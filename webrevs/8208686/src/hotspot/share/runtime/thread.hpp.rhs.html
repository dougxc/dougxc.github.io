<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_RUNTIME_THREAD_HPP
  26 #define SHARE_VM_RUNTIME_THREAD_HPP
  27 
  28 #include "jni.h"
  29 #include "gc/shared/gcThreadLocalData.hpp"
  30 #include "gc/shared/threadLocalAllocBuffer.hpp"
  31 #include "memory/allocation.hpp"
  32 #include "oops/oop.hpp"
  33 #include "prims/jvmtiExport.hpp"
  34 #include "runtime/frame.hpp"
  35 #include "runtime/globals.hpp"
  36 #include "runtime/handshake.hpp"
  37 #include "runtime/javaFrameAnchor.hpp"
  38 #include "runtime/jniHandles.hpp"
  39 #include "runtime/mutexLocker.hpp"
  40 #include "runtime/os.hpp"
  41 #include "runtime/osThread.hpp"
  42 #include "runtime/park.hpp"
  43 #include "runtime/safepoint.hpp"
  44 #include "runtime/stubRoutines.hpp"
  45 #include "runtime/threadHeapSampler.hpp"
  46 #include "runtime/threadLocalStorage.hpp"
  47 #include "runtime/threadStatisticalInfo.hpp"
  48 #include "runtime/unhandledOops.hpp"
  49 #include "utilities/align.hpp"
  50 #include "utilities/exceptions.hpp"
  51 #include "utilities/macros.hpp"
  52 #ifdef ZERO
  53 # include "stack_zero.hpp"
  54 #endif
  55 #if INCLUDE_JFR
  56 #include "jfr/support/jfrThreadExtension.hpp"
  57 #endif
  58 
  59 
  60 class SafeThreadsListPtr;
  61 class ThreadSafepointState;
  62 class ThreadsList;
  63 class ThreadsSMRSupport;
  64 
  65 class JvmtiThreadState;
  66 class ThreadStatistics;
  67 class ConcurrentLocksDump;
  68 class ParkEvent;
  69 class Parker;
  70 
  71 class ciEnv;
  72 class CompileThread;
  73 class CompileLog;
  74 class CompileTask;
  75 class CompileQueue;
  76 class CompilerCounters;
  77 class vframeArray;
  78 
  79 class DeoptResourceMark;
  80 class jvmtiDeferredLocalVariableSet;
  81 
  82 class GCTaskQueue;
  83 class ThreadClosure;
  84 class IdealGraphPrinter;
  85 
  86 class Metadata;
  87 template &lt;class T, MEMFLAGS F&gt; class ChunkedList;
  88 typedef ChunkedList&lt;Metadata*, mtInternal&gt; MetadataOnStackBuffer;
  89 
  90 DEBUG_ONLY(class ResourceMark;)
  91 
  92 class WorkerThread;
  93 
  94 // Class hierarchy
  95 // - Thread
  96 //   - JavaThread
  97 //     - various subclasses eg CompilerThread, ServiceThread
  98 //   - NonJavaThread
  99 //     - NamedThread
 100 //       - VMThread
 101 //       - ConcurrentGCThread
 102 //       - WorkerThread
 103 //         - GangWorker
 104 //         - GCTaskThread
 105 //     - WatcherThread
 106 //     - JfrThreadSampler
 107 //
 108 // All Thread subclasses must be either JavaThread or NonJavaThread.
 109 // This means !t-&gt;is_Java_thread() iff t is a NonJavaThread, or t is
 110 // a partially constructed/destroyed Thread.
 111 
 112 class Thread: public ThreadShadow {
 113   friend class VMStructs;
 114   friend class JVMCIVMStructs;
 115  private:
 116 
 117 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 118   // Current thread is maintained as a thread-local variable
 119   static THREAD_LOCAL_DECL Thread* _thr_current;
 120 #endif
 121 
 122  private:
 123   // Thread local data area available to the GC. The internal
 124   // structure and contents of this data area is GC-specific.
 125   // Only GC and GC barrier code should access this data area.
 126   GCThreadLocalData _gc_data;
 127 
 128  public:
 129   static ByteSize gc_data_offset() {
 130     return byte_offset_of(Thread, _gc_data);
 131   }
 132 
 133   template &lt;typename T&gt; T* gc_data() {
 134     STATIC_ASSERT(sizeof(T) &lt;= sizeof(_gc_data));
 135     return reinterpret_cast&lt;T*&gt;(&amp;_gc_data);
 136   }
 137 
 138   // Exception handling
 139   // (Note: _pending_exception and friends are in ThreadShadow)
 140   //oop       _pending_exception;                // pending exception for current thread
 141   // const char* _exception_file;                   // file information for exception (debugging only)
 142   // int         _exception_line;                   // line information for exception (debugging only)
 143  protected:
 144   // Support for forcing alignment of thread objects for biased locking
 145   void*       _real_malloc_address;
 146 
 147   // JavaThread lifecycle support:
 148   friend class SafeThreadsListPtr;  // for _threads_list_ptr, cmpxchg_threads_hazard_ptr(), {dec_,inc_,}nested_threads_hazard_ptr_cnt(), {g,s}et_threads_hazard_ptr(), inc_nested_handle_cnt(), tag_hazard_ptr() access
 149   friend class ScanHazardPtrGatherProtectedThreadsClosure;  // for cmpxchg_threads_hazard_ptr(), get_threads_hazard_ptr(), is_hazard_ptr_tagged() access
 150   friend class ScanHazardPtrGatherThreadsListClosure;  // for get_threads_hazard_ptr(), untag_hazard_ptr() access
 151   friend class ScanHazardPtrPrintMatchingThreadsClosure;  // for get_threads_hazard_ptr(), is_hazard_ptr_tagged() access
 152   friend class ThreadsSMRSupport;  // for _nested_threads_hazard_ptr_cnt, _threads_hazard_ptr, _threads_list_ptr access
 153 
 154   ThreadsList* volatile _threads_hazard_ptr;
 155   SafeThreadsListPtr*   _threads_list_ptr;
 156   ThreadsList*          cmpxchg_threads_hazard_ptr(ThreadsList* exchange_value, ThreadsList* compare_value);
 157   ThreadsList*          get_threads_hazard_ptr();
 158   void                  set_threads_hazard_ptr(ThreadsList* new_list);
 159   static bool           is_hazard_ptr_tagged(ThreadsList* list) {
 160     return (intptr_t(list) &amp; intptr_t(1)) == intptr_t(1);
 161   }
 162   static ThreadsList*   tag_hazard_ptr(ThreadsList* list) {
 163     return (ThreadsList*)(intptr_t(list) | intptr_t(1));
 164   }
 165   static ThreadsList*   untag_hazard_ptr(ThreadsList* list) {
 166     return (ThreadsList*)(intptr_t(list) &amp; ~intptr_t(1));
 167   }
 168   // This field is enabled via -XX:+EnableThreadSMRStatistics:
 169   uint _nested_threads_hazard_ptr_cnt;
 170   void dec_nested_threads_hazard_ptr_cnt() {
 171     assert(_nested_threads_hazard_ptr_cnt != 0, "mismatched {dec,inc}_nested_threads_hazard_ptr_cnt()");
 172     _nested_threads_hazard_ptr_cnt--;
 173   }
 174   void inc_nested_threads_hazard_ptr_cnt() {
 175     _nested_threads_hazard_ptr_cnt++;
 176   }
 177   uint nested_threads_hazard_ptr_cnt() {
 178     return _nested_threads_hazard_ptr_cnt;
 179   }
 180 
 181  public:
 182   void* operator new(size_t size) throw() { return allocate(size, true); }
 183   void* operator new(size_t size, const std::nothrow_t&amp; nothrow_constant) throw() {
 184     return allocate(size, false); }
 185   void  operator delete(void* p);
 186 
 187  protected:
 188   static void* allocate(size_t size, bool throw_excpt, MEMFLAGS flags = mtThread);
 189  private:
 190 
 191   // ***************************************************************
 192   // Suspend and resume support
 193   // ***************************************************************
 194   //
 195   // VM suspend/resume no longer exists - it was once used for various
 196   // things including safepoints but was deprecated and finally removed
 197   // in Java 7. Because VM suspension was considered "internal" Java-level
 198   // suspension was considered "external", and this legacy naming scheme
 199   // remains.
 200   //
 201   // External suspend/resume requests come from JVM_SuspendThread,
 202   // JVM_ResumeThread, JVMTI SuspendThread, and finally JVMTI
 203   // ResumeThread. External
 204   // suspend requests cause _external_suspend to be set and external
 205   // resume requests cause _external_suspend to be cleared.
 206   // External suspend requests do not nest on top of other external
 207   // suspend requests. The higher level APIs reject suspend requests
 208   // for already suspended threads.
 209   //
 210   // The external_suspend
 211   // flag is checked by has_special_runtime_exit_condition() and java thread
 212   // will self-suspend when handle_special_runtime_exit_condition() is
 213   // called. Most uses of the _thread_blocked state in JavaThreads are
 214   // considered the same as being externally suspended; if the blocking
 215   // condition lifts, the JavaThread will self-suspend. Other places
 216   // where VM checks for external_suspend include:
 217   //   + mutex granting (do not enter monitors when thread is suspended)
 218   //   + state transitions from _thread_in_native
 219   //
 220   // In general, java_suspend() does not wait for an external suspend
 221   // request to complete. When it returns, the only guarantee is that
 222   // the _external_suspend field is true.
 223   //
 224   // wait_for_ext_suspend_completion() is used to wait for an external
 225   // suspend request to complete. External suspend requests are usually
 226   // followed by some other interface call that requires the thread to
 227   // be quiescent, e.g., GetCallTrace(). By moving the "wait time" into
 228   // the interface that requires quiescence, we give the JavaThread a
 229   // chance to self-suspend before we need it to be quiescent. This
 230   // improves overall suspend/query performance.
 231   //
 232   // _suspend_flags controls the behavior of java_ suspend/resume.
 233   // It must be set under the protection of SR_lock. Read from the flag is
 234   // OK without SR_lock as long as the value is only used as a hint.
 235   // (e.g., check _external_suspend first without lock and then recheck
 236   // inside SR_lock and finish the suspension)
 237   //
 238   // _suspend_flags is also overloaded for other "special conditions" so
 239   // that a single check indicates whether any special action is needed
 240   // eg. for async exceptions.
 241   // -------------------------------------------------------------------
 242   // Notes:
 243   // 1. The suspend/resume logic no longer uses ThreadState in OSThread
 244   // but we still update its value to keep other part of the system (mainly
 245   // JVMTI) happy. ThreadState is legacy code (see notes in
 246   // osThread.hpp).
 247   //
 248   // 2. It would be more natural if set_external_suspend() is private and
 249   // part of java_suspend(), but that probably would affect the suspend/query
 250   // performance. Need more investigation on this.
 251 
 252   // suspend/resume lock: used for self-suspend
 253   Monitor* _SR_lock;
 254 
 255  protected:
 256   enum SuspendFlags {
 257     // NOTE: avoid using the sign-bit as cc generates different test code
 258     //       when the sign-bit is used, and sometimes incorrectly - see CR 6398077
 259 
 260     _external_suspend       = 0x20000000U, // thread is asked to self suspend
 261     _ext_suspended          = 0x40000000U, // thread has self-suspended
 262     _deopt_suspend          = 0x10000000U, // thread needs to self suspend for deopt
 263 
 264     _has_async_exception    = 0x00000001U, // there is a pending async exception
 265     _critical_native_unlock = 0x00000002U, // Must call back to unlock JNI critical lock
 266 
 267     _trace_flag             = 0x00000004U  // call tracing backend
 268   };
 269 
 270   // various suspension related flags - atomically updated
 271   // overloaded for async exception checking in check_special_condition_for_native_trans.
 272   volatile uint32_t _suspend_flags;
 273 
 274  private:
 275   int _num_nested_signal;
 276 
 277   DEBUG_ONLY(bool _suspendible_thread;)
 278 
 279  public:
 280   void enter_signal_handler() { _num_nested_signal++; }
 281   void leave_signal_handler() { _num_nested_signal--; }
 282   bool is_inside_signal_handler() const { return _num_nested_signal &gt; 0; }
 283 
<a name="1" id="anc1"></a><span class="new"> 284   // Determines if a heap allocation failure will be retried</span>
<span class="new"> 285   // (e.g., by deoptimizing and re-executing in the interpreter).</span>
<span class="new"> 286   // In this case, the failed allocation must raise</span>
<span class="new"> 287   // Universe::out_of_memory_error_retry() and omit side effects</span>
<span class="new"> 288   // such as JVMTI events and handling -XX:+HeapDumpOnOutOfMemoryError</span>
<span class="new"> 289   // and -XX:OnOutOfMemoryError.</span>
<span class="new"> 290   virtual bool in_retryable_allocation() const { return false; }</span>
<span class="new"> 291 </span>
 292 #ifdef ASSERT
 293   void set_suspendible_thread() {
 294     _suspendible_thread = true;
 295   }
 296 
 297   void clear_suspendible_thread() {
 298     _suspendible_thread = false;
 299   }
 300 
 301   bool is_suspendible_thread() { return _suspendible_thread; }
 302 #endif
 303 
 304  private:
 305   // Active_handles points to a block of handles
 306   JNIHandleBlock* _active_handles;
 307 
 308   // One-element thread local free list
 309   JNIHandleBlock* _free_handle_block;
 310 
 311   // Point to the last handle mark
 312   HandleMark* _last_handle_mark;
 313 
 314   // The parity of the last strong_roots iteration in which this thread was
 315   // claimed as a task.
 316   int _oops_do_parity;
 317 
 318   // Support for GlobalCounter
 319  private:
 320   volatile uintx _rcu_counter;
 321  public:
 322   volatile uintx* get_rcu_counter() {
 323     return &amp;_rcu_counter;
 324   }
 325 
 326  public:
 327   void set_last_handle_mark(HandleMark* mark)   { _last_handle_mark = mark; }
 328   HandleMark* last_handle_mark() const          { return _last_handle_mark; }
 329  private:
 330 
 331   // debug support for checking if code does allow safepoints or not
 332   // GC points in the VM can happen because of allocation, invoking a VM operation, or blocking on
 333   // mutex, or blocking on an object synchronizer (Java locking).
 334   // If !allow_safepoint(), then an assertion failure will happen in any of the above cases
 335   // If !allow_allocation(), then an assertion failure will happen during allocation
 336   // (Hence, !allow_safepoint() =&gt; !allow_allocation()).
 337   //
 338   // The two classes NoSafepointVerifier and No_Allocation_Verifier are used to set these counters.
 339   //
 340   NOT_PRODUCT(int _allow_safepoint_count;)      // If 0, thread allow a safepoint to happen
 341   debug_only(int _allow_allocation_count;)     // If 0, the thread is allowed to allocate oops.
 342 
 343   // Used by SkipGCALot class.
 344   NOT_PRODUCT(bool _skip_gcalot;)               // Should we elide gc-a-lot?
 345 
 346   friend class NoAllocVerifier;
 347   friend class NoSafepointVerifier;
 348   friend class PauseNoSafepointVerifier;
 349   friend class GCLocker;
 350 
 351   volatile void* _polling_page;                 // Thread local polling page
 352 
 353   ThreadLocalAllocBuffer _tlab;                 // Thread-local eden
 354   jlong _allocated_bytes;                       // Cumulative number of bytes allocated on
 355                                                 // the Java heap
 356   ThreadHeapSampler _heap_sampler;              // For use when sampling the memory.
 357 
 358   ThreadStatisticalInfo _statistical_info;      // Statistics about the thread
 359 
 360   JFR_ONLY(DEFINE_THREAD_LOCAL_FIELD_JFR;)      // Thread-local data for jfr
 361 
 362   int   _vm_operation_started_count;            // VM_Operation support
 363   int   _vm_operation_completed_count;          // VM_Operation support
 364 
 365   ObjectMonitor* _current_pending_monitor;      // ObjectMonitor this thread
 366                                                 // is waiting to lock
 367   bool _current_pending_monitor_is_from_java;   // locking is from Java code
 368 
 369   // ObjectMonitor on which this thread called Object.wait()
 370   ObjectMonitor* _current_waiting_monitor;
 371 
 372   // Private thread-local objectmonitor list - a simple cache organized as a SLL.
 373  public:
 374   ObjectMonitor* omFreeList;
 375   int omFreeCount;                              // length of omFreeList
 376   int omFreeProvision;                          // reload chunk size
 377   ObjectMonitor* omInUseList;                   // SLL to track monitors in circulation
 378   int omInUseCount;                             // length of omInUseList
 379 
 380 #ifdef ASSERT
 381  private:
 382   bool _visited_for_critical_count;
 383 
 384  public:
 385   void set_visited_for_critical_count(bool z) { _visited_for_critical_count = z; }
 386   bool was_visited_for_critical_count() const   { return _visited_for_critical_count; }
 387 #endif
 388 
 389  public:
 390   enum {
 391     is_definitely_current_thread = true
 392   };
 393 
 394   // Constructor
 395   Thread();
 396   virtual ~Thread() = 0;        // Thread is abstract.
 397 
 398   // Manage Thread::current()
 399   void initialize_thread_current();
 400   void clear_thread_current(); // TLS cleanup needed before threads terminate
 401 
 402  public:
 403   // thread entry point
 404   virtual void run();
 405 
 406   // Testers
 407   virtual bool is_VM_thread()       const            { return false; }
 408   virtual bool is_Java_thread()     const            { return false; }
 409   virtual bool is_Compiler_thread() const            { return false; }
 410   virtual bool is_Code_cache_sweeper_thread() const  { return false; }
 411   virtual bool is_hidden_from_external_view() const  { return false; }
 412   virtual bool is_jvmti_agent_thread() const         { return false; }
 413   // True iff the thread can perform GC operations at a safepoint.
 414   // Generally will be true only of VM thread and parallel GC WorkGang
 415   // threads.
 416   virtual bool is_GC_task_thread() const             { return false; }
 417   virtual bool is_Watcher_thread() const             { return false; }
 418   virtual bool is_ConcurrentGC_thread() const        { return false; }
 419   virtual bool is_Named_thread() const               { return false; }
 420   virtual bool is_Worker_thread() const              { return false; }
 421 
 422   // Can this thread make Java upcalls
 423   virtual bool can_call_java() const                 { return false; }
 424 
 425   // Casts
 426   virtual WorkerThread* as_Worker_thread() const     { return NULL; }
 427 
 428   virtual char* name() const { return (char*)"Unknown thread"; }
 429 
 430   // Returns the current thread (ASSERTS if NULL)
 431   static inline Thread* current();
 432   // Returns the current thread, or NULL if not attached
 433   static inline Thread* current_or_null();
 434   // Returns the current thread, or NULL if not attached, and is
 435   // safe for use from signal-handlers
 436   static inline Thread* current_or_null_safe();
 437 
 438   // Common thread operations
 439 #ifdef ASSERT
 440   static void check_for_dangling_thread_pointer(Thread *thread);
 441 #endif
 442   static void set_priority(Thread* thread, ThreadPriority priority);
 443   static ThreadPriority get_priority(const Thread* const thread);
 444   static void start(Thread* thread);
 445   static void interrupt(Thread* thr);
 446   static bool is_interrupted(Thread* thr, bool clear_interrupted);
 447 
 448   void set_native_thread_name(const char *name) {
 449     assert(Thread::current() == this, "set_native_thread_name can only be called on the current thread");
 450     os::set_native_thread_name(name);
 451   }
 452 
 453   ObjectMonitor** omInUseList_addr()             { return (ObjectMonitor **)&amp;omInUseList; }
 454   Monitor* SR_lock() const                       { return _SR_lock; }
 455 
 456   bool has_async_exception() const { return (_suspend_flags &amp; _has_async_exception) != 0; }
 457 
 458   inline void set_suspend_flag(SuspendFlags f);
 459   inline void clear_suspend_flag(SuspendFlags f);
 460 
 461   inline void set_has_async_exception();
 462   inline void clear_has_async_exception();
 463 
 464   bool do_critical_native_unlock() const { return (_suspend_flags &amp; _critical_native_unlock) != 0; }
 465 
 466   inline void set_critical_native_unlock();
 467   inline void clear_critical_native_unlock();
 468 
 469   inline void set_trace_flag();
 470   inline void clear_trace_flag();
 471 
 472   // Support for Unhandled Oop detection
 473   // Add the field for both, fastdebug and debug, builds to keep
 474   // Thread's fields layout the same.
 475   // Note: CHECK_UNHANDLED_OOPS is defined only for fastdebug build.
 476 #ifdef CHECK_UNHANDLED_OOPS
 477  private:
 478   UnhandledOops* _unhandled_oops;
 479 #elif defined(ASSERT)
 480  private:
 481   void* _unhandled_oops;
 482 #endif
 483 #ifdef CHECK_UNHANDLED_OOPS
 484  public:
 485   UnhandledOops* unhandled_oops() { return _unhandled_oops; }
 486   // Mark oop safe for gc.  It may be stack allocated but won't move.
 487   void allow_unhandled_oop(oop *op) {
 488     if (CheckUnhandledOops) unhandled_oops()-&gt;allow_unhandled_oop(op);
 489   }
 490   // Clear oops at safepoint so crashes point to unhandled oop violator
 491   void clear_unhandled_oops() {
 492     if (CheckUnhandledOops) unhandled_oops()-&gt;clear_unhandled_oops();
 493   }
 494 #endif // CHECK_UNHANDLED_OOPS
 495 
 496  public:
 497 #ifndef PRODUCT
 498   bool skip_gcalot()           { return _skip_gcalot; }
 499   void set_skip_gcalot(bool v) { _skip_gcalot = v;    }
 500 #endif
 501 
 502   // Installs a pending exception to be inserted later
 503   static void send_async_exception(oop thread_oop, oop java_throwable);
 504 
 505   // Resource area
 506   ResourceArea* resource_area() const            { return _resource_area; }
 507   void set_resource_area(ResourceArea* area)     { _resource_area = area; }
 508 
 509   OSThread* osthread() const                     { return _osthread;   }
 510   void set_osthread(OSThread* thread)            { _osthread = thread; }
 511 
 512   // JNI handle support
 513   JNIHandleBlock* active_handles() const         { return _active_handles; }
 514   void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }
 515   JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }
 516   void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }
 517 
 518   // Internal handle support
 519   HandleArea* handle_area() const                { return _handle_area; }
 520   void set_handle_area(HandleArea* area)         { _handle_area = area; }
 521 
 522   GrowableArray&lt;Metadata*&gt;* metadata_handles() const          { return _metadata_handles; }
 523   void set_metadata_handles(GrowableArray&lt;Metadata*&gt;* handles){ _metadata_handles = handles; }
 524 
 525   // Thread-Local Allocation Buffer (TLAB) support
 526   ThreadLocalAllocBuffer&amp; tlab()                 { return _tlab; }
 527   void initialize_tlab() {
 528     if (UseTLAB) {
 529       tlab().initialize();
 530     }
 531   }
 532 
 533   jlong allocated_bytes()               { return _allocated_bytes; }
 534   void set_allocated_bytes(jlong value) { _allocated_bytes = value; }
 535   void incr_allocated_bytes(jlong size) { _allocated_bytes += size; }
 536   inline jlong cooked_allocated_bytes();
 537 
 538   ThreadHeapSampler&amp; heap_sampler()     { return _heap_sampler; }
 539 
 540   ThreadStatisticalInfo&amp; statistical_info() { return _statistical_info; }
 541 
 542   JFR_ONLY(DEFINE_THREAD_LOCAL_ACCESSOR_JFR;)
 543 
 544   bool is_trace_suspend()               { return (_suspend_flags &amp; _trace_flag) != 0; }
 545 
 546   // VM operation support
 547   int vm_operation_ticket()                      { return ++_vm_operation_started_count; }
 548   int vm_operation_completed_count()             { return _vm_operation_completed_count; }
 549   void increment_vm_operation_completed_count()  { _vm_operation_completed_count++; }
 550 
 551   // For tracking the heavyweight monitor the thread is pending on.
 552   ObjectMonitor* current_pending_monitor() {
 553     return _current_pending_monitor;
 554   }
 555   void set_current_pending_monitor(ObjectMonitor* monitor) {
 556     _current_pending_monitor = monitor;
 557   }
 558   void set_current_pending_monitor_is_from_java(bool from_java) {
 559     _current_pending_monitor_is_from_java = from_java;
 560   }
 561   bool current_pending_monitor_is_from_java() {
 562     return _current_pending_monitor_is_from_java;
 563   }
 564 
 565   // For tracking the ObjectMonitor on which this thread called Object.wait()
 566   ObjectMonitor* current_waiting_monitor() {
 567     return _current_waiting_monitor;
 568   }
 569   void set_current_waiting_monitor(ObjectMonitor* monitor) {
 570     _current_waiting_monitor = monitor;
 571   }
 572 
 573   // GC support
 574   // Apply "f-&gt;do_oop" to all root oops in "this".
 575   //   Used by JavaThread::oops_do.
 576   // Apply "cf-&gt;do_code_blob" (if !NULL) to all code blobs active in frames
 577   virtual void oops_do(OopClosure* f, CodeBlobClosure* cf);
 578 
 579   // Handles the parallel case for the method below.
 580  private:
 581   bool claim_oops_do_par_case(int collection_parity);
 582  public:
 583   // Requires that "collection_parity" is that of the current roots
 584   // iteration.  If "is_par" is false, sets the parity of "this" to
 585   // "collection_parity", and returns "true".  If "is_par" is true,
 586   // uses an atomic instruction to set the current threads parity to
 587   // "collection_parity", if it is not already.  Returns "true" iff the
 588   // calling thread does the update, this indicates that the calling thread
 589   // has claimed the thread's stack as a root groop in the current
 590   // collection.
 591   bool claim_oops_do(bool is_par, int collection_parity) {
 592     if (!is_par) {
 593       _oops_do_parity = collection_parity;
 594       return true;
 595     } else {
 596       return claim_oops_do_par_case(collection_parity);
 597     }
 598   }
 599 
 600   // jvmtiRedefineClasses support
 601   void metadata_handles_do(void f(Metadata*));
 602 
 603   // Used by fast lock support
 604   virtual bool is_lock_owned(address adr) const;
 605 
 606   // Check if address is in the stack of the thread (not just for locks).
 607   // Warning: the method can only be used on the running thread
 608   bool is_in_stack(address adr) const;
 609   // Check if address is in the usable part of the stack (excludes protected
 610   // guard pages)
 611   bool is_in_usable_stack(address adr) const;
 612 
 613   // Sets this thread as starting thread. Returns failure if thread
 614   // creation fails due to lack of memory, too many threads etc.
 615   bool set_as_starting_thread();
 616 
 617 protected:
 618   // OS data associated with the thread
 619   OSThread* _osthread;  // Platform-specific thread information
 620 
 621   // Thread local resource area for temporary allocation within the VM
 622   ResourceArea* _resource_area;
 623 
 624   DEBUG_ONLY(ResourceMark* _current_resource_mark;)
 625 
 626   // Thread local handle area for allocation of handles within the VM
 627   HandleArea* _handle_area;
 628   GrowableArray&lt;Metadata*&gt;* _metadata_handles;
 629 
 630   // Support for stack overflow handling, get_thread, etc.
 631   address          _stack_base;
 632   size_t           _stack_size;
 633   uintptr_t        _self_raw_id;      // used by get_thread (mutable)
 634   int              _lgrp_id;
 635 
 636   volatile void** polling_page_addr() { return &amp;_polling_page; }
 637 
 638  public:
 639   // Stack overflow support
 640   address stack_base() const           { assert(_stack_base != NULL,"Sanity check"); return _stack_base; }
 641   void    set_stack_base(address base) { _stack_base = base; }
 642   size_t  stack_size() const           { return _stack_size; }
 643   void    set_stack_size(size_t size)  { _stack_size = size; }
 644   address stack_end()  const           { return stack_base() - stack_size(); }
 645   void    record_stack_base_and_size();
 646 
 647   bool    on_local_stack(address adr) const {
 648     // QQQ this has knowledge of direction, ought to be a stack method
 649     return (_stack_base &gt;= adr &amp;&amp; adr &gt;= stack_end());
 650   }
 651 
 652   uintptr_t self_raw_id()                    { return _self_raw_id; }
 653   void      set_self_raw_id(uintptr_t value) { _self_raw_id = value; }
 654 
 655   int     lgrp_id() const        { return _lgrp_id; }
 656   void    set_lgrp_id(int value) { _lgrp_id = value; }
 657 
 658   // Printing
 659   void print_on(outputStream* st, bool print_extended_info) const;
 660   virtual void print_on(outputStream* st) const { print_on(st, false); }
 661   void print() const { print_on(tty); }
 662   virtual void print_on_error(outputStream* st, char* buf, int buflen) const;
 663   void print_value_on(outputStream* st) const;
 664 
 665   // Debug-only code
 666 #ifdef ASSERT
 667  private:
 668   // Deadlock detection support for Mutex locks. List of locks own by thread.
 669   Monitor* _owned_locks;
 670   // Mutex::set_owner_implementation is the only place where _owned_locks is modified,
 671   // thus the friendship
 672   friend class Mutex;
 673   friend class Monitor;
 674 
 675  public:
 676   void print_owned_locks_on(outputStream* st) const;
 677   void print_owned_locks() const                 { print_owned_locks_on(tty);    }
 678   Monitor* owned_locks() const                   { return _owned_locks;          }
 679   bool owns_locks() const                        { return owned_locks() != NULL; }
 680   bool owns_locks_but_compiled_lock() const;
 681   int oops_do_parity() const                     { return _oops_do_parity; }
 682 
 683   // Deadlock detection
 684   bool allow_allocation()                        { return _allow_allocation_count == 0; }
 685   ResourceMark* current_resource_mark()          { return _current_resource_mark; }
 686   void set_current_resource_mark(ResourceMark* rm) { _current_resource_mark = rm; }
 687 #endif
 688 
 689   void check_for_valid_safepoint_state(bool potential_vm_operation) PRODUCT_RETURN;
 690 
 691  private:
 692   volatile int _jvmti_env_iteration_count;
 693 
 694  public:
 695   void entering_jvmti_env_iteration()            { ++_jvmti_env_iteration_count; }
 696   void leaving_jvmti_env_iteration()             { --_jvmti_env_iteration_count; }
 697   bool is_inside_jvmti_env_iteration()           { return _jvmti_env_iteration_count &gt; 0; }
 698 
 699   // Code generation
 700   static ByteSize exception_file_offset()        { return byte_offset_of(Thread, _exception_file); }
 701   static ByteSize exception_line_offset()        { return byte_offset_of(Thread, _exception_line); }
 702   static ByteSize active_handles_offset()        { return byte_offset_of(Thread, _active_handles); }
 703 
 704   static ByteSize stack_base_offset()            { return byte_offset_of(Thread, _stack_base); }
 705   static ByteSize stack_size_offset()            { return byte_offset_of(Thread, _stack_size); }
 706 
 707   static ByteSize polling_page_offset()          { return byte_offset_of(Thread, _polling_page); }
 708 
 709   static ByteSize tlab_start_offset()            { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::start_offset(); }
 710   static ByteSize tlab_end_offset()              { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::end_offset(); }
 711   static ByteSize tlab_top_offset()              { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::top_offset(); }
 712   static ByteSize tlab_pf_top_offset()           { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::pf_top_offset(); }
 713 
 714   static ByteSize allocated_bytes_offset()       { return byte_offset_of(Thread, _allocated_bytes); }
 715 
 716   JFR_ONLY(DEFINE_THREAD_LOCAL_OFFSET_JFR;)
 717 
 718  public:
 719   volatile intptr_t _Stalled;
 720   volatile int _TypeTag;
 721   ParkEvent * _ParkEvent;                     // for synchronized()
 722   ParkEvent * _SleepEvent;                    // for Thread.sleep
 723   ParkEvent * _MutexEvent;                    // for native internal Mutex/Monitor
 724   ParkEvent * _MuxEvent;                      // for low-level muxAcquire-muxRelease
 725   int NativeSyncRecursion;                    // diagnostic
 726 
 727   volatile int _OnTrap;                       // Resume-at IP delta
 728   jint _hashStateW;                           // Marsaglia Shift-XOR thread-local RNG
 729   jint _hashStateX;                           // thread-specific hashCode generator state
 730   jint _hashStateY;
 731   jint _hashStateZ;
 732 
 733   volatile jint rng[4];                      // RNG for spin loop
 734 
 735   // Low-level leaf-lock primitives used to implement synchronization
 736   // and native monitor-mutex infrastructure.
 737   // Not for general synchronization use.
 738   static void SpinAcquire(volatile int * Lock, const char * Name);
 739   static void SpinRelease(volatile int * Lock);
 740   static void muxAcquire(volatile intptr_t * Lock, const char * Name);
 741   static void muxAcquireW(volatile intptr_t * Lock, ParkEvent * ev);
 742   static void muxRelease(volatile intptr_t * Lock);
 743 };
 744 
 745 // Inline implementation of Thread::current()
 746 inline Thread* Thread::current() {
 747   Thread* current = current_or_null();
 748   assert(current != NULL, "Thread::current() called on detached thread");
 749   return current;
 750 }
 751 
 752 inline Thread* Thread::current_or_null() {
 753 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 754   return _thr_current;
 755 #else
 756   if (ThreadLocalStorage::is_initialized()) {
 757     return ThreadLocalStorage::thread();
 758   }
 759   return NULL;
 760 #endif
 761 }
 762 
 763 inline Thread* Thread::current_or_null_safe() {
 764   if (ThreadLocalStorage::is_initialized()) {
 765     return ThreadLocalStorage::thread();
 766   }
 767   return NULL;
 768 }
 769 
 770 class NonJavaThread: public Thread {
 771   friend class VMStructs;
 772 
 773   NonJavaThread* volatile _next;
 774 
 775   class List;
 776   static List _the_list;
 777 
 778  public:
 779   NonJavaThread();
 780   ~NonJavaThread();
 781 
 782   class Iterator;
 783 };
 784 
 785 // Provides iteration over the list of NonJavaThreads.  Because list
 786 // management occurs in the NonJavaThread constructor and destructor,
 787 // entries in the list may not be fully constructed instances of a
 788 // derived class.  Threads created after an iterator is constructed
 789 // will not be visited by the iterator.  The scope of an iterator is a
 790 // critical section; there must be no safepoint checks in that scope.
 791 class NonJavaThread::Iterator : public StackObj {
 792   uint _protect_enter;
 793   NonJavaThread* _current;
 794 
 795   // Noncopyable.
 796   Iterator(const Iterator&amp;);
 797   Iterator&amp; operator=(const Iterator&amp;);
 798 
 799 public:
 800   Iterator();
 801   ~Iterator();
 802 
 803   bool end() const { return _current == NULL; }
 804   NonJavaThread* current() const { return _current; }
 805   void step();
 806 };
 807 
 808 // Name support for threads.  non-JavaThread subclasses with multiple
 809 // uniquely named instances should derive from this.
 810 class NamedThread: public NonJavaThread {
 811   friend class VMStructs;
 812   enum {
 813     max_name_len = 64
 814   };
 815  private:
 816   char* _name;
 817   // log JavaThread being processed by oops_do
 818   JavaThread* _processed_thread;
 819   uint _gc_id; // The current GC id when a thread takes part in GC
 820 
 821  public:
 822   NamedThread();
 823   ~NamedThread();
 824   // May only be called once per thread.
 825   void set_name(const char* format, ...)  ATTRIBUTE_PRINTF(2, 3);
 826   void initialize_named_thread();
 827   virtual bool is_Named_thread() const { return true; }
 828   virtual char* name() const { return _name == NULL ? (char*)"Unknown Thread" : _name; }
 829   JavaThread *processed_thread() { return _processed_thread; }
 830   void set_processed_thread(JavaThread *thread) { _processed_thread = thread; }
 831   virtual void print_on(outputStream* st) const;
 832 
 833   void set_gc_id(uint gc_id) { _gc_id = gc_id; }
 834   uint gc_id() { return _gc_id; }
 835 };
 836 
 837 // Worker threads are named and have an id of an assigned work.
 838 class WorkerThread: public NamedThread {
 839  private:
 840   uint _id;
 841  public:
 842   WorkerThread() : _id(0)               { }
 843   virtual bool is_Worker_thread() const { return true; }
 844 
 845   virtual WorkerThread* as_Worker_thread() const {
 846     assert(is_Worker_thread(), "Dubious cast to WorkerThread*?");
 847     return (WorkerThread*) this;
 848   }
 849 
 850   void set_id(uint work_id)             { _id = work_id; }
 851   uint id() const                       { return _id; }
 852 };
 853 
 854 // A single WatcherThread is used for simulating timer interrupts.
 855 class WatcherThread: public NonJavaThread {
 856   friend class VMStructs;
 857  public:
 858   virtual void run();
 859 
 860  private:
 861   static WatcherThread* _watcher_thread;
 862 
 863   static bool _startable;
 864   // volatile due to at least one lock-free read
 865   volatile static bool _should_terminate;
 866  public:
 867   enum SomeConstants {
 868     delay_interval = 10                          // interrupt delay in milliseconds
 869   };
 870 
 871   // Constructor
 872   WatcherThread();
 873 
 874   // No destruction allowed
 875   ~WatcherThread() {
 876     guarantee(false, "WatcherThread deletion must fix the race with VM termination");
 877   }
 878 
 879   // Tester
 880   bool is_Watcher_thread() const                 { return true; }
 881 
 882   // Printing
 883   char* name() const { return (char*)"VM Periodic Task Thread"; }
 884   void print_on(outputStream* st) const;
 885   void unpark();
 886 
 887   // Returns the single instance of WatcherThread
 888   static WatcherThread* watcher_thread()         { return _watcher_thread; }
 889 
 890   // Create and start the single instance of WatcherThread, or stop it on shutdown
 891   static void start();
 892   static void stop();
 893   // Only allow start once the VM is sufficiently initialized
 894   // Otherwise the first task to enroll will trigger the start
 895   static void make_startable();
 896  private:
 897   int sleep() const;
 898 };
 899 
 900 
 901 class CompilerThread;
 902 
 903 typedef void (*ThreadFunction)(JavaThread*, TRAPS);
 904 
 905 class JavaThread: public Thread {
 906   friend class VMStructs;
 907   friend class JVMCIVMStructs;
 908   friend class WhiteBox;
 909  private:
 910   JavaThread*    _next;                          // The next thread in the Threads list
 911   bool           _on_thread_list;                // Is set when this JavaThread is added to the Threads list
 912   oop            _threadObj;                     // The Java level thread object
 913 
 914 #ifdef ASSERT
 915  private:
 916   int _java_call_counter;
 917 
 918  public:
 919   int  java_call_counter()                       { return _java_call_counter; }
 920   void inc_java_call_counter()                   { _java_call_counter++; }
 921   void dec_java_call_counter() {
 922     assert(_java_call_counter &gt; 0, "Invalid nesting of JavaCallWrapper");
 923     _java_call_counter--;
 924   }
 925  private:  // restore original namespace restriction
 926 #endif  // ifdef ASSERT
 927 
 928 #ifndef PRODUCT
 929  public:
 930   enum {
 931     jump_ring_buffer_size = 16
 932   };
 933  private:  // restore original namespace restriction
 934 #endif
 935 
 936   JavaFrameAnchor _anchor;                       // Encapsulation of current java frame and it state
 937 
 938   ThreadFunction _entry_point;
 939 
 940   JNIEnv        _jni_environment;
 941 
 942   // Deopt support
 943   DeoptResourceMark*  _deopt_mark;               // Holds special ResourceMark for deoptimization
 944 
 945   intptr_t*      _must_deopt_id;                 // id of frame that needs to be deopted once we
 946                                                  // transition out of native
 947   CompiledMethod*       _deopt_nmethod;         // CompiledMethod that is currently being deoptimized
 948   vframeArray*  _vframe_array_head;              // Holds the heap of the active vframeArrays
 949   vframeArray*  _vframe_array_last;              // Holds last vFrameArray we popped
 950   // Because deoptimization is lazy we must save jvmti requests to set locals
 951   // in compiled frames until we deoptimize and we have an interpreter frame.
 952   // This holds the pointer to array (yeah like there might be more than one) of
 953   // description of compiled vframes that have locals that need to be updated.
 954   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* _deferred_locals_updates;
 955 
 956   // Handshake value for fixing 6243940. We need a place for the i2c
 957   // adapter to store the callee Method*. This value is NEVER live
 958   // across a gc point so it does NOT have to be gc'd
 959   // The handshake is open ended since we can't be certain that it will
 960   // be NULLed. This is because we rarely ever see the race and end up
 961   // in handle_wrong_method which is the backend of the handshake. See
 962   // code in i2c adapters and handle_wrong_method.
 963 
 964   Method*       _callee_target;
 965 
 966   // Used to pass back results to the interpreter or generated code running Java code.
 967   oop           _vm_result;    // oop result is GC-preserved
 968   Metadata*     _vm_result_2;  // non-oop result
 969 
 970   // See ReduceInitialCardMarks: this holds the precise space interval of
 971   // the most recent slow path allocation for which compiled code has
 972   // elided card-marks for performance along the fast-path.
 973   MemRegion     _deferred_card_mark;
 974 
 975   MonitorChunk* _monitor_chunks;                 // Contains the off stack monitors
 976                                                  // allocated during deoptimization
 977                                                  // and by JNI_MonitorEnter/Exit
 978 
 979   // Async. requests support
 980   enum AsyncRequests {
 981     _no_async_condition = 0,
 982     _async_exception,
 983     _async_unsafe_access_error
 984   };
 985   AsyncRequests _special_runtime_exit_condition; // Enum indicating pending async. request
 986   oop           _pending_async_exception;
 987 
 988   // Safepoint support
 989  public:                                         // Expose _thread_state for SafeFetchInt()
 990   volatile JavaThreadState _thread_state;
 991  private:
 992   ThreadSafepointState *_safepoint_state;        // Holds information about a thread during a safepoint
 993   address               _saved_exception_pc;     // Saved pc of instruction where last implicit exception happened
 994 
 995   // JavaThread termination support
 996   enum TerminatedTypes {
 997     _not_terminated = 0xDEAD - 2,
 998     _thread_exiting,                             // JavaThread::exit() has been called for this thread
 999     _thread_terminated,                          // JavaThread is removed from thread list
1000     _vm_exited                                   // JavaThread is still executing native code, but VM is terminated
1001                                                  // only VM_Exit can set _vm_exited
1002   };
1003 
1004   // In general a JavaThread's _terminated field transitions as follows:
1005   //
1006   //   _not_terminated =&gt; _thread_exiting =&gt; _thread_terminated
1007   //
1008   // _vm_exited is a special value to cover the case of a JavaThread
1009   // executing native code after the VM itself is terminated.
1010   volatile TerminatedTypes _terminated;
1011   // suspend/resume support
1012   volatile bool         _suspend_equivalent;     // Suspend equivalent condition
1013   jint                  _in_deopt_handler;       // count of deoptimization
1014                                                  // handlers thread is in
1015   volatile bool         _doing_unsafe_access;    // Thread may fault due to unsafe access
1016   bool                  _do_not_unlock_if_synchronized;  // Do not unlock the receiver of a synchronized method (since it was
1017                                                          // never locked) when throwing an exception. Used by interpreter only.
1018 
1019   // JNI attach states:
1020   enum JNIAttachStates {
1021     _not_attaching_via_jni = 1,  // thread is not attaching via JNI
1022     _attaching_via_jni,          // thread is attaching via JNI
1023     _attached_via_jni            // thread has attached via JNI
1024   };
1025 
1026   // A regular JavaThread's _jni_attach_state is _not_attaching_via_jni.
1027   // A native thread that is attaching via JNI starts with a value
1028   // of _attaching_via_jni and transitions to _attached_via_jni.
1029   volatile JNIAttachStates _jni_attach_state;
1030 
1031  public:
1032   // State of the stack guard pages for this thread.
1033   enum StackGuardState {
1034     stack_guard_unused,         // not needed
1035     stack_guard_reserved_disabled,
1036     stack_guard_yellow_reserved_disabled,// disabled (temporarily) after stack overflow
1037     stack_guard_enabled         // enabled
1038   };
1039 
1040  private:
1041 
1042 #if INCLUDE_JVMCI
1043   // The _pending_* fields below are used to communicate extra information
1044   // from an uncommon trap in JVMCI compiled code to the uncommon trap handler.
1045 
1046   // Communicates the DeoptReason and DeoptAction of the uncommon trap
1047   int       _pending_deoptimization;
1048 
1049   // Specifies whether the uncommon trap is to bci 0 of a synchronized method
1050   // before the monitor has been acquired.
1051   bool      _pending_monitorenter;
1052 
1053   // Specifies if the DeoptReason for the last uncommon trap was Reason_transfer_to_interpreter
1054   bool      _pending_transfer_to_interpreter;
1055 
1056   // Guard for re-entrant call to JVMCIRuntime::adjust_comp_level
1057   bool      _adjusting_comp_level;
1058 
<a name="2" id="anc2"></a><span class="new">1059   // True if in a runtime call from compiled code that will deoptimize</span>
<span class="new">1060   // and re-execute a failed heap allocation in the interpreter.</span>
<span class="new">1061   bool      _in_retryable_allocation;</span>
<span class="new">1062 </span>
1063   // An id of a speculation that JVMCI compiled code can use to further describe and
1064   // uniquely identify the  speculative optimization guarded by the uncommon trap
1065   long       _pending_failed_speculation;
1066 
1067   // These fields are mutually exclusive in terms of live ranges.
1068   union {
1069     // Communicates the pc at which the most recent implicit exception occurred
1070     // from the signal handler to a deoptimization stub.
1071     address   _implicit_exception_pc;
1072 
1073     // Communicates an alternative call target to an i2c stub from a JavaCall .
1074     address   _alternate_call_target;
1075   } _jvmci;
1076 
1077   // Support for high precision, thread sensitive counters in JVMCI compiled code.
1078   jlong*    _jvmci_counters;
1079 
1080  public:
1081   static jlong* _jvmci_old_thread_counters;
1082   static void collect_counters(typeArrayOop array);
1083  private:
1084 #endif // INCLUDE_JVMCI
1085 
1086   StackGuardState  _stack_guard_state;
1087 
1088   // Precompute the limit of the stack as used in stack overflow checks.
1089   // We load it from here to simplify the stack overflow check in assembly.
1090   address          _stack_overflow_limit;
1091   address          _reserved_stack_activation;
1092 
1093   // Compiler exception handling (NOTE: The _exception_oop is *NOT* the same as _pending_exception. It is
1094   // used to temp. parsing values into and out of the runtime system during exception handling for compiled
1095   // code)
1096   volatile oop     _exception_oop;               // Exception thrown in compiled code
1097   volatile address _exception_pc;                // PC where exception happened
1098   volatile address _exception_handler_pc;        // PC for handler of exception
1099   volatile int     _is_method_handle_return;     // true (== 1) if the current exception PC is a MethodHandle call site.
1100 
1101  private:
1102   // support for JNI critical regions
1103   jint    _jni_active_critical;                  // count of entries into JNI critical region
1104 
1105   // Checked JNI: function name requires exception check
1106   char* _pending_jni_exception_check_fn;
1107 
1108   // For deadlock detection.
1109   int _depth_first_number;
1110 
1111   // JVMTI PopFrame support
1112   // This is set to popframe_pending to signal that top Java frame should be popped immediately
1113   int _popframe_condition;
1114 
1115   // If reallocation of scalar replaced objects fails, we throw OOM
1116   // and during exception propagation, pop the top
1117   // _frames_to_pop_failed_realloc frames, the ones that reference
1118   // failed reallocations.
1119   int _frames_to_pop_failed_realloc;
1120 
1121 #ifndef PRODUCT
1122   int _jmp_ring_index;
1123   struct {
1124     // We use intptr_t instead of address so debugger doesn't try and display strings
1125     intptr_t _target;
1126     intptr_t _instruction;
1127     const char*  _file;
1128     int _line;
1129   }   _jmp_ring[jump_ring_buffer_size];
1130 #endif // PRODUCT
1131 
1132   friend class VMThread;
1133   friend class ThreadWaitTransition;
1134   friend class VM_Exit;
1135 
1136   void initialize();                             // Initialized the instance variables
1137 
1138  public:
1139   // Constructor
1140   JavaThread(bool is_attaching_via_jni = false); // for main thread and JNI attached threads
1141   JavaThread(ThreadFunction entry_point, size_t stack_size = 0);
1142   ~JavaThread();
1143 
1144 #ifdef ASSERT
1145   // verify this JavaThread hasn't be published in the Threads::list yet
1146   void verify_not_published();
1147 #endif
1148 
1149   //JNI functiontable getter/setter for JVMTI jni function table interception API.
1150   void set_jni_functions(struct JNINativeInterface_* functionTable) {
1151     _jni_environment.functions = functionTable;
1152   }
1153   struct JNINativeInterface_* get_jni_functions() {
1154     return (struct JNINativeInterface_ *)_jni_environment.functions;
1155   }
1156 
1157   // This function is called at thread creation to allow
1158   // platform specific thread variables to be initialized.
1159   void cache_global_variables();
1160 
1161   // Executes Shutdown.shutdown()
1162   void invoke_shutdown_hooks();
1163 
1164   // Cleanup on thread exit
1165   enum ExitType {
1166     normal_exit,
1167     jni_detach
1168   };
1169   void exit(bool destroy_vm, ExitType exit_type = normal_exit);
1170 
1171   void cleanup_failed_attach_current_thread();
1172 
1173   // Testers
1174   virtual bool is_Java_thread() const            { return true;  }
1175   virtual bool can_call_java() const             { return true; }
1176 
1177   // Thread chain operations
1178   JavaThread* next() const                       { return _next; }
1179   void set_next(JavaThread* p)                   { _next = p; }
1180 
1181   // Thread oop. threadObj() can be NULL for initial JavaThread
1182   // (or for threads attached via JNI)
1183   oop threadObj() const                          { return _threadObj; }
1184   void set_threadObj(oop p)                      { _threadObj = p; }
1185 
1186   ThreadPriority java_priority() const;          // Read from threadObj()
1187 
1188   // Prepare thread and add to priority queue.  If a priority is
1189   // not specified, use the priority of the thread object. Threads_lock
1190   // must be held while this function is called.
1191   void prepare(jobject jni_thread, ThreadPriority prio=NoPriority);
1192 
1193   void set_saved_exception_pc(address pc)        { _saved_exception_pc = pc; }
1194   address saved_exception_pc()                   { return _saved_exception_pc; }
1195 
1196 
1197   ThreadFunction entry_point() const             { return _entry_point; }
1198 
1199   // Allocates a new Java level thread object for this thread. thread_name may be NULL.
1200   void allocate_threadObj(Handle thread_group, const char* thread_name, bool daemon, TRAPS);
1201 
1202   // Last frame anchor routines
1203 
1204   JavaFrameAnchor* frame_anchor(void)            { return &amp;_anchor; }
1205 
1206   // last_Java_sp
1207   bool has_last_Java_frame() const               { return _anchor.has_last_Java_frame(); }
1208   intptr_t* last_Java_sp() const                 { return _anchor.last_Java_sp(); }
1209 
1210   // last_Java_pc
1211 
1212   address last_Java_pc(void)                     { return _anchor.last_Java_pc(); }
1213 
1214   // Safepoint support
1215 #if !(defined(PPC64) || defined(AARCH64))
1216   JavaThreadState thread_state() const           { return _thread_state; }
1217   void set_thread_state(JavaThreadState s)       { _thread_state = s;    }
1218 #else
1219   // Use membars when accessing volatile _thread_state. See
1220   // Threads::create_vm() for size checks.
1221   inline JavaThreadState thread_state() const;
1222   inline void set_thread_state(JavaThreadState s);
1223 #endif
1224   ThreadSafepointState *safepoint_state() const  { return _safepoint_state; }
1225   void set_safepoint_state(ThreadSafepointState *state) { _safepoint_state = state; }
1226   bool is_at_poll_safepoint()                    { return _safepoint_state-&gt;is_at_poll_safepoint(); }
1227 
1228   // JavaThread termination and lifecycle support:
1229   void smr_delete();
1230   bool on_thread_list() const { return _on_thread_list; }
1231   void set_on_thread_list() { _on_thread_list = true; }
1232 
1233   // thread has called JavaThread::exit() or is terminated
1234   bool is_exiting() const;
1235   // thread is terminated (no longer on the threads list); we compare
1236   // against the two non-terminated values so that a freed JavaThread
1237   // will also be considered terminated.
1238   bool check_is_terminated(TerminatedTypes l_terminated) const {
1239     return l_terminated != _not_terminated &amp;&amp; l_terminated != _thread_exiting;
1240   }
1241   bool is_terminated() const;
1242   void set_terminated(TerminatedTypes t);
1243   // special for Threads::remove() which is static:
1244   void set_terminated_value();
1245   void block_if_vm_exited();
1246 
1247   bool doing_unsafe_access()                     { return _doing_unsafe_access; }
1248   void set_doing_unsafe_access(bool val)         { _doing_unsafe_access = val; }
1249 
1250   bool do_not_unlock_if_synchronized()             { return _do_not_unlock_if_synchronized; }
1251   void set_do_not_unlock_if_synchronized(bool val) { _do_not_unlock_if_synchronized = val; }
1252 
1253   inline void set_polling_page_release(void* poll_value);
1254   inline void set_polling_page(void* poll_value);
1255   inline volatile void* get_polling_page();
1256 
1257  private:
1258   // Support for thread handshake operations
1259   HandshakeState _handshake;
1260  public:
1261   void set_handshake_operation(HandshakeOperation* op) {
1262     _handshake.set_operation(this, op);
1263   }
1264 
1265   bool has_handshake() const {
1266     return _handshake.has_operation();
1267   }
1268 
1269   void cancel_handshake() {
1270     _handshake.cancel(this);
1271   }
1272 
1273   void handshake_process_by_self() {
1274     _handshake.process_by_self(this);
1275   }
1276 
1277   void handshake_process_by_vmthread() {
1278     _handshake.process_by_vmthread(this);
1279   }
1280 
1281   // Suspend/resume support for JavaThread
1282  private:
1283   inline void set_ext_suspended();
1284   inline void clear_ext_suspended();
1285 
1286  public:
1287   void java_suspend();
1288   void java_resume();
1289   int  java_suspend_self();
1290 
1291   void check_and_wait_while_suspended() {
1292     assert(JavaThread::current() == this, "sanity check");
1293 
1294     bool do_self_suspend;
1295     do {
1296       // were we externally suspended while we were waiting?
1297       do_self_suspend = handle_special_suspend_equivalent_condition();
1298       if (do_self_suspend) {
1299         // don't surprise the thread that suspended us by returning
1300         java_suspend_self();
1301         set_suspend_equivalent();
1302       }
1303     } while (do_self_suspend);
1304   }
1305   static void check_safepoint_and_suspend_for_native_trans(JavaThread *thread);
1306   // Check for async exception in addition to safepoint and suspend request.
1307   static void check_special_condition_for_native_trans(JavaThread *thread);
1308 
1309   // Same as check_special_condition_for_native_trans but finishes the
1310   // transition into thread_in_Java mode so that it can potentially
1311   // block.
1312   static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);
1313 
1314   bool is_ext_suspend_completed(bool called_by_wait, int delay, uint32_t *bits);
1315   bool is_ext_suspend_completed_with_lock(uint32_t *bits) {
1316     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1317     // Warning: is_ext_suspend_completed() may temporarily drop the
1318     // SR_lock to allow the thread to reach a stable thread state if
1319     // it is currently in a transient thread state.
1320     return is_ext_suspend_completed(false /* !called_by_wait */,
1321                                     SuspendRetryDelay, bits);
1322   }
1323 
1324   // We cannot allow wait_for_ext_suspend_completion() to run forever or
1325   // we could hang. SuspendRetryCount and SuspendRetryDelay are normally
1326   // passed as the count and delay parameters. Experiments with specific
1327   // calls to wait_for_ext_suspend_completion() can be done by passing
1328   // other values in the code. Experiments with all calls can be done
1329   // via the appropriate -XX options.
1330   bool wait_for_ext_suspend_completion(int count, int delay, uint32_t *bits);
1331 
1332   // test for suspend - most (all?) of these should go away
1333   bool is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits);
1334 
1335   inline void set_external_suspend();
1336   inline void clear_external_suspend();
1337 
1338   inline void set_deopt_suspend();
1339   inline void clear_deopt_suspend();
1340   bool is_deopt_suspend()         { return (_suspend_flags &amp; _deopt_suspend) != 0; }
1341 
1342   bool is_external_suspend() const {
1343     return (_suspend_flags &amp; _external_suspend) != 0;
1344   }
1345   // Whenever a thread transitions from native to vm/java it must suspend
1346   // if external|deopt suspend is present.
1347   bool is_suspend_after_native() const {
1348     return (_suspend_flags &amp; (_external_suspend | _deopt_suspend)) != 0;
1349   }
1350 
1351   // external suspend request is completed
1352   bool is_ext_suspended() const {
1353     return (_suspend_flags &amp; _ext_suspended) != 0;
1354   }
1355 
1356   bool is_external_suspend_with_lock() const {
1357     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1358     return is_external_suspend();
1359   }
1360 
1361   // Special method to handle a pending external suspend request
1362   // when a suspend equivalent condition lifts.
1363   bool handle_special_suspend_equivalent_condition() {
1364     assert(is_suspend_equivalent(),
1365            "should only be called in a suspend equivalence condition");
1366     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1367     bool ret = is_external_suspend();
1368     if (!ret) {
1369       // not about to self-suspend so clear suspend equivalence
1370       clear_suspend_equivalent();
1371     }
1372     // implied else:
1373     // We have a pending external suspend request so we leave the
1374     // suspend_equivalent flag set until java_suspend_self() sets
1375     // the ext_suspended flag and clears the suspend_equivalent
1376     // flag. This insures that wait_for_ext_suspend_completion()
1377     // will return consistent values.
1378     return ret;
1379   }
1380 
1381   // utility methods to see if we are doing some kind of suspension
1382   bool is_being_ext_suspended() const            {
1383     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1384     return is_ext_suspended() || is_external_suspend();
1385   }
1386 
1387   bool is_suspend_equivalent() const             { return _suspend_equivalent; }
1388 
1389   void set_suspend_equivalent()                  { _suspend_equivalent = true; }
1390   void clear_suspend_equivalent()                { _suspend_equivalent = false; }
1391 
1392   // Thread.stop support
1393   void send_thread_stop(oop throwable);
1394   AsyncRequests clear_special_runtime_exit_condition() {
1395     AsyncRequests x = _special_runtime_exit_condition;
1396     _special_runtime_exit_condition = _no_async_condition;
1397     return x;
1398   }
1399 
1400   // Are any async conditions present?
1401   bool has_async_condition() { return (_special_runtime_exit_condition != _no_async_condition); }
1402 
1403   void check_and_handle_async_exceptions(bool check_unsafe_error = true);
1404 
1405   // these next two are also used for self-suspension and async exception support
1406   void handle_special_runtime_exit_condition(bool check_asyncs = true);
1407 
1408   // Return true if JavaThread has an asynchronous condition or
1409   // if external suspension is requested.
1410   bool has_special_runtime_exit_condition() {
1411     // Because we don't use is_external_suspend_with_lock
1412     // it is possible that we won't see an asynchronous external suspend
1413     // request that has just gotten started, i.e., SR_lock grabbed but
1414     // _external_suspend field change either not made yet or not visible
1415     // yet. However, this is okay because the request is asynchronous and
1416     // we will see the new flag value the next time through. It's also
1417     // possible that the external suspend request is dropped after
1418     // we have checked is_external_suspend(), we will recheck its value
1419     // under SR_lock in java_suspend_self().
1420     return (_special_runtime_exit_condition != _no_async_condition) ||
1421             is_external_suspend() || is_trace_suspend();
1422   }
1423 
1424   void set_pending_unsafe_access_error()          { _special_runtime_exit_condition = _async_unsafe_access_error; }
1425 
1426   inline void set_pending_async_exception(oop e);
1427 
1428   // Fast-locking support
1429   bool is_lock_owned(address adr) const;
1430 
1431   // Accessors for vframe array top
1432   // The linked list of vframe arrays are sorted on sp. This means when we
1433   // unpack the head must contain the vframe array to unpack.
1434   void set_vframe_array_head(vframeArray* value) { _vframe_array_head = value; }
1435   vframeArray* vframe_array_head() const         { return _vframe_array_head;  }
1436 
1437   // Side structure for deferring update of java frame locals until deopt occurs
1438   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* deferred_locals() const { return _deferred_locals_updates; }
1439   void set_deferred_locals(GrowableArray&lt;jvmtiDeferredLocalVariableSet *&gt;* vf) { _deferred_locals_updates = vf; }
1440 
1441   // These only really exist to make debugging deopt problems simpler
1442 
1443   void set_vframe_array_last(vframeArray* value) { _vframe_array_last = value; }
1444   vframeArray* vframe_array_last() const         { return _vframe_array_last;  }
1445 
1446   // The special resourceMark used during deoptimization
1447 
1448   void set_deopt_mark(DeoptResourceMark* value)  { _deopt_mark = value; }
1449   DeoptResourceMark* deopt_mark(void)            { return _deopt_mark; }
1450 
1451   intptr_t* must_deopt_id()                      { return _must_deopt_id; }
1452   void     set_must_deopt_id(intptr_t* id)       { _must_deopt_id = id; }
1453   void     clear_must_deopt_id()                 { _must_deopt_id = NULL; }
1454 
1455   void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }
1456   CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }
1457 
1458   Method*    callee_target() const               { return _callee_target; }
1459   void set_callee_target  (Method* x)          { _callee_target   = x; }
1460 
1461   // Oop results of vm runtime calls
1462   oop  vm_result() const                         { return _vm_result; }
1463   void set_vm_result  (oop x)                    { _vm_result   = x; }
1464 
1465   Metadata*    vm_result_2() const               { return _vm_result_2; }
1466   void set_vm_result_2  (Metadata* x)          { _vm_result_2   = x; }
1467 
1468   MemRegion deferred_card_mark() const           { return _deferred_card_mark; }
1469   void set_deferred_card_mark(MemRegion mr)      { _deferred_card_mark = mr;   }
1470 
1471 #if INCLUDE_JVMCI
1472   int  pending_deoptimization() const             { return _pending_deoptimization; }
1473   long pending_failed_speculation() const         { return _pending_failed_speculation; }
1474   bool adjusting_comp_level() const               { return _adjusting_comp_level; }
1475   void set_adjusting_comp_level(bool b)           { _adjusting_comp_level = b; }
1476   bool has_pending_monitorenter() const           { return _pending_monitorenter; }
1477   void set_pending_monitorenter(bool b)           { _pending_monitorenter = b; }
1478   void set_pending_deoptimization(int reason)     { _pending_deoptimization = reason; }
1479   void set_pending_failed_speculation(long failed_speculation) { _pending_failed_speculation = failed_speculation; }
1480   void set_pending_transfer_to_interpreter(bool b) { _pending_transfer_to_interpreter = b; }
1481   void set_jvmci_alternate_call_target(address a) { assert(_jvmci._alternate_call_target == NULL, "must be"); _jvmci._alternate_call_target = a; }
1482   void set_jvmci_implicit_exception_pc(address a) { assert(_jvmci._implicit_exception_pc == NULL, "must be"); _jvmci._implicit_exception_pc = a; }
<a name="3" id="anc3"></a><span class="new">1483 </span>
<span class="new">1484   virtual bool in_retryable_allocation() const    { return _in_retryable_allocation; }</span>
<span class="new">1485   void set_in_retryable_allocation(bool b)        { _in_retryable_allocation = b; }</span>
1486 #endif // INCLUDE_JVMCI
1487 
1488   // Exception handling for compiled methods
1489   oop      exception_oop() const                 { return _exception_oop; }
1490   address  exception_pc() const                  { return _exception_pc; }
1491   address  exception_handler_pc() const          { return _exception_handler_pc; }
1492   bool     is_method_handle_return() const       { return _is_method_handle_return == 1; }
1493 
1494   void set_exception_oop(oop o)                  { (void)const_cast&lt;oop&amp;&gt;(_exception_oop = o); }
1495   void set_exception_pc(address a)               { _exception_pc = a; }
1496   void set_exception_handler_pc(address a)       { _exception_handler_pc = a; }
1497   void set_is_method_handle_return(bool value)   { _is_method_handle_return = value ? 1 : 0; }
1498 
1499   void clear_exception_oop_and_pc() {
1500     set_exception_oop(NULL);
1501     set_exception_pc(NULL);
1502   }
1503 
1504   // Stack overflow support
1505   //
1506   //  (small addresses)
1507   //
1508   //  --  &lt;-- stack_end()                   ---
1509   //  |                                      |
1510   //  |  red pages                           |
1511   //  |                                      |
1512   //  --  &lt;-- stack_red_zone_base()          |
1513   //  |                                      |
1514   //  |                                     guard
1515   //  |  yellow pages                       zone
1516   //  |                                      |
1517   //  |                                      |
1518   //  --  &lt;-- stack_yellow_zone_base()       |
1519   //  |                                      |
1520   //  |                                      |
1521   //  |  reserved pages                      |
1522   //  |                                      |
1523   //  --  &lt;-- stack_reserved_zone_base()    ---      ---
1524   //                                                 /|\  shadow     &lt;--  stack_overflow_limit() (somewhere in here)
1525   //                                                  |   zone
1526   //                                                 \|/  size
1527   //  some untouched memory                          ---
1528   //
1529   //
1530   //  --
1531   //  |
1532   //  |  shadow zone
1533   //  |
1534   //  --
1535   //  x    frame n
1536   //  --
1537   //  x    frame n-1
1538   //  x
1539   //  --
1540   //  ...
1541   //
1542   //  --
1543   //  x    frame 0
1544   //  --  &lt;-- stack_base()
1545   //
1546   //  (large addresses)
1547   //
1548 
1549  private:
1550   // These values are derived from flags StackRedPages, StackYellowPages,
1551   // StackReservedPages and StackShadowPages. The zone size is determined
1552   // ergonomically if page_size &gt; 4K.
1553   static size_t _stack_red_zone_size;
1554   static size_t _stack_yellow_zone_size;
1555   static size_t _stack_reserved_zone_size;
1556   static size_t _stack_shadow_zone_size;
1557  public:
1558   inline size_t stack_available(address cur_sp);
1559 
1560   static size_t stack_red_zone_size() {
1561     assert(_stack_red_zone_size &gt; 0, "Don't call this before the field is initialized.");
1562     return _stack_red_zone_size;
1563   }
1564   static void set_stack_red_zone_size(size_t s) {
1565     assert(is_aligned(s, os::vm_page_size()),
1566            "We can not protect if the red zone size is not page aligned.");
1567     assert(_stack_red_zone_size == 0, "This should be called only once.");
1568     _stack_red_zone_size = s;
1569   }
1570   address stack_red_zone_base() {
1571     return (address)(stack_end() + stack_red_zone_size());
1572   }
1573   bool in_stack_red_zone(address a) {
1574     return a &lt;= stack_red_zone_base() &amp;&amp; a &gt;= stack_end();
1575   }
1576 
1577   static size_t stack_yellow_zone_size() {
1578     assert(_stack_yellow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1579     return _stack_yellow_zone_size;
1580   }
1581   static void set_stack_yellow_zone_size(size_t s) {
1582     assert(is_aligned(s, os::vm_page_size()),
1583            "We can not protect if the yellow zone size is not page aligned.");
1584     assert(_stack_yellow_zone_size == 0, "This should be called only once.");
1585     _stack_yellow_zone_size = s;
1586   }
1587 
1588   static size_t stack_reserved_zone_size() {
1589     // _stack_reserved_zone_size may be 0. This indicates the feature is off.
1590     return _stack_reserved_zone_size;
1591   }
1592   static void set_stack_reserved_zone_size(size_t s) {
1593     assert(is_aligned(s, os::vm_page_size()),
1594            "We can not protect if the reserved zone size is not page aligned.");
1595     assert(_stack_reserved_zone_size == 0, "This should be called only once.");
1596     _stack_reserved_zone_size = s;
1597   }
1598   address stack_reserved_zone_base() {
1599     return (address)(stack_end() +
1600                      (stack_red_zone_size() + stack_yellow_zone_size() + stack_reserved_zone_size()));
1601   }
1602   bool in_stack_reserved_zone(address a) {
1603     return (a &lt;= stack_reserved_zone_base()) &amp;&amp;
1604            (a &gt;= (address)((intptr_t)stack_reserved_zone_base() - stack_reserved_zone_size()));
1605   }
1606 
1607   static size_t stack_yellow_reserved_zone_size() {
1608     return _stack_yellow_zone_size + _stack_reserved_zone_size;
1609   }
1610   bool in_stack_yellow_reserved_zone(address a) {
1611     return (a &lt;= stack_reserved_zone_base()) &amp;&amp; (a &gt;= stack_red_zone_base());
1612   }
1613 
1614   // Size of red + yellow + reserved zones.
1615   static size_t stack_guard_zone_size() {
1616     return stack_red_zone_size() + stack_yellow_reserved_zone_size();
1617   }
1618 
1619   static size_t stack_shadow_zone_size() {
1620     assert(_stack_shadow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1621     return _stack_shadow_zone_size;
1622   }
1623   static void set_stack_shadow_zone_size(size_t s) {
1624     // The shadow area is not allocated or protected, so
1625     // it needs not be page aligned.
1626     // But the stack bang currently assumes that it is a
1627     // multiple of page size. This guarantees that the bang
1628     // loop touches all pages in the shadow zone.
1629     // This can be guaranteed differently, as well.  E.g., if
1630     // the page size is a multiple of 4K, banging in 4K steps
1631     // suffices to touch all pages. (Some pages are banged
1632     // several times, though.)
1633     assert(is_aligned(s, os::vm_page_size()),
1634            "Stack bang assumes multiple of page size.");
1635     assert(_stack_shadow_zone_size == 0, "This should be called only once.");
1636     _stack_shadow_zone_size = s;
1637   }
1638 
1639   void create_stack_guard_pages();
1640   void remove_stack_guard_pages();
1641 
1642   void enable_stack_reserved_zone();
1643   void disable_stack_reserved_zone();
1644   void enable_stack_yellow_reserved_zone();
1645   void disable_stack_yellow_reserved_zone();
1646   void enable_stack_red_zone();
1647   void disable_stack_red_zone();
1648 
1649   inline bool stack_guard_zone_unused();
1650   inline bool stack_yellow_reserved_zone_disabled();
1651   inline bool stack_reserved_zone_disabled();
1652   inline bool stack_guards_enabled();
1653 
1654   address reserved_stack_activation() const { return _reserved_stack_activation; }
1655   void set_reserved_stack_activation(address addr) {
1656     assert(_reserved_stack_activation == stack_base()
1657             || _reserved_stack_activation == NULL
1658             || addr == stack_base(), "Must not be set twice");
1659     _reserved_stack_activation = addr;
1660   }
1661 
1662   // Attempt to reguard the stack after a stack overflow may have occurred.
1663   // Returns true if (a) guard pages are not needed on this thread, (b) the
1664   // pages are already guarded, or (c) the pages were successfully reguarded.
1665   // Returns false if there is not enough stack space to reguard the pages, in
1666   // which case the caller should unwind a frame and try again.  The argument
1667   // should be the caller's (approximate) sp.
1668   bool reguard_stack(address cur_sp);
1669   // Similar to above but see if current stackpoint is out of the guard area
1670   // and reguard if possible.
1671   bool reguard_stack(void);
1672 
1673   address stack_overflow_limit() { return _stack_overflow_limit; }
1674   void set_stack_overflow_limit() {
1675     _stack_overflow_limit =
1676       stack_end() + MAX2(JavaThread::stack_guard_zone_size(), JavaThread::stack_shadow_zone_size());
1677   }
1678 
1679   // Misc. accessors/mutators
1680   void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }
1681   void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }
1682   bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }
1683 
1684 #ifndef PRODUCT
1685   void record_jump(address target, address instr, const char* file, int line);
1686 #endif // PRODUCT
1687 
1688   // For assembly stub generation
1689   static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }
1690 #ifndef PRODUCT
1691   static ByteSize jmp_ring_index_offset()        { return byte_offset_of(JavaThread, _jmp_ring_index); }
1692   static ByteSize jmp_ring_offset()              { return byte_offset_of(JavaThread, _jmp_ring); }
1693 #endif // PRODUCT
1694   static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }
1695   static ByteSize pending_jni_exception_check_fn_offset() {
1696     return byte_offset_of(JavaThread, _pending_jni_exception_check_fn);
1697   }
1698   static ByteSize last_Java_sp_offset() {
1699     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();
1700   }
1701   static ByteSize last_Java_pc_offset() {
1702     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();
1703   }
1704   static ByteSize frame_anchor_offset() {
1705     return byte_offset_of(JavaThread, _anchor);
1706   }
1707   static ByteSize callee_target_offset()         { return byte_offset_of(JavaThread, _callee_target); }
1708   static ByteSize vm_result_offset()             { return byte_offset_of(JavaThread, _vm_result); }
1709   static ByteSize vm_result_2_offset()           { return byte_offset_of(JavaThread, _vm_result_2); }
1710   static ByteSize thread_state_offset()          { return byte_offset_of(JavaThread, _thread_state); }
1711   static ByteSize saved_exception_pc_offset()    { return byte_offset_of(JavaThread, _saved_exception_pc); }
1712   static ByteSize osthread_offset()              { return byte_offset_of(JavaThread, _osthread); }
1713 #if INCLUDE_JVMCI
1714   static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }
1715   static ByteSize pending_monitorenter_offset()  { return byte_offset_of(JavaThread, _pending_monitorenter); }
1716   static ByteSize pending_failed_speculation_offset() { return byte_offset_of(JavaThread, _pending_failed_speculation); }
1717   static ByteSize jvmci_alternate_call_target_offset() { return byte_offset_of(JavaThread, _jvmci._alternate_call_target); }
1718   static ByteSize jvmci_implicit_exception_pc_offset() { return byte_offset_of(JavaThread, _jvmci._implicit_exception_pc); }
1719   static ByteSize jvmci_counters_offset()        { return byte_offset_of(JavaThread, _jvmci_counters); }
1720 #endif // INCLUDE_JVMCI
1721   static ByteSize exception_oop_offset()         { return byte_offset_of(JavaThread, _exception_oop); }
1722   static ByteSize exception_pc_offset()          { return byte_offset_of(JavaThread, _exception_pc); }
1723   static ByteSize exception_handler_pc_offset()  { return byte_offset_of(JavaThread, _exception_handler_pc); }
1724   static ByteSize stack_overflow_limit_offset()  { return byte_offset_of(JavaThread, _stack_overflow_limit); }
1725   static ByteSize is_method_handle_return_offset() { return byte_offset_of(JavaThread, _is_method_handle_return); }
1726   static ByteSize stack_guard_state_offset()     { return byte_offset_of(JavaThread, _stack_guard_state); }
1727   static ByteSize reserved_stack_activation_offset() { return byte_offset_of(JavaThread, _reserved_stack_activation); }
1728   static ByteSize suspend_flags_offset()         { return byte_offset_of(JavaThread, _suspend_flags); }
1729 
1730   static ByteSize do_not_unlock_if_synchronized_offset() { return byte_offset_of(JavaThread, _do_not_unlock_if_synchronized); }
1731   static ByteSize should_post_on_exceptions_flag_offset() {
1732     return byte_offset_of(JavaThread, _should_post_on_exceptions_flag);
1733   }
1734 
1735   // Returns the jni environment for this thread
1736   JNIEnv* jni_environment()                      { return &amp;_jni_environment; }
1737 
1738   static JavaThread* thread_from_jni_environment(JNIEnv* env) {
1739     JavaThread *thread_from_jni_env = (JavaThread*)((intptr_t)env - in_bytes(jni_environment_offset()));
1740     // Only return NULL if thread is off the thread list; starting to
1741     // exit should not return NULL.
1742     if (thread_from_jni_env-&gt;is_terminated()) {
1743       thread_from_jni_env-&gt;block_if_vm_exited();
1744       return NULL;
1745     } else {
1746       return thread_from_jni_env;
1747     }
1748   }
1749 
1750   // JNI critical regions. These can nest.
1751   bool in_critical()    { return _jni_active_critical &gt; 0; }
1752   bool in_last_critical()  { return _jni_active_critical == 1; }
1753   void enter_critical() {
1754     assert(Thread::current() == this ||
1755            (Thread::current()-&gt;is_VM_thread() &amp;&amp;
1756            SafepointSynchronize::is_synchronizing()),
1757            "this must be current thread or synchronizing");
1758     _jni_active_critical++;
1759   }
1760   void exit_critical() {
1761     assert(Thread::current() == this, "this must be current thread");
1762     _jni_active_critical--;
1763     assert(_jni_active_critical &gt;= 0, "JNI critical nesting problem?");
1764   }
1765 
1766   // Checked JNI: is the programmer required to check for exceptions, if so specify
1767   // which function name. Returning to a Java frame should implicitly clear the
1768   // pending check, this is done for Native-&gt;Java transitions (i.e. user JNI code).
1769   // VM-&gt;Java transistions are not cleared, it is expected that JNI code enclosed
1770   // within ThreadToNativeFromVM makes proper exception checks (i.e. VM internal).
1771   bool is_pending_jni_exception_check() const { return _pending_jni_exception_check_fn != NULL; }
1772   void clear_pending_jni_exception_check() { _pending_jni_exception_check_fn = NULL; }
1773   const char* get_pending_jni_exception_check() const { return _pending_jni_exception_check_fn; }
1774   void set_pending_jni_exception_check(const char* fn_name) { _pending_jni_exception_check_fn = (char*) fn_name; }
1775 
1776   // For deadlock detection
1777   int depth_first_number() { return _depth_first_number; }
1778   void set_depth_first_number(int dfn) { _depth_first_number = dfn; }
1779 
1780  private:
1781   void set_monitor_chunks(MonitorChunk* monitor_chunks) { _monitor_chunks = monitor_chunks; }
1782 
1783  public:
1784   MonitorChunk* monitor_chunks() const           { return _monitor_chunks; }
1785   void add_monitor_chunk(MonitorChunk* chunk);
1786   void remove_monitor_chunk(MonitorChunk* chunk);
1787   bool in_deopt_handler() const                  { return _in_deopt_handler &gt; 0; }
1788   void inc_in_deopt_handler()                    { _in_deopt_handler++; }
1789   void dec_in_deopt_handler() {
1790     assert(_in_deopt_handler &gt; 0, "mismatched deopt nesting");
1791     if (_in_deopt_handler &gt; 0) { // robustness
1792       _in_deopt_handler--;
1793     }
1794   }
1795 
1796  private:
1797   void set_entry_point(ThreadFunction entry_point) { _entry_point = entry_point; }
1798 
1799  public:
1800 
1801   // Frame iteration; calls the function f for all frames on the stack
1802   void frames_do(void f(frame*, const RegisterMap*));
1803 
1804   // Memory operations
1805   void oops_do(OopClosure* f, CodeBlobClosure* cf);
1806 
1807   // Sweeper operations
1808   virtual void nmethods_do(CodeBlobClosure* cf);
1809 
1810   // RedefineClasses Support
1811   void metadata_do(void f(Metadata*));
1812 
1813   // Misc. operations
1814   char* name() const { return (char*)get_thread_name(); }
1815   void print_on(outputStream* st, bool print_extended_info) const;
1816   void print_on(outputStream* st) const { print_on(st, false); }
1817   void print_value();
1818   void print_thread_state_on(outputStream*) const      PRODUCT_RETURN;
1819   void print_thread_state() const                      PRODUCT_RETURN;
1820   void print_on_error(outputStream* st, char* buf, int buflen) const;
1821   void print_name_on_error(outputStream* st, char* buf, int buflen) const;
1822   void verify();
1823   const char* get_thread_name() const;
1824  private:
1825   // factor out low-level mechanics for use in both normal and error cases
1826   const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;
1827  public:
1828   const char* get_threadgroup_name() const;
1829   const char* get_parent_name() const;
1830 
1831   // Accessing frames
1832   frame last_frame() {
1833     _anchor.make_walkable(this);
1834     return pd_last_frame();
1835   }
1836   javaVFrame* last_java_vframe(RegisterMap* reg_map);
1837 
1838   // Returns method at 'depth' java or native frames down the stack
1839   // Used for security checks
1840   Klass* security_get_caller_class(int depth);
1841 
1842   // Print stack trace in external format
1843   void print_stack_on(outputStream* st);
1844   void print_stack() { print_stack_on(tty); }
1845 
1846   // Print stack traces in various internal formats
1847   void trace_stack()                             PRODUCT_RETURN;
1848   void trace_stack_from(vframe* start_vf)        PRODUCT_RETURN;
1849   void trace_frames()                            PRODUCT_RETURN;
1850   void trace_oops()                              PRODUCT_RETURN;
1851 
1852   // Print an annotated view of the stack frames
1853   void print_frame_layout(int depth = 0, bool validate_only = false) NOT_DEBUG_RETURN;
1854   void validate_frame_layout() {
1855     print_frame_layout(0, true);
1856   }
1857 
1858   // Returns the number of stack frames on the stack
1859   int depth() const;
1860 
1861   // Function for testing deoptimization
1862   void deoptimize();
1863   void make_zombies();
1864 
1865   void deoptimized_wrt_marked_nmethods();
1866 
1867  public:
1868   // Returns the running thread as a JavaThread
1869   static inline JavaThread* current();
1870 
1871   // Returns the active Java thread.  Do not use this if you know you are calling
1872   // from a JavaThread, as it's slower than JavaThread::current.  If called from
1873   // the VMThread, it also returns the JavaThread that instigated the VMThread's
1874   // operation.  You may not want that either.
1875   static JavaThread* active();
1876 
1877   inline CompilerThread* as_CompilerThread();
1878 
1879  public:
1880   virtual void run();
1881   void thread_main_inner();
1882 
1883  private:
1884   // PRIVILEGED STACK
1885   PrivilegedElement*  _privileged_stack_top;
1886   GrowableArray&lt;oop&gt;* _array_for_gc;
1887  public:
1888 
1889   // Returns the privileged_stack information.
1890   PrivilegedElement* privileged_stack_top() const       { return _privileged_stack_top; }
1891   void set_privileged_stack_top(PrivilegedElement *e)   { _privileged_stack_top = e; }
1892   void register_array_for_gc(GrowableArray&lt;oop&gt;* array) { _array_for_gc = array; }
1893 
1894  public:
1895   // Thread local information maintained by JVMTI.
1896   void set_jvmti_thread_state(JvmtiThreadState *value)                           { _jvmti_thread_state = value; }
1897   // A JvmtiThreadState is lazily allocated. This jvmti_thread_state()
1898   // getter is used to get this JavaThread's JvmtiThreadState if it has
1899   // one which means NULL can be returned. JvmtiThreadState::state_for()
1900   // is used to get the specified JavaThread's JvmtiThreadState if it has
1901   // one or it allocates a new JvmtiThreadState for the JavaThread and
1902   // returns it. JvmtiThreadState::state_for() will return NULL only if
1903   // the specified JavaThread is exiting.
1904   JvmtiThreadState *jvmti_thread_state() const                                   { return _jvmti_thread_state; }
1905   static ByteSize jvmti_thread_state_offset()                                    { return byte_offset_of(JavaThread, _jvmti_thread_state); }
1906 
1907   // JVMTI PopFrame support
1908   // Setting and clearing popframe_condition
1909   // All of these enumerated values are bits. popframe_pending
1910   // indicates that a PopFrame() has been requested and not yet been
1911   // completed. popframe_processing indicates that that PopFrame() is in
1912   // the process of being completed. popframe_force_deopt_reexecution_bit
1913   // indicates that special handling is required when returning to a
1914   // deoptimized caller.
1915   enum PopCondition {
1916     popframe_inactive                      = 0x00,
1917     popframe_pending_bit                   = 0x01,
1918     popframe_processing_bit                = 0x02,
1919     popframe_force_deopt_reexecution_bit   = 0x04
1920   };
1921   PopCondition popframe_condition()                   { return (PopCondition) _popframe_condition; }
1922   void set_popframe_condition(PopCondition c)         { _popframe_condition = c; }
1923   void set_popframe_condition_bit(PopCondition c)     { _popframe_condition |= c; }
1924   void clear_popframe_condition()                     { _popframe_condition = popframe_inactive; }
1925   static ByteSize popframe_condition_offset()         { return byte_offset_of(JavaThread, _popframe_condition); }
1926   bool has_pending_popframe()                         { return (popframe_condition() &amp; popframe_pending_bit) != 0; }
1927   bool popframe_forcing_deopt_reexecution()           { return (popframe_condition() &amp; popframe_force_deopt_reexecution_bit) != 0; }
1928   void clear_popframe_forcing_deopt_reexecution()     { _popframe_condition &amp;= ~popframe_force_deopt_reexecution_bit; }
1929 #ifdef CC_INTERP
1930   bool pop_frame_pending(void)                        { return ((_popframe_condition &amp; popframe_pending_bit) != 0); }
1931   void clr_pop_frame_pending(void)                    { _popframe_condition = popframe_inactive; }
1932   bool pop_frame_in_process(void)                     { return ((_popframe_condition &amp; popframe_processing_bit) != 0); }
1933   void set_pop_frame_in_process(void)                 { _popframe_condition |= popframe_processing_bit; }
1934   void clr_pop_frame_in_process(void)                 { _popframe_condition &amp;= ~popframe_processing_bit; }
1935 #endif
1936 
1937   int frames_to_pop_failed_realloc() const            { return _frames_to_pop_failed_realloc; }
1938   void set_frames_to_pop_failed_realloc(int nb)       { _frames_to_pop_failed_realloc = nb; }
1939   void dec_frames_to_pop_failed_realloc()             { _frames_to_pop_failed_realloc--; }
1940 
1941  private:
1942   // Saved incoming arguments to popped frame.
1943   // Used only when popped interpreted frame returns to deoptimized frame.
1944   void*    _popframe_preserved_args;
1945   int      _popframe_preserved_args_size;
1946 
1947  public:
1948   void  popframe_preserve_args(ByteSize size_in_bytes, void* start);
1949   void* popframe_preserved_args();
1950   ByteSize popframe_preserved_args_size();
1951   WordSize popframe_preserved_args_size_in_words();
1952   void  popframe_free_preserved_args();
1953 
1954 
1955  private:
1956   JvmtiThreadState *_jvmti_thread_state;
1957 
1958   // Used by the interpreter in fullspeed mode for frame pop, method
1959   // entry, method exit and single stepping support. This field is
1960   // only set to non-zero by the VM_EnterInterpOnlyMode VM operation.
1961   // It can be set to zero asynchronously (i.e., without a VM operation
1962   // or a lock) so we have to be very careful.
1963   int               _interp_only_mode;
1964 
1965  public:
1966   // used by the interpreter for fullspeed debugging support (see above)
1967   static ByteSize interp_only_mode_offset() { return byte_offset_of(JavaThread, _interp_only_mode); }
1968   bool is_interp_only_mode()                { return (_interp_only_mode != 0); }
1969   int get_interp_only_mode()                { return _interp_only_mode; }
1970   void increment_interp_only_mode()         { ++_interp_only_mode; }
1971   void decrement_interp_only_mode()         { --_interp_only_mode; }
1972 
1973   // support for cached flag that indicates whether exceptions need to be posted for this thread
1974   // if this is false, we can avoid deoptimizing when events are thrown
1975   // this gets set to reflect whether jvmtiExport::post_exception_throw would actually do anything
1976  private:
1977   int    _should_post_on_exceptions_flag;
1978 
1979  public:
1980   int   should_post_on_exceptions_flag()  { return _should_post_on_exceptions_flag; }
1981   void  set_should_post_on_exceptions_flag(int val)  { _should_post_on_exceptions_flag = val; }
1982 
1983  private:
1984   ThreadStatistics *_thread_stat;
1985 
1986  public:
1987   ThreadStatistics* get_thread_stat() const    { return _thread_stat; }
1988 
1989   // Return a blocker object for which this thread is blocked parking.
1990   oop current_park_blocker();
1991 
1992  private:
1993   static size_t _stack_size_at_create;
1994 
1995  public:
1996   static inline size_t stack_size_at_create(void) {
1997     return _stack_size_at_create;
1998   }
1999   static inline void set_stack_size_at_create(size_t value) {
2000     _stack_size_at_create = value;
2001   }
2002 
2003   // Machine dependent stuff
2004 #include OS_CPU_HEADER(thread)
2005 
2006  public:
2007   void set_blocked_on_compilation(bool value) {
2008     _blocked_on_compilation = value;
2009   }
2010 
2011   bool blocked_on_compilation() {
2012     return _blocked_on_compilation;
2013   }
2014  protected:
2015   bool         _blocked_on_compilation;
2016 
2017 
2018   // JSR166 per-thread parker
2019  private:
2020   Parker*    _parker;
2021  public:
2022   Parker*     parker() { return _parker; }
2023 
2024   // Biased locking support
2025  private:
2026   GrowableArray&lt;MonitorInfo*&gt;* _cached_monitor_info;
2027  public:
2028   GrowableArray&lt;MonitorInfo*&gt;* cached_monitor_info() { return _cached_monitor_info; }
2029   void set_cached_monitor_info(GrowableArray&lt;MonitorInfo*&gt;* info) { _cached_monitor_info = info; }
2030 
2031   // clearing/querying jni attach status
2032   bool is_attaching_via_jni() const { return _jni_attach_state == _attaching_via_jni; }
2033   bool has_attached_via_jni() const { return is_attaching_via_jni() || _jni_attach_state == _attached_via_jni; }
2034   inline void set_done_attaching_via_jni();
2035 };
2036 
2037 // Inline implementation of JavaThread::current
2038 inline JavaThread* JavaThread::current() {
2039   Thread* thread = Thread::current();
2040   assert(thread-&gt;is_Java_thread(), "just checking");
2041   return (JavaThread*)thread;
2042 }
2043 
2044 inline CompilerThread* JavaThread::as_CompilerThread() {
2045   assert(is_Compiler_thread(), "just checking");
2046   return (CompilerThread*)this;
2047 }
2048 
2049 // Dedicated thread to sweep the code cache
2050 class CodeCacheSweeperThread : public JavaThread {
2051   CompiledMethod*       _scanned_compiled_method; // nmethod being scanned by the sweeper
2052  public:
2053   CodeCacheSweeperThread();
2054   // Track the nmethod currently being scanned by the sweeper
2055   void set_scanned_compiled_method(CompiledMethod* cm) {
2056     assert(_scanned_compiled_method == NULL || cm == NULL, "should reset to NULL before writing a new value");
2057     _scanned_compiled_method = cm;
2058   }
2059 
2060   // Hide sweeper thread from external view.
2061   bool is_hidden_from_external_view() const { return true; }
2062 
2063   bool is_Code_cache_sweeper_thread() const { return true; }
2064 
2065   // Prevent GC from unloading _scanned_compiled_method
2066   void oops_do(OopClosure* f, CodeBlobClosure* cf);
2067   void nmethods_do(CodeBlobClosure* cf);
2068 };
2069 
2070 // A thread used for Compilation.
2071 class CompilerThread : public JavaThread {
2072   friend class VMStructs;
2073  private:
2074   CompilerCounters* _counters;
2075 
2076   ciEnv*                _env;
2077   CompileLog*           _log;
2078   CompileTask* volatile _task;  // print_threads_compiling can read this concurrently.
2079   CompileQueue*         _queue;
2080   BufferBlob*           _buffer_blob;
2081 
2082   AbstractCompiler*     _compiler;
2083   TimeStamp             _idle_time;
2084 
2085  public:
2086 
2087   static CompilerThread* current();
2088 
2089   CompilerThread(CompileQueue* queue, CompilerCounters* counters);
2090   ~CompilerThread();
2091 
2092   bool is_Compiler_thread() const                { return true; }
2093 
2094   virtual bool can_call_java() const;
2095 
2096   // Hide native compiler threads from external view.
2097   bool is_hidden_from_external_view() const      { return !can_call_java(); }
2098 
2099   void set_compiler(AbstractCompiler* c)         { _compiler = c; }
2100   AbstractCompiler* compiler() const             { return _compiler; }
2101 
2102   CompileQueue* queue()        const             { return _queue; }
2103   CompilerCounters* counters() const             { return _counters; }
2104 
2105   // Get/set the thread's compilation environment.
2106   ciEnv*        env()                            { return _env; }
2107   void          set_env(ciEnv* env)              { _env = env; }
2108 
2109   BufferBlob*   get_buffer_blob() const          { return _buffer_blob; }
2110   void          set_buffer_blob(BufferBlob* b)   { _buffer_blob = b; }
2111 
2112   // Get/set the thread's logging information
2113   CompileLog*   log()                            { return _log; }
2114   void          init_log(CompileLog* log) {
2115     // Set once, for good.
2116     assert(_log == NULL, "set only once");
2117     _log = log;
2118   }
2119 
2120   void start_idle_timer()                        { _idle_time.update(); }
2121   jlong idle_time_millis() {
2122     return TimeHelper::counter_to_millis(_idle_time.ticks_since_update());
2123   }
2124 
2125 #ifndef PRODUCT
2126  private:
2127   IdealGraphPrinter *_ideal_graph_printer;
2128  public:
2129   IdealGraphPrinter *ideal_graph_printer()           { return _ideal_graph_printer; }
2130   void set_ideal_graph_printer(IdealGraphPrinter *n) { _ideal_graph_printer = n; }
2131 #endif
2132 
2133   // Get/set the thread's current task
2134   CompileTask* task()                      { return _task; }
2135   void         set_task(CompileTask* task) { _task = task; }
2136 };
2137 
2138 inline CompilerThread* CompilerThread::current() {
2139   return JavaThread::current()-&gt;as_CompilerThread();
2140 }
2141 
2142 // The active thread queue. It also keeps track of the current used
2143 // thread priorities.
2144 class Threads: AllStatic {
2145   friend class VMStructs;
2146  private:
2147   static JavaThread* _thread_list;
2148   static int         _number_of_threads;
2149   static int         _number_of_non_daemon_threads;
2150   static int         _return_code;
2151   static int         _thread_claim_parity;
2152 #ifdef ASSERT
2153   static bool        _vm_complete;
2154   static size_t      _threads_before_barrier_set;
2155 #endif
2156 
2157   static void initialize_java_lang_classes(JavaThread* main_thread, TRAPS);
2158   static void initialize_jsr292_core_classes(TRAPS);
2159 
2160  public:
2161   // Thread management
2162   // force_daemon is a concession to JNI, where we may need to add a
2163   // thread to the thread list before allocating its thread object
2164   static void add(JavaThread* p, bool force_daemon = false);
2165   static void remove(JavaThread* p);
2166   static void non_java_threads_do(ThreadClosure* tc);
2167   static void java_threads_do(ThreadClosure* tc);
2168   static void java_threads_and_vm_thread_do(ThreadClosure* tc);
2169   static void threads_do(ThreadClosure* tc);
2170   static void possibly_parallel_threads_do(bool is_par, ThreadClosure* tc);
2171 
2172   // Initializes the vm and creates the vm thread
2173   static jint create_vm(JavaVMInitArgs* args, bool* canTryAgain);
2174   static void convert_vm_init_libraries_to_agents();
2175   static void create_vm_init_libraries();
2176   static void create_vm_init_agents();
2177   static void shutdown_vm_agents();
2178   static bool destroy_vm();
2179   // Supported VM versions via JNI
2180   // Includes JNI_VERSION_1_1
2181   static jboolean is_supported_jni_version_including_1_1(jint version);
2182   // Does not include JNI_VERSION_1_1
2183   static jboolean is_supported_jni_version(jint version);
2184 
2185   // The "thread claim parity" provides a way for threads to be claimed
2186   // by parallel worker tasks.
2187   //
2188   // Each thread contains a a "parity" field. A task will claim the
2189   // thread only if its parity field is the same as the global parity,
2190   // which is updated by calling change_thread_claim_parity().
2191   //
2192   // For this to work change_thread_claim_parity() needs to be called
2193   // exactly once in sequential code before starting parallel tasks
2194   // that should claim threads.
2195   //
2196   // New threads get their parity set to 0 and change_thread_claim_parity()
2197   // never set the global parity to 0.
2198   static int thread_claim_parity() { return _thread_claim_parity; }
2199   static void change_thread_claim_parity();
2200   static void assert_all_threads_claimed() NOT_DEBUG_RETURN;
2201 
2202   // Apply "f-&gt;do_oop" to all root oops in all threads.
2203   // This version may only be called by sequential code.
2204   static void oops_do(OopClosure* f, CodeBlobClosure* cf);
2205   // This version may be called by sequential or parallel code.
2206   static void possibly_parallel_oops_do(bool is_par, OopClosure* f, CodeBlobClosure* cf);
2207 
2208   // Apply "f-&gt;do_oop" to roots in all threads that
2209   // are part of compiled frames
2210   static void compiled_frame_oops_do(OopClosure* f, CodeBlobClosure* cf);
2211 
2212   static void convert_hcode_pointers();
2213   static void restore_hcode_pointers();
2214 
2215   // Sweeper
2216   static void nmethods_do(CodeBlobClosure* cf);
2217 
2218   // RedefineClasses support
2219   static void metadata_do(void f(Metadata*));
2220   static void metadata_handles_do(void f(Metadata*));
2221 
2222 #ifdef ASSERT
2223   static bool is_vm_complete() { return _vm_complete; }
2224 
2225   static size_t threads_before_barrier_set() {
2226     return _threads_before_barrier_set;
2227   }
2228 
2229   static void inc_threads_before_barrier_set() {
2230     ++_threads_before_barrier_set;
2231   }
2232 #endif // ASSERT
2233 
2234   // Verification
2235   static void verify();
2236   static void print_on(outputStream* st, bool print_stacks, bool internal_format, bool print_concurrent_locks, bool print_extended_info);
2237   static void print(bool print_stacks, bool internal_format) {
2238     // this function is only used by debug.cpp
2239     print_on(tty, print_stacks, internal_format, false /* no concurrent lock printed */, false /* simple format */);
2240   }
2241   static void print_on_error(outputStream* st, Thread* current, char* buf, int buflen);
2242   static void print_on_error(Thread* this_thread, outputStream* st, Thread* current, char* buf,
2243                              int buflen, bool* found_current);
2244   static void print_threads_compiling(outputStream* st, char* buf, int buflen);
2245 
2246   // Get Java threads that are waiting to enter a monitor.
2247   static GrowableArray&lt;JavaThread*&gt;* get_pending_threads(ThreadsList * t_list,
2248                                                          int count, address monitor);
2249 
2250   // Get owning Java thread from the monitor's owner field.
2251   static JavaThread *owning_thread_from_monitor_owner(ThreadsList * t_list,
2252                                                       address owner);
2253 
2254   // Number of threads on the active threads list
2255   static int number_of_threads()                 { return _number_of_threads; }
2256   // Number of non-daemon threads on the active threads list
2257   static int number_of_non_daemon_threads()      { return _number_of_non_daemon_threads; }
2258 
2259   // Deoptimizes all frames tied to marked nmethods
2260   static void deoptimized_wrt_marked_nmethods();
2261 };
2262 
2263 
2264 // Thread iterator
2265 class ThreadClosure: public StackObj {
2266  public:
2267   virtual void do_thread(Thread* thread) = 0;
2268 };
2269 
2270 class SignalHandlerMark: public StackObj {
2271  private:
2272   Thread* _thread;
2273  public:
2274   SignalHandlerMark(Thread* t) {
2275     _thread = t;
2276     if (_thread) _thread-&gt;enter_signal_handler();
2277   }
2278   ~SignalHandlerMark() {
2279     if (_thread) _thread-&gt;leave_signal_handler();
2280     _thread = NULL;
2281   }
2282 };
2283 
2284 
2285 #endif // SHARE_VM_RUNTIME_THREAD_HPP
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="4" type="hidden" /></form></body></html>
