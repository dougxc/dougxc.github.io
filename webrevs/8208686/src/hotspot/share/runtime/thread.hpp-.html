<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/hotspot/share/runtime/thread.hpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_RUNTIME_THREAD_HPP
  26 #define SHARE_VM_RUNTIME_THREAD_HPP
  27 
  28 #include "jni.h"
  29 #include "gc/shared/gcThreadLocalData.hpp"
  30 #include "gc/shared/threadLocalAllocBuffer.hpp"
  31 #include "memory/allocation.hpp"
  32 #include "oops/oop.hpp"
  33 #include "prims/jvmtiExport.hpp"
  34 #include "runtime/frame.hpp"
  35 #include "runtime/globals.hpp"
  36 #include "runtime/handshake.hpp"
  37 #include "runtime/javaFrameAnchor.hpp"
  38 #include "runtime/jniHandles.hpp"
  39 #include "runtime/mutexLocker.hpp"
  40 #include "runtime/os.hpp"
  41 #include "runtime/osThread.hpp"
  42 #include "runtime/park.hpp"
  43 #include "runtime/safepoint.hpp"
  44 #include "runtime/stubRoutines.hpp"
  45 #include "runtime/threadHeapSampler.hpp"
  46 #include "runtime/threadLocalStorage.hpp"
  47 #include "runtime/threadStatisticalInfo.hpp"
  48 #include "runtime/unhandledOops.hpp"
  49 #include "utilities/align.hpp"
  50 #include "utilities/exceptions.hpp"
  51 #include "utilities/macros.hpp"
  52 #ifdef ZERO
  53 # include "stack_zero.hpp"
  54 #endif
  55 #if INCLUDE_JFR
  56 #include "jfr/support/jfrThreadExtension.hpp"
  57 #endif
  58 
  59 
  60 class SafeThreadsListPtr;
  61 class ThreadSafepointState;
  62 class ThreadsList;
  63 class ThreadsSMRSupport;
  64 
  65 class JvmtiThreadState;
  66 class ThreadStatistics;
  67 class ConcurrentLocksDump;
  68 class ParkEvent;
  69 class Parker;
  70 
  71 class ciEnv;
  72 class CompileThread;
  73 class CompileLog;
  74 class CompileTask;
  75 class CompileQueue;
  76 class CompilerCounters;
  77 class vframeArray;
  78 
  79 class DeoptResourceMark;
  80 class jvmtiDeferredLocalVariableSet;
  81 
  82 class GCTaskQueue;
  83 class ThreadClosure;
  84 class IdealGraphPrinter;
  85 
  86 class Metadata;
  87 template &lt;class T, MEMFLAGS F&gt; class ChunkedList;
  88 typedef ChunkedList&lt;Metadata*, mtInternal&gt; MetadataOnStackBuffer;
  89 
  90 DEBUG_ONLY(class ResourceMark;)
  91 
  92 class WorkerThread;
  93 
  94 // Class hierarchy
  95 // - Thread
  96 //   - JavaThread
  97 //     - various subclasses eg CompilerThread, ServiceThread
  98 //   - NonJavaThread
  99 //     - NamedThread
 100 //       - VMThread
 101 //       - ConcurrentGCThread
 102 //       - WorkerThread
 103 //         - GangWorker
 104 //         - GCTaskThread
 105 //     - WatcherThread
 106 //     - JfrThreadSampler
 107 //
 108 // All Thread subclasses must be either JavaThread or NonJavaThread.
 109 // This means !t-&gt;is_Java_thread() iff t is a NonJavaThread, or t is
 110 // a partially constructed/destroyed Thread.
 111 
 112 class Thread: public ThreadShadow {
 113   friend class VMStructs;
 114   friend class JVMCIVMStructs;
 115  private:
 116 
 117 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 118   // Current thread is maintained as a thread-local variable
 119   static THREAD_LOCAL_DECL Thread* _thr_current;
 120 #endif
 121 
 122  private:
 123   // Thread local data area available to the GC. The internal
 124   // structure and contents of this data area is GC-specific.
 125   // Only GC and GC barrier code should access this data area.
 126   GCThreadLocalData _gc_data;
 127 
 128  public:
 129   static ByteSize gc_data_offset() {
 130     return byte_offset_of(Thread, _gc_data);
 131   }
 132 
 133   template &lt;typename T&gt; T* gc_data() {
 134     STATIC_ASSERT(sizeof(T) &lt;= sizeof(_gc_data));
 135     return reinterpret_cast&lt;T*&gt;(&amp;_gc_data);
 136   }
 137 
 138   // Exception handling
 139   // (Note: _pending_exception and friends are in ThreadShadow)
 140   //oop       _pending_exception;                // pending exception for current thread
 141   // const char* _exception_file;                   // file information for exception (debugging only)
 142   // int         _exception_line;                   // line information for exception (debugging only)
 143  protected:
 144   // Support for forcing alignment of thread objects for biased locking
 145   void*       _real_malloc_address;
 146 
 147   // JavaThread lifecycle support:
 148   friend class SafeThreadsListPtr;  // for _threads_list_ptr, cmpxchg_threads_hazard_ptr(), {dec_,inc_,}nested_threads_hazard_ptr_cnt(), {g,s}et_threads_hazard_ptr(), inc_nested_handle_cnt(), tag_hazard_ptr() access
 149   friend class ScanHazardPtrGatherProtectedThreadsClosure;  // for cmpxchg_threads_hazard_ptr(), get_threads_hazard_ptr(), is_hazard_ptr_tagged() access
 150   friend class ScanHazardPtrGatherThreadsListClosure;  // for get_threads_hazard_ptr(), untag_hazard_ptr() access
 151   friend class ScanHazardPtrPrintMatchingThreadsClosure;  // for get_threads_hazard_ptr(), is_hazard_ptr_tagged() access
 152   friend class ThreadsSMRSupport;  // for _nested_threads_hazard_ptr_cnt, _threads_hazard_ptr, _threads_list_ptr access
 153 
 154   ThreadsList* volatile _threads_hazard_ptr;
 155   SafeThreadsListPtr*   _threads_list_ptr;
 156   ThreadsList*          cmpxchg_threads_hazard_ptr(ThreadsList* exchange_value, ThreadsList* compare_value);
 157   ThreadsList*          get_threads_hazard_ptr();
 158   void                  set_threads_hazard_ptr(ThreadsList* new_list);
 159   static bool           is_hazard_ptr_tagged(ThreadsList* list) {
 160     return (intptr_t(list) &amp; intptr_t(1)) == intptr_t(1);
 161   }
 162   static ThreadsList*   tag_hazard_ptr(ThreadsList* list) {
 163     return (ThreadsList*)(intptr_t(list) | intptr_t(1));
 164   }
 165   static ThreadsList*   untag_hazard_ptr(ThreadsList* list) {
 166     return (ThreadsList*)(intptr_t(list) &amp; ~intptr_t(1));
 167   }
 168   // This field is enabled via -XX:+EnableThreadSMRStatistics:
 169   uint _nested_threads_hazard_ptr_cnt;
 170   void dec_nested_threads_hazard_ptr_cnt() {
 171     assert(_nested_threads_hazard_ptr_cnt != 0, "mismatched {dec,inc}_nested_threads_hazard_ptr_cnt()");
 172     _nested_threads_hazard_ptr_cnt--;
 173   }
 174   void inc_nested_threads_hazard_ptr_cnt() {
 175     _nested_threads_hazard_ptr_cnt++;
 176   }
 177   uint nested_threads_hazard_ptr_cnt() {
 178     return _nested_threads_hazard_ptr_cnt;
 179   }
 180 
 181  public:
 182   void* operator new(size_t size) throw() { return allocate(size, true); }
 183   void* operator new(size_t size, const std::nothrow_t&amp; nothrow_constant) throw() {
 184     return allocate(size, false); }
 185   void  operator delete(void* p);
 186 
 187  protected:
 188   static void* allocate(size_t size, bool throw_excpt, MEMFLAGS flags = mtThread);
 189  private:
 190 
 191   // ***************************************************************
 192   // Suspend and resume support
 193   // ***************************************************************
 194   //
 195   // VM suspend/resume no longer exists - it was once used for various
 196   // things including safepoints but was deprecated and finally removed
 197   // in Java 7. Because VM suspension was considered "internal" Java-level
 198   // suspension was considered "external", and this legacy naming scheme
 199   // remains.
 200   //
 201   // External suspend/resume requests come from JVM_SuspendThread,
 202   // JVM_ResumeThread, JVMTI SuspendThread, and finally JVMTI
 203   // ResumeThread. External
 204   // suspend requests cause _external_suspend to be set and external
 205   // resume requests cause _external_suspend to be cleared.
 206   // External suspend requests do not nest on top of other external
 207   // suspend requests. The higher level APIs reject suspend requests
 208   // for already suspended threads.
 209   //
 210   // The external_suspend
 211   // flag is checked by has_special_runtime_exit_condition() and java thread
 212   // will self-suspend when handle_special_runtime_exit_condition() is
 213   // called. Most uses of the _thread_blocked state in JavaThreads are
 214   // considered the same as being externally suspended; if the blocking
 215   // condition lifts, the JavaThread will self-suspend. Other places
 216   // where VM checks for external_suspend include:
 217   //   + mutex granting (do not enter monitors when thread is suspended)
 218   //   + state transitions from _thread_in_native
 219   //
 220   // In general, java_suspend() does not wait for an external suspend
 221   // request to complete. When it returns, the only guarantee is that
 222   // the _external_suspend field is true.
 223   //
 224   // wait_for_ext_suspend_completion() is used to wait for an external
 225   // suspend request to complete. External suspend requests are usually
 226   // followed by some other interface call that requires the thread to
 227   // be quiescent, e.g., GetCallTrace(). By moving the "wait time" into
 228   // the interface that requires quiescence, we give the JavaThread a
 229   // chance to self-suspend before we need it to be quiescent. This
 230   // improves overall suspend/query performance.
 231   //
 232   // _suspend_flags controls the behavior of java_ suspend/resume.
 233   // It must be set under the protection of SR_lock. Read from the flag is
 234   // OK without SR_lock as long as the value is only used as a hint.
 235   // (e.g., check _external_suspend first without lock and then recheck
 236   // inside SR_lock and finish the suspension)
 237   //
 238   // _suspend_flags is also overloaded for other "special conditions" so
 239   // that a single check indicates whether any special action is needed
 240   // eg. for async exceptions.
 241   // -------------------------------------------------------------------
 242   // Notes:
 243   // 1. The suspend/resume logic no longer uses ThreadState in OSThread
 244   // but we still update its value to keep other part of the system (mainly
 245   // JVMTI) happy. ThreadState is legacy code (see notes in
 246   // osThread.hpp).
 247   //
 248   // 2. It would be more natural if set_external_suspend() is private and
 249   // part of java_suspend(), but that probably would affect the suspend/query
 250   // performance. Need more investigation on this.
 251 
 252   // suspend/resume lock: used for self-suspend
 253   Monitor* _SR_lock;
 254 
 255  protected:
 256   enum SuspendFlags {
 257     // NOTE: avoid using the sign-bit as cc generates different test code
 258     //       when the sign-bit is used, and sometimes incorrectly - see CR 6398077
 259 
 260     _external_suspend       = 0x20000000U, // thread is asked to self suspend
 261     _ext_suspended          = 0x40000000U, // thread has self-suspended
 262     _deopt_suspend          = 0x10000000U, // thread needs to self suspend for deopt
 263 
 264     _has_async_exception    = 0x00000001U, // there is a pending async exception
 265     _critical_native_unlock = 0x00000002U, // Must call back to unlock JNI critical lock
 266 
 267     _trace_flag             = 0x00000004U  // call tracing backend
 268   };
 269 
 270   // various suspension related flags - atomically updated
 271   // overloaded for async exception checking in check_special_condition_for_native_trans.
 272   volatile uint32_t _suspend_flags;
 273 
 274  private:
 275   int _num_nested_signal;
 276 
 277   DEBUG_ONLY(bool _suspendible_thread;)
 278 
 279  public:
 280   void enter_signal_handler() { _num_nested_signal++; }
 281   void leave_signal_handler() { _num_nested_signal--; }
 282   bool is_inside_signal_handler() const { return _num_nested_signal &gt; 0; }
 283 
 284 #ifdef ASSERT
 285   void set_suspendible_thread() {
 286     _suspendible_thread = true;
 287   }
 288 
 289   void clear_suspendible_thread() {
 290     _suspendible_thread = false;
 291   }
 292 
 293   bool is_suspendible_thread() { return _suspendible_thread; }
 294 #endif
 295 
 296  private:
 297   // Active_handles points to a block of handles
 298   JNIHandleBlock* _active_handles;
 299 
 300   // One-element thread local free list
 301   JNIHandleBlock* _free_handle_block;
 302 
 303   // Point to the last handle mark
 304   HandleMark* _last_handle_mark;
 305 
 306   // The parity of the last strong_roots iteration in which this thread was
 307   // claimed as a task.
 308   int _oops_do_parity;
 309 
 310   // Support for GlobalCounter
 311  private:
 312   volatile uintx _rcu_counter;
 313  public:
 314   volatile uintx* get_rcu_counter() {
 315     return &amp;_rcu_counter;
 316   }
 317 
 318  public:
 319   void set_last_handle_mark(HandleMark* mark)   { _last_handle_mark = mark; }
 320   HandleMark* last_handle_mark() const          { return _last_handle_mark; }
 321  private:
 322 
 323   // debug support for checking if code does allow safepoints or not
 324   // GC points in the VM can happen because of allocation, invoking a VM operation, or blocking on
 325   // mutex, or blocking on an object synchronizer (Java locking).
 326   // If !allow_safepoint(), then an assertion failure will happen in any of the above cases
 327   // If !allow_allocation(), then an assertion failure will happen during allocation
 328   // (Hence, !allow_safepoint() =&gt; !allow_allocation()).
 329   //
 330   // The two classes NoSafepointVerifier and No_Allocation_Verifier are used to set these counters.
 331   //
 332   NOT_PRODUCT(int _allow_safepoint_count;)      // If 0, thread allow a safepoint to happen
 333   debug_only(int _allow_allocation_count;)     // If 0, the thread is allowed to allocate oops.
 334 
 335   // Used by SkipGCALot class.
 336   NOT_PRODUCT(bool _skip_gcalot;)               // Should we elide gc-a-lot?
 337 
 338   friend class NoAllocVerifier;
 339   friend class NoSafepointVerifier;
 340   friend class PauseNoSafepointVerifier;
 341   friend class GCLocker;
 342 
 343   volatile void* _polling_page;                 // Thread local polling page
 344 
 345   ThreadLocalAllocBuffer _tlab;                 // Thread-local eden
 346   jlong _allocated_bytes;                       // Cumulative number of bytes allocated on
 347                                                 // the Java heap
 348   ThreadHeapSampler _heap_sampler;              // For use when sampling the memory.
 349 
 350   ThreadStatisticalInfo _statistical_info;      // Statistics about the thread
 351 
 352   JFR_ONLY(DEFINE_THREAD_LOCAL_FIELD_JFR;)      // Thread-local data for jfr
 353 
 354   int   _vm_operation_started_count;            // VM_Operation support
 355   int   _vm_operation_completed_count;          // VM_Operation support
 356 
 357   ObjectMonitor* _current_pending_monitor;      // ObjectMonitor this thread
 358                                                 // is waiting to lock
 359   bool _current_pending_monitor_is_from_java;   // locking is from Java code
 360 
 361   // ObjectMonitor on which this thread called Object.wait()
 362   ObjectMonitor* _current_waiting_monitor;
 363 
 364   // Private thread-local objectmonitor list - a simple cache organized as a SLL.
 365  public:
 366   ObjectMonitor* omFreeList;
 367   int omFreeCount;                              // length of omFreeList
 368   int omFreeProvision;                          // reload chunk size
 369   ObjectMonitor* omInUseList;                   // SLL to track monitors in circulation
 370   int omInUseCount;                             // length of omInUseList
 371 
 372 #ifdef ASSERT
 373  private:
 374   bool _visited_for_critical_count;
 375 
 376  public:
 377   void set_visited_for_critical_count(bool z) { _visited_for_critical_count = z; }
 378   bool was_visited_for_critical_count() const   { return _visited_for_critical_count; }
 379 #endif
 380 
 381  public:
 382   enum {
 383     is_definitely_current_thread = true
 384   };
 385 
 386   // Constructor
 387   Thread();
 388   virtual ~Thread() = 0;        // Thread is abstract.
 389 
 390   // Manage Thread::current()
 391   void initialize_thread_current();
 392   void clear_thread_current(); // TLS cleanup needed before threads terminate
 393 
 394  public:
 395   // thread entry point
 396   virtual void run();
 397 
 398   // Testers
 399   virtual bool is_VM_thread()       const            { return false; }
 400   virtual bool is_Java_thread()     const            { return false; }
 401   virtual bool is_Compiler_thread() const            { return false; }
 402   virtual bool is_Code_cache_sweeper_thread() const  { return false; }
 403   virtual bool is_hidden_from_external_view() const  { return false; }
 404   virtual bool is_jvmti_agent_thread() const         { return false; }
 405   // True iff the thread can perform GC operations at a safepoint.
 406   // Generally will be true only of VM thread and parallel GC WorkGang
 407   // threads.
 408   virtual bool is_GC_task_thread() const             { return false; }
 409   virtual bool is_Watcher_thread() const             { return false; }
 410   virtual bool is_ConcurrentGC_thread() const        { return false; }
 411   virtual bool is_Named_thread() const               { return false; }
 412   virtual bool is_Worker_thread() const              { return false; }
 413 
 414   // Can this thread make Java upcalls
 415   virtual bool can_call_java() const                 { return false; }
 416 
 417   // Casts
 418   virtual WorkerThread* as_Worker_thread() const     { return NULL; }
 419 
 420   virtual char* name() const { return (char*)"Unknown thread"; }
 421 
 422   // Returns the current thread (ASSERTS if NULL)
 423   static inline Thread* current();
 424   // Returns the current thread, or NULL if not attached
 425   static inline Thread* current_or_null();
 426   // Returns the current thread, or NULL if not attached, and is
 427   // safe for use from signal-handlers
 428   static inline Thread* current_or_null_safe();
 429 
 430   // Common thread operations
 431 #ifdef ASSERT
 432   static void check_for_dangling_thread_pointer(Thread *thread);
 433 #endif
 434   static void set_priority(Thread* thread, ThreadPriority priority);
 435   static ThreadPriority get_priority(const Thread* const thread);
 436   static void start(Thread* thread);
 437   static void interrupt(Thread* thr);
 438   static bool is_interrupted(Thread* thr, bool clear_interrupted);
 439 
 440   void set_native_thread_name(const char *name) {
 441     assert(Thread::current() == this, "set_native_thread_name can only be called on the current thread");
 442     os::set_native_thread_name(name);
 443   }
 444 
 445   ObjectMonitor** omInUseList_addr()             { return (ObjectMonitor **)&amp;omInUseList; }
 446   Monitor* SR_lock() const                       { return _SR_lock; }
 447 
 448   bool has_async_exception() const { return (_suspend_flags &amp; _has_async_exception) != 0; }
 449 
 450   inline void set_suspend_flag(SuspendFlags f);
 451   inline void clear_suspend_flag(SuspendFlags f);
 452 
 453   inline void set_has_async_exception();
 454   inline void clear_has_async_exception();
 455 
 456   bool do_critical_native_unlock() const { return (_suspend_flags &amp; _critical_native_unlock) != 0; }
 457 
 458   inline void set_critical_native_unlock();
 459   inline void clear_critical_native_unlock();
 460 
 461   inline void set_trace_flag();
 462   inline void clear_trace_flag();
 463 
 464   // Support for Unhandled Oop detection
 465   // Add the field for both, fastdebug and debug, builds to keep
 466   // Thread's fields layout the same.
 467   // Note: CHECK_UNHANDLED_OOPS is defined only for fastdebug build.
 468 #ifdef CHECK_UNHANDLED_OOPS
 469  private:
 470   UnhandledOops* _unhandled_oops;
 471 #elif defined(ASSERT)
 472  private:
 473   void* _unhandled_oops;
 474 #endif
 475 #ifdef CHECK_UNHANDLED_OOPS
 476  public:
 477   UnhandledOops* unhandled_oops() { return _unhandled_oops; }
 478   // Mark oop safe for gc.  It may be stack allocated but won't move.
 479   void allow_unhandled_oop(oop *op) {
 480     if (CheckUnhandledOops) unhandled_oops()-&gt;allow_unhandled_oop(op);
 481   }
 482   // Clear oops at safepoint so crashes point to unhandled oop violator
 483   void clear_unhandled_oops() {
 484     if (CheckUnhandledOops) unhandled_oops()-&gt;clear_unhandled_oops();
 485   }
 486 #endif // CHECK_UNHANDLED_OOPS
 487 
 488  public:
 489 #ifndef PRODUCT
 490   bool skip_gcalot()           { return _skip_gcalot; }
 491   void set_skip_gcalot(bool v) { _skip_gcalot = v;    }
 492 #endif
 493 
 494   // Installs a pending exception to be inserted later
 495   static void send_async_exception(oop thread_oop, oop java_throwable);
 496 
 497   // Resource area
 498   ResourceArea* resource_area() const            { return _resource_area; }
 499   void set_resource_area(ResourceArea* area)     { _resource_area = area; }
 500 
 501   OSThread* osthread() const                     { return _osthread;   }
 502   void set_osthread(OSThread* thread)            { _osthread = thread; }
 503 
 504   // JNI handle support
 505   JNIHandleBlock* active_handles() const         { return _active_handles; }
 506   void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }
 507   JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }
 508   void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }
 509 
 510   // Internal handle support
 511   HandleArea* handle_area() const                { return _handle_area; }
 512   void set_handle_area(HandleArea* area)         { _handle_area = area; }
 513 
 514   GrowableArray&lt;Metadata*&gt;* metadata_handles() const          { return _metadata_handles; }
 515   void set_metadata_handles(GrowableArray&lt;Metadata*&gt;* handles){ _metadata_handles = handles; }
 516 
 517   // Thread-Local Allocation Buffer (TLAB) support
 518   ThreadLocalAllocBuffer&amp; tlab()                 { return _tlab; }
 519   void initialize_tlab() {
 520     if (UseTLAB) {
 521       tlab().initialize();
 522     }
 523   }
 524 
 525   jlong allocated_bytes()               { return _allocated_bytes; }
 526   void set_allocated_bytes(jlong value) { _allocated_bytes = value; }
 527   void incr_allocated_bytes(jlong size) { _allocated_bytes += size; }
 528   inline jlong cooked_allocated_bytes();
 529 
 530   ThreadHeapSampler&amp; heap_sampler()     { return _heap_sampler; }
 531 
 532   ThreadStatisticalInfo&amp; statistical_info() { return _statistical_info; }
 533 
 534   JFR_ONLY(DEFINE_THREAD_LOCAL_ACCESSOR_JFR;)
 535 
 536   bool is_trace_suspend()               { return (_suspend_flags &amp; _trace_flag) != 0; }
 537 
 538   // VM operation support
 539   int vm_operation_ticket()                      { return ++_vm_operation_started_count; }
 540   int vm_operation_completed_count()             { return _vm_operation_completed_count; }
 541   void increment_vm_operation_completed_count()  { _vm_operation_completed_count++; }
 542 
 543   // For tracking the heavyweight monitor the thread is pending on.
 544   ObjectMonitor* current_pending_monitor() {
 545     return _current_pending_monitor;
 546   }
 547   void set_current_pending_monitor(ObjectMonitor* monitor) {
 548     _current_pending_monitor = monitor;
 549   }
 550   void set_current_pending_monitor_is_from_java(bool from_java) {
 551     _current_pending_monitor_is_from_java = from_java;
 552   }
 553   bool current_pending_monitor_is_from_java() {
 554     return _current_pending_monitor_is_from_java;
 555   }
 556 
 557   // For tracking the ObjectMonitor on which this thread called Object.wait()
 558   ObjectMonitor* current_waiting_monitor() {
 559     return _current_waiting_monitor;
 560   }
 561   void set_current_waiting_monitor(ObjectMonitor* monitor) {
 562     _current_waiting_monitor = monitor;
 563   }
 564 
 565   // GC support
 566   // Apply "f-&gt;do_oop" to all root oops in "this".
 567   //   Used by JavaThread::oops_do.
 568   // Apply "cf-&gt;do_code_blob" (if !NULL) to all code blobs active in frames
 569   virtual void oops_do(OopClosure* f, CodeBlobClosure* cf);
 570 
 571   // Handles the parallel case for the method below.
 572  private:
 573   bool claim_oops_do_par_case(int collection_parity);
 574  public:
 575   // Requires that "collection_parity" is that of the current roots
 576   // iteration.  If "is_par" is false, sets the parity of "this" to
 577   // "collection_parity", and returns "true".  If "is_par" is true,
 578   // uses an atomic instruction to set the current threads parity to
 579   // "collection_parity", if it is not already.  Returns "true" iff the
 580   // calling thread does the update, this indicates that the calling thread
 581   // has claimed the thread's stack as a root groop in the current
 582   // collection.
 583   bool claim_oops_do(bool is_par, int collection_parity) {
 584     if (!is_par) {
 585       _oops_do_parity = collection_parity;
 586       return true;
 587     } else {
 588       return claim_oops_do_par_case(collection_parity);
 589     }
 590   }
 591 
 592   // jvmtiRedefineClasses support
 593   void metadata_handles_do(void f(Metadata*));
 594 
 595   // Used by fast lock support
 596   virtual bool is_lock_owned(address adr) const;
 597 
 598   // Check if address is in the stack of the thread (not just for locks).
 599   // Warning: the method can only be used on the running thread
 600   bool is_in_stack(address adr) const;
 601   // Check if address is in the usable part of the stack (excludes protected
 602   // guard pages)
 603   bool is_in_usable_stack(address adr) const;
 604 
 605   // Sets this thread as starting thread. Returns failure if thread
 606   // creation fails due to lack of memory, too many threads etc.
 607   bool set_as_starting_thread();
 608 
 609 protected:
 610   // OS data associated with the thread
 611   OSThread* _osthread;  // Platform-specific thread information
 612 
 613   // Thread local resource area for temporary allocation within the VM
 614   ResourceArea* _resource_area;
 615 
 616   DEBUG_ONLY(ResourceMark* _current_resource_mark;)
 617 
 618   // Thread local handle area for allocation of handles within the VM
 619   HandleArea* _handle_area;
 620   GrowableArray&lt;Metadata*&gt;* _metadata_handles;
 621 
 622   // Support for stack overflow handling, get_thread, etc.
 623   address          _stack_base;
 624   size_t           _stack_size;
 625   uintptr_t        _self_raw_id;      // used by get_thread (mutable)
 626   int              _lgrp_id;
 627 
 628   volatile void** polling_page_addr() { return &amp;_polling_page; }
 629 
 630  public:
 631   // Stack overflow support
 632   address stack_base() const           { assert(_stack_base != NULL,"Sanity check"); return _stack_base; }
 633   void    set_stack_base(address base) { _stack_base = base; }
 634   size_t  stack_size() const           { return _stack_size; }
 635   void    set_stack_size(size_t size)  { _stack_size = size; }
 636   address stack_end()  const           { return stack_base() - stack_size(); }
 637   void    record_stack_base_and_size();
 638 
 639   bool    on_local_stack(address adr) const {
 640     // QQQ this has knowledge of direction, ought to be a stack method
 641     return (_stack_base &gt;= adr &amp;&amp; adr &gt;= stack_end());
 642   }
 643 
 644   uintptr_t self_raw_id()                    { return _self_raw_id; }
 645   void      set_self_raw_id(uintptr_t value) { _self_raw_id = value; }
 646 
 647   int     lgrp_id() const        { return _lgrp_id; }
 648   void    set_lgrp_id(int value) { _lgrp_id = value; }
 649 
 650   // Printing
 651   void print_on(outputStream* st, bool print_extended_info) const;
 652   virtual void print_on(outputStream* st) const { print_on(st, false); }
 653   void print() const { print_on(tty); }
 654   virtual void print_on_error(outputStream* st, char* buf, int buflen) const;
 655   void print_value_on(outputStream* st) const;
 656 
 657   // Debug-only code
 658 #ifdef ASSERT
 659  private:
 660   // Deadlock detection support for Mutex locks. List of locks own by thread.
 661   Monitor* _owned_locks;
 662   // Mutex::set_owner_implementation is the only place where _owned_locks is modified,
 663   // thus the friendship
 664   friend class Mutex;
 665   friend class Monitor;
 666 
 667  public:
 668   void print_owned_locks_on(outputStream* st) const;
 669   void print_owned_locks() const                 { print_owned_locks_on(tty);    }
 670   Monitor* owned_locks() const                   { return _owned_locks;          }
 671   bool owns_locks() const                        { return owned_locks() != NULL; }
 672   bool owns_locks_but_compiled_lock() const;
 673   int oops_do_parity() const                     { return _oops_do_parity; }
 674 
 675   // Deadlock detection
 676   bool allow_allocation()                        { return _allow_allocation_count == 0; }
 677   ResourceMark* current_resource_mark()          { return _current_resource_mark; }
 678   void set_current_resource_mark(ResourceMark* rm) { _current_resource_mark = rm; }
 679 #endif
 680 
 681   void check_for_valid_safepoint_state(bool potential_vm_operation) PRODUCT_RETURN;
 682 
 683  private:
 684   volatile int _jvmti_env_iteration_count;
 685 
 686  public:
 687   void entering_jvmti_env_iteration()            { ++_jvmti_env_iteration_count; }
 688   void leaving_jvmti_env_iteration()             { --_jvmti_env_iteration_count; }
 689   bool is_inside_jvmti_env_iteration()           { return _jvmti_env_iteration_count &gt; 0; }
 690 
 691   // Code generation
 692   static ByteSize exception_file_offset()        { return byte_offset_of(Thread, _exception_file); }
 693   static ByteSize exception_line_offset()        { return byte_offset_of(Thread, _exception_line); }
 694   static ByteSize active_handles_offset()        { return byte_offset_of(Thread, _active_handles); }
 695 
 696   static ByteSize stack_base_offset()            { return byte_offset_of(Thread, _stack_base); }
 697   static ByteSize stack_size_offset()            { return byte_offset_of(Thread, _stack_size); }
 698 
 699   static ByteSize polling_page_offset()          { return byte_offset_of(Thread, _polling_page); }
 700 
 701   static ByteSize tlab_start_offset()            { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::start_offset(); }
 702   static ByteSize tlab_end_offset()              { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::end_offset(); }
 703   static ByteSize tlab_top_offset()              { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::top_offset(); }
 704   static ByteSize tlab_pf_top_offset()           { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::pf_top_offset(); }
 705 
 706   static ByteSize allocated_bytes_offset()       { return byte_offset_of(Thread, _allocated_bytes); }
 707 
 708   JFR_ONLY(DEFINE_THREAD_LOCAL_OFFSET_JFR;)
 709 
 710  public:
 711   volatile intptr_t _Stalled;
 712   volatile int _TypeTag;
 713   ParkEvent * _ParkEvent;                     // for synchronized()
 714   ParkEvent * _SleepEvent;                    // for Thread.sleep
 715   ParkEvent * _MutexEvent;                    // for native internal Mutex/Monitor
 716   ParkEvent * _MuxEvent;                      // for low-level muxAcquire-muxRelease
 717   int NativeSyncRecursion;                    // diagnostic
 718 
 719   volatile int _OnTrap;                       // Resume-at IP delta
 720   jint _hashStateW;                           // Marsaglia Shift-XOR thread-local RNG
 721   jint _hashStateX;                           // thread-specific hashCode generator state
 722   jint _hashStateY;
 723   jint _hashStateZ;
 724 
 725   volatile jint rng[4];                      // RNG for spin loop
 726 
 727   // Low-level leaf-lock primitives used to implement synchronization
 728   // and native monitor-mutex infrastructure.
 729   // Not for general synchronization use.
 730   static void SpinAcquire(volatile int * Lock, const char * Name);
 731   static void SpinRelease(volatile int * Lock);
 732   static void muxAcquire(volatile intptr_t * Lock, const char * Name);
 733   static void muxAcquireW(volatile intptr_t * Lock, ParkEvent * ev);
 734   static void muxRelease(volatile intptr_t * Lock);
 735 };
 736 
 737 // Inline implementation of Thread::current()
 738 inline Thread* Thread::current() {
 739   Thread* current = current_or_null();
 740   assert(current != NULL, "Thread::current() called on detached thread");
 741   return current;
 742 }
 743 
 744 inline Thread* Thread::current_or_null() {
 745 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 746   return _thr_current;
 747 #else
 748   if (ThreadLocalStorage::is_initialized()) {
 749     return ThreadLocalStorage::thread();
 750   }
 751   return NULL;
 752 #endif
 753 }
 754 
 755 inline Thread* Thread::current_or_null_safe() {
 756   if (ThreadLocalStorage::is_initialized()) {
 757     return ThreadLocalStorage::thread();
 758   }
 759   return NULL;
 760 }
 761 
 762 class NonJavaThread: public Thread {
 763   friend class VMStructs;
 764 
 765   NonJavaThread* volatile _next;
 766 
 767   class List;
 768   static List _the_list;
 769 
 770  public:
 771   NonJavaThread();
 772   ~NonJavaThread();
 773 
 774   class Iterator;
 775 };
 776 
 777 // Provides iteration over the list of NonJavaThreads.  Because list
 778 // management occurs in the NonJavaThread constructor and destructor,
 779 // entries in the list may not be fully constructed instances of a
 780 // derived class.  Threads created after an iterator is constructed
 781 // will not be visited by the iterator.  The scope of an iterator is a
 782 // critical section; there must be no safepoint checks in that scope.
 783 class NonJavaThread::Iterator : public StackObj {
 784   uint _protect_enter;
 785   NonJavaThread* _current;
 786 
 787   // Noncopyable.
 788   Iterator(const Iterator&amp;);
 789   Iterator&amp; operator=(const Iterator&amp;);
 790 
 791 public:
 792   Iterator();
 793   ~Iterator();
 794 
 795   bool end() const { return _current == NULL; }
 796   NonJavaThread* current() const { return _current; }
 797   void step();
 798 };
 799 
 800 // Name support for threads.  non-JavaThread subclasses with multiple
 801 // uniquely named instances should derive from this.
 802 class NamedThread: public NonJavaThread {
 803   friend class VMStructs;
 804   enum {
 805     max_name_len = 64
 806   };
 807  private:
 808   char* _name;
 809   // log JavaThread being processed by oops_do
 810   JavaThread* _processed_thread;
 811   uint _gc_id; // The current GC id when a thread takes part in GC
 812 
 813  public:
 814   NamedThread();
 815   ~NamedThread();
 816   // May only be called once per thread.
 817   void set_name(const char* format, ...)  ATTRIBUTE_PRINTF(2, 3);
 818   void initialize_named_thread();
 819   virtual bool is_Named_thread() const { return true; }
 820   virtual char* name() const { return _name == NULL ? (char*)"Unknown Thread" : _name; }
 821   JavaThread *processed_thread() { return _processed_thread; }
 822   void set_processed_thread(JavaThread *thread) { _processed_thread = thread; }
 823   virtual void print_on(outputStream* st) const;
 824 
 825   void set_gc_id(uint gc_id) { _gc_id = gc_id; }
 826   uint gc_id() { return _gc_id; }
 827 };
 828 
 829 // Worker threads are named and have an id of an assigned work.
 830 class WorkerThread: public NamedThread {
 831  private:
 832   uint _id;
 833  public:
 834   WorkerThread() : _id(0)               { }
 835   virtual bool is_Worker_thread() const { return true; }
 836 
 837   virtual WorkerThread* as_Worker_thread() const {
 838     assert(is_Worker_thread(), "Dubious cast to WorkerThread*?");
 839     return (WorkerThread*) this;
 840   }
 841 
 842   void set_id(uint work_id)             { _id = work_id; }
 843   uint id() const                       { return _id; }
 844 };
 845 
 846 // A single WatcherThread is used for simulating timer interrupts.
 847 class WatcherThread: public NonJavaThread {
 848   friend class VMStructs;
 849  public:
 850   virtual void run();
 851 
 852  private:
 853   static WatcherThread* _watcher_thread;
 854 
 855   static bool _startable;
 856   // volatile due to at least one lock-free read
 857   volatile static bool _should_terminate;
 858  public:
 859   enum SomeConstants {
 860     delay_interval = 10                          // interrupt delay in milliseconds
 861   };
 862 
 863   // Constructor
 864   WatcherThread();
 865 
 866   // No destruction allowed
 867   ~WatcherThread() {
 868     guarantee(false, "WatcherThread deletion must fix the race with VM termination");
 869   }
 870 
 871   // Tester
 872   bool is_Watcher_thread() const                 { return true; }
 873 
 874   // Printing
 875   char* name() const { return (char*)"VM Periodic Task Thread"; }
 876   void print_on(outputStream* st) const;
 877   void unpark();
 878 
 879   // Returns the single instance of WatcherThread
 880   static WatcherThread* watcher_thread()         { return _watcher_thread; }
 881 
 882   // Create and start the single instance of WatcherThread, or stop it on shutdown
 883   static void start();
 884   static void stop();
 885   // Only allow start once the VM is sufficiently initialized
 886   // Otherwise the first task to enroll will trigger the start
 887   static void make_startable();
 888  private:
 889   int sleep() const;
 890 };
 891 
 892 
 893 class CompilerThread;
 894 
 895 typedef void (*ThreadFunction)(JavaThread*, TRAPS);
 896 
 897 class JavaThread: public Thread {
 898   friend class VMStructs;
 899   friend class JVMCIVMStructs;
 900   friend class WhiteBox;
 901  private:
 902   JavaThread*    _next;                          // The next thread in the Threads list
 903   bool           _on_thread_list;                // Is set when this JavaThread is added to the Threads list
 904   oop            _threadObj;                     // The Java level thread object
 905 
 906 #ifdef ASSERT
 907  private:
 908   int _java_call_counter;
 909 
 910  public:
 911   int  java_call_counter()                       { return _java_call_counter; }
 912   void inc_java_call_counter()                   { _java_call_counter++; }
 913   void dec_java_call_counter() {
 914     assert(_java_call_counter &gt; 0, "Invalid nesting of JavaCallWrapper");
 915     _java_call_counter--;
 916   }
 917  private:  // restore original namespace restriction
 918 #endif  // ifdef ASSERT
 919 
 920 #ifndef PRODUCT
 921  public:
 922   enum {
 923     jump_ring_buffer_size = 16
 924   };
 925  private:  // restore original namespace restriction
 926 #endif
 927 
 928   JavaFrameAnchor _anchor;                       // Encapsulation of current java frame and it state
 929 
 930   ThreadFunction _entry_point;
 931 
 932   JNIEnv        _jni_environment;
 933 
 934   // Deopt support
 935   DeoptResourceMark*  _deopt_mark;               // Holds special ResourceMark for deoptimization
 936 
 937   intptr_t*      _must_deopt_id;                 // id of frame that needs to be deopted once we
 938                                                  // transition out of native
 939   CompiledMethod*       _deopt_nmethod;         // CompiledMethod that is currently being deoptimized
 940   vframeArray*  _vframe_array_head;              // Holds the heap of the active vframeArrays
 941   vframeArray*  _vframe_array_last;              // Holds last vFrameArray we popped
 942   // Because deoptimization is lazy we must save jvmti requests to set locals
 943   // in compiled frames until we deoptimize and we have an interpreter frame.
 944   // This holds the pointer to array (yeah like there might be more than one) of
 945   // description of compiled vframes that have locals that need to be updated.
 946   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* _deferred_locals_updates;
 947 
 948   // Handshake value for fixing 6243940. We need a place for the i2c
 949   // adapter to store the callee Method*. This value is NEVER live
 950   // across a gc point so it does NOT have to be gc'd
 951   // The handshake is open ended since we can't be certain that it will
 952   // be NULLed. This is because we rarely ever see the race and end up
 953   // in handle_wrong_method which is the backend of the handshake. See
 954   // code in i2c adapters and handle_wrong_method.
 955 
 956   Method*       _callee_target;
 957 
 958   // Used to pass back results to the interpreter or generated code running Java code.
 959   oop           _vm_result;    // oop result is GC-preserved
 960   Metadata*     _vm_result_2;  // non-oop result
 961 
 962   // See ReduceInitialCardMarks: this holds the precise space interval of
 963   // the most recent slow path allocation for which compiled code has
 964   // elided card-marks for performance along the fast-path.
 965   MemRegion     _deferred_card_mark;
 966 
 967   MonitorChunk* _monitor_chunks;                 // Contains the off stack monitors
 968                                                  // allocated during deoptimization
 969                                                  // and by JNI_MonitorEnter/Exit
 970 
 971   // Async. requests support
 972   enum AsyncRequests {
 973     _no_async_condition = 0,
 974     _async_exception,
 975     _async_unsafe_access_error
 976   };
 977   AsyncRequests _special_runtime_exit_condition; // Enum indicating pending async. request
 978   oop           _pending_async_exception;
 979 
 980   // Safepoint support
 981  public:                                         // Expose _thread_state for SafeFetchInt()
 982   volatile JavaThreadState _thread_state;
 983  private:
 984   ThreadSafepointState *_safepoint_state;        // Holds information about a thread during a safepoint
 985   address               _saved_exception_pc;     // Saved pc of instruction where last implicit exception happened
 986 
 987   // JavaThread termination support
 988   enum TerminatedTypes {
 989     _not_terminated = 0xDEAD - 2,
 990     _thread_exiting,                             // JavaThread::exit() has been called for this thread
 991     _thread_terminated,                          // JavaThread is removed from thread list
 992     _vm_exited                                   // JavaThread is still executing native code, but VM is terminated
 993                                                  // only VM_Exit can set _vm_exited
 994   };
 995 
 996   // In general a JavaThread's _terminated field transitions as follows:
 997   //
 998   //   _not_terminated =&gt; _thread_exiting =&gt; _thread_terminated
 999   //
1000   // _vm_exited is a special value to cover the case of a JavaThread
1001   // executing native code after the VM itself is terminated.
1002   volatile TerminatedTypes _terminated;
1003   // suspend/resume support
1004   volatile bool         _suspend_equivalent;     // Suspend equivalent condition
1005   jint                  _in_deopt_handler;       // count of deoptimization
1006                                                  // handlers thread is in
1007   volatile bool         _doing_unsafe_access;    // Thread may fault due to unsafe access
1008   bool                  _do_not_unlock_if_synchronized;  // Do not unlock the receiver of a synchronized method (since it was
1009                                                          // never locked) when throwing an exception. Used by interpreter only.
1010 
1011   // JNI attach states:
1012   enum JNIAttachStates {
1013     _not_attaching_via_jni = 1,  // thread is not attaching via JNI
1014     _attaching_via_jni,          // thread is attaching via JNI
1015     _attached_via_jni            // thread has attached via JNI
1016   };
1017 
1018   // A regular JavaThread's _jni_attach_state is _not_attaching_via_jni.
1019   // A native thread that is attaching via JNI starts with a value
1020   // of _attaching_via_jni and transitions to _attached_via_jni.
1021   volatile JNIAttachStates _jni_attach_state;
1022 
1023  public:
1024   // State of the stack guard pages for this thread.
1025   enum StackGuardState {
1026     stack_guard_unused,         // not needed
1027     stack_guard_reserved_disabled,
1028     stack_guard_yellow_reserved_disabled,// disabled (temporarily) after stack overflow
1029     stack_guard_enabled         // enabled
1030   };
1031 
1032  private:
1033 
1034 #if INCLUDE_JVMCI
1035   // The _pending_* fields below are used to communicate extra information
1036   // from an uncommon trap in JVMCI compiled code to the uncommon trap handler.
1037 
1038   // Communicates the DeoptReason and DeoptAction of the uncommon trap
1039   int       _pending_deoptimization;
1040 
1041   // Specifies whether the uncommon trap is to bci 0 of a synchronized method
1042   // before the monitor has been acquired.
1043   bool      _pending_monitorenter;
1044 
1045   // Specifies if the DeoptReason for the last uncommon trap was Reason_transfer_to_interpreter
1046   bool      _pending_transfer_to_interpreter;
1047 
1048   // Guard for re-entrant call to JVMCIRuntime::adjust_comp_level
1049   bool      _adjusting_comp_level;
1050 
1051   // An id of a speculation that JVMCI compiled code can use to further describe and
1052   // uniquely identify the  speculative optimization guarded by the uncommon trap
1053   long       _pending_failed_speculation;
1054 
1055   // These fields are mutually exclusive in terms of live ranges.
1056   union {
1057     // Communicates the pc at which the most recent implicit exception occurred
1058     // from the signal handler to a deoptimization stub.
1059     address   _implicit_exception_pc;
1060 
1061     // Communicates an alternative call target to an i2c stub from a JavaCall .
1062     address   _alternate_call_target;
1063   } _jvmci;
1064 
1065   // Support for high precision, thread sensitive counters in JVMCI compiled code.
1066   jlong*    _jvmci_counters;
1067 
1068  public:
1069   static jlong* _jvmci_old_thread_counters;
1070   static void collect_counters(typeArrayOop array);
1071  private:
1072 #endif // INCLUDE_JVMCI
1073 
1074   StackGuardState  _stack_guard_state;
1075 
1076   // Precompute the limit of the stack as used in stack overflow checks.
1077   // We load it from here to simplify the stack overflow check in assembly.
1078   address          _stack_overflow_limit;
1079   address          _reserved_stack_activation;
1080 
1081   // Compiler exception handling (NOTE: The _exception_oop is *NOT* the same as _pending_exception. It is
1082   // used to temp. parsing values into and out of the runtime system during exception handling for compiled
1083   // code)
1084   volatile oop     _exception_oop;               // Exception thrown in compiled code
1085   volatile address _exception_pc;                // PC where exception happened
1086   volatile address _exception_handler_pc;        // PC for handler of exception
1087   volatile int     _is_method_handle_return;     // true (== 1) if the current exception PC is a MethodHandle call site.
1088 
1089  private:
1090   // support for JNI critical regions
1091   jint    _jni_active_critical;                  // count of entries into JNI critical region
1092 
1093   // Checked JNI: function name requires exception check
1094   char* _pending_jni_exception_check_fn;
1095 
1096   // For deadlock detection.
1097   int _depth_first_number;
1098 
1099   // JVMTI PopFrame support
1100   // This is set to popframe_pending to signal that top Java frame should be popped immediately
1101   int _popframe_condition;
1102 
1103   // If reallocation of scalar replaced objects fails, we throw OOM
1104   // and during exception propagation, pop the top
1105   // _frames_to_pop_failed_realloc frames, the ones that reference
1106   // failed reallocations.
1107   int _frames_to_pop_failed_realloc;
1108 
1109 #ifndef PRODUCT
1110   int _jmp_ring_index;
1111   struct {
1112     // We use intptr_t instead of address so debugger doesn't try and display strings
1113     intptr_t _target;
1114     intptr_t _instruction;
1115     const char*  _file;
1116     int _line;
1117   }   _jmp_ring[jump_ring_buffer_size];
1118 #endif // PRODUCT
1119 
1120   friend class VMThread;
1121   friend class ThreadWaitTransition;
1122   friend class VM_Exit;
1123 
1124   void initialize();                             // Initialized the instance variables
1125 
1126  public:
1127   // Constructor
1128   JavaThread(bool is_attaching_via_jni = false); // for main thread and JNI attached threads
1129   JavaThread(ThreadFunction entry_point, size_t stack_size = 0);
1130   ~JavaThread();
1131 
1132 #ifdef ASSERT
1133   // verify this JavaThread hasn't be published in the Threads::list yet
1134   void verify_not_published();
1135 #endif
1136 
1137   //JNI functiontable getter/setter for JVMTI jni function table interception API.
1138   void set_jni_functions(struct JNINativeInterface_* functionTable) {
1139     _jni_environment.functions = functionTable;
1140   }
1141   struct JNINativeInterface_* get_jni_functions() {
1142     return (struct JNINativeInterface_ *)_jni_environment.functions;
1143   }
1144 
1145   // This function is called at thread creation to allow
1146   // platform specific thread variables to be initialized.
1147   void cache_global_variables();
1148 
1149   // Executes Shutdown.shutdown()
1150   void invoke_shutdown_hooks();
1151 
1152   // Cleanup on thread exit
1153   enum ExitType {
1154     normal_exit,
1155     jni_detach
1156   };
1157   void exit(bool destroy_vm, ExitType exit_type = normal_exit);
1158 
1159   void cleanup_failed_attach_current_thread();
1160 
1161   // Testers
1162   virtual bool is_Java_thread() const            { return true;  }
1163   virtual bool can_call_java() const             { return true; }
1164 
1165   // Thread chain operations
1166   JavaThread* next() const                       { return _next; }
1167   void set_next(JavaThread* p)                   { _next = p; }
1168 
1169   // Thread oop. threadObj() can be NULL for initial JavaThread
1170   // (or for threads attached via JNI)
1171   oop threadObj() const                          { return _threadObj; }
1172   void set_threadObj(oop p)                      { _threadObj = p; }
1173 
1174   ThreadPriority java_priority() const;          // Read from threadObj()
1175 
1176   // Prepare thread and add to priority queue.  If a priority is
1177   // not specified, use the priority of the thread object. Threads_lock
1178   // must be held while this function is called.
1179   void prepare(jobject jni_thread, ThreadPriority prio=NoPriority);
1180 
1181   void set_saved_exception_pc(address pc)        { _saved_exception_pc = pc; }
1182   address saved_exception_pc()                   { return _saved_exception_pc; }
1183 
1184 
1185   ThreadFunction entry_point() const             { return _entry_point; }
1186 
1187   // Allocates a new Java level thread object for this thread. thread_name may be NULL.
1188   void allocate_threadObj(Handle thread_group, const char* thread_name, bool daemon, TRAPS);
1189 
1190   // Last frame anchor routines
1191 
1192   JavaFrameAnchor* frame_anchor(void)            { return &amp;_anchor; }
1193 
1194   // last_Java_sp
1195   bool has_last_Java_frame() const               { return _anchor.has_last_Java_frame(); }
1196   intptr_t* last_Java_sp() const                 { return _anchor.last_Java_sp(); }
1197 
1198   // last_Java_pc
1199 
1200   address last_Java_pc(void)                     { return _anchor.last_Java_pc(); }
1201 
1202   // Safepoint support
1203 #if !(defined(PPC64) || defined(AARCH64))
1204   JavaThreadState thread_state() const           { return _thread_state; }
1205   void set_thread_state(JavaThreadState s)       { _thread_state = s;    }
1206 #else
1207   // Use membars when accessing volatile _thread_state. See
1208   // Threads::create_vm() for size checks.
1209   inline JavaThreadState thread_state() const;
1210   inline void set_thread_state(JavaThreadState s);
1211 #endif
1212   ThreadSafepointState *safepoint_state() const  { return _safepoint_state; }
1213   void set_safepoint_state(ThreadSafepointState *state) { _safepoint_state = state; }
1214   bool is_at_poll_safepoint()                    { return _safepoint_state-&gt;is_at_poll_safepoint(); }
1215 
1216   // JavaThread termination and lifecycle support:
1217   void smr_delete();
1218   bool on_thread_list() const { return _on_thread_list; }
1219   void set_on_thread_list() { _on_thread_list = true; }
1220 
1221   // thread has called JavaThread::exit() or is terminated
1222   bool is_exiting() const;
1223   // thread is terminated (no longer on the threads list); we compare
1224   // against the two non-terminated values so that a freed JavaThread
1225   // will also be considered terminated.
1226   bool check_is_terminated(TerminatedTypes l_terminated) const {
1227     return l_terminated != _not_terminated &amp;&amp; l_terminated != _thread_exiting;
1228   }
1229   bool is_terminated() const;
1230   void set_terminated(TerminatedTypes t);
1231   // special for Threads::remove() which is static:
1232   void set_terminated_value();
1233   void block_if_vm_exited();
1234 
1235   bool doing_unsafe_access()                     { return _doing_unsafe_access; }
1236   void set_doing_unsafe_access(bool val)         { _doing_unsafe_access = val; }
1237 
1238   bool do_not_unlock_if_synchronized()             { return _do_not_unlock_if_synchronized; }
1239   void set_do_not_unlock_if_synchronized(bool val) { _do_not_unlock_if_synchronized = val; }
1240 
1241   inline void set_polling_page_release(void* poll_value);
1242   inline void set_polling_page(void* poll_value);
1243   inline volatile void* get_polling_page();
1244 
1245  private:
1246   // Support for thread handshake operations
1247   HandshakeState _handshake;
1248  public:
1249   void set_handshake_operation(HandshakeOperation* op) {
1250     _handshake.set_operation(this, op);
1251   }
1252 
1253   bool has_handshake() const {
1254     return _handshake.has_operation();
1255   }
1256 
1257   void cancel_handshake() {
1258     _handshake.cancel(this);
1259   }
1260 
1261   void handshake_process_by_self() {
1262     _handshake.process_by_self(this);
1263   }
1264 
1265   void handshake_process_by_vmthread() {
1266     _handshake.process_by_vmthread(this);
1267   }
1268 
1269   // Suspend/resume support for JavaThread
1270  private:
1271   inline void set_ext_suspended();
1272   inline void clear_ext_suspended();
1273 
1274  public:
1275   void java_suspend();
1276   void java_resume();
1277   int  java_suspend_self();
1278 
1279   void check_and_wait_while_suspended() {
1280     assert(JavaThread::current() == this, "sanity check");
1281 
1282     bool do_self_suspend;
1283     do {
1284       // were we externally suspended while we were waiting?
1285       do_self_suspend = handle_special_suspend_equivalent_condition();
1286       if (do_self_suspend) {
1287         // don't surprise the thread that suspended us by returning
1288         java_suspend_self();
1289         set_suspend_equivalent();
1290       }
1291     } while (do_self_suspend);
1292   }
1293   static void check_safepoint_and_suspend_for_native_trans(JavaThread *thread);
1294   // Check for async exception in addition to safepoint and suspend request.
1295   static void check_special_condition_for_native_trans(JavaThread *thread);
1296 
1297   // Same as check_special_condition_for_native_trans but finishes the
1298   // transition into thread_in_Java mode so that it can potentially
1299   // block.
1300   static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);
1301 
1302   bool is_ext_suspend_completed(bool called_by_wait, int delay, uint32_t *bits);
1303   bool is_ext_suspend_completed_with_lock(uint32_t *bits) {
1304     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1305     // Warning: is_ext_suspend_completed() may temporarily drop the
1306     // SR_lock to allow the thread to reach a stable thread state if
1307     // it is currently in a transient thread state.
1308     return is_ext_suspend_completed(false /* !called_by_wait */,
1309                                     SuspendRetryDelay, bits);
1310   }
1311 
1312   // We cannot allow wait_for_ext_suspend_completion() to run forever or
1313   // we could hang. SuspendRetryCount and SuspendRetryDelay are normally
1314   // passed as the count and delay parameters. Experiments with specific
1315   // calls to wait_for_ext_suspend_completion() can be done by passing
1316   // other values in the code. Experiments with all calls can be done
1317   // via the appropriate -XX options.
1318   bool wait_for_ext_suspend_completion(int count, int delay, uint32_t *bits);
1319 
1320   // test for suspend - most (all?) of these should go away
1321   bool is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits);
1322 
1323   inline void set_external_suspend();
1324   inline void clear_external_suspend();
1325 
1326   inline void set_deopt_suspend();
1327   inline void clear_deopt_suspend();
1328   bool is_deopt_suspend()         { return (_suspend_flags &amp; _deopt_suspend) != 0; }
1329 
1330   bool is_external_suspend() const {
1331     return (_suspend_flags &amp; _external_suspend) != 0;
1332   }
1333   // Whenever a thread transitions from native to vm/java it must suspend
1334   // if external|deopt suspend is present.
1335   bool is_suspend_after_native() const {
1336     return (_suspend_flags &amp; (_external_suspend | _deopt_suspend)) != 0;
1337   }
1338 
1339   // external suspend request is completed
1340   bool is_ext_suspended() const {
1341     return (_suspend_flags &amp; _ext_suspended) != 0;
1342   }
1343 
1344   bool is_external_suspend_with_lock() const {
1345     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1346     return is_external_suspend();
1347   }
1348 
1349   // Special method to handle a pending external suspend request
1350   // when a suspend equivalent condition lifts.
1351   bool handle_special_suspend_equivalent_condition() {
1352     assert(is_suspend_equivalent(),
1353            "should only be called in a suspend equivalence condition");
1354     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1355     bool ret = is_external_suspend();
1356     if (!ret) {
1357       // not about to self-suspend so clear suspend equivalence
1358       clear_suspend_equivalent();
1359     }
1360     // implied else:
1361     // We have a pending external suspend request so we leave the
1362     // suspend_equivalent flag set until java_suspend_self() sets
1363     // the ext_suspended flag and clears the suspend_equivalent
1364     // flag. This insures that wait_for_ext_suspend_completion()
1365     // will return consistent values.
1366     return ret;
1367   }
1368 
1369   // utility methods to see if we are doing some kind of suspension
1370   bool is_being_ext_suspended() const            {
1371     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1372     return is_ext_suspended() || is_external_suspend();
1373   }
1374 
1375   bool is_suspend_equivalent() const             { return _suspend_equivalent; }
1376 
1377   void set_suspend_equivalent()                  { _suspend_equivalent = true; }
1378   void clear_suspend_equivalent()                { _suspend_equivalent = false; }
1379 
1380   // Thread.stop support
1381   void send_thread_stop(oop throwable);
1382   AsyncRequests clear_special_runtime_exit_condition() {
1383     AsyncRequests x = _special_runtime_exit_condition;
1384     _special_runtime_exit_condition = _no_async_condition;
1385     return x;
1386   }
1387 
1388   // Are any async conditions present?
1389   bool has_async_condition() { return (_special_runtime_exit_condition != _no_async_condition); }
1390 
1391   void check_and_handle_async_exceptions(bool check_unsafe_error = true);
1392 
1393   // these next two are also used for self-suspension and async exception support
1394   void handle_special_runtime_exit_condition(bool check_asyncs = true);
1395 
1396   // Return true if JavaThread has an asynchronous condition or
1397   // if external suspension is requested.
1398   bool has_special_runtime_exit_condition() {
1399     // Because we don't use is_external_suspend_with_lock
1400     // it is possible that we won't see an asynchronous external suspend
1401     // request that has just gotten started, i.e., SR_lock grabbed but
1402     // _external_suspend field change either not made yet or not visible
1403     // yet. However, this is okay because the request is asynchronous and
1404     // we will see the new flag value the next time through. It's also
1405     // possible that the external suspend request is dropped after
1406     // we have checked is_external_suspend(), we will recheck its value
1407     // under SR_lock in java_suspend_self().
1408     return (_special_runtime_exit_condition != _no_async_condition) ||
1409             is_external_suspend() || is_trace_suspend();
1410   }
1411 
1412   void set_pending_unsafe_access_error()          { _special_runtime_exit_condition = _async_unsafe_access_error; }
1413 
1414   inline void set_pending_async_exception(oop e);
1415 
1416   // Fast-locking support
1417   bool is_lock_owned(address adr) const;
1418 
1419   // Accessors for vframe array top
1420   // The linked list of vframe arrays are sorted on sp. This means when we
1421   // unpack the head must contain the vframe array to unpack.
1422   void set_vframe_array_head(vframeArray* value) { _vframe_array_head = value; }
1423   vframeArray* vframe_array_head() const         { return _vframe_array_head;  }
1424 
1425   // Side structure for deferring update of java frame locals until deopt occurs
1426   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* deferred_locals() const { return _deferred_locals_updates; }
1427   void set_deferred_locals(GrowableArray&lt;jvmtiDeferredLocalVariableSet *&gt;* vf) { _deferred_locals_updates = vf; }
1428 
1429   // These only really exist to make debugging deopt problems simpler
1430 
1431   void set_vframe_array_last(vframeArray* value) { _vframe_array_last = value; }
1432   vframeArray* vframe_array_last() const         { return _vframe_array_last;  }
1433 
1434   // The special resourceMark used during deoptimization
1435 
1436   void set_deopt_mark(DeoptResourceMark* value)  { _deopt_mark = value; }
1437   DeoptResourceMark* deopt_mark(void)            { return _deopt_mark; }
1438 
1439   intptr_t* must_deopt_id()                      { return _must_deopt_id; }
1440   void     set_must_deopt_id(intptr_t* id)       { _must_deopt_id = id; }
1441   void     clear_must_deopt_id()                 { _must_deopt_id = NULL; }
1442 
1443   void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }
1444   CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }
1445 
1446   Method*    callee_target() const               { return _callee_target; }
1447   void set_callee_target  (Method* x)          { _callee_target   = x; }
1448 
1449   // Oop results of vm runtime calls
1450   oop  vm_result() const                         { return _vm_result; }
1451   void set_vm_result  (oop x)                    { _vm_result   = x; }
1452 
1453   Metadata*    vm_result_2() const               { return _vm_result_2; }
1454   void set_vm_result_2  (Metadata* x)          { _vm_result_2   = x; }
1455 
1456   MemRegion deferred_card_mark() const           { return _deferred_card_mark; }
1457   void set_deferred_card_mark(MemRegion mr)      { _deferred_card_mark = mr;   }
1458 
1459 #if INCLUDE_JVMCI
1460   int  pending_deoptimization() const             { return _pending_deoptimization; }
1461   long  pending_failed_speculation() const         { return _pending_failed_speculation; }
1462   bool adjusting_comp_level() const               { return _adjusting_comp_level; }
1463   void set_adjusting_comp_level(bool b)           { _adjusting_comp_level = b; }
1464   bool has_pending_monitorenter() const           { return _pending_monitorenter; }
1465   void set_pending_monitorenter(bool b)           { _pending_monitorenter = b; }
1466   void set_pending_deoptimization(int reason)     { _pending_deoptimization = reason; }
1467   void set_pending_failed_speculation(long failed_speculation) { _pending_failed_speculation = failed_speculation; }
1468   void set_pending_transfer_to_interpreter(bool b) { _pending_transfer_to_interpreter = b; }
1469   void set_jvmci_alternate_call_target(address a) { assert(_jvmci._alternate_call_target == NULL, "must be"); _jvmci._alternate_call_target = a; }
1470   void set_jvmci_implicit_exception_pc(address a) { assert(_jvmci._implicit_exception_pc == NULL, "must be"); _jvmci._implicit_exception_pc = a; }
1471 #endif // INCLUDE_JVMCI
1472 
1473   // Exception handling for compiled methods
1474   oop      exception_oop() const                 { return _exception_oop; }
1475   address  exception_pc() const                  { return _exception_pc; }
1476   address  exception_handler_pc() const          { return _exception_handler_pc; }
1477   bool     is_method_handle_return() const       { return _is_method_handle_return == 1; }
1478 
1479   void set_exception_oop(oop o)                  { (void)const_cast&lt;oop&amp;&gt;(_exception_oop = o); }
1480   void set_exception_pc(address a)               { _exception_pc = a; }
1481   void set_exception_handler_pc(address a)       { _exception_handler_pc = a; }
1482   void set_is_method_handle_return(bool value)   { _is_method_handle_return = value ? 1 : 0; }
1483 
1484   void clear_exception_oop_and_pc() {
1485     set_exception_oop(NULL);
1486     set_exception_pc(NULL);
1487   }
1488 
1489   // Stack overflow support
1490   //
1491   //  (small addresses)
1492   //
1493   //  --  &lt;-- stack_end()                   ---
1494   //  |                                      |
1495   //  |  red pages                           |
1496   //  |                                      |
1497   //  --  &lt;-- stack_red_zone_base()          |
1498   //  |                                      |
1499   //  |                                     guard
1500   //  |  yellow pages                       zone
1501   //  |                                      |
1502   //  |                                      |
1503   //  --  &lt;-- stack_yellow_zone_base()       |
1504   //  |                                      |
1505   //  |                                      |
1506   //  |  reserved pages                      |
1507   //  |                                      |
1508   //  --  &lt;-- stack_reserved_zone_base()    ---      ---
1509   //                                                 /|\  shadow     &lt;--  stack_overflow_limit() (somewhere in here)
1510   //                                                  |   zone
1511   //                                                 \|/  size
1512   //  some untouched memory                          ---
1513   //
1514   //
1515   //  --
1516   //  |
1517   //  |  shadow zone
1518   //  |
1519   //  --
1520   //  x    frame n
1521   //  --
1522   //  x    frame n-1
1523   //  x
1524   //  --
1525   //  ...
1526   //
1527   //  --
1528   //  x    frame 0
1529   //  --  &lt;-- stack_base()
1530   //
1531   //  (large addresses)
1532   //
1533 
1534  private:
1535   // These values are derived from flags StackRedPages, StackYellowPages,
1536   // StackReservedPages and StackShadowPages. The zone size is determined
1537   // ergonomically if page_size &gt; 4K.
1538   static size_t _stack_red_zone_size;
1539   static size_t _stack_yellow_zone_size;
1540   static size_t _stack_reserved_zone_size;
1541   static size_t _stack_shadow_zone_size;
1542  public:
1543   inline size_t stack_available(address cur_sp);
1544 
1545   static size_t stack_red_zone_size() {
1546     assert(_stack_red_zone_size &gt; 0, "Don't call this before the field is initialized.");
1547     return _stack_red_zone_size;
1548   }
1549   static void set_stack_red_zone_size(size_t s) {
1550     assert(is_aligned(s, os::vm_page_size()),
1551            "We can not protect if the red zone size is not page aligned.");
1552     assert(_stack_red_zone_size == 0, "This should be called only once.");
1553     _stack_red_zone_size = s;
1554   }
1555   address stack_red_zone_base() {
1556     return (address)(stack_end() + stack_red_zone_size());
1557   }
1558   bool in_stack_red_zone(address a) {
1559     return a &lt;= stack_red_zone_base() &amp;&amp; a &gt;= stack_end();
1560   }
1561 
1562   static size_t stack_yellow_zone_size() {
1563     assert(_stack_yellow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1564     return _stack_yellow_zone_size;
1565   }
1566   static void set_stack_yellow_zone_size(size_t s) {
1567     assert(is_aligned(s, os::vm_page_size()),
1568            "We can not protect if the yellow zone size is not page aligned.");
1569     assert(_stack_yellow_zone_size == 0, "This should be called only once.");
1570     _stack_yellow_zone_size = s;
1571   }
1572 
1573   static size_t stack_reserved_zone_size() {
1574     // _stack_reserved_zone_size may be 0. This indicates the feature is off.
1575     return _stack_reserved_zone_size;
1576   }
1577   static void set_stack_reserved_zone_size(size_t s) {
1578     assert(is_aligned(s, os::vm_page_size()),
1579            "We can not protect if the reserved zone size is not page aligned.");
1580     assert(_stack_reserved_zone_size == 0, "This should be called only once.");
1581     _stack_reserved_zone_size = s;
1582   }
1583   address stack_reserved_zone_base() {
1584     return (address)(stack_end() +
1585                      (stack_red_zone_size() + stack_yellow_zone_size() + stack_reserved_zone_size()));
1586   }
1587   bool in_stack_reserved_zone(address a) {
1588     return (a &lt;= stack_reserved_zone_base()) &amp;&amp;
1589            (a &gt;= (address)((intptr_t)stack_reserved_zone_base() - stack_reserved_zone_size()));
1590   }
1591 
1592   static size_t stack_yellow_reserved_zone_size() {
1593     return _stack_yellow_zone_size + _stack_reserved_zone_size;
1594   }
1595   bool in_stack_yellow_reserved_zone(address a) {
1596     return (a &lt;= stack_reserved_zone_base()) &amp;&amp; (a &gt;= stack_red_zone_base());
1597   }
1598 
1599   // Size of red + yellow + reserved zones.
1600   static size_t stack_guard_zone_size() {
1601     return stack_red_zone_size() + stack_yellow_reserved_zone_size();
1602   }
1603 
1604   static size_t stack_shadow_zone_size() {
1605     assert(_stack_shadow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1606     return _stack_shadow_zone_size;
1607   }
1608   static void set_stack_shadow_zone_size(size_t s) {
1609     // The shadow area is not allocated or protected, so
1610     // it needs not be page aligned.
1611     // But the stack bang currently assumes that it is a
1612     // multiple of page size. This guarantees that the bang
1613     // loop touches all pages in the shadow zone.
1614     // This can be guaranteed differently, as well.  E.g., if
1615     // the page size is a multiple of 4K, banging in 4K steps
1616     // suffices to touch all pages. (Some pages are banged
1617     // several times, though.)
1618     assert(is_aligned(s, os::vm_page_size()),
1619            "Stack bang assumes multiple of page size.");
1620     assert(_stack_shadow_zone_size == 0, "This should be called only once.");
1621     _stack_shadow_zone_size = s;
1622   }
1623 
1624   void create_stack_guard_pages();
1625   void remove_stack_guard_pages();
1626 
1627   void enable_stack_reserved_zone();
1628   void disable_stack_reserved_zone();
1629   void enable_stack_yellow_reserved_zone();
1630   void disable_stack_yellow_reserved_zone();
1631   void enable_stack_red_zone();
1632   void disable_stack_red_zone();
1633 
1634   inline bool stack_guard_zone_unused();
1635   inline bool stack_yellow_reserved_zone_disabled();
1636   inline bool stack_reserved_zone_disabled();
1637   inline bool stack_guards_enabled();
1638 
1639   address reserved_stack_activation() const { return _reserved_stack_activation; }
1640   void set_reserved_stack_activation(address addr) {
1641     assert(_reserved_stack_activation == stack_base()
1642             || _reserved_stack_activation == NULL
1643             || addr == stack_base(), "Must not be set twice");
1644     _reserved_stack_activation = addr;
1645   }
1646 
1647   // Attempt to reguard the stack after a stack overflow may have occurred.
1648   // Returns true if (a) guard pages are not needed on this thread, (b) the
1649   // pages are already guarded, or (c) the pages were successfully reguarded.
1650   // Returns false if there is not enough stack space to reguard the pages, in
1651   // which case the caller should unwind a frame and try again.  The argument
1652   // should be the caller's (approximate) sp.
1653   bool reguard_stack(address cur_sp);
1654   // Similar to above but see if current stackpoint is out of the guard area
1655   // and reguard if possible.
1656   bool reguard_stack(void);
1657 
1658   address stack_overflow_limit() { return _stack_overflow_limit; }
1659   void set_stack_overflow_limit() {
1660     _stack_overflow_limit =
1661       stack_end() + MAX2(JavaThread::stack_guard_zone_size(), JavaThread::stack_shadow_zone_size());
1662   }
1663 
1664   // Misc. accessors/mutators
1665   void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }
1666   void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }
1667   bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }
1668 
1669 #ifndef PRODUCT
1670   void record_jump(address target, address instr, const char* file, int line);
1671 #endif // PRODUCT
1672 
1673   // For assembly stub generation
1674   static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }
1675 #ifndef PRODUCT
1676   static ByteSize jmp_ring_index_offset()        { return byte_offset_of(JavaThread, _jmp_ring_index); }
1677   static ByteSize jmp_ring_offset()              { return byte_offset_of(JavaThread, _jmp_ring); }
1678 #endif // PRODUCT
1679   static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }
1680   static ByteSize pending_jni_exception_check_fn_offset() {
1681     return byte_offset_of(JavaThread, _pending_jni_exception_check_fn);
1682   }
1683   static ByteSize last_Java_sp_offset() {
1684     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();
1685   }
1686   static ByteSize last_Java_pc_offset() {
1687     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();
1688   }
1689   static ByteSize frame_anchor_offset() {
1690     return byte_offset_of(JavaThread, _anchor);
1691   }
1692   static ByteSize callee_target_offset()         { return byte_offset_of(JavaThread, _callee_target); }
1693   static ByteSize vm_result_offset()             { return byte_offset_of(JavaThread, _vm_result); }
1694   static ByteSize vm_result_2_offset()           { return byte_offset_of(JavaThread, _vm_result_2); }
1695   static ByteSize thread_state_offset()          { return byte_offset_of(JavaThread, _thread_state); }
1696   static ByteSize saved_exception_pc_offset()    { return byte_offset_of(JavaThread, _saved_exception_pc); }
1697   static ByteSize osthread_offset()              { return byte_offset_of(JavaThread, _osthread); }
1698 #if INCLUDE_JVMCI
1699   static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }
1700   static ByteSize pending_monitorenter_offset()  { return byte_offset_of(JavaThread, _pending_monitorenter); }
1701   static ByteSize pending_failed_speculation_offset() { return byte_offset_of(JavaThread, _pending_failed_speculation); }
1702   static ByteSize jvmci_alternate_call_target_offset() { return byte_offset_of(JavaThread, _jvmci._alternate_call_target); }
1703   static ByteSize jvmci_implicit_exception_pc_offset() { return byte_offset_of(JavaThread, _jvmci._implicit_exception_pc); }
1704   static ByteSize jvmci_counters_offset()        { return byte_offset_of(JavaThread, _jvmci_counters); }
1705 #endif // INCLUDE_JVMCI
1706   static ByteSize exception_oop_offset()         { return byte_offset_of(JavaThread, _exception_oop); }
1707   static ByteSize exception_pc_offset()          { return byte_offset_of(JavaThread, _exception_pc); }
1708   static ByteSize exception_handler_pc_offset()  { return byte_offset_of(JavaThread, _exception_handler_pc); }
1709   static ByteSize stack_overflow_limit_offset()  { return byte_offset_of(JavaThread, _stack_overflow_limit); }
1710   static ByteSize is_method_handle_return_offset() { return byte_offset_of(JavaThread, _is_method_handle_return); }
1711   static ByteSize stack_guard_state_offset()     { return byte_offset_of(JavaThread, _stack_guard_state); }
1712   static ByteSize reserved_stack_activation_offset() { return byte_offset_of(JavaThread, _reserved_stack_activation); }
1713   static ByteSize suspend_flags_offset()         { return byte_offset_of(JavaThread, _suspend_flags); }
1714 
1715   static ByteSize do_not_unlock_if_synchronized_offset() { return byte_offset_of(JavaThread, _do_not_unlock_if_synchronized); }
1716   static ByteSize should_post_on_exceptions_flag_offset() {
1717     return byte_offset_of(JavaThread, _should_post_on_exceptions_flag);
1718   }
1719 
1720   // Returns the jni environment for this thread
1721   JNIEnv* jni_environment()                      { return &amp;_jni_environment; }
1722 
1723   static JavaThread* thread_from_jni_environment(JNIEnv* env) {
1724     JavaThread *thread_from_jni_env = (JavaThread*)((intptr_t)env - in_bytes(jni_environment_offset()));
1725     // Only return NULL if thread is off the thread list; starting to
1726     // exit should not return NULL.
1727     if (thread_from_jni_env-&gt;is_terminated()) {
1728       thread_from_jni_env-&gt;block_if_vm_exited();
1729       return NULL;
1730     } else {
1731       return thread_from_jni_env;
1732     }
1733   }
1734 
1735   // JNI critical regions. These can nest.
1736   bool in_critical()    { return _jni_active_critical &gt; 0; }
1737   bool in_last_critical()  { return _jni_active_critical == 1; }
1738   void enter_critical() {
1739     assert(Thread::current() == this ||
1740            (Thread::current()-&gt;is_VM_thread() &amp;&amp;
1741            SafepointSynchronize::is_synchronizing()),
1742            "this must be current thread or synchronizing");
1743     _jni_active_critical++;
1744   }
1745   void exit_critical() {
1746     assert(Thread::current() == this, "this must be current thread");
1747     _jni_active_critical--;
1748     assert(_jni_active_critical &gt;= 0, "JNI critical nesting problem?");
1749   }
1750 
1751   // Checked JNI: is the programmer required to check for exceptions, if so specify
1752   // which function name. Returning to a Java frame should implicitly clear the
1753   // pending check, this is done for Native-&gt;Java transitions (i.e. user JNI code).
1754   // VM-&gt;Java transistions are not cleared, it is expected that JNI code enclosed
1755   // within ThreadToNativeFromVM makes proper exception checks (i.e. VM internal).
1756   bool is_pending_jni_exception_check() const { return _pending_jni_exception_check_fn != NULL; }
1757   void clear_pending_jni_exception_check() { _pending_jni_exception_check_fn = NULL; }
1758   const char* get_pending_jni_exception_check() const { return _pending_jni_exception_check_fn; }
1759   void set_pending_jni_exception_check(const char* fn_name) { _pending_jni_exception_check_fn = (char*) fn_name; }
1760 
1761   // For deadlock detection
1762   int depth_first_number() { return _depth_first_number; }
1763   void set_depth_first_number(int dfn) { _depth_first_number = dfn; }
1764 
1765  private:
1766   void set_monitor_chunks(MonitorChunk* monitor_chunks) { _monitor_chunks = monitor_chunks; }
1767 
1768  public:
1769   MonitorChunk* monitor_chunks() const           { return _monitor_chunks; }
1770   void add_monitor_chunk(MonitorChunk* chunk);
1771   void remove_monitor_chunk(MonitorChunk* chunk);
1772   bool in_deopt_handler() const                  { return _in_deopt_handler &gt; 0; }
1773   void inc_in_deopt_handler()                    { _in_deopt_handler++; }
1774   void dec_in_deopt_handler() {
1775     assert(_in_deopt_handler &gt; 0, "mismatched deopt nesting");
1776     if (_in_deopt_handler &gt; 0) { // robustness
1777       _in_deopt_handler--;
1778     }
1779   }
1780 
1781  private:
1782   void set_entry_point(ThreadFunction entry_point) { _entry_point = entry_point; }
1783 
1784  public:
1785 
1786   // Frame iteration; calls the function f for all frames on the stack
1787   void frames_do(void f(frame*, const RegisterMap*));
1788 
1789   // Memory operations
1790   void oops_do(OopClosure* f, CodeBlobClosure* cf);
1791 
1792   // Sweeper operations
1793   virtual void nmethods_do(CodeBlobClosure* cf);
1794 
1795   // RedefineClasses Support
1796   void metadata_do(void f(Metadata*));
1797 
1798   // Misc. operations
1799   char* name() const { return (char*)get_thread_name(); }
1800   void print_on(outputStream* st, bool print_extended_info) const;
1801   void print_on(outputStream* st) const { print_on(st, false); }
1802   void print_value();
1803   void print_thread_state_on(outputStream*) const      PRODUCT_RETURN;
1804   void print_thread_state() const                      PRODUCT_RETURN;
1805   void print_on_error(outputStream* st, char* buf, int buflen) const;
1806   void print_name_on_error(outputStream* st, char* buf, int buflen) const;
1807   void verify();
1808   const char* get_thread_name() const;
1809  private:
1810   // factor out low-level mechanics for use in both normal and error cases
1811   const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;
1812  public:
1813   const char* get_threadgroup_name() const;
1814   const char* get_parent_name() const;
1815 
1816   // Accessing frames
1817   frame last_frame() {
1818     _anchor.make_walkable(this);
1819     return pd_last_frame();
1820   }
1821   javaVFrame* last_java_vframe(RegisterMap* reg_map);
1822 
1823   // Returns method at 'depth' java or native frames down the stack
1824   // Used for security checks
1825   Klass* security_get_caller_class(int depth);
1826 
1827   // Print stack trace in external format
1828   void print_stack_on(outputStream* st);
1829   void print_stack() { print_stack_on(tty); }
1830 
1831   // Print stack traces in various internal formats
1832   void trace_stack()                             PRODUCT_RETURN;
1833   void trace_stack_from(vframe* start_vf)        PRODUCT_RETURN;
1834   void trace_frames()                            PRODUCT_RETURN;
1835   void trace_oops()                              PRODUCT_RETURN;
1836 
1837   // Print an annotated view of the stack frames
1838   void print_frame_layout(int depth = 0, bool validate_only = false) NOT_DEBUG_RETURN;
1839   void validate_frame_layout() {
1840     print_frame_layout(0, true);
1841   }
1842 
1843   // Returns the number of stack frames on the stack
1844   int depth() const;
1845 
1846   // Function for testing deoptimization
1847   void deoptimize();
1848   void make_zombies();
1849 
1850   void deoptimized_wrt_marked_nmethods();
1851 
1852  public:
1853   // Returns the running thread as a JavaThread
1854   static inline JavaThread* current();
1855 
1856   // Returns the active Java thread.  Do not use this if you know you are calling
1857   // from a JavaThread, as it's slower than JavaThread::current.  If called from
1858   // the VMThread, it also returns the JavaThread that instigated the VMThread's
1859   // operation.  You may not want that either.
1860   static JavaThread* active();
1861 
1862   inline CompilerThread* as_CompilerThread();
1863 
1864  public:
1865   virtual void run();
1866   void thread_main_inner();
1867 
1868  private:
1869   // PRIVILEGED STACK
1870   PrivilegedElement*  _privileged_stack_top;
1871   GrowableArray&lt;oop&gt;* _array_for_gc;
1872  public:
1873 
1874   // Returns the privileged_stack information.
1875   PrivilegedElement* privileged_stack_top() const       { return _privileged_stack_top; }
1876   void set_privileged_stack_top(PrivilegedElement *e)   { _privileged_stack_top = e; }
1877   void register_array_for_gc(GrowableArray&lt;oop&gt;* array) { _array_for_gc = array; }
1878 
1879  public:
1880   // Thread local information maintained by JVMTI.
1881   void set_jvmti_thread_state(JvmtiThreadState *value)                           { _jvmti_thread_state = value; }
1882   // A JvmtiThreadState is lazily allocated. This jvmti_thread_state()
1883   // getter is used to get this JavaThread's JvmtiThreadState if it has
1884   // one which means NULL can be returned. JvmtiThreadState::state_for()
1885   // is used to get the specified JavaThread's JvmtiThreadState if it has
1886   // one or it allocates a new JvmtiThreadState for the JavaThread and
1887   // returns it. JvmtiThreadState::state_for() will return NULL only if
1888   // the specified JavaThread is exiting.
1889   JvmtiThreadState *jvmti_thread_state() const                                   { return _jvmti_thread_state; }
1890   static ByteSize jvmti_thread_state_offset()                                    { return byte_offset_of(JavaThread, _jvmti_thread_state); }
1891 
1892   // JVMTI PopFrame support
1893   // Setting and clearing popframe_condition
1894   // All of these enumerated values are bits. popframe_pending
1895   // indicates that a PopFrame() has been requested and not yet been
1896   // completed. popframe_processing indicates that that PopFrame() is in
1897   // the process of being completed. popframe_force_deopt_reexecution_bit
1898   // indicates that special handling is required when returning to a
1899   // deoptimized caller.
1900   enum PopCondition {
1901     popframe_inactive                      = 0x00,
1902     popframe_pending_bit                   = 0x01,
1903     popframe_processing_bit                = 0x02,
1904     popframe_force_deopt_reexecution_bit   = 0x04
1905   };
1906   PopCondition popframe_condition()                   { return (PopCondition) _popframe_condition; }
1907   void set_popframe_condition(PopCondition c)         { _popframe_condition = c; }
1908   void set_popframe_condition_bit(PopCondition c)     { _popframe_condition |= c; }
1909   void clear_popframe_condition()                     { _popframe_condition = popframe_inactive; }
1910   static ByteSize popframe_condition_offset()         { return byte_offset_of(JavaThread, _popframe_condition); }
1911   bool has_pending_popframe()                         { return (popframe_condition() &amp; popframe_pending_bit) != 0; }
1912   bool popframe_forcing_deopt_reexecution()           { return (popframe_condition() &amp; popframe_force_deopt_reexecution_bit) != 0; }
1913   void clear_popframe_forcing_deopt_reexecution()     { _popframe_condition &amp;= ~popframe_force_deopt_reexecution_bit; }
1914 #ifdef CC_INTERP
1915   bool pop_frame_pending(void)                        { return ((_popframe_condition &amp; popframe_pending_bit) != 0); }
1916   void clr_pop_frame_pending(void)                    { _popframe_condition = popframe_inactive; }
1917   bool pop_frame_in_process(void)                     { return ((_popframe_condition &amp; popframe_processing_bit) != 0); }
1918   void set_pop_frame_in_process(void)                 { _popframe_condition |= popframe_processing_bit; }
1919   void clr_pop_frame_in_process(void)                 { _popframe_condition &amp;= ~popframe_processing_bit; }
1920 #endif
1921 
1922   int frames_to_pop_failed_realloc() const            { return _frames_to_pop_failed_realloc; }
1923   void set_frames_to_pop_failed_realloc(int nb)       { _frames_to_pop_failed_realloc = nb; }
1924   void dec_frames_to_pop_failed_realloc()             { _frames_to_pop_failed_realloc--; }
1925 
1926  private:
1927   // Saved incoming arguments to popped frame.
1928   // Used only when popped interpreted frame returns to deoptimized frame.
1929   void*    _popframe_preserved_args;
1930   int      _popframe_preserved_args_size;
1931 
1932  public:
1933   void  popframe_preserve_args(ByteSize size_in_bytes, void* start);
1934   void* popframe_preserved_args();
1935   ByteSize popframe_preserved_args_size();
1936   WordSize popframe_preserved_args_size_in_words();
1937   void  popframe_free_preserved_args();
1938 
1939 
1940  private:
1941   JvmtiThreadState *_jvmti_thread_state;
1942 
1943   // Used by the interpreter in fullspeed mode for frame pop, method
1944   // entry, method exit and single stepping support. This field is
1945   // only set to non-zero by the VM_EnterInterpOnlyMode VM operation.
1946   // It can be set to zero asynchronously (i.e., without a VM operation
1947   // or a lock) so we have to be very careful.
1948   int               _interp_only_mode;
1949 
1950  public:
1951   // used by the interpreter for fullspeed debugging support (see above)
1952   static ByteSize interp_only_mode_offset() { return byte_offset_of(JavaThread, _interp_only_mode); }
1953   bool is_interp_only_mode()                { return (_interp_only_mode != 0); }
1954   int get_interp_only_mode()                { return _interp_only_mode; }
1955   void increment_interp_only_mode()         { ++_interp_only_mode; }
1956   void decrement_interp_only_mode()         { --_interp_only_mode; }
1957 
1958   // support for cached flag that indicates whether exceptions need to be posted for this thread
1959   // if this is false, we can avoid deoptimizing when events are thrown
1960   // this gets set to reflect whether jvmtiExport::post_exception_throw would actually do anything
1961  private:
1962   int    _should_post_on_exceptions_flag;
1963 
1964  public:
1965   int   should_post_on_exceptions_flag()  { return _should_post_on_exceptions_flag; }
1966   void  set_should_post_on_exceptions_flag(int val)  { _should_post_on_exceptions_flag = val; }
1967 
1968  private:
1969   ThreadStatistics *_thread_stat;
1970 
1971  public:
1972   ThreadStatistics* get_thread_stat() const    { return _thread_stat; }
1973 
1974   // Return a blocker object for which this thread is blocked parking.
1975   oop current_park_blocker();
1976 
1977  private:
1978   static size_t _stack_size_at_create;
1979 
1980  public:
1981   static inline size_t stack_size_at_create(void) {
1982     return _stack_size_at_create;
1983   }
1984   static inline void set_stack_size_at_create(size_t value) {
1985     _stack_size_at_create = value;
1986   }
1987 
1988   // Machine dependent stuff
1989 #include OS_CPU_HEADER(thread)
1990 
1991  public:
1992   void set_blocked_on_compilation(bool value) {
1993     _blocked_on_compilation = value;
1994   }
1995 
1996   bool blocked_on_compilation() {
1997     return _blocked_on_compilation;
1998   }
1999  protected:
2000   bool         _blocked_on_compilation;
2001 
2002 
2003   // JSR166 per-thread parker
2004  private:
2005   Parker*    _parker;
2006  public:
2007   Parker*     parker() { return _parker; }
2008 
2009   // Biased locking support
2010  private:
2011   GrowableArray&lt;MonitorInfo*&gt;* _cached_monitor_info;
2012  public:
2013   GrowableArray&lt;MonitorInfo*&gt;* cached_monitor_info() { return _cached_monitor_info; }
2014   void set_cached_monitor_info(GrowableArray&lt;MonitorInfo*&gt;* info) { _cached_monitor_info = info; }
2015 
2016   // clearing/querying jni attach status
2017   bool is_attaching_via_jni() const { return _jni_attach_state == _attaching_via_jni; }
2018   bool has_attached_via_jni() const { return is_attaching_via_jni() || _jni_attach_state == _attached_via_jni; }
2019   inline void set_done_attaching_via_jni();
2020 };
2021 
2022 // Inline implementation of JavaThread::current
2023 inline JavaThread* JavaThread::current() {
2024   Thread* thread = Thread::current();
2025   assert(thread-&gt;is_Java_thread(), "just checking");
2026   return (JavaThread*)thread;
2027 }
2028 
2029 inline CompilerThread* JavaThread::as_CompilerThread() {
2030   assert(is_Compiler_thread(), "just checking");
2031   return (CompilerThread*)this;
2032 }
2033 
2034 // Dedicated thread to sweep the code cache
2035 class CodeCacheSweeperThread : public JavaThread {
2036   CompiledMethod*       _scanned_compiled_method; // nmethod being scanned by the sweeper
2037  public:
2038   CodeCacheSweeperThread();
2039   // Track the nmethod currently being scanned by the sweeper
2040   void set_scanned_compiled_method(CompiledMethod* cm) {
2041     assert(_scanned_compiled_method == NULL || cm == NULL, "should reset to NULL before writing a new value");
2042     _scanned_compiled_method = cm;
2043   }
2044 
2045   // Hide sweeper thread from external view.
2046   bool is_hidden_from_external_view() const { return true; }
2047 
2048   bool is_Code_cache_sweeper_thread() const { return true; }
2049 
2050   // Prevent GC from unloading _scanned_compiled_method
2051   void oops_do(OopClosure* f, CodeBlobClosure* cf);
2052   void nmethods_do(CodeBlobClosure* cf);
2053 };
2054 
2055 // A thread used for Compilation.
2056 class CompilerThread : public JavaThread {
2057   friend class VMStructs;
2058  private:
2059   CompilerCounters* _counters;
2060 
2061   ciEnv*                _env;
2062   CompileLog*           _log;
2063   CompileTask* volatile _task;  // print_threads_compiling can read this concurrently.
2064   CompileQueue*         _queue;
2065   BufferBlob*           _buffer_blob;
2066 
2067   AbstractCompiler*     _compiler;
2068   TimeStamp             _idle_time;
2069 
2070  public:
2071 
2072   static CompilerThread* current();
2073 
2074   CompilerThread(CompileQueue* queue, CompilerCounters* counters);
2075   ~CompilerThread();
2076 
2077   bool is_Compiler_thread() const                { return true; }
2078 
2079   virtual bool can_call_java() const;
2080 
2081   // Hide native compiler threads from external view.
2082   bool is_hidden_from_external_view() const      { return !can_call_java(); }
2083 
2084   void set_compiler(AbstractCompiler* c)         { _compiler = c; }
2085   AbstractCompiler* compiler() const             { return _compiler; }
2086 
2087   CompileQueue* queue()        const             { return _queue; }
2088   CompilerCounters* counters() const             { return _counters; }
2089 
2090   // Get/set the thread's compilation environment.
2091   ciEnv*        env()                            { return _env; }
2092   void          set_env(ciEnv* env)              { _env = env; }
2093 
2094   BufferBlob*   get_buffer_blob() const          { return _buffer_blob; }
2095   void          set_buffer_blob(BufferBlob* b)   { _buffer_blob = b; }
2096 
2097   // Get/set the thread's logging information
2098   CompileLog*   log()                            { return _log; }
2099   void          init_log(CompileLog* log) {
2100     // Set once, for good.
2101     assert(_log == NULL, "set only once");
2102     _log = log;
2103   }
2104 
2105   void start_idle_timer()                        { _idle_time.update(); }
2106   jlong idle_time_millis() {
2107     return TimeHelper::counter_to_millis(_idle_time.ticks_since_update());
2108   }
2109 
2110 #ifndef PRODUCT
2111  private:
2112   IdealGraphPrinter *_ideal_graph_printer;
2113  public:
2114   IdealGraphPrinter *ideal_graph_printer()           { return _ideal_graph_printer; }
2115   void set_ideal_graph_printer(IdealGraphPrinter *n) { _ideal_graph_printer = n; }
2116 #endif
2117 
2118   // Get/set the thread's current task
2119   CompileTask* task()                      { return _task; }
2120   void         set_task(CompileTask* task) { _task = task; }
2121 };
2122 
2123 inline CompilerThread* CompilerThread::current() {
2124   return JavaThread::current()-&gt;as_CompilerThread();
2125 }
2126 
2127 // The active thread queue. It also keeps track of the current used
2128 // thread priorities.
2129 class Threads: AllStatic {
2130   friend class VMStructs;
2131  private:
2132   static JavaThread* _thread_list;
2133   static int         _number_of_threads;
2134   static int         _number_of_non_daemon_threads;
2135   static int         _return_code;
2136   static int         _thread_claim_parity;
2137 #ifdef ASSERT
2138   static bool        _vm_complete;
2139   static size_t      _threads_before_barrier_set;
2140 #endif
2141 
2142   static void initialize_java_lang_classes(JavaThread* main_thread, TRAPS);
2143   static void initialize_jsr292_core_classes(TRAPS);
2144 
2145  public:
2146   // Thread management
2147   // force_daemon is a concession to JNI, where we may need to add a
2148   // thread to the thread list before allocating its thread object
2149   static void add(JavaThread* p, bool force_daemon = false);
2150   static void remove(JavaThread* p);
2151   static void non_java_threads_do(ThreadClosure* tc);
2152   static void java_threads_do(ThreadClosure* tc);
2153   static void java_threads_and_vm_thread_do(ThreadClosure* tc);
2154   static void threads_do(ThreadClosure* tc);
2155   static void possibly_parallel_threads_do(bool is_par, ThreadClosure* tc);
2156 
2157   // Initializes the vm and creates the vm thread
2158   static jint create_vm(JavaVMInitArgs* args, bool* canTryAgain);
2159   static void convert_vm_init_libraries_to_agents();
2160   static void create_vm_init_libraries();
2161   static void create_vm_init_agents();
2162   static void shutdown_vm_agents();
2163   static bool destroy_vm();
2164   // Supported VM versions via JNI
2165   // Includes JNI_VERSION_1_1
2166   static jboolean is_supported_jni_version_including_1_1(jint version);
2167   // Does not include JNI_VERSION_1_1
2168   static jboolean is_supported_jni_version(jint version);
2169 
2170   // The "thread claim parity" provides a way for threads to be claimed
2171   // by parallel worker tasks.
2172   //
2173   // Each thread contains a a "parity" field. A task will claim the
2174   // thread only if its parity field is the same as the global parity,
2175   // which is updated by calling change_thread_claim_parity().
2176   //
2177   // For this to work change_thread_claim_parity() needs to be called
2178   // exactly once in sequential code before starting parallel tasks
2179   // that should claim threads.
2180   //
2181   // New threads get their parity set to 0 and change_thread_claim_parity()
2182   // never set the global parity to 0.
2183   static int thread_claim_parity() { return _thread_claim_parity; }
2184   static void change_thread_claim_parity();
2185   static void assert_all_threads_claimed() NOT_DEBUG_RETURN;
2186 
2187   // Apply "f-&gt;do_oop" to all root oops in all threads.
2188   // This version may only be called by sequential code.
2189   static void oops_do(OopClosure* f, CodeBlobClosure* cf);
2190   // This version may be called by sequential or parallel code.
2191   static void possibly_parallel_oops_do(bool is_par, OopClosure* f, CodeBlobClosure* cf);
2192 
2193   // Apply "f-&gt;do_oop" to roots in all threads that
2194   // are part of compiled frames
2195   static void compiled_frame_oops_do(OopClosure* f, CodeBlobClosure* cf);
2196 
2197   static void convert_hcode_pointers();
2198   static void restore_hcode_pointers();
2199 
2200   // Sweeper
2201   static void nmethods_do(CodeBlobClosure* cf);
2202 
2203   // RedefineClasses support
2204   static void metadata_do(void f(Metadata*));
2205   static void metadata_handles_do(void f(Metadata*));
2206 
2207 #ifdef ASSERT
2208   static bool is_vm_complete() { return _vm_complete; }
2209 
2210   static size_t threads_before_barrier_set() {
2211     return _threads_before_barrier_set;
2212   }
2213 
2214   static void inc_threads_before_barrier_set() {
2215     ++_threads_before_barrier_set;
2216   }
2217 #endif // ASSERT
2218 
2219   // Verification
2220   static void verify();
2221   static void print_on(outputStream* st, bool print_stacks, bool internal_format, bool print_concurrent_locks, bool print_extended_info);
2222   static void print(bool print_stacks, bool internal_format) {
2223     // this function is only used by debug.cpp
2224     print_on(tty, print_stacks, internal_format, false /* no concurrent lock printed */, false /* simple format */);
2225   }
2226   static void print_on_error(outputStream* st, Thread* current, char* buf, int buflen);
2227   static void print_on_error(Thread* this_thread, outputStream* st, Thread* current, char* buf,
2228                              int buflen, bool* found_current);
2229   static void print_threads_compiling(outputStream* st, char* buf, int buflen);
2230 
2231   // Get Java threads that are waiting to enter a monitor.
2232   static GrowableArray&lt;JavaThread*&gt;* get_pending_threads(ThreadsList * t_list,
2233                                                          int count, address monitor);
2234 
2235   // Get owning Java thread from the monitor's owner field.
2236   static JavaThread *owning_thread_from_monitor_owner(ThreadsList * t_list,
2237                                                       address owner);
2238 
2239   // Number of threads on the active threads list
2240   static int number_of_threads()                 { return _number_of_threads; }
2241   // Number of non-daemon threads on the active threads list
2242   static int number_of_non_daemon_threads()      { return _number_of_non_daemon_threads; }
2243 
2244   // Deoptimizes all frames tied to marked nmethods
2245   static void deoptimized_wrt_marked_nmethods();
2246 };
2247 
2248 
2249 // Thread iterator
2250 class ThreadClosure: public StackObj {
2251  public:
2252   virtual void do_thread(Thread* thread) = 0;
2253 };
2254 
2255 class SignalHandlerMark: public StackObj {
2256  private:
2257   Thread* _thread;
2258  public:
2259   SignalHandlerMark(Thread* t) {
2260     _thread = t;
2261     if (_thread) _thread-&gt;enter_signal_handler();
2262   }
2263   ~SignalHandlerMark() {
2264     if (_thread) _thread-&gt;leave_signal_handler();
2265     _thread = NULL;
2266   }
2267 };
2268 
2269 
2270 #endif // SHARE_VM_RUNTIME_THREAD_HPP
</pre></body></html>
