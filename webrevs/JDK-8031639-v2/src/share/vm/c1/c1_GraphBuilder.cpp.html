<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/c1/c1_GraphBuilder.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "c1/c1_CFGPrinter.hpp"
  27 #include "c1/c1_Canonicalizer.hpp"
  28 #include "c1/c1_Compilation.hpp"
  29 #include "c1/c1_GraphBuilder.hpp"
  30 #include "c1/c1_InstructionPrinter.hpp"
  31 #include "ci/ciCallSite.hpp"
  32 #include "ci/ciField.hpp"
  33 #include "ci/ciKlass.hpp"
  34 #include "ci/ciMemberName.hpp"
  35 #include "compiler/compileBroker.hpp"
  36 #include "interpreter/bytecode.hpp"
  37 #include "runtime/sharedRuntime.hpp"
  38 #include "runtime/compilationPolicy.hpp"
  39 #include "utilities/bitMap.inline.hpp"
  40 
  41 class BlockListBuilder VALUE_OBJ_CLASS_SPEC {
  42  private:
  43   Compilation* _compilation;
  44   IRScope*     _scope;
  45 
  46   BlockList    _blocks;                // internal list of all blocks
  47   BlockList*   _bci2block;             // mapping from bci to blocks for GraphBuilder
  48 
  49   // fields used by mark_loops
  50   BitMap       _active;                // for iteration of control flow graph
  51   BitMap       _visited;               // for iteration of control flow graph
  52   intArray     _loop_map;              // caches the information if a block is contained in a loop
  53   int          _next_loop_index;       // next free loop number
  54   int          _next_block_number;     // for reverse postorder numbering of blocks
  55 
  56   // accessors
  57   Compilation*  compilation() const              { return _compilation; }
  58   IRScope*      scope() const                    { return _scope; }
  59   ciMethod*     method() const                   { return scope()-&gt;method(); }
  60   XHandlers*    xhandlers() const                { return scope()-&gt;xhandlers(); }
  61 
  62   // unified bailout support
  63   void          bailout(const char* msg) const   { compilation()-&gt;bailout(msg); }
  64   bool          bailed_out() const               { return compilation()-&gt;bailed_out(); }
  65 
  66   // helper functions
  67   BlockBegin* make_block_at(int bci, BlockBegin* predecessor);
  68   void handle_exceptions(BlockBegin* current, int cur_bci);
  69   void handle_jsr(BlockBegin* current, int sr_bci, int next_bci);
  70   void store_one(BlockBegin* current, int local);
  71   void store_two(BlockBegin* current, int local);
  72   void set_entries(int osr_bci);
  73   void set_leaders();
  74 
  75   void make_loop_header(BlockBegin* block);
  76   void mark_loops();
  77   int  mark_loops(BlockBegin* b, bool in_subroutine);
  78 
  79   // debugging
  80 #ifndef PRODUCT
  81   void print();
  82 #endif
  83 
  84  public:
  85   // creation
  86   BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci);
  87 
  88   // accessors for GraphBuilder
  89   BlockList*    bci2block() const                { return _bci2block; }
  90 };
  91 
  92 
  93 // Implementation of BlockListBuilder
  94 
  95 BlockListBuilder::BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci)
  96  : _compilation(compilation)
  97  , _scope(scope)
  98  , _blocks(16)
  99  , _bci2block(new BlockList(scope-&gt;method()-&gt;code_size(), NULL))
 100  , _next_block_number(0)
 101  , _active()         // size not known yet
 102  , _visited()        // size not known yet
 103  , _next_loop_index(0)
 104  , _loop_map() // size not known yet
 105 {
 106   set_entries(osr_bci);
 107   set_leaders();
 108   CHECK_BAILOUT();
 109 
 110   mark_loops();
 111   NOT_PRODUCT(if (PrintInitialBlockList) print());
 112 
 113 #ifndef PRODUCT
 114   if (PrintCFGToFile) {
 115     stringStream title;
 116     title.print("BlockListBuilder ");
 117     scope-&gt;method()-&gt;print_name(&amp;title);
 118     CFGPrinter::print_cfg(_bci2block, title.as_string(), false, false);
 119   }
 120 #endif
 121 }
 122 
 123 
 124 void BlockListBuilder::set_entries(int osr_bci) {
 125   // generate start blocks
 126   BlockBegin* std_entry = make_block_at(0, NULL);
 127   if (scope()-&gt;caller() == NULL) {
 128     std_entry-&gt;set(BlockBegin::std_entry_flag);
 129   }
 130   if (osr_bci != -1) {
 131     BlockBegin* osr_entry = make_block_at(osr_bci, NULL);
 132     osr_entry-&gt;set(BlockBegin::osr_entry_flag);
 133   }
 134 
 135   // generate exception entry blocks
 136   XHandlers* list = xhandlers();
 137   const int n = list-&gt;length();
 138   for (int i = 0; i &lt; n; i++) {
 139     XHandler* h = list-&gt;handler_at(i);
 140     BlockBegin* entry = make_block_at(h-&gt;handler_bci(), NULL);
 141     entry-&gt;set(BlockBegin::exception_entry_flag);
 142     h-&gt;set_entry_block(entry);
 143   }
 144 }
 145 
 146 
 147 BlockBegin* BlockListBuilder::make_block_at(int cur_bci, BlockBegin* predecessor) {
 148   assert(method()-&gt;bci_block_start().at(cur_bci), "wrong block starts of MethodLivenessAnalyzer");
 149 
 150   BlockBegin* block = _bci2block-&gt;at(cur_bci);
 151   if (block == NULL) {
 152     block = new BlockBegin(cur_bci);
 153     block-&gt;init_stores_to_locals(method()-&gt;max_locals());
 154     _bci2block-&gt;at_put(cur_bci, block);
 155     _blocks.append(block);
 156 
 157     assert(predecessor == NULL || predecessor-&gt;bci() &lt; cur_bci, "targets for backward branches must already exist");
 158   }
 159 
 160   if (predecessor != NULL) {
 161     if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 162       BAILOUT_("Exception handler can be reached by both normal and exceptional control flow", block);
 163     }
 164 
 165     predecessor-&gt;add_successor(block);
 166     block-&gt;increment_total_preds();
 167   }
 168 
 169   return block;
 170 }
 171 
 172 
 173 inline void BlockListBuilder::store_one(BlockBegin* current, int local) {
 174   current-&gt;stores_to_locals().set_bit(local);
 175 }
 176 inline void BlockListBuilder::store_two(BlockBegin* current, int local) {
 177   store_one(current, local);
 178   store_one(current, local + 1);
 179 }
 180 
 181 
 182 void BlockListBuilder::handle_exceptions(BlockBegin* current, int cur_bci) {
 183   // Draws edges from a block to its exception handlers
 184   XHandlers* list = xhandlers();
 185   const int n = list-&gt;length();
 186 
 187   for (int i = 0; i &lt; n; i++) {
 188     XHandler* h = list-&gt;handler_at(i);
 189 
 190     if (h-&gt;covers(cur_bci)) {
 191       BlockBegin* entry = h-&gt;entry_block();
 192       assert(entry != NULL &amp;&amp; entry == _bci2block-&gt;at(h-&gt;handler_bci()), "entry must be set");
 193       assert(entry-&gt;is_set(BlockBegin::exception_entry_flag), "flag must be set");
 194 
 195       // add each exception handler only once
 196       if (!current-&gt;is_successor(entry)) {
 197         current-&gt;add_successor(entry);
 198         entry-&gt;increment_total_preds();
 199       }
 200 
 201       // stop when reaching catchall
 202       if (h-&gt;catch_type() == 0) break;
 203     }
 204   }
 205 }
 206 
 207 void BlockListBuilder::handle_jsr(BlockBegin* current, int sr_bci, int next_bci) {
 208   // start a new block after jsr-bytecode and link this block into cfg
 209   make_block_at(next_bci, current);
 210 
 211   // start a new block at the subroutine entry at mark it with special flag
 212   BlockBegin* sr_block = make_block_at(sr_bci, current);
 213   if (!sr_block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 214     sr_block-&gt;set(BlockBegin::subroutine_entry_flag);
 215   }
 216 }
 217 
 218 
 219 void BlockListBuilder::set_leaders() {
 220   bool has_xhandlers = xhandlers()-&gt;has_handlers();
 221   BlockBegin* current = NULL;
 222 
 223   // The information which bci starts a new block simplifies the analysis
 224   // Without it, backward branches could jump to a bci where no block was created
 225   // during bytecode iteration. This would require the creation of a new block at the
 226   // branch target and a modification of the successor lists.
 227   BitMap bci_block_start = method()-&gt;bci_block_start();
 228 
 229   ciBytecodeStream s(method());
 230   while (s.next() != ciBytecodeStream::EOBC()) {
 231     int cur_bci = s.cur_bci();
 232 
 233     if (bci_block_start.at(cur_bci)) {
 234       current = make_block_at(cur_bci, current);
 235     }
 236     assert(current != NULL, "must have current block");
 237 
 238     if (has_xhandlers &amp;&amp; GraphBuilder::can_trap(method(), s.cur_bc())) {
 239       handle_exceptions(current, cur_bci);
 240     }
 241 
 242     switch (s.cur_bc()) {
 243       // track stores to local variables for selective creation of phi functions
 244       case Bytecodes::_iinc:     store_one(current, s.get_index()); break;
 245       case Bytecodes::_istore:   store_one(current, s.get_index()); break;
 246       case Bytecodes::_lstore:   store_two(current, s.get_index()); break;
 247       case Bytecodes::_fstore:   store_one(current, s.get_index()); break;
 248       case Bytecodes::_dstore:   store_two(current, s.get_index()); break;
 249       case Bytecodes::_astore:   store_one(current, s.get_index()); break;
 250       case Bytecodes::_istore_0: store_one(current, 0); break;
 251       case Bytecodes::_istore_1: store_one(current, 1); break;
 252       case Bytecodes::_istore_2: store_one(current, 2); break;
 253       case Bytecodes::_istore_3: store_one(current, 3); break;
 254       case Bytecodes::_lstore_0: store_two(current, 0); break;
 255       case Bytecodes::_lstore_1: store_two(current, 1); break;
 256       case Bytecodes::_lstore_2: store_two(current, 2); break;
 257       case Bytecodes::_lstore_3: store_two(current, 3); break;
 258       case Bytecodes::_fstore_0: store_one(current, 0); break;
 259       case Bytecodes::_fstore_1: store_one(current, 1); break;
 260       case Bytecodes::_fstore_2: store_one(current, 2); break;
 261       case Bytecodes::_fstore_3: store_one(current, 3); break;
 262       case Bytecodes::_dstore_0: store_two(current, 0); break;
 263       case Bytecodes::_dstore_1: store_two(current, 1); break;
 264       case Bytecodes::_dstore_2: store_two(current, 2); break;
 265       case Bytecodes::_dstore_3: store_two(current, 3); break;
 266       case Bytecodes::_astore_0: store_one(current, 0); break;
 267       case Bytecodes::_astore_1: store_one(current, 1); break;
 268       case Bytecodes::_astore_2: store_one(current, 2); break;
 269       case Bytecodes::_astore_3: store_one(current, 3); break;
 270 
 271       // track bytecodes that affect the control flow
 272       case Bytecodes::_athrow:  // fall through
 273       case Bytecodes::_ret:     // fall through
 274       case Bytecodes::_ireturn: // fall through
 275       case Bytecodes::_lreturn: // fall through
 276       case Bytecodes::_freturn: // fall through
 277       case Bytecodes::_dreturn: // fall through
 278       case Bytecodes::_areturn: // fall through
 279       case Bytecodes::_return:
 280         current = NULL;
 281         break;
 282 
 283       case Bytecodes::_ifeq:      // fall through
 284       case Bytecodes::_ifne:      // fall through
 285       case Bytecodes::_iflt:      // fall through
 286       case Bytecodes::_ifge:      // fall through
 287       case Bytecodes::_ifgt:      // fall through
 288       case Bytecodes::_ifle:      // fall through
 289       case Bytecodes::_if_icmpeq: // fall through
 290       case Bytecodes::_if_icmpne: // fall through
 291       case Bytecodes::_if_icmplt: // fall through
 292       case Bytecodes::_if_icmpge: // fall through
 293       case Bytecodes::_if_icmpgt: // fall through
 294       case Bytecodes::_if_icmple: // fall through
 295       case Bytecodes::_if_acmpeq: // fall through
 296       case Bytecodes::_if_acmpne: // fall through
 297       case Bytecodes::_ifnull:    // fall through
 298       case Bytecodes::_ifnonnull:
 299         make_block_at(s.next_bci(), current);
 300         make_block_at(s.get_dest(), current);
 301         current = NULL;
 302         break;
 303 
 304       case Bytecodes::_goto:
 305         make_block_at(s.get_dest(), current);
 306         current = NULL;
 307         break;
 308 
 309       case Bytecodes::_goto_w:
 310         make_block_at(s.get_far_dest(), current);
 311         current = NULL;
 312         break;
 313 
 314       case Bytecodes::_jsr:
 315         handle_jsr(current, s.get_dest(), s.next_bci());
 316         current = NULL;
 317         break;
 318 
 319       case Bytecodes::_jsr_w:
 320         handle_jsr(current, s.get_far_dest(), s.next_bci());
 321         current = NULL;
 322         break;
 323 
 324       case Bytecodes::_tableswitch: {
 325         // set block for each case
 326         Bytecode_tableswitch sw(&amp;s);
 327         int l = sw.length();
 328         for (int i = 0; i &lt; l; i++) {
 329           make_block_at(cur_bci + sw.dest_offset_at(i), current);
 330         }
 331         make_block_at(cur_bci + sw.default_offset(), current);
 332         current = NULL;
 333         break;
 334       }
 335 
 336       case Bytecodes::_lookupswitch: {
 337         // set block for each case
 338         Bytecode_lookupswitch sw(&amp;s);
 339         int l = sw.number_of_pairs();
 340         for (int i = 0; i &lt; l; i++) {
 341           make_block_at(cur_bci + sw.pair_at(i).offset(), current);
 342         }
 343         make_block_at(cur_bci + sw.default_offset(), current);
 344         current = NULL;
 345         break;
 346       }
 347     }
 348   }
 349 }
 350 
 351 
 352 void BlockListBuilder::mark_loops() {
 353   ResourceMark rm;
 354 
 355   _active = BitMap(BlockBegin::number_of_blocks());         _active.clear();
 356   _visited = BitMap(BlockBegin::number_of_blocks());        _visited.clear();
 357   _loop_map = intArray(BlockBegin::number_of_blocks(), 0);
 358   _next_loop_index = 0;
 359   _next_block_number = _blocks.length();
 360 
 361   // recursively iterate the control flow graph
 362   mark_loops(_bci2block-&gt;at(0), false);
 363   assert(_next_block_number &gt;= 0, "invalid block numbers");
 364 }
 365 
 366 void BlockListBuilder::make_loop_header(BlockBegin* block) {
 367   if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 368     // exception edges may look like loops but don't mark them as such
 369     // since it screws up block ordering.
 370     return;
 371   }
 372   if (!block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 373     block-&gt;set(BlockBegin::parser_loop_header_flag);
 374 
 375     assert(_loop_map.at(block-&gt;block_id()) == 0, "must not be set yet");
 376     assert(0 &lt;= _next_loop_index &amp;&amp; _next_loop_index &lt; BitsPerInt, "_next_loop_index is used as a bit-index in integer");
 377     _loop_map.at_put(block-&gt;block_id(), 1 &lt;&lt; _next_loop_index);
 378     if (_next_loop_index &lt; 31) _next_loop_index++;
 379   } else {
 380     // block already marked as loop header
 381     assert(is_power_of_2((unsigned int)_loop_map.at(block-&gt;block_id())), "exactly one bit must be set");
 382   }
 383 }
 384 
 385 int BlockListBuilder::mark_loops(BlockBegin* block, bool in_subroutine) {
 386   int block_id = block-&gt;block_id();
 387 
 388   if (_visited.at(block_id)) {
 389     if (_active.at(block_id)) {
 390       // reached block via backward branch
 391       make_loop_header(block);
 392     }
 393     // return cached loop information for this block
 394     return _loop_map.at(block_id);
 395   }
 396 
 397   if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 398     in_subroutine = true;
 399   }
 400 
 401   // set active and visited bits before successors are processed
 402   _visited.set_bit(block_id);
 403   _active.set_bit(block_id);
 404 
 405   intptr_t loop_state = 0;
 406   for (int i = block-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
 407     // recursively process all successors
 408     loop_state |= mark_loops(block-&gt;sux_at(i), in_subroutine);
 409   }
 410 
 411   // clear active-bit after all successors are processed
 412   _active.clear_bit(block_id);
 413 
 414   // reverse-post-order numbering of all blocks
 415   block-&gt;set_depth_first_number(_next_block_number);
 416   _next_block_number--;
 417 
 418   if (loop_state != 0 || in_subroutine ) {
 419     // block is contained at least in one loop, so phi functions are necessary
 420     // phi functions are also necessary for all locals stored in a subroutine
 421     scope()-&gt;requires_phi_function().set_union(block-&gt;stores_to_locals());
 422   }
 423 
 424   if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 425     int header_loop_state = _loop_map.at(block_id);
 426     assert(is_power_of_2((unsigned)header_loop_state), "exactly one bit must be set");
 427 
 428     // If the highest bit is set (i.e. when integer value is negative), the method
 429     // has 32 or more loops. This bit is never cleared because it is used for multiple loops
 430     if (header_loop_state &gt;= 0) {
 431       clear_bits(loop_state, header_loop_state);
 432     }
 433   }
 434 
 435   // cache and return loop information for this block
 436   _loop_map.at_put(block_id, loop_state);
 437   return loop_state;
 438 }
 439 
 440 
 441 #ifndef PRODUCT
 442 
 443 int compare_depth_first(BlockBegin** a, BlockBegin** b) {
 444   return (*a)-&gt;depth_first_number() - (*b)-&gt;depth_first_number();
 445 }
 446 
 447 void BlockListBuilder::print() {
 448   tty-&gt;print("----- initial block list of BlockListBuilder for method ");
 449   method()-&gt;print_short_name();
 450   tty-&gt;cr();
 451 
 452   // better readability if blocks are sorted in processing order
 453   _blocks.sort(compare_depth_first);
 454 
 455   for (int i = 0; i &lt; _blocks.length(); i++) {
 456     BlockBegin* cur = _blocks.at(i);
 457     tty-&gt;print("%4d: B%-4d bci: %-4d  preds: %-4d ", cur-&gt;depth_first_number(), cur-&gt;block_id(), cur-&gt;bci(), cur-&gt;total_preds());
 458 
 459     tty-&gt;print(cur-&gt;is_set(BlockBegin::std_entry_flag)               ? " std" : "    ");
 460     tty-&gt;print(cur-&gt;is_set(BlockBegin::osr_entry_flag)               ? " osr" : "    ");
 461     tty-&gt;print(cur-&gt;is_set(BlockBegin::exception_entry_flag)         ? " ex" : "   ");
 462     tty-&gt;print(cur-&gt;is_set(BlockBegin::subroutine_entry_flag)        ? " sr" : "   ");
 463     tty-&gt;print(cur-&gt;is_set(BlockBegin::parser_loop_header_flag)      ? " lh" : "   ");
 464 
 465     if (cur-&gt;number_of_sux() &gt; 0) {
 466       tty-&gt;print("    sux: ");
 467       for (int j = 0; j &lt; cur-&gt;number_of_sux(); j++) {
 468         BlockBegin* sux = cur-&gt;sux_at(j);
 469         tty-&gt;print("B%d ", sux-&gt;block_id());
 470       }
 471     }
 472     tty-&gt;cr();
 473   }
 474 }
 475 
 476 #endif
 477 
 478 
 479 // A simple growable array of Values indexed by ciFields
 480 class FieldBuffer: public CompilationResourceObj {
 481  private:
 482   GrowableArray&lt;Value&gt; _values;
 483 
 484  public:
 485   FieldBuffer() {}
 486 
 487   void kill() {
 488     _values.trunc_to(0);
 489   }
 490 
 491   Value at(ciField* field) {
 492     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 493     int offset = field-&gt;offset();
 494     if (offset &lt; _values.length()) {
 495       return _values.at(offset);
 496     } else {
 497       return NULL;
 498     }
 499   }
 500 
 501   void at_put(ciField* field, Value value) {
 502     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 503     int offset = field-&gt;offset();
 504     _values.at_put_grow(offset, value, NULL);
 505   }
 506 
 507 };
 508 
 509 
 510 // MemoryBuffer is fairly simple model of the current state of memory.
 511 // It partitions memory into several pieces.  The first piece is
 512 // generic memory where little is known about the owner of the memory.
 513 // This is conceptually represented by the tuple &lt;O, F, V&gt; which says
 514 // that the field F of object O has value V.  This is flattened so
 515 // that F is represented by the offset of the field and the parallel
 516 // arrays _objects and _values are used for O and V.  Loads of O.F can
 517 // simply use V.  Newly allocated objects are kept in a separate list
 518 // along with a parallel array for each object which represents the
 519 // current value of its fields.  Stores of the default value to fields
 520 // which have never been stored to before are eliminated since they
 521 // are redundant.  Once newly allocated objects are stored into
 522 // another object or they are passed out of the current compile they
 523 // are treated like generic memory.
 524 
 525 class MemoryBuffer: public CompilationResourceObj {
 526  private:
 527   FieldBuffer                 _values;
 528   GrowableArray&lt;Value&gt;        _objects;
 529   GrowableArray&lt;Value&gt;        _newobjects;
 530   GrowableArray&lt;FieldBuffer*&gt; _fields;
 531 
 532  public:
 533   MemoryBuffer() {}
 534 
 535   StoreField* store(StoreField* st) {
 536     if (!EliminateFieldAccess) {
 537       return st;
 538     }
 539 
 540     Value object = st-&gt;obj();
 541     Value value = st-&gt;value();
 542     ciField* field = st-&gt;field();
 543     if (field-&gt;holder()-&gt;is_loaded()) {
 544       int offset = field-&gt;offset();
 545       int index = _newobjects.find(object);
 546       if (index != -1) {
 547         // newly allocated object with no other stores performed on this field
 548         FieldBuffer* buf = _fields.at(index);
 549         if (buf-&gt;at(field) == NULL &amp;&amp; is_default_value(value)) {
 550 #ifndef PRODUCT
 551           if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 552             tty-&gt;print_cr("Eliminated store for object %d:", index);
 553             st-&gt;print_line();
 554           }
 555 #endif
 556           return NULL;
 557         } else {
 558           buf-&gt;at_put(field, value);
 559         }
 560       } else {
 561         _objects.at_put_grow(offset, object, NULL);
 562         _values.at_put(field, value);
 563       }
 564 
 565       store_value(value);
 566     } else {
 567       // if we held onto field names we could alias based on names but
 568       // we don't know what's being stored to so kill it all.
 569       kill();
 570     }
 571     return st;
 572   }
 573 
 574 
 575   // return true if this value correspond to the default value of a field.
 576   bool is_default_value(Value value) {
 577     Constant* con = value-&gt;as_Constant();
 578     if (con) {
 579       switch (con-&gt;type()-&gt;tag()) {
 580         case intTag:    return con-&gt;type()-&gt;as_IntConstant()-&gt;value() == 0;
 581         case longTag:   return con-&gt;type()-&gt;as_LongConstant()-&gt;value() == 0;
 582         case floatTag:  return jint_cast(con-&gt;type()-&gt;as_FloatConstant()-&gt;value()) == 0;
 583         case doubleTag: return jlong_cast(con-&gt;type()-&gt;as_DoubleConstant()-&gt;value()) == jlong_cast(0);
 584         case objectTag: return con-&gt;type() == objectNull;
 585         default:  ShouldNotReachHere();
 586       }
 587     }
 588     return false;
 589   }
 590 
 591 
 592   // return either the actual value of a load or the load itself
 593   Value load(LoadField* load) {
 594     if (!EliminateFieldAccess) {
 595       return load;
 596     }
 597 
 598     if (RoundFPResults &amp;&amp; UseSSE &lt; 2 &amp;&amp; load-&gt;type()-&gt;is_float_kind()) {
 599       // can't skip load since value might get rounded as a side effect
 600       return load;
 601     }
 602 
 603     ciField* field = load-&gt;field();
 604     Value object   = load-&gt;obj();
 605     if (field-&gt;holder()-&gt;is_loaded() &amp;&amp; !field-&gt;is_volatile()) {
 606       int offset = field-&gt;offset();
 607       Value result = NULL;
 608       int index = _newobjects.find(object);
 609       if (index != -1) {
 610         result = _fields.at(index)-&gt;at(field);
 611       } else if (_objects.at_grow(offset, NULL) == object) {
 612         result = _values.at(field);
 613       }
 614       if (result != NULL) {
 615 #ifndef PRODUCT
 616         if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 617           tty-&gt;print_cr("Eliminated load: ");
 618           load-&gt;print_line();
 619         }
 620 #endif
 621         assert(result-&gt;type()-&gt;tag() == load-&gt;type()-&gt;tag(), "wrong types");
 622         return result;
 623       }
 624     }
 625     return load;
 626   }
 627 
 628   // Record this newly allocated object
 629   void new_instance(NewInstance* object) {
 630     int index = _newobjects.length();
 631     _newobjects.append(object);
 632     if (_fields.at_grow(index, NULL) == NULL) {
 633       _fields.at_put(index, new FieldBuffer());
 634     } else {
 635       _fields.at(index)-&gt;kill();
 636     }
 637   }
 638 
 639   void store_value(Value value) {
 640     int index = _newobjects.find(value);
 641     if (index != -1) {
 642       // stored a newly allocated object into another object.
 643       // Assume we've lost track of it as separate slice of memory.
 644       // We could do better by keeping track of whether individual
 645       // fields could alias each other.
 646       _newobjects.remove_at(index);
 647       // pull out the field info and store it at the end up the list
 648       // of field info list to be reused later.
 649       _fields.append(_fields.at(index));
 650       _fields.remove_at(index);
 651     }
 652   }
 653 
 654   void kill() {
 655     _newobjects.trunc_to(0);
 656     _objects.trunc_to(0);
 657     _values.kill();
 658   }
 659 };
 660 
 661 
 662 // Implementation of GraphBuilder's ScopeData
 663 
 664 GraphBuilder::ScopeData::ScopeData(ScopeData* parent)
 665   : _parent(parent)
 666   , _bci2block(NULL)
 667   , _scope(NULL)
 668   , _has_handler(false)
 669   , _stream(NULL)
 670   , _work_list(NULL)
 671   , _parsing_jsr(false)
 672   , _jsr_xhandlers(NULL)
 673   , _caller_stack_size(-1)
 674   , _continuation(NULL)
 675   , _num_returns(0)
 676   , _cleanup_block(NULL)
 677   , _cleanup_return_prev(NULL)
 678   , _cleanup_state(NULL)
 679 {
 680   if (parent != NULL) {
 681     _max_inline_size = (intx) ((float) NestedInliningSizeRatio * (float) parent-&gt;max_inline_size() / 100.0f);
 682   } else {
 683     _max_inline_size = MaxInlineSize;
 684   }
 685   if (_max_inline_size &lt; MaxTrivialSize) {
 686     _max_inline_size = MaxTrivialSize;
 687   }
 688 }
 689 
 690 
 691 void GraphBuilder::kill_all() {
 692   if (UseLocalValueNumbering) {
 693     vmap()-&gt;kill_all();
 694   }
 695   _memory-&gt;kill();
 696 }
 697 
 698 
 699 BlockBegin* GraphBuilder::ScopeData::block_at(int bci) {
 700   if (parsing_jsr()) {
 701     // It is necessary to clone all blocks associated with a
 702     // subroutine, including those for exception handlers in the scope
 703     // of the method containing the jsr (because those exception
 704     // handlers may contain ret instructions in some cases).
 705     BlockBegin* block = bci2block()-&gt;at(bci);
 706     if (block != NULL &amp;&amp; block == parent()-&gt;bci2block()-&gt;at(bci)) {
 707       BlockBegin* new_block = new BlockBegin(block-&gt;bci());
 708 #ifndef PRODUCT
 709       if (PrintInitialBlockList) {
 710         tty-&gt;print_cr("CFG: cloned block %d (bci %d) as block %d for jsr",
 711                       block-&gt;block_id(), block-&gt;bci(), new_block-&gt;block_id());
 712       }
 713 #endif
 714       // copy data from cloned blocked
 715       new_block-&gt;set_depth_first_number(block-&gt;depth_first_number());
 716       if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) new_block-&gt;set(BlockBegin::parser_loop_header_flag);
 717       // Preserve certain flags for assertion checking
 718       if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) new_block-&gt;set(BlockBegin::subroutine_entry_flag);
 719       if (block-&gt;is_set(BlockBegin::exception_entry_flag))  new_block-&gt;set(BlockBegin::exception_entry_flag);
 720 
 721       // copy was_visited_flag to allow early detection of bailouts
 722       // if a block that is used in a jsr has already been visited before,
 723       // it is shared between the normal control flow and a subroutine
 724       // BlockBegin::try_merge returns false when the flag is set, this leads
 725       // to a compilation bailout
 726       if (block-&gt;is_set(BlockBegin::was_visited_flag))  new_block-&gt;set(BlockBegin::was_visited_flag);
 727 
 728       bci2block()-&gt;at_put(bci, new_block);
 729       block = new_block;
 730     }
 731     return block;
 732   } else {
 733     return bci2block()-&gt;at(bci);
 734   }
 735 }
 736 
 737 
 738 XHandlers* GraphBuilder::ScopeData::xhandlers() const {
 739   if (_jsr_xhandlers == NULL) {
 740     assert(!parsing_jsr(), "");
 741     return scope()-&gt;xhandlers();
 742   }
 743   assert(parsing_jsr(), "");
 744   return _jsr_xhandlers;
 745 }
 746 
 747 
 748 void GraphBuilder::ScopeData::set_scope(IRScope* scope) {
 749   _scope = scope;
 750   bool parent_has_handler = false;
 751   if (parent() != NULL) {
 752     parent_has_handler = parent()-&gt;has_handler();
 753   }
 754   _has_handler = parent_has_handler || scope-&gt;xhandlers()-&gt;has_handlers();
 755 }
 756 
 757 
 758 void GraphBuilder::ScopeData::set_inline_cleanup_info(BlockBegin* block,
 759                                                       Instruction* return_prev,
 760                                                       ValueStack* return_state) {
 761   _cleanup_block       = block;
 762   _cleanup_return_prev = return_prev;
 763   _cleanup_state       = return_state;
 764 }
 765 
 766 
 767 void GraphBuilder::ScopeData::add_to_work_list(BlockBegin* block) {
 768   if (_work_list == NULL) {
 769     _work_list = new BlockList();
 770   }
 771 
 772   if (!block-&gt;is_set(BlockBegin::is_on_work_list_flag)) {
 773     // Do not start parsing the continuation block while in a
 774     // sub-scope
 775     if (parsing_jsr()) {
 776       if (block == jsr_continuation()) {
 777         return;
 778       }
 779     } else {
 780       if (block == continuation()) {
 781         return;
 782       }
 783     }
 784     block-&gt;set(BlockBegin::is_on_work_list_flag);
 785     _work_list-&gt;push(block);
 786 
 787     sort_top_into_worklist(_work_list, block);
 788   }
 789 }
 790 
 791 
 792 void GraphBuilder::sort_top_into_worklist(BlockList* worklist, BlockBegin* top) {
 793   assert(worklist-&gt;top() == top, "");
 794   // sort block descending into work list
 795   const int dfn = top-&gt;depth_first_number();
 796   assert(dfn != -1, "unknown depth first number");
 797   int i = worklist-&gt;length()-2;
 798   while (i &gt;= 0) {
 799     BlockBegin* b = worklist-&gt;at(i);
 800     if (b-&gt;depth_first_number() &lt; dfn) {
 801       worklist-&gt;at_put(i+1, b);
 802     } else {
 803       break;
 804     }
 805     i --;
 806   }
 807   if (i &gt;= -1) worklist-&gt;at_put(i + 1, top);
 808 }
 809 
 810 
 811 BlockBegin* GraphBuilder::ScopeData::remove_from_work_list() {
 812   if (is_work_list_empty()) {
 813     return NULL;
 814   }
 815   return _work_list-&gt;pop();
 816 }
 817 
 818 
 819 bool GraphBuilder::ScopeData::is_work_list_empty() const {
 820   return (_work_list == NULL || _work_list-&gt;length() == 0);
 821 }
 822 
 823 
 824 void GraphBuilder::ScopeData::setup_jsr_xhandlers() {
 825   assert(parsing_jsr(), "");
 826   // clone all the exception handlers from the scope
 827   XHandlers* handlers = new XHandlers(scope()-&gt;xhandlers());
 828   const int n = handlers-&gt;length();
 829   for (int i = 0; i &lt; n; i++) {
 830     // The XHandlers need to be adjusted to dispatch to the cloned
 831     // handler block instead of the default one but the synthetic
 832     // unlocker needs to be handled specially.  The synthetic unlocker
 833     // should be left alone since there can be only one and all code
 834     // should dispatch to the same one.
 835     XHandler* h = handlers-&gt;handler_at(i);
 836     assert(h-&gt;handler_bci() != SynchronizationEntryBCI, "must be real");
 837     h-&gt;set_entry_block(block_at(h-&gt;handler_bci()));
 838   }
 839   _jsr_xhandlers = handlers;
 840 }
 841 
 842 
 843 int GraphBuilder::ScopeData::num_returns() {
 844   if (parsing_jsr()) {
 845     return parent()-&gt;num_returns();
 846   }
 847   return _num_returns;
 848 }
 849 
 850 
 851 void GraphBuilder::ScopeData::incr_num_returns() {
 852   if (parsing_jsr()) {
 853     parent()-&gt;incr_num_returns();
 854   } else {
 855     ++_num_returns;
 856   }
 857 }
 858 
 859 
 860 // Implementation of GraphBuilder
 861 
 862 #define INLINE_BAILOUT(msg)        { inline_bailout(msg); return false; }
 863 
 864 
 865 void GraphBuilder::load_constant() {
 866   ciConstant con = stream()-&gt;get_constant();
 867   if (con.basic_type() == T_ILLEGAL) {
 868     BAILOUT("could not resolve a constant");
 869   } else {
 870     ValueType* t = illegalType;
 871     ValueStack* patch_state = NULL;
 872     switch (con.basic_type()) {
 873       case T_BOOLEAN: t = new IntConstant     (con.as_boolean()); break;
 874       case T_BYTE   : t = new IntConstant     (con.as_byte   ()); break;
 875       case T_CHAR   : t = new IntConstant     (con.as_char   ()); break;
 876       case T_SHORT  : t = new IntConstant     (con.as_short  ()); break;
 877       case T_INT    : t = new IntConstant     (con.as_int    ()); break;
 878       case T_LONG   : t = new LongConstant    (con.as_long   ()); break;
 879       case T_FLOAT  : t = new FloatConstant   (con.as_float  ()); break;
 880       case T_DOUBLE : t = new DoubleConstant  (con.as_double ()); break;
 881       case T_ARRAY  : t = new ArrayConstant   (con.as_object ()-&gt;as_array   ()); break;
 882       case T_OBJECT :
 883        {
 884         ciObject* obj = con.as_object();
 885         if (!obj-&gt;is_loaded()
 886             || (PatchALot &amp;&amp; obj-&gt;klass() != ciEnv::current()-&gt;String_klass())) {
 887           patch_state = copy_state_before();
 888           t = new ObjectConstant(obj);
 889         } else {
 890           assert(obj-&gt;is_instance(), "must be java_mirror of klass");
 891           t = new InstanceConstant(obj-&gt;as_instance());
 892         }
 893         break;
 894        }
 895       default       : ShouldNotReachHere();
 896     }
 897     Value x;
 898     if (patch_state != NULL) {
 899       x = new Constant(t, patch_state);
 900     } else {
 901       x = new Constant(t);
 902     }
 903     push(t, append(x));
 904   }
 905 }
 906 
 907 
 908 void GraphBuilder::load_local(ValueType* type, int index) {
 909   Value x = state()-&gt;local_at(index);
 910   assert(x != NULL &amp;&amp; !x-&gt;type()-&gt;is_illegal(), "access of illegal local variable");
 911   push(type, x);
 912 }
 913 
 914 
 915 void GraphBuilder::store_local(ValueType* type, int index) {
 916   Value x = pop(type);
 917   store_local(state(), x, index);
 918 }
 919 
 920 
 921 void GraphBuilder::store_local(ValueStack* state, Value x, int index) {
 922   if (parsing_jsr()) {
 923     // We need to do additional tracking of the location of the return
 924     // address for jsrs since we don't handle arbitrary jsr/ret
 925     // constructs. Here we are figuring out in which circumstances we
 926     // need to bail out.
 927     if (x-&gt;type()-&gt;is_address()) {
 928       scope_data()-&gt;set_jsr_return_address_local(index);
 929 
 930       // Also check parent jsrs (if any) at this time to see whether
 931       // they are using this local. We don't handle skipping over a
 932       // ret.
 933       for (ScopeData* cur_scope_data = scope_data()-&gt;parent();
 934            cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
 935            cur_scope_data = cur_scope_data-&gt;parent()) {
 936         if (cur_scope_data-&gt;jsr_return_address_local() == index) {
 937           BAILOUT("subroutine overwrites return address from previous subroutine");
 938         }
 939       }
 940     } else if (index == scope_data()-&gt;jsr_return_address_local()) {
 941       scope_data()-&gt;set_jsr_return_address_local(-1);
 942     }
 943   }
 944 
 945   state-&gt;store_local(index, round_fp(x));
 946 }
 947 
 948 
 949 void GraphBuilder::load_indexed(BasicType type) {
 950   // In case of in block code motion in range check elimination
 951   ValueStack* state_before = copy_state_indexed_access();
 952   compilation()-&gt;set_has_access_indexed(true);
 953   Value index = ipop();
 954   Value array = apop();
 955   Value length = NULL;
 956   if (CSEArrayLength ||
 957       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 958       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 959     length = append(new ArrayLength(array, state_before));
 960   }
 961   push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));
 962 }
 963 
 964 
 965 void GraphBuilder::store_indexed(BasicType type) {
 966   // In case of in block code motion in range check elimination
 967   ValueStack* state_before = copy_state_indexed_access();
 968   compilation()-&gt;set_has_access_indexed(true);
 969   Value value = pop(as_ValueType(type));
 970   Value index = ipop();
 971   Value array = apop();
 972   Value length = NULL;
 973   if (CSEArrayLength ||
 974       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 975       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 976     length = append(new ArrayLength(array, state_before));
 977   }
 978   StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before);
 979   append(result);
 980   _memory-&gt;store_value(value);
 981 
 982   if (type == T_OBJECT &amp;&amp; is_profiling()) {
 983     // Note that we'd collect profile data in this method if we wanted it.
 984     compilation()-&gt;set_would_profile(true);
 985 
 986     if (profile_checkcasts()) {
 987       result-&gt;set_profiled_method(method());
 988       result-&gt;set_profiled_bci(bci());
 989       result-&gt;set_should_profile(true);
 990     }
 991   }
 992 }
 993 
 994 
 995 void GraphBuilder::stack_op(Bytecodes::Code code) {
 996   switch (code) {
 997     case Bytecodes::_pop:
 998       { state()-&gt;raw_pop();
 999       }
1000       break;
1001     case Bytecodes::_pop2:
1002       { state()-&gt;raw_pop();
1003         state()-&gt;raw_pop();
1004       }
1005       break;
1006     case Bytecodes::_dup:
1007       { Value w = state()-&gt;raw_pop();
1008         state()-&gt;raw_push(w);
1009         state()-&gt;raw_push(w);
1010       }
1011       break;
1012     case Bytecodes::_dup_x1:
1013       { Value w1 = state()-&gt;raw_pop();
1014         Value w2 = state()-&gt;raw_pop();
1015         state()-&gt;raw_push(w1);
1016         state()-&gt;raw_push(w2);
1017         state()-&gt;raw_push(w1);
1018       }
1019       break;
1020     case Bytecodes::_dup_x2:
1021       { Value w1 = state()-&gt;raw_pop();
1022         Value w2 = state()-&gt;raw_pop();
1023         Value w3 = state()-&gt;raw_pop();
1024         state()-&gt;raw_push(w1);
1025         state()-&gt;raw_push(w3);
1026         state()-&gt;raw_push(w2);
1027         state()-&gt;raw_push(w1);
1028       }
1029       break;
1030     case Bytecodes::_dup2:
1031       { Value w1 = state()-&gt;raw_pop();
1032         Value w2 = state()-&gt;raw_pop();
1033         state()-&gt;raw_push(w2);
1034         state()-&gt;raw_push(w1);
1035         state()-&gt;raw_push(w2);
1036         state()-&gt;raw_push(w1);
1037       }
1038       break;
1039     case Bytecodes::_dup2_x1:
1040       { Value w1 = state()-&gt;raw_pop();
1041         Value w2 = state()-&gt;raw_pop();
1042         Value w3 = state()-&gt;raw_pop();
1043         state()-&gt;raw_push(w2);
1044         state()-&gt;raw_push(w1);
1045         state()-&gt;raw_push(w3);
1046         state()-&gt;raw_push(w2);
1047         state()-&gt;raw_push(w1);
1048       }
1049       break;
1050     case Bytecodes::_dup2_x2:
1051       { Value w1 = state()-&gt;raw_pop();
1052         Value w2 = state()-&gt;raw_pop();
1053         Value w3 = state()-&gt;raw_pop();
1054         Value w4 = state()-&gt;raw_pop();
1055         state()-&gt;raw_push(w2);
1056         state()-&gt;raw_push(w1);
1057         state()-&gt;raw_push(w4);
1058         state()-&gt;raw_push(w3);
1059         state()-&gt;raw_push(w2);
1060         state()-&gt;raw_push(w1);
1061       }
1062       break;
1063     case Bytecodes::_swap:
1064       { Value w1 = state()-&gt;raw_pop();
1065         Value w2 = state()-&gt;raw_pop();
1066         state()-&gt;raw_push(w1);
1067         state()-&gt;raw_push(w2);
1068       }
1069       break;
1070     default:
1071       ShouldNotReachHere();
1072       break;
1073   }
1074 }
1075 
1076 
1077 void GraphBuilder::arithmetic_op(ValueType* type, Bytecodes::Code code, ValueStack* state_before) {
1078   Value y = pop(type);
1079   Value x = pop(type);
1080   // NOTE: strictfp can be queried from current method since we don't
1081   // inline methods with differing strictfp bits
1082   Value res = new ArithmeticOp(code, x, y, method()-&gt;is_strict(), state_before);
1083   // Note: currently single-precision floating-point rounding on Intel is handled at the LIRGenerator level
1084   res = append(res);
1085   if (method()-&gt;is_strict()) {
1086     res = round_fp(res);
1087   }
1088   push(type, res);
1089 }
1090 
1091 
1092 void GraphBuilder::negate_op(ValueType* type) {
1093   push(type, append(new NegateOp(pop(type))));
1094 }
1095 
1096 
1097 void GraphBuilder::shift_op(ValueType* type, Bytecodes::Code code) {
1098   Value s = ipop();
1099   Value x = pop(type);
1100   // try to simplify
1101   // Note: This code should go into the canonicalizer as soon as it can
1102   //       can handle canonicalized forms that contain more than one node.
1103   if (CanonicalizeNodes &amp;&amp; code == Bytecodes::_iushr) {
1104     // pattern: x &gt;&gt;&gt; s
1105     IntConstant* s1 = s-&gt;type()-&gt;as_IntConstant();
1106     if (s1 != NULL) {
1107       // pattern: x &gt;&gt;&gt; s1, with s1 constant
1108       ShiftOp* l = x-&gt;as_ShiftOp();
1109       if (l != NULL &amp;&amp; l-&gt;op() == Bytecodes::_ishl) {
1110         // pattern: (a &lt;&lt; b) &gt;&gt;&gt; s1
1111         IntConstant* s0 = l-&gt;y()-&gt;type()-&gt;as_IntConstant();
1112         if (s0 != NULL) {
1113           // pattern: (a &lt;&lt; s0) &gt;&gt;&gt; s1
1114           const int s0c = s0-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1115           const int s1c = s1-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1116           if (s0c == s1c) {
1117             if (s0c == 0) {
1118               // pattern: (a &lt;&lt; 0) &gt;&gt;&gt; 0 =&gt; simplify to: a
1119               ipush(l-&gt;x());
1120             } else {
1121               // pattern: (a &lt;&lt; s0c) &gt;&gt;&gt; s0c =&gt; simplify to: a &amp; m, with m constant
1122               assert(0 &lt; s0c &amp;&amp; s0c &lt; BitsPerInt, "adjust code below to handle corner cases");
1123               const int m = (1 &lt;&lt; (BitsPerInt - s0c)) - 1;
1124               Value s = append(new Constant(new IntConstant(m)));
1125               ipush(append(new LogicOp(Bytecodes::_iand, l-&gt;x(), s)));
1126             }
1127             return;
1128           }
1129         }
1130       }
1131     }
1132   }
1133   // could not simplify
1134   push(type, append(new ShiftOp(code, x, s)));
1135 }
1136 
1137 
1138 void GraphBuilder::logic_op(ValueType* type, Bytecodes::Code code) {
1139   Value y = pop(type);
1140   Value x = pop(type);
1141   push(type, append(new LogicOp(code, x, y)));
1142 }
1143 
1144 
1145 void GraphBuilder::compare_op(ValueType* type, Bytecodes::Code code) {
1146   ValueStack* state_before = copy_state_before();
1147   Value y = pop(type);
1148   Value x = pop(type);
1149   ipush(append(new CompareOp(code, x, y, state_before)));
1150 }
1151 
1152 
1153 void GraphBuilder::convert(Bytecodes::Code op, BasicType from, BasicType to) {
1154   push(as_ValueType(to), append(new Convert(op, pop(as_ValueType(from)), as_ValueType(to))));
1155 }
1156 
1157 
1158 void GraphBuilder::increment() {
1159   int index = stream()-&gt;get_index();
1160   int delta = stream()-&gt;is_wide() ? (signed short)Bytes::get_Java_u2(stream()-&gt;cur_bcp() + 4) : (signed char)(stream()-&gt;cur_bcp()[2]);
1161   load_local(intType, index);
1162   ipush(append(new Constant(new IntConstant(delta))));
1163   arithmetic_op(intType, Bytecodes::_iadd);
1164   store_local(intType, index);
1165 }
1166 
1167 
1168 void GraphBuilder::_goto(int from_bci, int to_bci) {
1169   Goto *x = new Goto(block_at(to_bci), to_bci &lt;= from_bci);
1170   if (is_profiling()) {
1171     compilation()-&gt;set_would_profile(true);
1172     x-&gt;set_profiled_bci(bci());
1173     if (profile_branches()) {
1174       x-&gt;set_profiled_method(method());
1175       x-&gt;set_should_profile(true);
1176     }
1177   }
1178   append(x);
1179 }
1180 
1181 
1182 void GraphBuilder::if_node(Value x, If::Condition cond, Value y, ValueStack* state_before) {
1183   BlockBegin* tsux = block_at(stream()-&gt;get_dest());
1184   BlockBegin* fsux = block_at(stream()-&gt;next_bci());
1185   bool is_bb = tsux-&gt;bci() &lt; stream()-&gt;cur_bci() || fsux-&gt;bci() &lt; stream()-&gt;cur_bci();
1186   // In case of loop invariant code motion or predicate insertion
1187   // before the body of a loop the state is needed
1188   Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()-&gt;is_optimistic()) ? state_before : NULL, is_bb));
1189 
1190   assert(i-&gt;as_Goto() == NULL ||
1191          (i-&gt;as_Goto()-&gt;sux_at(0) == tsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == tsux-&gt;bci() &lt; stream()-&gt;cur_bci()) ||
1192          (i-&gt;as_Goto()-&gt;sux_at(0) == fsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == fsux-&gt;bci() &lt; stream()-&gt;cur_bci()),
1193          "safepoint state of Goto returned by canonicalizer incorrect");
1194 
1195   if (is_profiling()) {
1196     If* if_node = i-&gt;as_If();
1197     if (if_node != NULL) {
1198       // Note that we'd collect profile data in this method if we wanted it.
1199       compilation()-&gt;set_would_profile(true);
1200       // At level 2 we need the proper bci to count backedges
1201       if_node-&gt;set_profiled_bci(bci());
1202       if (profile_branches()) {
1203         // Successors can be rotated by the canonicalizer, check for this case.
1204         if_node-&gt;set_profiled_method(method());
1205         if_node-&gt;set_should_profile(true);
1206         if (if_node-&gt;tsux() == fsux) {
1207           if_node-&gt;set_swapped(true);
1208         }
1209       }
1210       return;
1211     }
1212 
1213     // Check if this If was reduced to Goto.
1214     Goto *goto_node = i-&gt;as_Goto();
1215     if (goto_node != NULL) {
1216       compilation()-&gt;set_would_profile(true);
1217       goto_node-&gt;set_profiled_bci(bci());
1218       if (profile_branches()) {
1219         goto_node-&gt;set_profiled_method(method());
1220         goto_node-&gt;set_should_profile(true);
1221         // Find out which successor is used.
1222         if (goto_node-&gt;default_sux() == tsux) {
1223           goto_node-&gt;set_direction(Goto::taken);
1224         } else if (goto_node-&gt;default_sux() == fsux) {
1225           goto_node-&gt;set_direction(Goto::not_taken);
1226         } else {
1227           ShouldNotReachHere();
1228         }
1229       }
1230       return;
1231     }
1232   }
1233 }
1234 
1235 
1236 void GraphBuilder::if_zero(ValueType* type, If::Condition cond) {
1237   Value y = append(new Constant(intZero));
1238   ValueStack* state_before = copy_state_before();
1239   Value x = ipop();
1240   if_node(x, cond, y, state_before);
1241 }
1242 
1243 
1244 void GraphBuilder::if_null(ValueType* type, If::Condition cond) {
1245   Value y = append(new Constant(objectNull));
1246   ValueStack* state_before = copy_state_before();
1247   Value x = apop();
1248   if_node(x, cond, y, state_before);
1249 }
1250 
1251 
1252 void GraphBuilder::if_same(ValueType* type, If::Condition cond) {
1253   ValueStack* state_before = copy_state_before();
1254   Value y = pop(type);
1255   Value x = pop(type);
1256   if_node(x, cond, y, state_before);
1257 }
1258 
1259 
1260 void GraphBuilder::jsr(int dest) {
1261   // We only handle well-formed jsrs (those which are "block-structured").
1262   // If the bytecodes are strange (jumping out of a jsr block) then we
1263   // might end up trying to re-parse a block containing a jsr which
1264   // has already been activated. Watch for this case and bail out.
1265   for (ScopeData* cur_scope_data = scope_data();
1266        cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
1267        cur_scope_data = cur_scope_data-&gt;parent()) {
1268     if (cur_scope_data-&gt;jsr_entry_bci() == dest) {
1269       BAILOUT("too-complicated jsr/ret structure");
1270     }
1271   }
1272 
1273   push(addressType, append(new Constant(new AddressConstant(next_bci()))));
1274   if (!try_inline_jsr(dest)) {
1275     return; // bailed out while parsing and inlining subroutine
1276   }
1277 }
1278 
1279 
1280 void GraphBuilder::ret(int local_index) {
1281   if (!parsing_jsr()) BAILOUT("ret encountered while not parsing subroutine");
1282 
1283   if (local_index != scope_data()-&gt;jsr_return_address_local()) {
1284     BAILOUT("can not handle complicated jsr/ret constructs");
1285   }
1286 
1287   // Rets simply become (NON-SAFEPOINT) gotos to the jsr continuation
1288   append(new Goto(scope_data()-&gt;jsr_continuation(), false));
1289 }
1290 
1291 
1292 void GraphBuilder::table_switch() {
1293   Bytecode_tableswitch sw(stream());
1294   const int l = sw.length();
1295   if (CanonicalizeNodes &amp;&amp; l == 1) {
1296     // total of 2 successors =&gt; use If instead of switch
1297     // Note: This code should go into the canonicalizer as soon as it can
1298     //       can handle canonicalized forms that contain more than one node.
1299     Value key = append(new Constant(new IntConstant(sw.low_key())));
1300     BlockBegin* tsux = block_at(bci() + sw.dest_offset_at(0));
1301     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1302     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1303     // In case of loop invariant code motion or predicate insertion
1304     // before the body of a loop the state is needed
1305     ValueStack* state_before = copy_state_if_bb(is_bb);
1306     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1307   } else {
1308     // collect successors
1309     BlockList* sux = new BlockList(l + 1, NULL);
1310     int i;
1311     bool has_bb = false;
1312     for (i = 0; i &lt; l; i++) {
1313       sux-&gt;at_put(i, block_at(bci() + sw.dest_offset_at(i)));
1314       if (sw.dest_offset_at(i) &lt; 0) has_bb = true;
1315     }
1316     // add default successor
1317     if (sw.default_offset() &lt; 0) has_bb = true;
1318     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1319     // In case of loop invariant code motion or predicate insertion
1320     // before the body of a loop the state is needed
1321     ValueStack* state_before = copy_state_if_bb(has_bb);
1322     Instruction* res = append(new TableSwitch(ipop(), sux, sw.low_key(), state_before, has_bb));
1323 #ifdef ASSERT
1324     if (res-&gt;as_Goto()) {
1325       for (i = 0; i &lt; l; i++) {
1326         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1327           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.dest_offset_at(i) &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1328         }
1329       }
1330     }
1331 #endif
1332   }
1333 }
1334 
1335 
1336 void GraphBuilder::lookup_switch() {
1337   Bytecode_lookupswitch sw(stream());
1338   const int l = sw.number_of_pairs();
1339   if (CanonicalizeNodes &amp;&amp; l == 1) {
1340     // total of 2 successors =&gt; use If instead of switch
1341     // Note: This code should go into the canonicalizer as soon as it can
1342     //       can handle canonicalized forms that contain more than one node.
1343     // simplify to If
1344     LookupswitchPair pair = sw.pair_at(0);
1345     Value key = append(new Constant(new IntConstant(pair.match())));
1346     BlockBegin* tsux = block_at(bci() + pair.offset());
1347     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1348     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1349     // In case of loop invariant code motion or predicate insertion
1350     // before the body of a loop the state is needed
1351     ValueStack* state_before = copy_state_if_bb(is_bb);;
1352     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1353   } else {
1354     // collect successors &amp; keys
1355     BlockList* sux = new BlockList(l + 1, NULL);
1356     intArray* keys = new intArray(l, 0);
1357     int i;
1358     bool has_bb = false;
1359     for (i = 0; i &lt; l; i++) {
1360       LookupswitchPair pair = sw.pair_at(i);
1361       if (pair.offset() &lt; 0) has_bb = true;
1362       sux-&gt;at_put(i, block_at(bci() + pair.offset()));
1363       keys-&gt;at_put(i, pair.match());
1364     }
1365     // add default successor
1366     if (sw.default_offset() &lt; 0) has_bb = true;
1367     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1368     // In case of loop invariant code motion or predicate insertion
1369     // before the body of a loop the state is needed
1370     ValueStack* state_before = copy_state_if_bb(has_bb);
1371     Instruction* res = append(new LookupSwitch(ipop(), sux, keys, state_before, has_bb));
1372 #ifdef ASSERT
1373     if (res-&gt;as_Goto()) {
1374       for (i = 0; i &lt; l; i++) {
1375         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1376           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.pair_at(i).offset() &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1377         }
1378       }
1379     }
1380 #endif
1381   }
1382 }
1383 
1384 void GraphBuilder::call_register_finalizer() {
1385   // If the receiver requires finalization then emit code to perform
1386   // the registration on return.
1387 
1388   // Gather some type information about the receiver
1389   Value receiver = state()-&gt;local_at(0);
1390   assert(receiver != NULL, "must have a receiver");
1391   ciType* declared_type = receiver-&gt;declared_type();
1392   ciType* exact_type = receiver-&gt;exact_type();
1393   if (exact_type == NULL &amp;&amp;
1394       receiver-&gt;as_Local() &amp;&amp;
1395       receiver-&gt;as_Local()-&gt;java_index() == 0) {
1396     ciInstanceKlass* ik = compilation()-&gt;method()-&gt;holder();
1397     if (ik-&gt;is_final()) {
1398       exact_type = ik;
1399     } else if (UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
1400       // test class is leaf class
1401       compilation()-&gt;dependency_recorder()-&gt;assert_leaf_type(ik);
1402       exact_type = ik;
1403     } else {
1404       declared_type = ik;
1405     }
1406   }
1407 
1408   // see if we know statically that registration isn't required
1409   bool needs_check = true;
1410   if (exact_type != NULL) {
1411     needs_check = exact_type-&gt;as_instance_klass()-&gt;has_finalizer();
1412   } else if (declared_type != NULL) {
1413     ciInstanceKlass* ik = declared_type-&gt;as_instance_klass();
1414     if (!Dependencies::has_finalizable_subclass(ik)) {
1415       compilation()-&gt;dependency_recorder()-&gt;assert_has_no_finalizable_subclasses(ik);
1416       needs_check = false;
1417     }
1418   }
1419 
1420   if (needs_check) {
1421     // Perform the registration of finalizable objects.
1422     ValueStack* state_before = copy_state_for_exception();
1423     load_local(objectType, 0);
1424     append_split(new Intrinsic(voidType, vmIntrinsics::_Object_init,
1425                                state()-&gt;pop_arguments(1),
1426                                true, state_before, true));
1427   }
1428 }
1429 
1430 
1431 void GraphBuilder::method_return(Value x) {
1432   if (RegisterFinalizersAtInit &amp;&amp;
1433       method()-&gt;intrinsic_id() == vmIntrinsics::_Object_init) {
1434     call_register_finalizer();
1435   }
1436 
1437   bool need_mem_bar = false;
1438   if (method()-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
1439       scope()-&gt;wrote_final()) {
1440     need_mem_bar = true;
1441   }
1442 
1443   // Check to see whether we are inlining. If so, Return
1444   // instructions become Gotos to the continuation point.
1445   if (continuation() != NULL) {
1446     assert(!method()-&gt;is_synchronized() || InlineSynchronizedMethods, "can not inline synchronized methods yet");
1447 
1448     if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
1449       // Report exit from inline methods
1450       Values* args = new Values(1);
1451       args-&gt;push(append(new Constant(new MethodConstant(method()))));
1452       append(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args));
1453     }
1454 
1455     // If the inlined method is synchronized, the monitor must be
1456     // released before we jump to the continuation block.
1457     if (method()-&gt;is_synchronized()) {
1458       assert(state()-&gt;locks_size() == 1, "receiver must be locked here");
1459       monitorexit(state()-&gt;lock_at(0), SynchronizationEntryBCI);
1460     }
1461 
1462     if (need_mem_bar) {
1463       append(new MemBar(lir_membar_storestore));
1464     }
1465 
1466     // State at end of inlined method is the state of the caller
1467     // without the method parameters on stack, including the
1468     // return value, if any, of the inlined method on operand stack.
1469     int invoke_bci = state()-&gt;caller_state()-&gt;bci();
1470     set_state(state()-&gt;caller_state()-&gt;copy_for_parsing());
1471     if (x != NULL) {
1472       state()-&gt;push(x-&gt;type(), x);
1473       if (profile_return() &amp;&amp; x-&gt;type()-&gt;is_object_kind()) {
1474         ciMethod* caller = state()-&gt;scope()-&gt;method();
1475         ciMethodData* md = caller-&gt;method_data_or_null();
1476         ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
1477         if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1478           bool has_return = data-&gt;is_CallTypeData() ? ((ciCallTypeData*)data)-&gt;has_return() : ((ciVirtualCallTypeData*)data)-&gt;has_return();
1479           // May not be true in case of an inlined call through a method handle intrinsic.
1480           if (has_return) {
1481             profile_return_type(x, method(), caller, invoke_bci);
1482           }
1483         }
1484       }
1485     }
1486     Goto* goto_callee = new Goto(continuation(), false);
1487 
1488     // See whether this is the first return; if so, store off some
1489     // of the state for later examination
1490     if (num_returns() == 0) {
1491       set_inline_cleanup_info();
1492     }
1493 
1494     // The current bci() is in the wrong scope, so use the bci() of
1495     // the continuation point.
1496     append_with_bci(goto_callee, scope_data()-&gt;continuation()-&gt;bci());
1497     incr_num_returns();
1498     return;
1499   }
1500 
1501   state()-&gt;truncate_stack(0);
1502   if (method()-&gt;is_synchronized()) {
1503     // perform the unlocking before exiting the method
1504     Value receiver;
1505     if (!method()-&gt;is_static()) {
1506       receiver = _initial_state-&gt;local_at(0);
1507     } else {
1508       receiver = append(new Constant(new ClassConstant(method()-&gt;holder())));
1509     }
1510     append_split(new MonitorExit(receiver, state()-&gt;unlock()));
1511   }
1512 
1513   if (need_mem_bar) {
1514       append(new MemBar(lir_membar_storestore));
1515   }
1516 
1517   append(new Return(x));
1518 }
1519 
1520 
1521 void GraphBuilder::access_field(Bytecodes::Code code) {
1522   bool will_link;
1523   ciField* field = stream()-&gt;get_field(will_link);
1524   ciInstanceKlass* holder = field-&gt;holder();
1525   BasicType field_type = field-&gt;type()-&gt;basic_type();
1526   ValueType* type = as_ValueType(field_type);
1527   // call will_link again to determine if the field is valid.
1528   const bool needs_patching = !holder-&gt;is_loaded() ||
1529                               !field-&gt;will_link(method()-&gt;holder(), code) ||
1530                               PatchALot;
1531 
1532   ValueStack* state_before = NULL;
1533   if (!holder-&gt;is_initialized() || needs_patching) {
1534     // save state before instruction for debug info when
1535     // deoptimization happens during patching
1536     state_before = copy_state_before();
1537   }
1538 
1539   Value obj = NULL;
1540   if (code == Bytecodes::_getstatic || code == Bytecodes::_putstatic) {
1541     if (state_before != NULL) {
1542       // build a patching constant
1543       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()), state_before);
1544     } else {
1545       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()));
1546     }
1547   }
1548 
1549   if (field-&gt;is_final() &amp;&amp; (code == Bytecodes::_putfield)) {
1550     scope()-&gt;set_wrote_final();
1551   }
1552 
1553   const int offset = !needs_patching ? field-&gt;offset() : -1;
1554   switch (code) {
1555     case Bytecodes::_getstatic: {
1556       // check for compile-time constants, i.e., initialized static final fields
1557       Instruction* constant = NULL;
1558       if (field-&gt;is_constant() &amp;&amp; !PatchALot) {
1559         ciConstant field_val = field-&gt;constant_value();
1560         BasicType field_type = field_val.basic_type();
1561         switch (field_type) {
1562         case T_ARRAY:
1563         case T_OBJECT:
1564           if (field_val.as_object()-&gt;should_be_constant()) {
1565             constant = new Constant(as_ValueType(field_val));
1566           }
1567           break;
1568 
1569         default:
1570           constant = new Constant(as_ValueType(field_val));
1571         }
1572       }
1573       if (constant != NULL) {
1574         push(type, append(constant));
1575       } else {
1576         if (state_before == NULL) {
1577           state_before = copy_state_for_exception();
1578         }
1579         push(type, append(new LoadField(append(obj), offset, field, true,
1580                                         state_before, needs_patching)));
1581       }
1582       break;
1583     }
1584     case Bytecodes::_putstatic:
1585       { Value val = pop(type);
1586         if (state_before == NULL) {
1587           state_before = copy_state_for_exception();
1588         }
1589         append(new StoreField(append(obj), offset, field, val, true, state_before, needs_patching));
1590       }
1591       break;
1592     case Bytecodes::_getfield: {
1593       // Check for compile-time constants, i.e., trusted final non-static fields.
1594       Instruction* constant = NULL;
1595       obj = apop();
1596       ObjectType* obj_type = obj-&gt;type()-&gt;as_ObjectType();
1597       if (obj_type-&gt;is_constant() &amp;&amp; !PatchALot) {
1598         ciObject* const_oop = obj_type-&gt;constant_value();
1599         if (!const_oop-&gt;is_null_object() &amp;&amp; const_oop-&gt;is_loaded()) {
1600           if (field-&gt;is_constant()) {
1601             ciConstant field_val = field-&gt;constant_value_of(const_oop);
1602             BasicType field_type = field_val.basic_type();
1603             switch (field_type) {
1604             case T_ARRAY:
1605             case T_OBJECT:
1606               if (field_val.as_object()-&gt;should_be_constant()) {
1607                 constant = new Constant(as_ValueType(field_val));
1608               }
1609               break;
1610             default:
1611               constant = new Constant(as_ValueType(field_val));
1612             }
1613           } else {
1614             // For CallSite objects treat the target field as a compile time constant.
1615             if (const_oop-&gt;is_call_site()) {
1616               ciCallSite* call_site = const_oop-&gt;as_call_site();
1617               if (field-&gt;is_call_site_target()) {
1618                 ciMethodHandle* target = call_site-&gt;get_target();
1619                 if (target != NULL) {  // just in case
1620                   ciConstant field_val(T_OBJECT, target);
1621                   constant = new Constant(as_ValueType(field_val));
1622                   // Add a dependence for invalidation of the optimization.
1623                   if (!call_site-&gt;is_constant_call_site()) {
1624                     VM_ENTRY_MARK;
1625                     dependency_recorder()-&gt;assert_call_site_target_value(call_site, target);
1626                   }
1627                 }
1628               }
1629             }
1630           }
1631         }
1632       }
1633       if (constant != NULL) {
1634         push(type, append(constant));
1635       } else {
1636         if (state_before == NULL) {
1637           state_before = copy_state_for_exception();
1638         }
1639         LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);
1640         Value replacement = !needs_patching ? _memory-&gt;load(load) : load;
1641         if (replacement != load) {
1642           assert(replacement-&gt;is_linked() || !replacement-&gt;can_be_linked(), "should already by linked");
1643           push(type, replacement);
1644         } else {
1645           push(type, append(load));
1646         }
1647       }
1648       break;
1649     }
1650     case Bytecodes::_putfield: {
1651       Value val = pop(type);
1652       obj = apop();
1653       if (state_before == NULL) {
1654         state_before = copy_state_for_exception();
1655       }
1656       StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);
1657       if (!needs_patching) store = _memory-&gt;store(store);
1658       if (store != NULL) {
1659         append(store);
1660       }
1661       break;
1662     }
1663     default:
1664       ShouldNotReachHere();
1665       break;
1666   }
1667 }
1668 
1669 
1670 Dependencies* GraphBuilder::dependency_recorder() const {
1671   assert(DeoptC1, "need debug information");
1672   return compilation()-&gt;dependency_recorder();
1673 }
1674 
1675 // How many arguments do we want to profile?
1676 Values* GraphBuilder::args_list_for_profiling(ciMethod* target, int&amp; start, bool may_have_receiver) {
1677   int n = 0;
1678   bool has_receiver = may_have_receiver &amp;&amp; Bytecodes::has_receiver(method()-&gt;java_code_at_bci(bci()));
1679   start = has_receiver ? 1 : 0;
1680   if (profile_arguments()) {
1681     ciProfileData* data = method()-&gt;method_data()-&gt;bci_to_data(bci());
1682     if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1683       n = data-&gt;is_CallTypeData() ? data-&gt;as_CallTypeData()-&gt;number_of_arguments() : data-&gt;as_VirtualCallTypeData()-&gt;number_of_arguments();
1684     }
1685   }
1686   // If we are inlining then we need to collect arguments to profile parameters for the target
1687   if (profile_parameters() &amp;&amp; target != NULL) {
1688     if (target-&gt;method_data() != NULL &amp;&amp; target-&gt;method_data()-&gt;parameters_type_data() != NULL) {
1689       // The receiver is profiled on method entry so it's included in
1690       // the number of parameters but here we're only interested in
1691       // actual arguments.
1692       n = MAX2(n, target-&gt;method_data()-&gt;parameters_type_data()-&gt;number_of_parameters() - start);
1693     }
1694   }
1695   if (n &gt; 0) {
1696     return new Values(n);
1697   }
1698   return NULL;
1699 }
1700 
1701 // Collect arguments that we want to profile in a list
1702 Values* GraphBuilder::collect_args_for_profiling(Values* args, ciMethod* target, bool may_have_receiver) {
1703   int start = 0;
1704   Values* obj_args = args_list_for_profiling(target, start, may_have_receiver);
1705   if (obj_args == NULL) {
1706     return NULL;
1707   }
1708   int s = obj_args-&gt;size();
1709   for (int i = start, j = 0; j &lt; s; i++) {
1710     if (args-&gt;at(i)-&gt;type()-&gt;is_object_kind()) {
1711       obj_args-&gt;push(args-&gt;at(i));
1712       j++;
1713     }
1714   }
1715   assert(s == obj_args-&gt;length(), "missed on arg?");
1716   return obj_args;
1717 }
1718 
1719 
1720 void GraphBuilder::invoke(Bytecodes::Code code) {
1721   bool will_link;
1722   ciSignature* declared_signature = NULL;
1723   ciMethod*             target = stream()-&gt;get_method(will_link, &amp;declared_signature);
1724   ciKlass*              holder = stream()-&gt;get_declared_method_holder();
1725   const Bytecodes::Code bc_raw = stream()-&gt;cur_bc_raw();
1726   assert(declared_signature != NULL, "cannot be null");
1727 
1728   if (!C1PatchInvokeDynamic &amp;&amp; Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; !will_link) {
1729     BAILOUT("unlinked call site (C1PatchInvokeDynamic is off)");
1730   }
1731 
1732   // we have to make sure the argument size (incl. the receiver)
1733   // is correct for compilation (the call would fail later during
1734   // linkage anyway) - was bug (gri 7/28/99)
1735   {
1736     // Use raw to get rewritten bytecode.
1737     const bool is_invokestatic = bc_raw == Bytecodes::_invokestatic;
1738     const bool allow_static =
1739           is_invokestatic ||
1740           bc_raw == Bytecodes::_invokehandle ||
1741           bc_raw == Bytecodes::_invokedynamic;
1742     if (target-&gt;is_loaded()) {
1743       if (( target-&gt;is_static() &amp;&amp; !allow_static) ||
1744           (!target-&gt;is_static() &amp;&amp;  is_invokestatic)) {
1745         BAILOUT("will cause link error");
1746       }
1747     }
1748   }
1749   ciInstanceKlass* klass = target-&gt;holder();
1750 
1751   // check if CHA possible: if so, change the code to invoke_special
1752   ciInstanceKlass* calling_klass = method()-&gt;holder();
1753   ciInstanceKlass* callee_holder = ciEnv::get_instance_klass_for_declared_method_holder(holder);
1754   ciInstanceKlass* actual_recv = callee_holder;
1755 
1756   CompileLog* log = compilation()-&gt;log();
1757   if (log != NULL)
1758       log-&gt;elem("call method='%d' instr='%s'",
1759                 log-&gt;identify(target),
1760                 Bytecodes::name(code));
1761 
1762   // Some methods are obviously bindable without any type checks so
1763   // convert them directly to an invokespecial or invokestatic.
1764   if (target-&gt;is_loaded() &amp;&amp; !target-&gt;is_abstract() &amp;&amp; target-&gt;can_be_statically_bound()) {
1765     switch (bc_raw) {
1766     case Bytecodes::_invokevirtual:
1767       code = Bytecodes::_invokespecial;
1768       break;
1769     case Bytecodes::_invokehandle:
1770       code = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokespecial;
1771       break;
1772     }
1773   } else {
1774     if (bc_raw == Bytecodes::_invokehandle) {
1775       assert(!will_link, "should come here only for unlinked call");
1776       code = Bytecodes::_invokespecial;
1777     }
1778   }
1779 
1780   // Push appendix argument (MethodType, CallSite, etc.), if one.
1781   bool patch_for_appendix = false;
1782   int patching_appendix_arg = 0;
1783   if (C1PatchInvokeDynamic &amp;&amp;
1784       (Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; (!will_link || PatchALot))) {
1785     Value arg = append(new Constant(new ObjectConstant(compilation()-&gt;env()-&gt;unloaded_ciinstance()), copy_state_before()));
1786     apush(arg);
1787     patch_for_appendix = true;
1788     patching_appendix_arg = (will_link &amp;&amp; stream()-&gt;has_appendix()) ? 0 : 1;
1789   } else if (stream()-&gt;has_appendix()) {
1790     ciObject* appendix = stream()-&gt;get_appendix();
1791     Value arg = append(new Constant(new ObjectConstant(appendix)));
1792     apush(arg);
1793   }
1794 
1795   // NEEDS_CLEANUP
1796   // I've added the target-&gt;is_loaded() test below but I don't really understand
1797   // how klass-&gt;is_loaded() can be true and yet target-&gt;is_loaded() is false.
1798   // this happened while running the JCK invokevirtual tests under doit.  TKR
1799   ciMethod* cha_monomorphic_target = NULL;
1800   ciMethod* exact_target = NULL;
1801   Value better_receiver = NULL;
1802   if (UseCHA &amp;&amp; DeoptC1 &amp;&amp; klass-&gt;is_loaded() &amp;&amp; target-&gt;is_loaded() &amp;&amp;
1803       !(// %%% FIXME: Are both of these relevant?
1804         target-&gt;is_method_handle_intrinsic() ||
1805         target-&gt;is_compiled_lambda_form()) &amp;&amp;
1806       !patch_for_appendix) {
1807     Value receiver = NULL;
1808     ciInstanceKlass* receiver_klass = NULL;
1809     bool type_is_exact = false;
1810     // try to find a precise receiver type
1811     if (will_link &amp;&amp; !target-&gt;is_static()) {
1812       int index = state()-&gt;stack_size() - (target-&gt;arg_size_no_receiver() + 1);
1813       receiver = state()-&gt;stack_at(index);
1814       ciType* type = receiver-&gt;exact_type();
1815       if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1816           type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1817         receiver_klass = (ciInstanceKlass*) type;
1818         type_is_exact = true;
1819       }
1820       if (type == NULL) {
1821         type = receiver-&gt;declared_type();
1822         if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1823             type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1824           receiver_klass = (ciInstanceKlass*) type;
1825           if (receiver_klass-&gt;is_leaf_type() &amp;&amp; !receiver_klass-&gt;is_final()) {
1826             // Insert a dependency on this type since
1827             // find_monomorphic_target may assume it's already done.
1828             dependency_recorder()-&gt;assert_leaf_type(receiver_klass);
1829             type_is_exact = true;
1830           }
1831         }
1832       }
1833     }
1834     if (receiver_klass != NULL &amp;&amp; type_is_exact &amp;&amp;
1835         receiver_klass-&gt;is_loaded() &amp;&amp; code != Bytecodes::_invokespecial) {
1836       // If we have the exact receiver type we can bind directly to
1837       // the method to call.
1838       exact_target = target-&gt;resolve_invoke(calling_klass, receiver_klass);
1839       if (exact_target != NULL) {
1840         target = exact_target;
1841         code = Bytecodes::_invokespecial;
1842       }
1843     }
1844     if (receiver_klass != NULL &amp;&amp;
1845         receiver_klass-&gt;is_subtype_of(actual_recv) &amp;&amp;
1846         actual_recv-&gt;is_initialized()) {
1847       actual_recv = receiver_klass;
1848     }
1849 
1850     if ((code == Bytecodes::_invokevirtual &amp;&amp; callee_holder-&gt;is_initialized()) ||
1851         (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_initialized() &amp;&amp; !actual_recv-&gt;is_interface())) {
1852       // Use CHA on the receiver to select a more precise method.
1853       cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, callee_holder, actual_recv);
1854     } else if (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_loaded() &amp;&amp; receiver != NULL) {
1855       // if there is only one implementor of this interface then we
1856       // may be able bind this invoke directly to the implementing
1857       // klass but we need both a dependence on the single interface
1858       // and on the method we bind to.  Additionally since all we know
1859       // about the receiver type is the it's supposed to implement the
1860       // interface we have to insert a check that it's the class we
1861       // expect.  Interface types are not checked by the verifier so
1862       // they are roughly equivalent to Object.
1863       ciInstanceKlass* singleton = NULL;
1864       if (target-&gt;holder()-&gt;nof_implementors() == 1) {
1865         singleton = target-&gt;holder()-&gt;implementor();
1866         assert(singleton != NULL &amp;&amp; singleton != target-&gt;holder(),
1867                "just checking");
1868 
1869         assert(holder-&gt;is_interface(), "invokeinterface to non interface?");
1870         ciInstanceKlass* decl_interface = (ciInstanceKlass*)holder;
1871         // the number of implementors for decl_interface is less or
1872         // equal to the number of implementors for target-&gt;holder() so
1873         // if number of implementors of target-&gt;holder() == 1 then
1874         // number of implementors for decl_interface is 0 or 1. If
1875         // it's 0 then no class implements decl_interface and there's
1876         // no point in inlining.
1877         if (!holder-&gt;is_loaded() || decl_interface-&gt;nof_implementors() != 1 || decl_interface-&gt;has_default_methods()) {
1878           singleton = NULL;
1879         }
1880       }
1881       if (singleton) {
1882         cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, target-&gt;holder(), singleton);
1883         if (cha_monomorphic_target != NULL) {
1884           // If CHA is able to bind this invoke then update the class
1885           // to match that class, otherwise klass will refer to the
1886           // interface.
1887           klass = cha_monomorphic_target-&gt;holder();
1888           actual_recv = target-&gt;holder();
1889 
1890           // insert a check it's really the expected class.
1891           CheckCast* c = new CheckCast(klass, receiver, copy_state_for_exception());
1892           c-&gt;set_incompatible_class_change_check();
1893           c-&gt;set_direct_compare(klass-&gt;is_final());
1894           // pass the result of the checkcast so that the compiler has
1895           // more accurate type info in the inlinee
1896           better_receiver = append_split(c);
1897         }
1898       }
1899     }
1900   }
1901 
1902   if (cha_monomorphic_target != NULL) {
1903     if (cha_monomorphic_target-&gt;is_abstract()) {
1904       // Do not optimize for abstract methods
1905       cha_monomorphic_target = NULL;
1906     }
1907   }
1908 
1909   if (cha_monomorphic_target != NULL) {
1910     if (!(target-&gt;is_final_method())) {
1911       // If we inlined because CHA revealed only a single target method,
1912       // then we are dependent on that target method not getting overridden
1913       // by dynamic class loading.  Be sure to test the "static" receiver
1914       // dest_method here, as opposed to the actual receiver, which may
1915       // falsely lead us to believe that the receiver is final or private.
1916       dependency_recorder()-&gt;assert_unique_concrete_method(actual_recv, cha_monomorphic_target);
1917     }
1918     code = Bytecodes::_invokespecial;
1919   }
1920 
1921   // check if we could do inlining
1922   if (!PatchALot &amp;&amp; Inline &amp;&amp; klass-&gt;is_loaded() &amp;&amp;
1923       (klass-&gt;is_initialized() || klass-&gt;is_interface() &amp;&amp; target-&gt;holder()-&gt;is_initialized())
1924       &amp;&amp; target-&gt;is_loaded()
1925       &amp;&amp; !patch_for_appendix) {
1926     // callee is known =&gt; check if we have static binding
1927     assert(target-&gt;is_loaded(), "callee must be known");
1928     if (code == Bytecodes::_invokestatic  ||
1929         code == Bytecodes::_invokespecial ||
1930         code == Bytecodes::_invokevirtual &amp;&amp; target-&gt;is_final_method() ||
1931         code == Bytecodes::_invokedynamic) {
1932       ciMethod* inline_target = (cha_monomorphic_target != NULL) ? cha_monomorphic_target : target;
1933       // static binding =&gt; check if callee is ok
1934       bool success = try_inline(inline_target, (cha_monomorphic_target != NULL) || (exact_target != NULL), code, better_receiver);
1935 
1936       CHECK_BAILOUT();
1937       clear_inline_bailout();
1938 
1939       if (success) {
1940         // Register dependence if JVMTI has either breakpoint
1941         // setting or hotswapping of methods capabilities since they may
1942         // cause deoptimization.
1943         if (compilation()-&gt;env()-&gt;jvmti_can_hotswap_or_post_breakpoint()) {
1944           dependency_recorder()-&gt;assert_evol_method(inline_target);
1945         }
1946         return;
1947       }
1948     } else {
1949       print_inlining(target, "no static binding", /*success*/ false);
1950     }
1951   } else {
1952     print_inlining(target, "not inlineable", /*success*/ false);
1953   }
1954 
1955   // If we attempted an inline which did not succeed because of a
1956   // bailout during construction of the callee graph, the entire
1957   // compilation has to be aborted. This is fairly rare and currently
1958   // seems to only occur for jasm-generated classes which contain
1959   // jsr/ret pairs which are not associated with finally clauses and
1960   // do not have exception handlers in the containing method, and are
1961   // therefore not caught early enough to abort the inlining without
1962   // corrupting the graph. (We currently bail out with a non-empty
1963   // stack at a ret in these situations.)
1964   CHECK_BAILOUT();
1965 
1966   // inlining not successful =&gt; standard invoke
1967   bool is_loaded = target-&gt;is_loaded();
1968   ValueType* result_type = as_ValueType(declared_signature-&gt;return_type());
1969   ValueStack* state_before = copy_state_exhandling();
1970 
1971   // The bytecode (code) might change in this method so we are checking this very late.
1972   const bool has_receiver =
1973     code == Bytecodes::_invokespecial   ||
1974     code == Bytecodes::_invokevirtual   ||
1975     code == Bytecodes::_invokeinterface;
1976   Values* args = state()-&gt;pop_arguments(target-&gt;arg_size_no_receiver() + patching_appendix_arg);
1977   Value recv = has_receiver ? apop() : NULL;
1978   int vtable_index = Method::invalid_vtable_index;
1979 
1980 #ifdef SPARC
1981   // Currently only supported on Sparc.
1982   // The UseInlineCaches only controls dispatch to invokevirtuals for
1983   // loaded classes which we weren't able to statically bind.
1984   if (!UseInlineCaches &amp;&amp; is_loaded &amp;&amp; code == Bytecodes::_invokevirtual
1985       &amp;&amp; !target-&gt;can_be_statically_bound()) {
1986     // Find a vtable index if one is available
1987     vtable_index = target-&gt;resolve_vtable_index(calling_klass, callee_holder);
1988   }
1989 #endif
1990 
1991   if (recv != NULL &amp;&amp;
1992       (code == Bytecodes::_invokespecial ||
1993        !is_loaded || target-&gt;is_final())) {
1994     // invokespecial always needs a NULL check.  invokevirtual where
1995     // the target is final or where it's not known that whether the
1996     // target is final requires a NULL check.  Otherwise normal
1997     // invokevirtual will perform the null check during the lookup
1998     // logic or the unverified entry point.  Profiling of calls
1999     // requires that the null check is performed in all cases.
2000     null_check(recv);
2001   }
2002 
2003   if (is_profiling()) {
2004     if (recv != NULL &amp;&amp; profile_calls()) {
2005       null_check(recv);
2006     }
2007     // Note that we'd collect profile data in this method if we wanted it.
2008     compilation()-&gt;set_would_profile(true);
2009 
2010     if (profile_calls()) {
2011       assert(cha_monomorphic_target == NULL || exact_target == NULL, "both can not be set");
2012       ciKlass* target_klass = NULL;
2013       if (cha_monomorphic_target != NULL) {
2014         target_klass = cha_monomorphic_target-&gt;holder();
2015       } else if (exact_target != NULL) {
2016         target_klass = exact_target-&gt;holder();
2017       }
2018       profile_call(target, recv, target_klass, collect_args_for_profiling(args, NULL, false), false);
2019     }
2020   }
2021 
2022   Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);
2023   // push result
2024   append_split(result);
2025 
2026   if (result_type != voidType) {
2027     if (method()-&gt;is_strict()) {
2028       push(result_type, round_fp(result));
2029     } else {
2030       push(result_type, result);
2031     }
2032   }
2033   if (profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
2034     profile_return_type(result, target);
2035   }
2036 }
2037 
2038 
2039 void GraphBuilder::new_instance(int klass_index) {
2040   ValueStack* state_before = copy_state_exhandling();
2041   bool will_link;
2042   ciKlass* klass = stream()-&gt;get_klass(will_link);
2043   assert(klass-&gt;is_instance_klass(), "must be an instance klass");
2044   NewInstance* new_instance = new NewInstance(klass-&gt;as_instance_klass(), state_before);
2045   _memory-&gt;new_instance(new_instance);
2046   apush(append_split(new_instance));
2047 }
2048 
2049 
2050 void GraphBuilder::new_type_array() {
2051   ValueStack* state_before = copy_state_exhandling();
2052   apush(append_split(new NewTypeArray(ipop(), (BasicType)stream()-&gt;get_index(), state_before)));
2053 }
2054 
2055 
2056 void GraphBuilder::new_object_array() {
2057   bool will_link;
2058   ciKlass* klass = stream()-&gt;get_klass(will_link);
2059   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2060   NewArray* n = new NewObjectArray(klass, ipop(), state_before);
2061   apush(append_split(n));
2062 }
2063 
2064 
2065 bool GraphBuilder::direct_compare(ciKlass* k) {
2066   if (k-&gt;is_loaded() &amp;&amp; k-&gt;is_instance_klass() &amp;&amp; !UseSlowPath) {
2067     ciInstanceKlass* ik = k-&gt;as_instance_klass();
2068     if (ik-&gt;is_final()) {
2069       return true;
2070     } else {
2071       if (DeoptC1 &amp;&amp; UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
2072         // test class is leaf class
2073         dependency_recorder()-&gt;assert_leaf_type(ik);
2074         return true;
2075       }
2076     }
2077   }
2078   return false;
2079 }
2080 
2081 
2082 void GraphBuilder::check_cast(int klass_index) {
2083   bool will_link;
2084   ciKlass* klass = stream()-&gt;get_klass(will_link);
2085   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
2086   CheckCast* c = new CheckCast(klass, apop(), state_before);
2087   apush(append_split(c));
2088   c-&gt;set_direct_compare(direct_compare(klass));
2089 
2090   if (is_profiling()) {
2091     // Note that we'd collect profile data in this method if we wanted it.
2092     compilation()-&gt;set_would_profile(true);
2093 
2094     if (profile_checkcasts()) {
2095       c-&gt;set_profiled_method(method());
2096       c-&gt;set_profiled_bci(bci());
2097       c-&gt;set_should_profile(true);
2098     }
2099   }
2100 }
2101 
2102 
2103 void GraphBuilder::instance_of(int klass_index) {
2104   bool will_link;
2105   ciKlass* klass = stream()-&gt;get_klass(will_link);
2106   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2107   InstanceOf* i = new InstanceOf(klass, apop(), state_before);
2108   ipush(append_split(i));
2109   i-&gt;set_direct_compare(direct_compare(klass));
2110 
2111   if (is_profiling()) {
2112     // Note that we'd collect profile data in this method if we wanted it.
2113     compilation()-&gt;set_would_profile(true);
2114 
2115     if (profile_checkcasts()) {
2116       i-&gt;set_profiled_method(method());
2117       i-&gt;set_profiled_bci(bci());
2118       i-&gt;set_should_profile(true);
2119     }
2120   }
2121 }
2122 
2123 
2124 void GraphBuilder::monitorenter(Value x, int bci) {
2125   // save state before locking in case of deoptimization after a NullPointerException
2126   ValueStack* state_before = copy_state_for_exception_with_bci(bci);
2127   append_with_bci(new MonitorEnter(x, state()-&gt;lock(x), state_before), bci);
2128   kill_all();
2129 }
2130 
2131 
2132 void GraphBuilder::monitorexit(Value x, int bci) {
2133   append_with_bci(new MonitorExit(x, state()-&gt;unlock()), bci);
2134   kill_all();
2135 }
2136 
2137 
2138 void GraphBuilder::new_multi_array(int dimensions) {
2139   bool will_link;
2140   ciKlass* klass = stream()-&gt;get_klass(will_link);
2141   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2142 
2143   Values* dims = new Values(dimensions, NULL);
2144   // fill in all dimensions
2145   int i = dimensions;
2146   while (i-- &gt; 0) dims-&gt;at_put(i, ipop());
2147   // create array
2148   NewArray* n = new NewMultiArray(klass, dims, state_before);
2149   apush(append_split(n));
2150 }
2151 
2152 
2153 void GraphBuilder::throw_op(int bci) {
2154   // We require that the debug info for a Throw be the "state before"
2155   // the Throw (i.e., exception oop is still on TOS)
2156   ValueStack* state_before = copy_state_before_with_bci(bci);
2157   Throw* t = new Throw(apop(), state_before);
2158   // operand stack not needed after a throw
2159   state()-&gt;truncate_stack(0);
2160   append_with_bci(t, bci);
2161 }
2162 
2163 
2164 Value GraphBuilder::round_fp(Value fp_value) {
2165   // no rounding needed if SSE2 is used
2166   if (RoundFPResults &amp;&amp; UseSSE &lt; 2) {
2167     // Must currently insert rounding node for doubleword values that
2168     // are results of expressions (i.e., not loads from memory or
2169     // constants)
2170     if (fp_value-&gt;type()-&gt;tag() == doubleTag &amp;&amp;
2171         fp_value-&gt;as_Constant() == NULL &amp;&amp;
2172         fp_value-&gt;as_Local() == NULL &amp;&amp;       // method parameters need no rounding
2173         fp_value-&gt;as_RoundFP() == NULL) {
2174       return append(new RoundFP(fp_value));
2175     }
2176   }
2177   return fp_value;
2178 }
2179 
2180 
2181 Instruction* GraphBuilder::append_with_bci(Instruction* instr, int bci) {
2182   Canonicalizer canon(compilation(), instr, bci);
2183   Instruction* i1 = canon.canonical();
2184   if (i1-&gt;is_linked() || !i1-&gt;can_be_linked()) {
2185     // Canonicalizer returned an instruction which was already
2186     // appended so simply return it.
2187     return i1;
2188   }
2189 
2190   if (UseLocalValueNumbering) {
2191     // Lookup the instruction in the ValueMap and add it to the map if
2192     // it's not found.
2193     Instruction* i2 = vmap()-&gt;find_insert(i1);
2194     if (i2 != i1) {
2195       // found an entry in the value map, so just return it.
2196       assert(i2-&gt;is_linked(), "should already be linked");
2197       return i2;
2198     }
2199     ValueNumberingEffects vne(vmap());
2200     i1-&gt;visit(&amp;vne);
2201   }
2202 
2203   // i1 was not eliminated =&gt; append it
2204   assert(i1-&gt;next() == NULL, "shouldn't already be linked");
2205   _last = _last-&gt;set_next(i1, canon.bci());
2206 
2207   if (++_instruction_count &gt;= InstructionCountCutoff &amp;&amp; !bailed_out()) {
2208     // set the bailout state but complete normal processing.  We
2209     // might do a little more work before noticing the bailout so we
2210     // want processing to continue normally until it's noticed.
2211     bailout("Method and/or inlining is too large");
2212   }
2213 
2214 #ifndef PRODUCT
2215   if (PrintIRDuringConstruction) {
2216     InstructionPrinter ip;
2217     ip.print_line(i1);
2218     if (Verbose) {
2219       state()-&gt;print();
2220     }
2221   }
2222 #endif
2223 
2224   // save state after modification of operand stack for StateSplit instructions
2225   StateSplit* s = i1-&gt;as_StateSplit();
2226   if (s != NULL) {
2227     if (EliminateFieldAccess) {
2228       Intrinsic* intrinsic = s-&gt;as_Intrinsic();
2229       if (s-&gt;as_Invoke() != NULL || (intrinsic &amp;&amp; !intrinsic-&gt;preserves_state())) {
2230         _memory-&gt;kill();
2231       }
2232     }
2233     s-&gt;set_state(state()-&gt;copy(ValueStack::StateAfter, canon.bci()));
2234   }
2235 
2236   // set up exception handlers for this instruction if necessary
2237   if (i1-&gt;can_trap()) {
2238     i1-&gt;set_exception_handlers(handle_exception(i1));
2239     assert(i1-&gt;exception_state() != NULL || !i1-&gt;needs_exception_state() || bailed_out(), "handle_exception must set exception state");
2240   }
2241   return i1;
2242 }
2243 
2244 
2245 Instruction* GraphBuilder::append(Instruction* instr) {
2246   assert(instr-&gt;as_StateSplit() == NULL || instr-&gt;as_BlockEnd() != NULL, "wrong append used");
2247   return append_with_bci(instr, bci());
2248 }
2249 
2250 
2251 Instruction* GraphBuilder::append_split(StateSplit* instr) {
2252   return append_with_bci(instr, bci());
2253 }
2254 
2255 
2256 void GraphBuilder::null_check(Value value) {
2257   if (value-&gt;as_NewArray() != NULL || value-&gt;as_NewInstance() != NULL) {
2258     return;
2259   } else {
2260     Constant* con = value-&gt;as_Constant();
2261     if (con) {
2262       ObjectType* c = con-&gt;type()-&gt;as_ObjectType();
2263       if (c &amp;&amp; c-&gt;is_loaded()) {
2264         ObjectConstant* oc = c-&gt;as_ObjectConstant();
2265         if (!oc || !oc-&gt;value()-&gt;is_null_object()) {
2266           return;
2267         }
2268       }
2269     }
2270   }
2271   append(new NullCheck(value, copy_state_for_exception()));
2272 }
2273 
2274 
2275 
2276 XHandlers* GraphBuilder::handle_exception(Instruction* instruction) {
2277   if (!has_handler() &amp;&amp; (!instruction-&gt;needs_exception_state() || instruction-&gt;exception_state() != NULL)) {
2278     assert(instruction-&gt;exception_state() == NULL
2279            || instruction-&gt;exception_state()-&gt;kind() == ValueStack::EmptyExceptionState
2280            || (instruction-&gt;exception_state()-&gt;kind() == ValueStack::ExceptionState &amp;&amp; _compilation-&gt;env()-&gt;jvmti_can_access_local_variables()),
2281            "exception_state should be of exception kind");
2282     return new XHandlers();
2283   }
2284 
2285   XHandlers*  exception_handlers = new XHandlers();
2286   ScopeData*  cur_scope_data = scope_data();
2287   ValueStack* cur_state = instruction-&gt;state_before();
2288   ValueStack* prev_state = NULL;
2289   int scope_count = 0;
2290 
2291   assert(cur_state != NULL, "state_before must be set");
2292   do {
2293     int cur_bci = cur_state-&gt;bci();
2294     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2295     assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data-&gt;stream()-&gt;cur_bci(), "invalid bci");
2296 
2297     // join with all potential exception handlers
2298     XHandlers* list = cur_scope_data-&gt;xhandlers();
2299     const int n = list-&gt;length();
2300     for (int i = 0; i &lt; n; i++) {
2301       XHandler* h = list-&gt;handler_at(i);
2302       if (h-&gt;covers(cur_bci)) {
2303         // h is a potential exception handler =&gt; join it
2304         compilation()-&gt;set_has_exception_handlers(true);
2305 
2306         BlockBegin* entry = h-&gt;entry_block();
2307         if (entry == block()) {
2308           // It's acceptable for an exception handler to cover itself
2309           // but we don't handle that in the parser currently.  It's
2310           // very rare so we bailout instead of trying to handle it.
2311           BAILOUT_("exception handler covers itself", exception_handlers);
2312         }
2313         assert(entry-&gt;bci() == h-&gt;handler_bci(), "must match");
2314         assert(entry-&gt;bci() == -1 || entry == cur_scope_data-&gt;block_at(entry-&gt;bci()), "blocks must correspond");
2315 
2316         // previously this was a BAILOUT, but this is not necessary
2317         // now because asynchronous exceptions are not handled this way.
2318         assert(entry-&gt;state() == NULL || cur_state-&gt;total_locks_size() == entry-&gt;state()-&gt;total_locks_size(), "locks do not match");
2319 
2320         // xhandler start with an empty expression stack
2321         if (cur_state-&gt;stack_size() != 0) {
2322           cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2323         }
2324         if (instruction-&gt;exception_state() == NULL) {
2325           instruction-&gt;set_exception_state(cur_state);
2326         }
2327 
2328         // Note: Usually this join must work. However, very
2329         // complicated jsr-ret structures where we don't ret from
2330         // the subroutine can cause the objects on the monitor
2331         // stacks to not match because blocks can be parsed twice.
2332         // The only test case we've seen so far which exhibits this
2333         // problem is caught by the infinite recursion test in
2334         // GraphBuilder::jsr() if the join doesn't work.
2335         if (!entry-&gt;try_merge(cur_state)) {
2336           BAILOUT_("error while joining with exception handler, prob. due to complicated jsr/rets", exception_handlers);
2337         }
2338 
2339         // add current state for correct handling of phi functions at begin of xhandler
2340         int phi_operand = entry-&gt;add_exception_state(cur_state);
2341 
2342         // add entry to the list of xhandlers of this block
2343         _block-&gt;add_exception_handler(entry);
2344 
2345         // add back-edge from xhandler entry to this block
2346         if (!entry-&gt;is_predecessor(_block)) {
2347           entry-&gt;add_predecessor(_block);
2348         }
2349 
2350         // clone XHandler because phi_operand and scope_count can not be shared
2351         XHandler* new_xhandler = new XHandler(h);
2352         new_xhandler-&gt;set_phi_operand(phi_operand);
2353         new_xhandler-&gt;set_scope_count(scope_count);
2354         exception_handlers-&gt;append(new_xhandler);
2355 
2356         // fill in exception handler subgraph lazily
2357         assert(!entry-&gt;is_set(BlockBegin::was_visited_flag), "entry must not be visited yet");
2358         cur_scope_data-&gt;add_to_work_list(entry);
2359 
2360         // stop when reaching catchall
2361         if (h-&gt;catch_type() == 0) {
2362           return exception_handlers;
2363         }
2364       }
2365     }
2366 
2367     if (exception_handlers-&gt;length() == 0) {
2368       // This scope and all callees do not handle exceptions, so the local
2369       // variables of this scope are not needed. However, the scope itself is
2370       // required for a correct exception stack trace -&gt; clear out the locals.
2371       if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
2372         cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2373       } else {
2374         cur_state = cur_state-&gt;copy(ValueStack::EmptyExceptionState, cur_state-&gt;bci());
2375       }
2376       if (prev_state != NULL) {
2377         prev_state-&gt;set_caller_state(cur_state);
2378       }
2379       if (instruction-&gt;exception_state() == NULL) {
2380         instruction-&gt;set_exception_state(cur_state);
2381       }
2382     }
2383 
2384     // Set up iteration for next time.
2385     // If parsing a jsr, do not grab exception handlers from the
2386     // parent scopes for this method (already got them, and they
2387     // needed to be cloned)
2388 
2389     while (cur_scope_data-&gt;parsing_jsr()) {
2390       cur_scope_data = cur_scope_data-&gt;parent();
2391     }
2392 
2393     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2394     assert(cur_state-&gt;locks_size() == 0 || cur_state-&gt;locks_size() == 1, "unlocking must be done in a catchall exception handler");
2395 
2396     prev_state = cur_state;
2397     cur_state = cur_state-&gt;caller_state();
2398     cur_scope_data = cur_scope_data-&gt;parent();
2399     scope_count++;
2400   } while (cur_scope_data != NULL);
2401 
2402   return exception_handlers;
2403 }
2404 
2405 
2406 // Helper class for simplifying Phis.
2407 class PhiSimplifier : public BlockClosure {
2408  private:
2409   bool _has_substitutions;
2410   Value simplify(Value v);
2411 
2412  public:
2413   PhiSimplifier(BlockBegin* start) : _has_substitutions(false) {
2414     start-&gt;iterate_preorder(this);
2415     if (_has_substitutions) {
2416       SubstitutionResolver sr(start);
2417     }
2418   }
2419   void block_do(BlockBegin* b);
2420   bool has_substitutions() const { return _has_substitutions; }
2421 };
2422 
2423 
2424 Value PhiSimplifier::simplify(Value v) {
2425   Phi* phi = v-&gt;as_Phi();
2426 
2427   if (phi == NULL) {
2428     // no phi function
2429     return v;
2430   } else if (v-&gt;has_subst()) {
2431     // already substituted; subst can be phi itself -&gt; simplify
2432     return simplify(v-&gt;subst());
2433   } else if (phi-&gt;is_set(Phi::cannot_simplify)) {
2434     // already tried to simplify phi before
2435     return phi;
2436   } else if (phi-&gt;is_set(Phi::visited)) {
2437     // break cycles in phi functions
2438     return phi;
2439   } else if (phi-&gt;type()-&gt;is_illegal()) {
2440     // illegal phi functions are ignored anyway
2441     return phi;
2442 
2443   } else {
2444     // mark phi function as processed to break cycles in phi functions
2445     phi-&gt;set(Phi::visited);
2446 
2447     // simplify x = [y, x] and x = [y, y] to y
2448     Value subst = NULL;
2449     int opd_count = phi-&gt;operand_count();
2450     for (int i = 0; i &lt; opd_count; i++) {
2451       Value opd = phi-&gt;operand_at(i);
2452       assert(opd != NULL, "Operand must exist!");
2453 
2454       if (opd-&gt;type()-&gt;is_illegal()) {
2455         // if one operand is illegal, the entire phi function is illegal
2456         phi-&gt;make_illegal();
2457         phi-&gt;clear(Phi::visited);
2458         return phi;
2459       }
2460 
2461       Value new_opd = simplify(opd);
2462       assert(new_opd != NULL, "Simplified operand must exist!");
2463 
2464       if (new_opd != phi &amp;&amp; new_opd != subst) {
2465         if (subst == NULL) {
2466           subst = new_opd;
2467         } else {
2468           // no simplification possible
2469           phi-&gt;set(Phi::cannot_simplify);
2470           phi-&gt;clear(Phi::visited);
2471           return phi;
2472         }
2473       }
2474     }
2475 
2476     // sucessfully simplified phi function
2477     assert(subst != NULL, "illegal phi function");
2478     _has_substitutions = true;
2479     phi-&gt;clear(Phi::visited);
2480     phi-&gt;set_subst(subst);
2481 
2482 #ifndef PRODUCT
2483     if (PrintPhiFunctions) {
2484       tty-&gt;print_cr("simplified phi function %c%d to %c%d (Block B%d)", phi-&gt;type()-&gt;tchar(), phi-&gt;id(), subst-&gt;type()-&gt;tchar(), subst-&gt;id(), phi-&gt;block()-&gt;block_id());
2485     }
2486 #endif
2487 
2488     return subst;
2489   }
2490 }
2491 
2492 
2493 void PhiSimplifier::block_do(BlockBegin* b) {
2494   for_each_phi_fun(b, phi,
2495     simplify(phi);
2496   );
2497 
2498 #ifdef ASSERT
2499   for_each_phi_fun(b, phi,
2500                    assert(phi-&gt;operand_count() != 1 || phi-&gt;subst() != phi, "missed trivial simplification");
2501   );
2502 
2503   ValueStack* state = b-&gt;state()-&gt;caller_state();
2504   for_each_state_value(state, value,
2505     Phi* phi = value-&gt;as_Phi();
2506     assert(phi == NULL || phi-&gt;block() != b, "must not have phi function to simplify in caller state");
2507   );
2508 #endif
2509 }
2510 
2511 // This method is called after all blocks are filled with HIR instructions
2512 // It eliminates all Phi functions of the form x = [y, y] and x = [y, x]
2513 void GraphBuilder::eliminate_redundant_phis(BlockBegin* start) {
2514   PhiSimplifier simplifier(start);
2515 }
2516 
2517 
2518 void GraphBuilder::connect_to_end(BlockBegin* beg) {
2519   // setup iteration
2520   kill_all();
2521   _block = beg;
2522   _state = beg-&gt;state()-&gt;copy_for_parsing();
2523   _last  = beg;
2524   iterate_bytecodes_for_block(beg-&gt;bci());
2525 }
2526 
2527 
2528 BlockEnd* GraphBuilder::iterate_bytecodes_for_block(int bci) {
2529 #ifndef PRODUCT
2530   if (PrintIRDuringConstruction) {
2531     tty-&gt;cr();
2532     InstructionPrinter ip;
2533     ip.print_instr(_block); tty-&gt;cr();
2534     ip.print_stack(_block-&gt;state()); tty-&gt;cr();
2535     ip.print_inline_level(_block);
2536     ip.print_head();
2537     tty-&gt;print_cr("locals size: %d stack size: %d", state()-&gt;locals_size(), state()-&gt;stack_size());
2538   }
2539 #endif
2540   _skip_block = false;
2541   assert(state() != NULL, "ValueStack missing!");
2542   CompileLog* log = compilation()-&gt;log();
2543   ciBytecodeStream s(method());
2544   s.reset_to_bci(bci);
2545   int prev_bci = bci;
2546   scope_data()-&gt;set_stream(&amp;s);
2547   // iterate
2548   Bytecodes::Code code = Bytecodes::_illegal;
2549   bool push_exception = false;
2550 
2551   if (block()-&gt;is_set(BlockBegin::exception_entry_flag) &amp;&amp; block()-&gt;next() == NULL) {
2552     // first thing in the exception entry block should be the exception object.
2553     push_exception = true;
2554   }
2555 
2556   while (!bailed_out() &amp;&amp; last()-&gt;as_BlockEnd() == NULL &amp;&amp;
2557          (code = stream()-&gt;next()) != ciBytecodeStream::EOBC() &amp;&amp;
2558          (block_at(s.cur_bci()) == NULL || block_at(s.cur_bci()) == block())) {
2559     assert(state()-&gt;kind() == ValueStack::Parsing, "invalid state kind");
2560 
2561     if (log != NULL)
2562       log-&gt;set_context("bc code='%d' bci='%d'", (int)code, s.cur_bci());
2563 
2564     // Check for active jsr during OSR compilation
2565     if (compilation()-&gt;is_osr_compile()
2566         &amp;&amp; scope()-&gt;is_top_scope()
2567         &amp;&amp; parsing_jsr()
2568         &amp;&amp; s.cur_bci() == compilation()-&gt;osr_bci()) {
2569       bailout("OSR not supported while a jsr is active");
2570     }
2571 
2572     if (push_exception) {
2573       apush(append(new ExceptionObject()));
2574       push_exception = false;
2575     }
2576 
2577     // handle bytecode
2578     switch (code) {
2579       case Bytecodes::_nop            : /* nothing to do */ break;
2580       case Bytecodes::_aconst_null    : apush(append(new Constant(objectNull            ))); break;
2581       case Bytecodes::_iconst_m1      : ipush(append(new Constant(new IntConstant   (-1)))); break;
2582       case Bytecodes::_iconst_0       : ipush(append(new Constant(intZero               ))); break;
2583       case Bytecodes::_iconst_1       : ipush(append(new Constant(intOne                ))); break;
2584       case Bytecodes::_iconst_2       : ipush(append(new Constant(new IntConstant   ( 2)))); break;
2585       case Bytecodes::_iconst_3       : ipush(append(new Constant(new IntConstant   ( 3)))); break;
2586       case Bytecodes::_iconst_4       : ipush(append(new Constant(new IntConstant   ( 4)))); break;
2587       case Bytecodes::_iconst_5       : ipush(append(new Constant(new IntConstant   ( 5)))); break;
2588       case Bytecodes::_lconst_0       : lpush(append(new Constant(new LongConstant  ( 0)))); break;
2589       case Bytecodes::_lconst_1       : lpush(append(new Constant(new LongConstant  ( 1)))); break;
2590       case Bytecodes::_fconst_0       : fpush(append(new Constant(new FloatConstant ( 0)))); break;
2591       case Bytecodes::_fconst_1       : fpush(append(new Constant(new FloatConstant ( 1)))); break;
2592       case Bytecodes::_fconst_2       : fpush(append(new Constant(new FloatConstant ( 2)))); break;
2593       case Bytecodes::_dconst_0       : dpush(append(new Constant(new DoubleConstant( 0)))); break;
2594       case Bytecodes::_dconst_1       : dpush(append(new Constant(new DoubleConstant( 1)))); break;
2595       case Bytecodes::_bipush         : ipush(append(new Constant(new IntConstant(((signed char*)s.cur_bcp())[1])))); break;
2596       case Bytecodes::_sipush         : ipush(append(new Constant(new IntConstant((short)Bytes::get_Java_u2(s.cur_bcp()+1))))); break;
2597       case Bytecodes::_ldc            : // fall through
2598       case Bytecodes::_ldc_w          : // fall through
2599       case Bytecodes::_ldc2_w         : load_constant(); break;
2600       case Bytecodes::_iload          : load_local(intType     , s.get_index()); break;
2601       case Bytecodes::_lload          : load_local(longType    , s.get_index()); break;
2602       case Bytecodes::_fload          : load_local(floatType   , s.get_index()); break;
2603       case Bytecodes::_dload          : load_local(doubleType  , s.get_index()); break;
2604       case Bytecodes::_aload          : load_local(instanceType, s.get_index()); break;
2605       case Bytecodes::_iload_0        : load_local(intType   , 0); break;
2606       case Bytecodes::_iload_1        : load_local(intType   , 1); break;
2607       case Bytecodes::_iload_2        : load_local(intType   , 2); break;
2608       case Bytecodes::_iload_3        : load_local(intType   , 3); break;
2609       case Bytecodes::_lload_0        : load_local(longType  , 0); break;
2610       case Bytecodes::_lload_1        : load_local(longType  , 1); break;
2611       case Bytecodes::_lload_2        : load_local(longType  , 2); break;
2612       case Bytecodes::_lload_3        : load_local(longType  , 3); break;
2613       case Bytecodes::_fload_0        : load_local(floatType , 0); break;
2614       case Bytecodes::_fload_1        : load_local(floatType , 1); break;
2615       case Bytecodes::_fload_2        : load_local(floatType , 2); break;
2616       case Bytecodes::_fload_3        : load_local(floatType , 3); break;
2617       case Bytecodes::_dload_0        : load_local(doubleType, 0); break;
2618       case Bytecodes::_dload_1        : load_local(doubleType, 1); break;
2619       case Bytecodes::_dload_2        : load_local(doubleType, 2); break;
2620       case Bytecodes::_dload_3        : load_local(doubleType, 3); break;
2621       case Bytecodes::_aload_0        : load_local(objectType, 0); break;
2622       case Bytecodes::_aload_1        : load_local(objectType, 1); break;
2623       case Bytecodes::_aload_2        : load_local(objectType, 2); break;
2624       case Bytecodes::_aload_3        : load_local(objectType, 3); break;
2625       case Bytecodes::_iaload         : load_indexed(T_INT   ); break;
2626       case Bytecodes::_laload         : load_indexed(T_LONG  ); break;
2627       case Bytecodes::_faload         : load_indexed(T_FLOAT ); break;
2628       case Bytecodes::_daload         : load_indexed(T_DOUBLE); break;
2629       case Bytecodes::_aaload         : load_indexed(T_OBJECT); break;
2630       case Bytecodes::_baload         : load_indexed(T_BYTE  ); break;
2631       case Bytecodes::_caload         : load_indexed(T_CHAR  ); break;
2632       case Bytecodes::_saload         : load_indexed(T_SHORT ); break;
2633       case Bytecodes::_istore         : store_local(intType   , s.get_index()); break;
2634       case Bytecodes::_lstore         : store_local(longType  , s.get_index()); break;
2635       case Bytecodes::_fstore         : store_local(floatType , s.get_index()); break;
2636       case Bytecodes::_dstore         : store_local(doubleType, s.get_index()); break;
2637       case Bytecodes::_astore         : store_local(objectType, s.get_index()); break;
2638       case Bytecodes::_istore_0       : store_local(intType   , 0); break;
2639       case Bytecodes::_istore_1       : store_local(intType   , 1); break;
2640       case Bytecodes::_istore_2       : store_local(intType   , 2); break;
2641       case Bytecodes::_istore_3       : store_local(intType   , 3); break;
2642       case Bytecodes::_lstore_0       : store_local(longType  , 0); break;
2643       case Bytecodes::_lstore_1       : store_local(longType  , 1); break;
2644       case Bytecodes::_lstore_2       : store_local(longType  , 2); break;
2645       case Bytecodes::_lstore_3       : store_local(longType  , 3); break;
2646       case Bytecodes::_fstore_0       : store_local(floatType , 0); break;
2647       case Bytecodes::_fstore_1       : store_local(floatType , 1); break;
2648       case Bytecodes::_fstore_2       : store_local(floatType , 2); break;
2649       case Bytecodes::_fstore_3       : store_local(floatType , 3); break;
2650       case Bytecodes::_dstore_0       : store_local(doubleType, 0); break;
2651       case Bytecodes::_dstore_1       : store_local(doubleType, 1); break;
2652       case Bytecodes::_dstore_2       : store_local(doubleType, 2); break;
2653       case Bytecodes::_dstore_3       : store_local(doubleType, 3); break;
2654       case Bytecodes::_astore_0       : store_local(objectType, 0); break;
2655       case Bytecodes::_astore_1       : store_local(objectType, 1); break;
2656       case Bytecodes::_astore_2       : store_local(objectType, 2); break;
2657       case Bytecodes::_astore_3       : store_local(objectType, 3); break;
2658       case Bytecodes::_iastore        : store_indexed(T_INT   ); break;
2659       case Bytecodes::_lastore        : store_indexed(T_LONG  ); break;
2660       case Bytecodes::_fastore        : store_indexed(T_FLOAT ); break;
2661       case Bytecodes::_dastore        : store_indexed(T_DOUBLE); break;
2662       case Bytecodes::_aastore        : store_indexed(T_OBJECT); break;
2663       case Bytecodes::_bastore        : store_indexed(T_BYTE  ); break;
2664       case Bytecodes::_castore        : store_indexed(T_CHAR  ); break;
2665       case Bytecodes::_sastore        : store_indexed(T_SHORT ); break;
2666       case Bytecodes::_pop            : // fall through
2667       case Bytecodes::_pop2           : // fall through
2668       case Bytecodes::_dup            : // fall through
2669       case Bytecodes::_dup_x1         : // fall through
2670       case Bytecodes::_dup_x2         : // fall through
2671       case Bytecodes::_dup2           : // fall through
2672       case Bytecodes::_dup2_x1        : // fall through
2673       case Bytecodes::_dup2_x2        : // fall through
2674       case Bytecodes::_swap           : stack_op(code); break;
2675       case Bytecodes::_iadd           : arithmetic_op(intType   , code); break;
2676       case Bytecodes::_ladd           : arithmetic_op(longType  , code); break;
2677       case Bytecodes::_fadd           : arithmetic_op(floatType , code); break;
2678       case Bytecodes::_dadd           : arithmetic_op(doubleType, code); break;
2679       case Bytecodes::_isub           : arithmetic_op(intType   , code); break;
2680       case Bytecodes::_lsub           : arithmetic_op(longType  , code); break;
2681       case Bytecodes::_fsub           : arithmetic_op(floatType , code); break;
2682       case Bytecodes::_dsub           : arithmetic_op(doubleType, code); break;
2683       case Bytecodes::_imul           : arithmetic_op(intType   , code); break;
2684       case Bytecodes::_lmul           : arithmetic_op(longType  , code); break;
2685       case Bytecodes::_fmul           : arithmetic_op(floatType , code); break;
2686       case Bytecodes::_dmul           : arithmetic_op(doubleType, code); break;
2687       case Bytecodes::_idiv           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2688       case Bytecodes::_ldiv           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2689       case Bytecodes::_fdiv           : arithmetic_op(floatType , code); break;
2690       case Bytecodes::_ddiv           : arithmetic_op(doubleType, code); break;
2691       case Bytecodes::_irem           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2692       case Bytecodes::_lrem           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2693       case Bytecodes::_frem           : arithmetic_op(floatType , code); break;
2694       case Bytecodes::_drem           : arithmetic_op(doubleType, code); break;
2695       case Bytecodes::_ineg           : negate_op(intType   ); break;
2696       case Bytecodes::_lneg           : negate_op(longType  ); break;
2697       case Bytecodes::_fneg           : negate_op(floatType ); break;
2698       case Bytecodes::_dneg           : negate_op(doubleType); break;
2699       case Bytecodes::_ishl           : shift_op(intType , code); break;
2700       case Bytecodes::_lshl           : shift_op(longType, code); break;
2701       case Bytecodes::_ishr           : shift_op(intType , code); break;
2702       case Bytecodes::_lshr           : shift_op(longType, code); break;
2703       case Bytecodes::_iushr          : shift_op(intType , code); break;
2704       case Bytecodes::_lushr          : shift_op(longType, code); break;
2705       case Bytecodes::_iand           : logic_op(intType , code); break;
2706       case Bytecodes::_land           : logic_op(longType, code); break;
2707       case Bytecodes::_ior            : logic_op(intType , code); break;
2708       case Bytecodes::_lor            : logic_op(longType, code); break;
2709       case Bytecodes::_ixor           : logic_op(intType , code); break;
2710       case Bytecodes::_lxor           : logic_op(longType, code); break;
2711       case Bytecodes::_iinc           : increment(); break;
2712       case Bytecodes::_i2l            : convert(code, T_INT   , T_LONG  ); break;
2713       case Bytecodes::_i2f            : convert(code, T_INT   , T_FLOAT ); break;
2714       case Bytecodes::_i2d            : convert(code, T_INT   , T_DOUBLE); break;
2715       case Bytecodes::_l2i            : convert(code, T_LONG  , T_INT   ); break;
2716       case Bytecodes::_l2f            : convert(code, T_LONG  , T_FLOAT ); break;
2717       case Bytecodes::_l2d            : convert(code, T_LONG  , T_DOUBLE); break;
2718       case Bytecodes::_f2i            : convert(code, T_FLOAT , T_INT   ); break;
2719       case Bytecodes::_f2l            : convert(code, T_FLOAT , T_LONG  ); break;
2720       case Bytecodes::_f2d            : convert(code, T_FLOAT , T_DOUBLE); break;
2721       case Bytecodes::_d2i            : convert(code, T_DOUBLE, T_INT   ); break;
2722       case Bytecodes::_d2l            : convert(code, T_DOUBLE, T_LONG  ); break;
2723       case Bytecodes::_d2f            : convert(code, T_DOUBLE, T_FLOAT ); break;
2724       case Bytecodes::_i2b            : convert(code, T_INT   , T_BYTE  ); break;
2725       case Bytecodes::_i2c            : convert(code, T_INT   , T_CHAR  ); break;
2726       case Bytecodes::_i2s            : convert(code, T_INT   , T_SHORT ); break;
2727       case Bytecodes::_lcmp           : compare_op(longType  , code); break;
2728       case Bytecodes::_fcmpl          : compare_op(floatType , code); break;
2729       case Bytecodes::_fcmpg          : compare_op(floatType , code); break;
2730       case Bytecodes::_dcmpl          : compare_op(doubleType, code); break;
2731       case Bytecodes::_dcmpg          : compare_op(doubleType, code); break;
2732       case Bytecodes::_ifeq           : if_zero(intType   , If::eql); break;
2733       case Bytecodes::_ifne           : if_zero(intType   , If::neq); break;
2734       case Bytecodes::_iflt           : if_zero(intType   , If::lss); break;
2735       case Bytecodes::_ifge           : if_zero(intType   , If::geq); break;
2736       case Bytecodes::_ifgt           : if_zero(intType   , If::gtr); break;
2737       case Bytecodes::_ifle           : if_zero(intType   , If::leq); break;
2738       case Bytecodes::_if_icmpeq      : if_same(intType   , If::eql); break;
2739       case Bytecodes::_if_icmpne      : if_same(intType   , If::neq); break;
2740       case Bytecodes::_if_icmplt      : if_same(intType   , If::lss); break;
2741       case Bytecodes::_if_icmpge      : if_same(intType   , If::geq); break;
2742       case Bytecodes::_if_icmpgt      : if_same(intType   , If::gtr); break;
2743       case Bytecodes::_if_icmple      : if_same(intType   , If::leq); break;
2744       case Bytecodes::_if_acmpeq      : if_same(objectType, If::eql); break;
2745       case Bytecodes::_if_acmpne      : if_same(objectType, If::neq); break;
2746       case Bytecodes::_goto           : _goto(s.cur_bci(), s.get_dest()); break;
2747       case Bytecodes::_jsr            : jsr(s.get_dest()); break;
2748       case Bytecodes::_ret            : ret(s.get_index()); break;
2749       case Bytecodes::_tableswitch    : table_switch(); break;
2750       case Bytecodes::_lookupswitch   : lookup_switch(); break;
2751       case Bytecodes::_ireturn        : method_return(ipop()); break;
2752       case Bytecodes::_lreturn        : method_return(lpop()); break;
2753       case Bytecodes::_freturn        : method_return(fpop()); break;
2754       case Bytecodes::_dreturn        : method_return(dpop()); break;
2755       case Bytecodes::_areturn        : method_return(apop()); break;
2756       case Bytecodes::_return         : method_return(NULL  ); break;
2757       case Bytecodes::_getstatic      : // fall through
2758       case Bytecodes::_putstatic      : // fall through
2759       case Bytecodes::_getfield       : // fall through
2760       case Bytecodes::_putfield       : access_field(code); break;
2761       case Bytecodes::_invokevirtual  : // fall through
2762       case Bytecodes::_invokespecial  : // fall through
2763       case Bytecodes::_invokestatic   : // fall through
2764       case Bytecodes::_invokedynamic  : // fall through
2765       case Bytecodes::_invokeinterface: invoke(code); break;
2766       case Bytecodes::_new            : new_instance(s.get_index_u2()); break;
2767       case Bytecodes::_newarray       : new_type_array(); break;
2768       case Bytecodes::_anewarray      : new_object_array(); break;
2769       case Bytecodes::_arraylength    : { ValueStack* state_before = copy_state_for_exception(); ipush(append(new ArrayLength(apop(), state_before))); break; }
2770       case Bytecodes::_athrow         : throw_op(s.cur_bci()); break;
2771       case Bytecodes::_checkcast      : check_cast(s.get_index_u2()); break;
2772       case Bytecodes::_instanceof     : instance_of(s.get_index_u2()); break;
2773       case Bytecodes::_monitorenter   : monitorenter(apop(), s.cur_bci()); break;
2774       case Bytecodes::_monitorexit    : monitorexit (apop(), s.cur_bci()); break;
2775       case Bytecodes::_wide           : ShouldNotReachHere(); break;
2776       case Bytecodes::_multianewarray : new_multi_array(s.cur_bcp()[3]); break;
2777       case Bytecodes::_ifnull         : if_null(objectType, If::eql); break;
2778       case Bytecodes::_ifnonnull      : if_null(objectType, If::neq); break;
2779       case Bytecodes::_goto_w         : _goto(s.cur_bci(), s.get_far_dest()); break;
2780       case Bytecodes::_jsr_w          : jsr(s.get_far_dest()); break;
2781       case Bytecodes::_breakpoint     : BAILOUT_("concurrent setting of breakpoint", NULL);
2782       default                         : ShouldNotReachHere(); break;
2783     }
2784 
2785     if (log != NULL)
2786       log-&gt;clear_context(); // skip marker if nothing was printed
2787 
2788     // save current bci to setup Goto at the end
2789     prev_bci = s.cur_bci();
2790 
2791   }
2792   CHECK_BAILOUT_(NULL);
2793   // stop processing of this block (see try_inline_full)
2794   if (_skip_block) {
2795     _skip_block = false;
2796     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
2797     return _last-&gt;as_BlockEnd();
2798   }
2799   // if there are any, check if last instruction is a BlockEnd instruction
2800   BlockEnd* end = last()-&gt;as_BlockEnd();
2801   if (end == NULL) {
2802     // all blocks must end with a BlockEnd instruction =&gt; add a Goto
2803     end = new Goto(block_at(s.cur_bci()), false);
2804     append(end);
2805   }
2806   assert(end == last()-&gt;as_BlockEnd(), "inconsistency");
2807 
2808   assert(end-&gt;state() != NULL, "state must already be present");
2809   assert(end-&gt;as_Return() == NULL || end-&gt;as_Throw() == NULL || end-&gt;state()-&gt;stack_size() == 0, "stack not needed for return and throw");
2810 
2811   // connect to begin &amp; set state
2812   // NOTE that inlining may have changed the block we are parsing
2813   block()-&gt;set_end(end);
2814   // propagate state
2815   for (int i = end-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
2816     BlockBegin* sux = end-&gt;sux_at(i);
2817     assert(sux-&gt;is_predecessor(block()), "predecessor missing");
2818     // be careful, bailout if bytecodes are strange
2819     if (!sux-&gt;try_merge(end-&gt;state())) BAILOUT_("block join failed", NULL);
2820     scope_data()-&gt;add_to_work_list(end-&gt;sux_at(i));
2821   }
2822 
2823   scope_data()-&gt;set_stream(NULL);
2824 
2825   // done
2826   return end;
2827 }
2828 
2829 
2830 void GraphBuilder::iterate_all_blocks(bool start_in_current_block_for_inlining) {
2831   do {
2832     if (start_in_current_block_for_inlining &amp;&amp; !bailed_out()) {
2833       iterate_bytecodes_for_block(0);
2834       start_in_current_block_for_inlining = false;
2835     } else {
2836       BlockBegin* b;
2837       while ((b = scope_data()-&gt;remove_from_work_list()) != NULL) {
2838         if (!b-&gt;is_set(BlockBegin::was_visited_flag)) {
2839           if (b-&gt;is_set(BlockBegin::osr_entry_flag)) {
2840             // we're about to parse the osr entry block, so make sure
2841             // we setup the OSR edge leading into this block so that
2842             // Phis get setup correctly.
2843             setup_osr_entry_block();
2844             // this is no longer the osr entry block, so clear it.
2845             b-&gt;clear(BlockBegin::osr_entry_flag);
2846           }
2847           b-&gt;set(BlockBegin::was_visited_flag);
2848           connect_to_end(b);
2849         }
2850       }
2851     }
2852   } while (!bailed_out() &amp;&amp; !scope_data()-&gt;is_work_list_empty());
2853 }
2854 
2855 
2856 bool GraphBuilder::_can_trap      [Bytecodes::number_of_java_codes];
2857 
2858 void GraphBuilder::initialize() {
2859   // the following bytecodes are assumed to potentially
2860   // throw exceptions in compiled code - note that e.g.
2861   // monitorexit &amp; the return bytecodes do not throw
2862   // exceptions since monitor pairing proved that they
2863   // succeed (if monitor pairing succeeded)
2864   Bytecodes::Code can_trap_list[] =
2865     { Bytecodes::_ldc
2866     , Bytecodes::_ldc_w
2867     , Bytecodes::_ldc2_w
2868     , Bytecodes::_iaload
2869     , Bytecodes::_laload
2870     , Bytecodes::_faload
2871     , Bytecodes::_daload
2872     , Bytecodes::_aaload
2873     , Bytecodes::_baload
2874     , Bytecodes::_caload
2875     , Bytecodes::_saload
2876     , Bytecodes::_iastore
2877     , Bytecodes::_lastore
2878     , Bytecodes::_fastore
2879     , Bytecodes::_dastore
2880     , Bytecodes::_aastore
2881     , Bytecodes::_bastore
2882     , Bytecodes::_castore
2883     , Bytecodes::_sastore
2884     , Bytecodes::_idiv
2885     , Bytecodes::_ldiv
2886     , Bytecodes::_irem
2887     , Bytecodes::_lrem
2888     , Bytecodes::_getstatic
2889     , Bytecodes::_putstatic
2890     , Bytecodes::_getfield
2891     , Bytecodes::_putfield
2892     , Bytecodes::_invokevirtual
2893     , Bytecodes::_invokespecial
2894     , Bytecodes::_invokestatic
2895     , Bytecodes::_invokedynamic
2896     , Bytecodes::_invokeinterface
2897     , Bytecodes::_new
2898     , Bytecodes::_newarray
2899     , Bytecodes::_anewarray
2900     , Bytecodes::_arraylength
2901     , Bytecodes::_athrow
2902     , Bytecodes::_checkcast
2903     , Bytecodes::_instanceof
2904     , Bytecodes::_monitorenter
2905     , Bytecodes::_multianewarray
2906     };
2907 
2908   // inititialize trap tables
2909   for (int i = 0; i &lt; Bytecodes::number_of_java_codes; i++) {
2910     _can_trap[i] = false;
2911   }
2912   // set standard trap info
2913   for (uint j = 0; j &lt; ARRAY_SIZE(can_trap_list); j++) {
2914     _can_trap[can_trap_list[j]] = true;
2915   }
2916 }
2917 
2918 
2919 BlockBegin* GraphBuilder::header_block(BlockBegin* entry, BlockBegin::Flag f, ValueStack* state) {
2920   assert(entry-&gt;is_set(f), "entry/flag mismatch");
2921   // create header block
2922   BlockBegin* h = new BlockBegin(entry-&gt;bci());
2923   h-&gt;set_depth_first_number(0);
2924 
2925   Value l = h;
2926   BlockEnd* g = new Goto(entry, false);
2927   l-&gt;set_next(g, entry-&gt;bci());
2928   h-&gt;set_end(g);
2929   h-&gt;set(f);
2930   // setup header block end state
2931   ValueStack* s = state-&gt;copy(ValueStack::StateAfter, entry-&gt;bci()); // can use copy since stack is empty (=&gt; no phis)
2932   assert(s-&gt;stack_is_empty(), "must have empty stack at entry point");
2933   g-&gt;set_state(s);
2934   return h;
2935 }
2936 
2937 
2938 
2939 BlockBegin* GraphBuilder::setup_start_block(int osr_bci, BlockBegin* std_entry, BlockBegin* osr_entry, ValueStack* state) {
2940   BlockBegin* start = new BlockBegin(0);
2941 
2942   // This code eliminates the empty start block at the beginning of
2943   // each method.  Previously, each method started with the
2944   // start-block created below, and this block was followed by the
2945   // header block that was always empty.  This header block is only
2946   // necesary if std_entry is also a backward branch target because
2947   // then phi functions may be necessary in the header block.  It's
2948   // also necessary when profiling so that there's a single block that
2949   // can increment the interpreter_invocation_count.
2950   BlockBegin* new_header_block;
2951   if (std_entry-&gt;number_of_preds() &gt; 0 || count_invocations() || count_backedges()) {
2952     new_header_block = header_block(std_entry, BlockBegin::std_entry_flag, state);
2953   } else {
2954     new_header_block = std_entry;
2955   }
2956 
2957   // setup start block (root for the IR graph)
2958   Base* base =
2959     new Base(
2960       new_header_block,
2961       osr_entry
2962     );
2963   start-&gt;set_next(base, 0);
2964   start-&gt;set_end(base);
2965   // create &amp; setup state for start block
2966   start-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2967   base-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2968 
2969   if (base-&gt;std_entry()-&gt;state() == NULL) {
2970     // setup states for header blocks
2971     base-&gt;std_entry()-&gt;merge(state);
2972   }
2973 
2974   assert(base-&gt;std_entry()-&gt;state() != NULL, "");
2975   return start;
2976 }
2977 
2978 
2979 void GraphBuilder::setup_osr_entry_block() {
2980   assert(compilation()-&gt;is_osr_compile(), "only for osrs");
2981 
2982   int osr_bci = compilation()-&gt;osr_bci();
2983   ciBytecodeStream s(method());
2984   s.reset_to_bci(osr_bci);
2985   s.next();
2986   scope_data()-&gt;set_stream(&amp;s);
2987 
2988   // create a new block to be the osr setup code
2989   _osr_entry = new BlockBegin(osr_bci);
2990   _osr_entry-&gt;set(BlockBegin::osr_entry_flag);
2991   _osr_entry-&gt;set_depth_first_number(0);
2992   BlockBegin* target = bci2block()-&gt;at(osr_bci);
2993   assert(target != NULL &amp;&amp; target-&gt;is_set(BlockBegin::osr_entry_flag), "must be there");
2994   // the osr entry has no values for locals
2995   ValueStack* state = target-&gt;state()-&gt;copy();
2996   _osr_entry-&gt;set_state(state);
2997 
2998   kill_all();
2999   _block = _osr_entry;
3000   _state = _osr_entry-&gt;state()-&gt;copy();
3001   assert(_state-&gt;bci() == osr_bci, "mismatch");
3002   _last  = _osr_entry;
3003   Value e = append(new OsrEntry());
3004   e-&gt;set_needs_null_check(false);
3005 
3006   // OSR buffer is
3007   //
3008   // locals[nlocals-1..0]
3009   // monitors[number_of_locks-1..0]
3010   //
3011   // locals is a direct copy of the interpreter frame so in the osr buffer
3012   // so first slot in the local array is the last local from the interpreter
3013   // and last slot is local[0] (receiver) from the interpreter
3014   //
3015   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
3016   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
3017   // in the interpreter frame (the method lock if a sync method)
3018 
3019   // Initialize monitors in the compiled activation.
3020 
3021   int index;
3022   Value local;
3023 
3024   // find all the locals that the interpreter thinks contain live oops
3025   const BitMap live_oops = method()-&gt;live_local_oops_at_bci(osr_bci);
3026 
3027   // compute the offset into the locals so that we can treat the buffer
3028   // as if the locals were still in the interpreter frame
3029   int locals_offset = BytesPerWord * (method()-&gt;max_locals() - 1);
3030   for_each_local_value(state, index, local) {
3031     int offset = locals_offset - (index + local-&gt;type()-&gt;size() - 1) * BytesPerWord;
3032     Value get;
3033     if (local-&gt;type()-&gt;is_object_kind() &amp;&amp; !live_oops.at(index)) {
3034       // The interpreter thinks this local is dead but the compiler
3035       // doesn't so pretend that the interpreter passed in null.
3036       get = append(new Constant(objectNull));
3037     } else {
3038       get = append(new UnsafeGetRaw(as_BasicType(local-&gt;type()), e,
3039                                     append(new Constant(new IntConstant(offset))),
3040                                     0,
3041                                     true /*unaligned*/, true /*wide*/));
3042     }
3043     _state-&gt;store_local(index, get);
3044   }
3045 
3046   // the storage for the OSR buffer is freed manually in the LIRGenerator.
3047 
3048   assert(state-&gt;caller_state() == NULL, "should be top scope");
3049   state-&gt;clear_locals();
3050   Goto* g = new Goto(target, false);
3051   append(g);
3052   _osr_entry-&gt;set_end(g);
3053   target-&gt;merge(_osr_entry-&gt;end()-&gt;state());
3054 
3055   scope_data()-&gt;set_stream(NULL);
3056 }
3057 
3058 
3059 ValueStack* GraphBuilder::state_at_entry() {
3060   ValueStack* state = new ValueStack(scope(), NULL);
3061 
3062   // Set up locals for receiver
3063   int idx = 0;
3064   if (!method()-&gt;is_static()) {
3065     // we should always see the receiver
3066     state-&gt;store_local(idx, new Local(method()-&gt;holder(), objectType, idx));
3067     idx = 1;
3068   }
3069 
3070   // Set up locals for incoming arguments
3071   ciSignature* sig = method()-&gt;signature();
3072   for (int i = 0; i &lt; sig-&gt;count(); i++) {
3073     ciType* type = sig-&gt;type_at(i);
3074     BasicType basic_type = type-&gt;basic_type();
3075     // don't allow T_ARRAY to propagate into locals types
3076     if (basic_type == T_ARRAY) basic_type = T_OBJECT;
3077     ValueType* vt = as_ValueType(basic_type);
3078     state-&gt;store_local(idx, new Local(type, vt, idx));
3079     idx += type-&gt;size();
3080   }
3081 
3082   // lock synchronized method
3083   if (method()-&gt;is_synchronized()) {
3084     state-&gt;lock(NULL);
3085   }
3086 
3087   return state;
3088 }
3089 
3090 
3091 GraphBuilder::GraphBuilder(Compilation* compilation, IRScope* scope)
3092   : _scope_data(NULL)
3093   , _instruction_count(0)
3094   , _osr_entry(NULL)
3095   , _memory(new MemoryBuffer())
3096   , _compilation(compilation)
3097   , _inline_bailout_msg(NULL)
3098 {
3099   int osr_bci = compilation-&gt;osr_bci();
3100 
3101   // determine entry points and bci2block mapping
3102   BlockListBuilder blm(compilation, scope, osr_bci);
3103   CHECK_BAILOUT();
3104 
3105   BlockList* bci2block = blm.bci2block();
3106   BlockBegin* start_block = bci2block-&gt;at(0);
3107 
3108   push_root_scope(scope, bci2block, start_block);
3109 
3110   // setup state for std entry
3111   _initial_state = state_at_entry();
3112   start_block-&gt;merge(_initial_state);
3113 
3114   // complete graph
3115   _vmap        = new ValueMap();
3116   switch (scope-&gt;method()-&gt;intrinsic_id()) {
3117   case vmIntrinsics::_dabs          : // fall through
3118   case vmIntrinsics::_dsqrt         : // fall through
3119   case vmIntrinsics::_dsin          : // fall through
3120   case vmIntrinsics::_dcos          : // fall through
3121   case vmIntrinsics::_dtan          : // fall through
3122   case vmIntrinsics::_dlog          : // fall through
3123   case vmIntrinsics::_dlog10        : // fall through
3124   case vmIntrinsics::_dexp          : // fall through
3125   case vmIntrinsics::_dpow          : // fall through
3126     {
3127       // Compiles where the root method is an intrinsic need a special
3128       // compilation environment because the bytecodes for the method
3129       // shouldn't be parsed during the compilation, only the special
3130       // Intrinsic node should be emitted.  If this isn't done the the
3131       // code for the inlined version will be different than the root
3132       // compiled version which could lead to monotonicity problems on
3133       // intel.
3134 
3135       // Set up a stream so that appending instructions works properly.
3136       ciBytecodeStream s(scope-&gt;method());
3137       s.reset_to_bci(0);
3138       scope_data()-&gt;set_stream(&amp;s);
3139       s.next();
3140 
3141       // setup the initial block state
3142       _block = start_block;
3143       _state = start_block-&gt;state()-&gt;copy_for_parsing();
3144       _last  = start_block;
3145       load_local(doubleType, 0);
3146       if (scope-&gt;method()-&gt;intrinsic_id() == vmIntrinsics::_dpow) {
3147         load_local(doubleType, 2);
3148       }
3149 
3150       // Emit the intrinsic node.
3151       bool result = try_inline_intrinsics(scope-&gt;method());
3152       if (!result) BAILOUT("failed to inline intrinsic");
3153       method_return(dpop());
3154 
3155       // connect the begin and end blocks and we're all done.
3156       BlockEnd* end = last()-&gt;as_BlockEnd();
3157       block()-&gt;set_end(end);
3158       break;
3159     }
3160 
3161   case vmIntrinsics::_Reference_get:
3162     {
3163       {
3164         // With java.lang.ref.reference.get() we must go through the
3165         // intrinsic - when G1 is enabled - even when get() is the root
3166         // method of the compile so that, if necessary, the value in
3167         // the referent field of the reference object gets recorded by
3168         // the pre-barrier code.
3169         // Specifically, if G1 is enabled, the value in the referent
3170         // field is recorded by the G1 SATB pre barrier. This will
3171         // result in the referent being marked live and the reference
3172         // object removed from the list of discovered references during
3173         // reference processing.
3174 
3175         // Also we need intrinsic to prevent commoning reads from this field
3176         // across safepoint since GC can change its value.
3177 
3178         // Set up a stream so that appending instructions works properly.
3179         ciBytecodeStream s(scope-&gt;method());
3180         s.reset_to_bci(0);
3181         scope_data()-&gt;set_stream(&amp;s);
3182         s.next();
3183 
3184         // setup the initial block state
3185         _block = start_block;
3186         _state = start_block-&gt;state()-&gt;copy_for_parsing();
3187         _last  = start_block;
3188         load_local(objectType, 0);
3189 
3190         // Emit the intrinsic node.
3191         bool result = try_inline_intrinsics(scope-&gt;method());
3192         if (!result) BAILOUT("failed to inline intrinsic");
3193         method_return(apop());
3194 
3195         // connect the begin and end blocks and we're all done.
3196         BlockEnd* end = last()-&gt;as_BlockEnd();
3197         block()-&gt;set_end(end);
3198         break;
3199       }
3200       // Otherwise, fall thru
3201     }
3202 
3203   default:
3204     scope_data()-&gt;add_to_work_list(start_block);
3205     iterate_all_blocks();
3206     break;
3207   }
3208   CHECK_BAILOUT();
3209 
3210   _start = setup_start_block(osr_bci, start_block, _osr_entry, _initial_state);
3211 
3212   eliminate_redundant_phis(_start);
3213 
3214   NOT_PRODUCT(if (PrintValueNumbering &amp;&amp; Verbose) print_stats());
3215   // for osr compile, bailout if some requirements are not fulfilled
3216   if (osr_bci != -1) {
3217     BlockBegin* osr_block = blm.bci2block()-&gt;at(osr_bci);
3218     assert(osr_block-&gt;is_set(BlockBegin::was_visited_flag),"osr entry must have been visited for osr compile");
3219 
3220     // check if osr entry point has empty stack - we cannot handle non-empty stacks at osr entry points
3221     if (!osr_block-&gt;state()-&gt;stack_is_empty()) {
3222       BAILOUT("stack not empty at OSR entry point");
3223     }
3224   }
3225 #ifndef PRODUCT
3226   if (PrintCompilation &amp;&amp; Verbose) tty-&gt;print_cr("Created %d Instructions", _instruction_count);
3227 #endif
3228 }
3229 
3230 
3231 ValueStack* GraphBuilder::copy_state_before() {
3232   return copy_state_before_with_bci(bci());
3233 }
3234 
3235 ValueStack* GraphBuilder::copy_state_exhandling() {
3236   return copy_state_exhandling_with_bci(bci());
3237 }
3238 
3239 ValueStack* GraphBuilder::copy_state_for_exception() {
3240   return copy_state_for_exception_with_bci(bci());
3241 }
3242 
3243 ValueStack* GraphBuilder::copy_state_before_with_bci(int bci) {
3244   return state()-&gt;copy(ValueStack::StateBefore, bci);
3245 }
3246 
3247 ValueStack* GraphBuilder::copy_state_exhandling_with_bci(int bci) {
3248   if (!has_handler()) return NULL;
3249   return state()-&gt;copy(ValueStack::StateBefore, bci);
3250 }
3251 
3252 ValueStack* GraphBuilder::copy_state_for_exception_with_bci(int bci) {
3253   ValueStack* s = copy_state_exhandling_with_bci(bci);
3254   if (s == NULL) {
3255     if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
3256       s = state()-&gt;copy(ValueStack::ExceptionState, bci);
3257     } else {
3258       s = state()-&gt;copy(ValueStack::EmptyExceptionState, bci);
3259     }
3260   }
3261   return s;
3262 }
3263 
3264 int GraphBuilder::recursive_inline_level(ciMethod* cur_callee) const {
3265   int recur_level = 0;
3266   for (IRScope* s = scope(); s != NULL; s = s-&gt;caller()) {
3267     if (s-&gt;method() == cur_callee) {
3268       ++recur_level;
3269     }
3270   }
3271   return recur_level;
3272 }
3273 
3274 
3275 bool GraphBuilder::try_inline(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3276   const char* msg = NULL;
3277 
3278   // clear out any existing inline bailout condition
3279   clear_inline_bailout();
3280 
3281   // exclude methods we don't want to inline
3282   msg = should_not_inline(callee);
3283   if (msg != NULL) {
3284     print_inlining(callee, msg, /*success*/ false);
3285     return false;
3286   }
3287 
3288   // method handle invokes
3289   if (callee-&gt;is_method_handle_intrinsic()) {
3290     return try_method_handle_inline(callee);
3291   }
3292 
3293   // handle intrinsics
3294   if (callee-&gt;intrinsic_id() != vmIntrinsics::_none) {
3295     if (try_inline_intrinsics(callee)) {
3296       print_inlining(callee, "intrinsic");
3297       return true;
3298     }
3299     // try normal inlining
3300   }
3301 
3302   // certain methods cannot be parsed at all
3303   msg = check_can_parse(callee);
3304   if (msg != NULL) {
3305     print_inlining(callee, msg, /*success*/ false);
3306     return false;
3307   }
3308 
3309   // If bytecode not set use the current one.
3310   if (bc == Bytecodes::_illegal) {
3311     bc = code();
3312   }
3313   if (try_inline_full(callee, holder_known, bc, receiver))
3314     return true;
3315 
3316   // Entire compilation could fail during try_inline_full call.
3317   // In that case printing inlining decision info is useless.
3318   if (!bailed_out())
3319     print_inlining(callee, _inline_bailout_msg, /*success*/ false);
3320 
3321   return false;
3322 }
3323 
3324 
3325 const char* GraphBuilder::check_can_parse(ciMethod* callee) const {
3326   // Certain methods cannot be parsed at all:
3327   if ( callee-&gt;is_native())            return "native method";
3328   if ( callee-&gt;is_abstract())          return "abstract method";
3329   if (!callee-&gt;can_be_compiled())      return "not compilable (disabled)";
3330   return NULL;
3331 }
3332 
3333 
3334 // negative filter: should callee NOT be inlined?  returns NULL, ok to inline, or rejection msg
3335 const char* GraphBuilder::should_not_inline(ciMethod* callee) const {
3336   if ( callee-&gt;should_exclude())       return "excluded by CompilerOracle";
3337   if ( callee-&gt;should_not_inline())    return "disallowed by CompilerOracle";
3338   if ( callee-&gt;dont_inline())          return "don't inline by annotation";
3339   return NULL;
3340 }
3341 
3342 
3343 bool GraphBuilder::try_inline_intrinsics(ciMethod* callee) {
3344   if (callee-&gt;is_synchronized()) {
3345     // We don't currently support any synchronized intrinsics
3346     return false;
3347   }
3348 
3349   // callee seems like a good candidate
3350   // determine id
3351   vmIntrinsics::ID id = callee-&gt;intrinsic_id();
3352   if (!InlineNatives &amp;&amp; id != vmIntrinsics::_Reference_get) {
3353     // InlineNatives does not control Reference.get
3354     INLINE_BAILOUT("intrinsic method inlining disabled");
3355   }
3356   bool preserves_state = false;
3357   bool cantrap = true;
3358   switch (id) {
3359     case vmIntrinsics::_arraycopy:
3360       if (!InlineArrayCopy) return false;
3361       break;
3362 
3363 #ifdef TRACE_HAVE_INTRINSICS
3364     case vmIntrinsics::_classID:
3365     case vmIntrinsics::_threadID:
3366       preserves_state = true;
3367       cantrap = true;
3368       break;
3369 
3370     case vmIntrinsics::_counterTime:
3371       preserves_state = true;
3372       cantrap = false;
3373       break;
3374 #endif
3375 
3376     case vmIntrinsics::_currentTimeMillis:
3377     case vmIntrinsics::_nanoTime:
3378       preserves_state = true;
3379       cantrap = false;
3380       break;
3381 
3382     case vmIntrinsics::_floatToRawIntBits   :
3383     case vmIntrinsics::_intBitsToFloat      :
3384     case vmIntrinsics::_doubleToRawLongBits :
3385     case vmIntrinsics::_longBitsToDouble    :
3386       if (!InlineMathNatives) return false;
3387       preserves_state = true;
3388       cantrap = false;
3389       break;
3390 
3391     case vmIntrinsics::_getClass      :
3392     case vmIntrinsics::_isInstance    :
3393       if (!InlineClassNatives) return false;
3394       preserves_state = true;
3395       break;
3396 
3397     case vmIntrinsics::_currentThread :
3398       if (!InlineThreadNatives) return false;
3399       preserves_state = true;
3400       cantrap = false;
3401       break;
3402 
3403     case vmIntrinsics::_dabs          : // fall through
3404     case vmIntrinsics::_dsqrt         : // fall through
3405     case vmIntrinsics::_dsin          : // fall through
3406     case vmIntrinsics::_dcos          : // fall through
3407     case vmIntrinsics::_dtan          : // fall through
3408     case vmIntrinsics::_dlog          : // fall through
3409     case vmIntrinsics::_dlog10        : // fall through
3410     case vmIntrinsics::_dexp          : // fall through
3411     case vmIntrinsics::_dpow          : // fall through
3412       if (!InlineMathNatives) return false;
3413       cantrap = false;
3414       preserves_state = true;
3415       break;
3416 
3417     // Use special nodes for Unsafe instructions so we can more easily
3418     // perform an address-mode optimization on the raw variants
3419     case vmIntrinsics::_getObject : return append_unsafe_get_obj(callee, T_OBJECT,  false);
3420     case vmIntrinsics::_getBoolean: return append_unsafe_get_obj(callee, T_BOOLEAN, false);
3421     case vmIntrinsics::_getByte   : return append_unsafe_get_obj(callee, T_BYTE,    false);
3422     case vmIntrinsics::_getShort  : return append_unsafe_get_obj(callee, T_SHORT,   false);
3423     case vmIntrinsics::_getChar   : return append_unsafe_get_obj(callee, T_CHAR,    false);
3424     case vmIntrinsics::_getInt    : return append_unsafe_get_obj(callee, T_INT,     false);
3425     case vmIntrinsics::_getLong   : return append_unsafe_get_obj(callee, T_LONG,    false);
3426     case vmIntrinsics::_getFloat  : return append_unsafe_get_obj(callee, T_FLOAT,   false);
3427     case vmIntrinsics::_getDouble : return append_unsafe_get_obj(callee, T_DOUBLE,  false);
3428 
3429     case vmIntrinsics::_putObject : return append_unsafe_put_obj(callee, T_OBJECT,  false);
3430     case vmIntrinsics::_putBoolean: return append_unsafe_put_obj(callee, T_BOOLEAN, false);
3431     case vmIntrinsics::_putByte   : return append_unsafe_put_obj(callee, T_BYTE,    false);
3432     case vmIntrinsics::_putShort  : return append_unsafe_put_obj(callee, T_SHORT,   false);
3433     case vmIntrinsics::_putChar   : return append_unsafe_put_obj(callee, T_CHAR,    false);
3434     case vmIntrinsics::_putInt    : return append_unsafe_put_obj(callee, T_INT,     false);
3435     case vmIntrinsics::_putLong   : return append_unsafe_put_obj(callee, T_LONG,    false);
3436     case vmIntrinsics::_putFloat  : return append_unsafe_put_obj(callee, T_FLOAT,   false);
3437     case vmIntrinsics::_putDouble : return append_unsafe_put_obj(callee, T_DOUBLE,  false);
3438 
3439     case vmIntrinsics::_getObjectVolatile : return append_unsafe_get_obj(callee, T_OBJECT,  true);
3440     case vmIntrinsics::_getBooleanVolatile: return append_unsafe_get_obj(callee, T_BOOLEAN, true);
3441     case vmIntrinsics::_getByteVolatile   : return append_unsafe_get_obj(callee, T_BYTE,    true);
3442     case vmIntrinsics::_getShortVolatile  : return append_unsafe_get_obj(callee, T_SHORT,   true);
3443     case vmIntrinsics::_getCharVolatile   : return append_unsafe_get_obj(callee, T_CHAR,    true);
3444     case vmIntrinsics::_getIntVolatile    : return append_unsafe_get_obj(callee, T_INT,     true);
3445     case vmIntrinsics::_getLongVolatile   : return append_unsafe_get_obj(callee, T_LONG,    true);
3446     case vmIntrinsics::_getFloatVolatile  : return append_unsafe_get_obj(callee, T_FLOAT,   true);
3447     case vmIntrinsics::_getDoubleVolatile : return append_unsafe_get_obj(callee, T_DOUBLE,  true);
3448 
3449     case vmIntrinsics::_putObjectVolatile : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3450     case vmIntrinsics::_putBooleanVolatile: return append_unsafe_put_obj(callee, T_BOOLEAN, true);
3451     case vmIntrinsics::_putByteVolatile   : return append_unsafe_put_obj(callee, T_BYTE,    true);
3452     case vmIntrinsics::_putShortVolatile  : return append_unsafe_put_obj(callee, T_SHORT,   true);
3453     case vmIntrinsics::_putCharVolatile   : return append_unsafe_put_obj(callee, T_CHAR,    true);
3454     case vmIntrinsics::_putIntVolatile    : return append_unsafe_put_obj(callee, T_INT,     true);
3455     case vmIntrinsics::_putLongVolatile   : return append_unsafe_put_obj(callee, T_LONG,    true);
3456     case vmIntrinsics::_putFloatVolatile  : return append_unsafe_put_obj(callee, T_FLOAT,   true);
3457     case vmIntrinsics::_putDoubleVolatile : return append_unsafe_put_obj(callee, T_DOUBLE,  true);
3458 
3459     case vmIntrinsics::_getByte_raw   : return append_unsafe_get_raw(callee, T_BYTE);
3460     case vmIntrinsics::_getShort_raw  : return append_unsafe_get_raw(callee, T_SHORT);
3461     case vmIntrinsics::_getChar_raw   : return append_unsafe_get_raw(callee, T_CHAR);
3462     case vmIntrinsics::_getInt_raw    : return append_unsafe_get_raw(callee, T_INT);
3463     case vmIntrinsics::_getLong_raw   : return append_unsafe_get_raw(callee, T_LONG);
3464     case vmIntrinsics::_getFloat_raw  : return append_unsafe_get_raw(callee, T_FLOAT);
3465     case vmIntrinsics::_getDouble_raw : return append_unsafe_get_raw(callee, T_DOUBLE);
3466 
3467     case vmIntrinsics::_putByte_raw   : return append_unsafe_put_raw(callee, T_BYTE);
3468     case vmIntrinsics::_putShort_raw  : return append_unsafe_put_raw(callee, T_SHORT);
3469     case vmIntrinsics::_putChar_raw   : return append_unsafe_put_raw(callee, T_CHAR);
3470     case vmIntrinsics::_putInt_raw    : return append_unsafe_put_raw(callee, T_INT);
3471     case vmIntrinsics::_putLong_raw   : return append_unsafe_put_raw(callee, T_LONG);
3472     case vmIntrinsics::_putFloat_raw  : return append_unsafe_put_raw(callee, T_FLOAT);
3473     case vmIntrinsics::_putDouble_raw : return append_unsafe_put_raw(callee, T_DOUBLE);
3474 
3475     case vmIntrinsics::_prefetchRead        : return append_unsafe_prefetch(callee, false, false);
3476     case vmIntrinsics::_prefetchWrite       : return append_unsafe_prefetch(callee, false, true);
3477     case vmIntrinsics::_prefetchReadStatic  : return append_unsafe_prefetch(callee, true,  false);
3478     case vmIntrinsics::_prefetchWriteStatic : return append_unsafe_prefetch(callee, true,  true);
3479 
3480     case vmIntrinsics::_checkIndex    :
3481       if (!InlineNIOCheckIndex) return false;
3482       preserves_state = true;
3483       break;
3484     case vmIntrinsics::_putOrderedObject : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3485     case vmIntrinsics::_putOrderedInt    : return append_unsafe_put_obj(callee, T_INT,     true);
3486     case vmIntrinsics::_putOrderedLong   : return append_unsafe_put_obj(callee, T_LONG,    true);
3487 
3488     case vmIntrinsics::_compareAndSwapLong:
3489       if (!VM_Version::supports_cx8()) return false;
3490       // fall through
3491     case vmIntrinsics::_compareAndSwapInt:
3492     case vmIntrinsics::_compareAndSwapObject:
3493       append_unsafe_CAS(callee);
3494       return true;
3495 
3496     case vmIntrinsics::_getAndAddInt:
3497       if (!VM_Version::supports_atomic_getadd4()) {
3498         return false;
3499       }
3500       return append_unsafe_get_and_set_obj(callee, true);
3501     case vmIntrinsics::_getAndAddLong:
3502       if (!VM_Version::supports_atomic_getadd8()) {
3503         return false;
3504       }
3505       return append_unsafe_get_and_set_obj(callee, true);
3506     case vmIntrinsics::_getAndSetInt:
3507       if (!VM_Version::supports_atomic_getset4()) {
3508         return false;
3509       }
3510       return append_unsafe_get_and_set_obj(callee, false);
3511     case vmIntrinsics::_getAndSetLong:
3512       if (!VM_Version::supports_atomic_getset8()) {
3513         return false;
3514       }
3515       return append_unsafe_get_and_set_obj(callee, false);
3516     case vmIntrinsics::_getAndSetObject:
3517 #ifdef _LP64
3518       if (!UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset8()) {
3519         return false;
3520       }
3521       if (UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset4()) {
3522         return false;
3523       }
3524 #else
3525       if (!VM_Version::supports_atomic_getset4()) {
3526         return false;
3527       }
3528 #endif
3529       return append_unsafe_get_and_set_obj(callee, false);
3530 
3531     case vmIntrinsics::_Reference_get:
3532       // Use the intrinsic version of Reference.get() so that the value in
3533       // the referent field can be registered by the G1 pre-barrier code.
3534       // Also to prevent commoning reads from this field across safepoint
3535       // since GC can change its value.
3536       preserves_state = true;
3537       break;
3538 
3539     case vmIntrinsics::_updateCRC32:
3540     case vmIntrinsics::_updateBytesCRC32:
3541     case vmIntrinsics::_updateByteBufferCRC32:
3542       if (!UseCRC32Intrinsics) return false;
3543       cantrap = false;
3544       preserves_state = true;
3545       break;
3546 
3547     case vmIntrinsics::_loadFence :
3548     case vmIntrinsics::_storeFence:
3549     case vmIntrinsics::_fullFence :
3550       break;
3551 
3552     default                       : return false; // do not inline
3553   }
3554   // create intrinsic node
3555   const bool has_receiver = !callee-&gt;is_static();
3556   ValueType* result_type = as_ValueType(callee-&gt;return_type());
3557   ValueStack* state_before = copy_state_for_exception();
3558 
3559   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
3560 
3561   if (is_profiling()) {
3562     // Don't profile in the special case where the root method
3563     // is the intrinsic
3564     if (callee != method()) {
3565       // Note that we'd collect profile data in this method if we wanted it.
3566       compilation()-&gt;set_would_profile(true);
3567       if (profile_calls()) {
3568         Value recv = NULL;
3569         if (has_receiver) {
3570           recv = args-&gt;at(0);
3571           null_check(recv);
3572         }
3573         profile_call(callee, recv, NULL, collect_args_for_profiling(args, callee, true), true);
3574       }
3575     }
3576   }
3577 
3578   Intrinsic* result = new Intrinsic(result_type, id, args, has_receiver, state_before,
3579                                     preserves_state, cantrap);
3580   // append instruction &amp; push result
3581   Value value = append_split(result);
3582   if (result_type != voidType) push(result_type, value);
3583 
3584   if (callee != method() &amp;&amp; profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
3585     profile_return_type(result, callee);
3586   }
3587 
3588   // done
3589   return true;
3590 }
3591 
3592 
3593 bool GraphBuilder::try_inline_jsr(int jsr_dest_bci) {
3594   // Introduce a new callee continuation point - all Ret instructions
3595   // will be replaced with Gotos to this point.
3596   BlockBegin* cont = block_at(next_bci());
3597   assert(cont != NULL, "continuation must exist (BlockListBuilder starts a new block after a jsr");
3598 
3599   // Note: can not assign state to continuation yet, as we have to
3600   // pick up the state from the Ret instructions.
3601 
3602   // Push callee scope
3603   push_scope_for_jsr(cont, jsr_dest_bci);
3604 
3605   // Temporarily set up bytecode stream so we can append instructions
3606   // (only using the bci of this stream)
3607   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3608 
3609   BlockBegin* jsr_start_block = block_at(jsr_dest_bci);
3610   assert(jsr_start_block != NULL, "jsr start block must exist");
3611   assert(!jsr_start_block-&gt;is_set(BlockBegin::was_visited_flag), "should not have visited jsr yet");
3612   Goto* goto_sub = new Goto(jsr_start_block, false);
3613   // Must copy state to avoid wrong sharing when parsing bytecodes
3614   assert(jsr_start_block-&gt;state() == NULL, "should have fresh jsr starting block");
3615   jsr_start_block-&gt;set_state(copy_state_before_with_bci(jsr_dest_bci));
3616   append(goto_sub);
3617   _block-&gt;set_end(goto_sub);
3618   _last = _block = jsr_start_block;
3619 
3620   // Clear out bytecode stream
3621   scope_data()-&gt;set_stream(NULL);
3622 
3623   scope_data()-&gt;add_to_work_list(jsr_start_block);
3624 
3625   // Ready to resume parsing in subroutine
3626   iterate_all_blocks();
3627 
3628   // If we bailed out during parsing, return immediately (this is bad news)
3629   CHECK_BAILOUT_(false);
3630 
3631   // Detect whether the continuation can actually be reached. If not,
3632   // it has not had state set by the join() operations in
3633   // iterate_bytecodes_for_block()/ret() and we should not touch the
3634   // iteration state. The calling activation of
3635   // iterate_bytecodes_for_block will then complete normally.
3636   if (cont-&gt;state() != NULL) {
3637     if (!cont-&gt;is_set(BlockBegin::was_visited_flag)) {
3638       // add continuation to work list instead of parsing it immediately
3639       scope_data()-&gt;parent()-&gt;add_to_work_list(cont);
3640     }
3641   }
3642 
3643   assert(jsr_continuation() == cont, "continuation must not have changed");
3644   assert(!jsr_continuation()-&gt;is_set(BlockBegin::was_visited_flag) ||
3645          jsr_continuation()-&gt;is_set(BlockBegin::parser_loop_header_flag),
3646          "continuation can only be visited in case of backward branches");
3647   assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "block must have end");
3648 
3649   // continuation is in work list, so end iteration of current block
3650   _skip_block = true;
3651   pop_scope_for_jsr();
3652 
3653   return true;
3654 }
3655 
3656 
3657 // Inline the entry of a synchronized method as a monitor enter and
3658 // register the exception handler which releases the monitor if an
3659 // exception is thrown within the callee. Note that the monitor enter
3660 // cannot throw an exception itself, because the receiver is
3661 // guaranteed to be non-null by the explicit null check at the
3662 // beginning of inlining.
3663 void GraphBuilder::inline_sync_entry(Value lock, BlockBegin* sync_handler) {
3664   assert(lock != NULL &amp;&amp; sync_handler != NULL, "lock or handler missing");
3665 
3666   monitorenter(lock, SynchronizationEntryBCI);
3667   assert(_last-&gt;as_MonitorEnter() != NULL, "monitor enter expected");
3668   _last-&gt;set_needs_null_check(false);
3669 
3670   sync_handler-&gt;set(BlockBegin::exception_entry_flag);
3671   sync_handler-&gt;set(BlockBegin::is_on_work_list_flag);
3672 
3673   ciExceptionHandler* desc = new ciExceptionHandler(method()-&gt;holder(), 0, method()-&gt;code_size(), -1, 0);
3674   XHandler* h = new XHandler(desc);
3675   h-&gt;set_entry_block(sync_handler);
3676   scope_data()-&gt;xhandlers()-&gt;append(h);
3677   scope_data()-&gt;set_has_handler();
3678 }
3679 
3680 
3681 // If an exception is thrown and not handled within an inlined
3682 // synchronized method, the monitor must be released before the
3683 // exception is rethrown in the outer scope. Generate the appropriate
3684 // instructions here.
3685 void GraphBuilder::fill_sync_handler(Value lock, BlockBegin* sync_handler, bool default_handler) {
3686   BlockBegin* orig_block = _block;
3687   ValueStack* orig_state = _state;
3688   Instruction* orig_last = _last;
3689   _last = _block = sync_handler;
3690   _state = sync_handler-&gt;state()-&gt;copy();
3691 
3692   assert(sync_handler != NULL, "handler missing");
3693   assert(!sync_handler-&gt;is_set(BlockBegin::was_visited_flag), "is visited here");
3694 
3695   assert(lock != NULL || default_handler, "lock or handler missing");
3696 
3697   XHandler* h = scope_data()-&gt;xhandlers()-&gt;remove_last();
3698   assert(h-&gt;entry_block() == sync_handler, "corrupt list of handlers");
3699 
3700   block()-&gt;set(BlockBegin::was_visited_flag);
3701   Value exception = append_with_bci(new ExceptionObject(), SynchronizationEntryBCI);
3702   assert(exception-&gt;is_pinned(), "must be");
3703 
3704   int bci = SynchronizationEntryBCI;
3705   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3706     // Report exit from inline methods.  We don't have a stream here
3707     // so pass an explicit bci of SynchronizationEntryBCI.
3708     Values* args = new Values(1);
3709     args-&gt;push(append_with_bci(new Constant(new MethodConstant(method())), bci));
3710     append_with_bci(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args), bci);
3711   }
3712 
3713   if (lock) {
3714     assert(state()-&gt;locks_size() &gt; 0 &amp;&amp; state()-&gt;lock_at(state()-&gt;locks_size() - 1) == lock, "lock is missing");
3715     if (!lock-&gt;is_linked()) {
3716       lock = append_with_bci(lock, bci);
3717     }
3718 
3719     // exit the monitor in the context of the synchronized method
3720     monitorexit(lock, bci);
3721 
3722     // exit the context of the synchronized method
3723     if (!default_handler) {
3724       pop_scope();
3725       bci = _state-&gt;caller_state()-&gt;bci();
3726       _state = _state-&gt;caller_state()-&gt;copy_for_parsing();
3727     }
3728   }
3729 
3730   // perform the throw as if at the the call site
3731   apush(exception);
3732   throw_op(bci);
3733 
3734   BlockEnd* end = last()-&gt;as_BlockEnd();
3735   block()-&gt;set_end(end);
3736 
3737   _block = orig_block;
3738   _state = orig_state;
3739   _last = orig_last;
3740 }
3741 
3742 
3743 bool GraphBuilder::try_inline_full(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3744   assert(!callee-&gt;is_native(), "callee must not be native");
3745   if (CompilationPolicy::policy()-&gt;should_not_inline(compilation()-&gt;env(), callee)) {
3746     INLINE_BAILOUT("inlining prohibited by policy");
3747   }
3748   // first perform tests of things it's not possible to inline
3749   if (callee-&gt;has_exception_handlers() &amp;&amp;
3750       !InlineMethodsWithExceptionHandlers) INLINE_BAILOUT("callee has exception handlers");
3751   if (callee-&gt;is_synchronized() &amp;&amp;
3752       !InlineSynchronizedMethods         ) INLINE_BAILOUT("callee is synchronized");
3753   if (!callee-&gt;holder()-&gt;is_initialized()) INLINE_BAILOUT("callee's klass not initialized yet");
3754   if (!callee-&gt;has_balanced_monitors())    INLINE_BAILOUT("callee's monitors do not match");
3755 
3756   // Proper inlining of methods with jsrs requires a little more work.
3757   if (callee-&gt;has_jsrs()                 ) INLINE_BAILOUT("jsrs not handled properly by inliner yet");
3758 
3759   // When SSE2 is used on intel, then no special handling is needed
3760   // for strictfp because the enum-constant is fixed at compile time,
3761   // the check for UseSSE2 is needed here
3762   if (strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt; 2 &amp;&amp; method()-&gt;is_strict() != callee-&gt;is_strict()) {
3763     INLINE_BAILOUT("caller and callee have different strict fp requirements");
3764   }
3765 
3766   if (is_profiling() &amp;&amp; !callee-&gt;ensure_method_data()) {
3767     INLINE_BAILOUT("mdo allocation failed");
3768   }
3769 
3770   // now perform tests that are based on flag settings
3771   if (callee-&gt;force_inline()) {
3772     if (inline_level() &gt; MaxForceInlineLevel) INLINE_BAILOUT("MaxForceInlineLevel");
3773     print_inlining(callee, "force inline by annotation");
3774   } else if (callee-&gt;should_inline()) {
3775     print_inlining(callee, "force inline by CompileOracle");
3776   } else {
3777     // use heuristic controls on inlining
3778     if (inline_level() &gt; MaxInlineLevel                         ) INLINE_BAILOUT("inlining too deep");
3779     if (recursive_inline_level(callee) &gt; MaxRecursiveInlineLevel) INLINE_BAILOUT("recursive inlining too deep");
3780     if (callee-&gt;code_size_for_inlining() &gt; max_inline_size()    ) INLINE_BAILOUT("callee is too large");
3781 
3782     // don't inline throwable methods unless the inlining tree is rooted in a throwable class
3783     if (callee-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
3784         callee-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3785       // Throwable constructor call
3786       IRScope* top = scope();
3787       while (top-&gt;caller() != NULL) {
3788         top = top-&gt;caller();
3789       }
3790       if (!top-&gt;method()-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3791         INLINE_BAILOUT("don't inline Throwable constructors");
3792       }
3793     }
3794 
3795     if (compilation()-&gt;env()-&gt;num_inlined_bytecodes() &gt; DesiredMethodLimit) {
3796       INLINE_BAILOUT("total inlining greater than DesiredMethodLimit");
3797     }
3798     // printing
3799     print_inlining(callee);
3800   }
3801 
3802   // NOTE: Bailouts from this point on, which occur at the
3803   // GraphBuilder level, do not cause bailout just of the inlining but
3804   // in fact of the entire compilation.
3805 
3806   BlockBegin* orig_block = block();
3807 
3808   const bool is_invokedynamic = bc == Bytecodes::_invokedynamic;
3809   const bool has_receiver = (bc != Bytecodes::_invokestatic &amp;&amp; !is_invokedynamic);
3810 
3811   const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
3812   assert(args_base &gt;= 0, "stack underflow during inlining");
3813 
3814   // Insert null check if necessary
3815   Value recv = NULL;
3816   if (has_receiver) {
3817     // note: null check must happen even if first instruction of callee does
3818     //       an implicit null check since the callee is in a different scope
3819     //       and we must make sure exception handling does the right thing
3820     assert(!callee-&gt;is_static(), "callee must not be static");
3821     assert(callee-&gt;arg_size() &gt; 0, "must have at least a receiver");
3822     recv = state()-&gt;stack_at(args_base);
3823     null_check(recv);
3824   }
3825 
3826   if (is_profiling()) {
3827     // Note that we'd collect profile data in this method if we wanted it.
3828     // this may be redundant here...
3829     compilation()-&gt;set_would_profile(true);
3830 
3831     if (profile_calls()) {
3832       int start = 0;
3833       Values* obj_args = args_list_for_profiling(callee, start, has_receiver);
3834       if (obj_args != NULL) {
3835         int s = obj_args-&gt;size();
3836         // if called through method handle invoke, some arguments may have been popped
3837         for (int i = args_base+start, j = 0; j &lt; obj_args-&gt;size() &amp;&amp; i &lt; state()-&gt;stack_size(); ) {
3838           Value v = state()-&gt;stack_at_inc(i);
3839           if (v-&gt;type()-&gt;is_object_kind()) {
3840             obj_args-&gt;push(v);
3841             j++;
3842           }
3843         }
3844 #ifdef ASSERT
3845         {
3846           bool ignored_will_link;
3847           ciSignature* declared_signature = NULL;
3848           ciMethod* real_target = method()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
3849           assert(s == obj_args-&gt;length() || real_target-&gt;is_method_handle_intrinsic(), "missed on arg?");
3850         }
3851 #endif
3852       }
3853       profile_call(callee, recv, holder_known ? callee-&gt;holder() : NULL, obj_args, true);
3854     }
3855   }
3856 
3857   // Introduce a new callee continuation point - if the callee has
3858   // more than one return instruction or the return does not allow
3859   // fall-through of control flow, all return instructions of the
3860   // callee will need to be replaced by Goto's pointing to this
3861   // continuation point.
3862   BlockBegin* cont = block_at(next_bci());
3863   bool continuation_existed = true;
3864   if (cont == NULL) {
3865     cont = new BlockBegin(next_bci());
3866     // low number so that continuation gets parsed as early as possible
3867     cont-&gt;set_depth_first_number(0);
3868 #ifndef PRODUCT
3869     if (PrintInitialBlockList) {
3870       tty-&gt;print_cr("CFG: created block %d (bci %d) as continuation for inline at bci %d",
3871                     cont-&gt;block_id(), cont-&gt;bci(), bci());
3872     }
3873 #endif
3874     continuation_existed = false;
3875   }
3876   // Record number of predecessors of continuation block before
3877   // inlining, to detect if inlined method has edges to its
3878   // continuation after inlining.
3879   int continuation_preds = cont-&gt;number_of_preds();
3880 
3881   // Push callee scope
3882   push_scope(callee, cont);
3883 
3884   // the BlockListBuilder for the callee could have bailed out
3885   if (bailed_out())
3886       return false;
3887 
3888   // Temporarily set up bytecode stream so we can append instructions
3889   // (only using the bci of this stream)
3890   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3891 
3892   // Pass parameters into callee state: add assignments
3893   // note: this will also ensure that all arguments are computed before being passed
3894   ValueStack* callee_state = state();
3895   ValueStack* caller_state = state()-&gt;caller_state();
3896   for (int i = args_base; i &lt; caller_state-&gt;stack_size(); ) {
3897     const int arg_no = i - args_base;
3898     Value arg = caller_state-&gt;stack_at_inc(i);
3899     store_local(callee_state, arg, arg_no);
3900   }
3901 
3902   // Remove args from stack.
3903   // Note that we preserve locals state in case we can use it later
3904   // (see use of pop_scope() below)
3905   caller_state-&gt;truncate_stack(args_base);
3906   assert(callee_state-&gt;stack_size() == 0, "callee stack must be empty");
3907 
3908   Value lock;
3909   BlockBegin* sync_handler;
3910 
3911   // Inline the locking of the receiver if the callee is synchronized
3912   if (callee-&gt;is_synchronized()) {
3913     lock = callee-&gt;is_static() ? append(new Constant(new InstanceConstant(callee-&gt;holder()-&gt;java_mirror())))
3914                                : state()-&gt;local_at(0);
3915     sync_handler = new BlockBegin(SynchronizationEntryBCI);
3916     inline_sync_entry(lock, sync_handler);
3917   }
3918 
3919   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3920     Values* args = new Values(1);
3921     args-&gt;push(append(new Constant(new MethodConstant(method()))));
3922     append(new RuntimeCall(voidType, "dtrace_method_entry", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), args));
3923   }
3924 
3925   if (profile_inlined_calls()) {
3926     profile_invocation(callee, copy_state_before_with_bci(SynchronizationEntryBCI));
3927   }
3928 
3929   BlockBegin* callee_start_block = block_at(0);
3930   if (callee_start_block != NULL) {
3931     assert(callee_start_block-&gt;is_set(BlockBegin::parser_loop_header_flag), "must be loop header");
3932     Goto* goto_callee = new Goto(callee_start_block, false);
3933     // The state for this goto is in the scope of the callee, so use
3934     // the entry bci for the callee instead of the call site bci.
3935     append_with_bci(goto_callee, 0);
3936     _block-&gt;set_end(goto_callee);
3937     callee_start_block-&gt;merge(callee_state);
3938 
3939     _last = _block = callee_start_block;
3940 
3941     scope_data()-&gt;add_to_work_list(callee_start_block);
3942   }
3943 
3944   // Clear out bytecode stream
3945   scope_data()-&gt;set_stream(NULL);
3946 
3947   // Ready to resume parsing in callee (either in the same block we
3948   // were in before or in the callee's start block)
3949   iterate_all_blocks(callee_start_block == NULL);
3950 
3951   // If we bailed out during parsing, return immediately (this is bad news)
3952   if (bailed_out())
3953       return false;
3954 
3955   // iterate_all_blocks theoretically traverses in random order; in
3956   // practice, we have only traversed the continuation if we are
3957   // inlining into a subroutine
3958   assert(continuation_existed ||
3959          !continuation()-&gt;is_set(BlockBegin::was_visited_flag),
3960          "continuation should not have been parsed yet if we created it");
3961 
3962   // At this point we are almost ready to return and resume parsing of
3963   // the caller back in the GraphBuilder. The only thing we want to do
3964   // first is an optimization: during parsing of the callee we
3965   // generated at least one Goto to the continuation block. If we
3966   // generated exactly one, and if the inlined method spanned exactly
3967   // one block (and we didn't have to Goto its entry), then we snip
3968   // off the Goto to the continuation, allowing control to fall
3969   // through back into the caller block and effectively performing
3970   // block merging. This allows load elimination and CSE to take place
3971   // across multiple callee scopes if they are relatively simple, and
3972   // is currently essential to making inlining profitable.
3973   if (num_returns() == 1
3974       &amp;&amp; block() == orig_block
3975       &amp;&amp; block() == inline_cleanup_block()) {
3976     _last  = inline_cleanup_return_prev();
3977     _state = inline_cleanup_state();
3978   } else if (continuation_preds == cont-&gt;number_of_preds()) {
3979     // Inlining caused that the instructions after the invoke in the
3980     // caller are not reachable any more. So skip filling this block
3981     // with instructions!
3982     assert(cont == continuation(), "");
3983     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
3984     _skip_block = true;
3985   } else {
3986     // Resume parsing in continuation block unless it was already parsed.
3987     // Note that if we don't change _last here, iteration in
3988     // iterate_bytecodes_for_block will stop when we return.
3989     if (!continuation()-&gt;is_set(BlockBegin::was_visited_flag)) {
3990       // add continuation to work list instead of parsing it immediately
3991       assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
3992       scope_data()-&gt;parent()-&gt;add_to_work_list(continuation());
3993       _skip_block = true;
3994     }
3995   }
3996 
3997   // Fill the exception handler for synchronized methods with instructions
3998   if (callee-&gt;is_synchronized() &amp;&amp; sync_handler-&gt;state() != NULL) {
3999     fill_sync_handler(lock, sync_handler);
4000   } else {
4001     pop_scope();
4002   }
4003 
4004   compilation()-&gt;notice_inlined_method(callee);
4005 
4006   return true;
4007 }
4008 
4009 
4010 bool GraphBuilder::try_method_handle_inline(ciMethod* callee) {
4011   ValueStack* state_before = state()-&gt;copy_for_parsing();
4012   vmIntrinsics::ID iid = callee-&gt;intrinsic_id();
4013   switch (iid) {
4014   case vmIntrinsics::_invokeBasic:
4015     {
4016       // get MethodHandle receiver
4017       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4018       ValueType* type = state()-&gt;stack_at(args_base)-&gt;type();
4019       if (type-&gt;is_constant()) {
4020         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_method_handle()-&gt;get_vmtarget();
4021         // We don't do CHA here so only inline static and statically bindable methods.
4022         if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4023           Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4024           if (try_inline(target, /*holder_known*/ true, bc)) {
4025             return true;
4026           }
4027         } else {
4028           print_inlining(target, "not static or statically bindable", /*success*/ false);
4029         }
4030       } else {
4031         print_inlining(callee, "receiver not constant", /*success*/ false);
4032       }
4033     }
4034     break;
4035 
4036   case vmIntrinsics::_linkToVirtual:
4037   case vmIntrinsics::_linkToStatic:
4038   case vmIntrinsics::_linkToSpecial:
4039   case vmIntrinsics::_linkToInterface:
4040     {
4041       // pop MemberName argument
4042       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4043       ValueType* type = apop()-&gt;type();
4044       if (type-&gt;is_constant()) {
4045         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_member_name()-&gt;get_vmtarget();
4046         // If the target is another method handle invoke try recursivly to get
4047         // a better target.
4048         if (target-&gt;is_method_handle_intrinsic()) {
4049           if (try_method_handle_inline(target)) {
4050             return true;
4051           }
4052         } else {
4053           ciSignature* signature = target-&gt;signature();
4054           const int receiver_skip = target-&gt;is_static() ? 0 : 1;
4055           // Cast receiver to its type.
4056           if (!target-&gt;is_static()) {
4057             ciKlass* tk = signature-&gt;accessing_klass();
4058             Value obj = state()-&gt;stack_at(args_base);
4059             if (obj-&gt;exact_type() == NULL &amp;&amp;
4060                 obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4061               TypeCast* c = new TypeCast(tk, obj, state_before);
4062               append(c);
4063               state()-&gt;stack_at_put(args_base, c);
4064             }
4065           }
4066           // Cast reference arguments to its type.
4067           for (int i = 0, j = 0; i &lt; signature-&gt;count(); i++) {
4068             ciType* t = signature-&gt;type_at(i);
4069             if (t-&gt;is_klass()) {
4070               ciKlass* tk = t-&gt;as_klass();
4071               Value obj = state()-&gt;stack_at(args_base + receiver_skip + j);
4072               if (obj-&gt;exact_type() == NULL &amp;&amp;
4073                   obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4074                 TypeCast* c = new TypeCast(t, obj, state_before);
4075                 append(c);
4076                 state()-&gt;stack_at_put(args_base + receiver_skip + j, c);
4077               }
4078             }
4079             j += t-&gt;size();  // long and double take two slots
4080           }
4081           // We don't do CHA here so only inline static and statically bindable methods.
4082           if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4083             Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4084             if (try_inline(target, /*holder_known*/ true, bc)) {
4085               return true;
4086             }
4087           } else {
4088             print_inlining(target, "not static or statically bindable", /*success*/ false);
4089           }
4090         }
4091       } else {
4092         print_inlining(callee, "MemberName not constant", /*success*/ false);
4093       }
4094     }
4095     break;
4096 
4097   default:
4098     fatal(err_msg("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
4099     break;
4100   }
4101   set_state(state_before);
4102   return false;
4103 }
4104 
4105 
4106 void GraphBuilder::inline_bailout(const char* msg) {
4107   assert(msg != NULL, "inline bailout msg must exist");
4108   _inline_bailout_msg = msg;
4109 }
4110 
4111 
4112 void GraphBuilder::clear_inline_bailout() {
4113   _inline_bailout_msg = NULL;
4114 }
4115 
4116 
4117 void GraphBuilder::push_root_scope(IRScope* scope, BlockList* bci2block, BlockBegin* start) {
4118   ScopeData* data = new ScopeData(NULL);
4119   data-&gt;set_scope(scope);
4120   data-&gt;set_bci2block(bci2block);
4121   _scope_data = data;
4122   _block = start;
4123 }
4124 
4125 
4126 void GraphBuilder::push_scope(ciMethod* callee, BlockBegin* continuation) {
4127   IRScope* callee_scope = new IRScope(compilation(), scope(), bci(), callee, -1, false);
4128   scope()-&gt;add_callee(callee_scope);
4129 
4130   BlockListBuilder blb(compilation(), callee_scope, -1);
4131   CHECK_BAILOUT();
4132 
4133   if (!blb.bci2block()-&gt;at(0)-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
4134     // this scope can be inlined directly into the caller so remove
4135     // the block at bci 0.
4136     blb.bci2block()-&gt;at_put(0, NULL);
4137   }
4138 
4139   set_state(new ValueStack(callee_scope, state()-&gt;copy(ValueStack::CallerState, bci())));
4140 
4141   ScopeData* data = new ScopeData(scope_data());
4142   data-&gt;set_scope(callee_scope);
4143   data-&gt;set_bci2block(blb.bci2block());
4144   data-&gt;set_continuation(continuation);
4145   _scope_data = data;
4146 }
4147 
4148 
4149 void GraphBuilder::push_scope_for_jsr(BlockBegin* jsr_continuation, int jsr_dest_bci) {
4150   ScopeData* data = new ScopeData(scope_data());
4151   data-&gt;set_parsing_jsr();
4152   data-&gt;set_jsr_entry_bci(jsr_dest_bci);
4153   data-&gt;set_jsr_return_address_local(-1);
4154   // Must clone bci2block list as we will be mutating it in order to
4155   // properly clone all blocks in jsr region as well as exception
4156   // handlers containing rets
4157   BlockList* new_bci2block = new BlockList(bci2block()-&gt;length());
4158   new_bci2block-&gt;push_all(bci2block());
4159   data-&gt;set_bci2block(new_bci2block);
4160   data-&gt;set_scope(scope());
4161   data-&gt;setup_jsr_xhandlers();
4162   data-&gt;set_continuation(continuation());
4163   data-&gt;set_jsr_continuation(jsr_continuation);
4164   _scope_data = data;
4165 }
4166 
4167 
4168 void GraphBuilder::pop_scope() {
4169   int number_of_locks = scope()-&gt;number_of_locks();
4170   _scope_data = scope_data()-&gt;parent();
4171   // accumulate minimum number of monitor slots to be reserved
4172   scope()-&gt;set_min_number_of_locks(number_of_locks);
4173 }
4174 
4175 
4176 void GraphBuilder::pop_scope_for_jsr() {
4177   _scope_data = scope_data()-&gt;parent();
4178 }
4179 
4180 bool GraphBuilder::append_unsafe_get_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4181   if (InlineUnsafeOps) {
4182     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4183     null_check(args-&gt;at(0));
4184     Instruction* offset = args-&gt;at(2);
4185 #ifndef _LP64
4186     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4187 #endif
4188     Instruction* op = append(new UnsafeGetObject(t, args-&gt;at(1), offset, is_volatile));
4189     push(op-&gt;type(), op);
4190     compilation()-&gt;set_has_unsafe_access(true);
4191   }
4192   return InlineUnsafeOps;
4193 }
4194 
4195 
4196 bool GraphBuilder::append_unsafe_put_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4197   if (InlineUnsafeOps) {
4198     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4199     null_check(args-&gt;at(0));
4200     Instruction* offset = args-&gt;at(2);
4201 #ifndef _LP64
4202     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4203 #endif
4204     Instruction* op = append(new UnsafePutObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_volatile));
4205     compilation()-&gt;set_has_unsafe_access(true);
4206     kill_all();
4207   }
4208   return InlineUnsafeOps;
4209 }
4210 
4211 
4212 bool GraphBuilder::append_unsafe_get_raw(ciMethod* callee, BasicType t) {
4213   if (InlineUnsafeOps) {
4214     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4215     null_check(args-&gt;at(0));
4216     Instruction* op = append(new UnsafeGetRaw(t, args-&gt;at(1), false));
4217     push(op-&gt;type(), op);
4218     compilation()-&gt;set_has_unsafe_access(true);
4219   }
4220   return InlineUnsafeOps;
4221 }
4222 
4223 
4224 bool GraphBuilder::append_unsafe_put_raw(ciMethod* callee, BasicType t) {
4225   if (InlineUnsafeOps) {
4226     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4227     null_check(args-&gt;at(0));
4228     Instruction* op = append(new UnsafePutRaw(t, args-&gt;at(1), args-&gt;at(2)));
4229     compilation()-&gt;set_has_unsafe_access(true);
4230   }
4231   return InlineUnsafeOps;
4232 }
4233 
4234 
4235 bool GraphBuilder::append_unsafe_prefetch(ciMethod* callee, bool is_static, bool is_store) {
4236   if (InlineUnsafeOps) {
4237     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4238     int obj_arg_index = 1; // Assume non-static case
4239     if (is_static) {
4240       obj_arg_index = 0;
4241     } else {
4242       null_check(args-&gt;at(0));
4243     }
4244     Instruction* offset = args-&gt;at(obj_arg_index + 1);
4245 #ifndef _LP64
4246     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4247 #endif
4248     Instruction* op = is_store ? append(new UnsafePrefetchWrite(args-&gt;at(obj_arg_index), offset))
4249                                : append(new UnsafePrefetchRead (args-&gt;at(obj_arg_index), offset));
4250     compilation()-&gt;set_has_unsafe_access(true);
4251   }
4252   return InlineUnsafeOps;
4253 }
4254 
4255 
4256 void GraphBuilder::append_unsafe_CAS(ciMethod* callee) {
4257   ValueStack* state_before = copy_state_for_exception();
4258   ValueType* result_type = as_ValueType(callee-&gt;return_type());
4259   assert(result_type-&gt;is_int(), "int result");
4260   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4261 
4262   // Pop off some args to speically handle, then push back
4263   Value newval = args-&gt;pop();
4264   Value cmpval = args-&gt;pop();
4265   Value offset = args-&gt;pop();
4266   Value src = args-&gt;pop();
4267   Value unsafe_obj = args-&gt;pop();
4268 
4269   // Separately handle the unsafe arg. It is not needed for code
4270   // generation, but must be null checked
4271   null_check(unsafe_obj);
4272 
4273 #ifndef _LP64
4274   offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4275 #endif
4276 
4277   args-&gt;push(src);
4278   args-&gt;push(offset);
4279   args-&gt;push(cmpval);
4280   args-&gt;push(newval);
4281 
4282   // An unsafe CAS can alias with other field accesses, but we don't
4283   // know which ones so mark the state as no preserved.  This will
4284   // cause CSE to invalidate memory across it.
4285   bool preserves_state = false;
4286   Intrinsic* result = new Intrinsic(result_type, callee-&gt;intrinsic_id(), args, false, state_before, preserves_state);
4287   append_split(result);
4288   push(result_type, result);
4289   compilation()-&gt;set_has_unsafe_access(true);
4290 }
4291 
4292 
4293 void GraphBuilder::print_inlining(ciMethod* callee, const char* msg, bool success) {
4294   CompileLog* log = compilation()-&gt;log();
4295   if (log != NULL) {
4296     if (success) {
4297       if (msg != NULL)
4298         log-&gt;inline_success(msg);
4299       else
4300         log-&gt;inline_success("receiver is statically known");
4301     } else {
4302       if (msg != NULL)
4303         log-&gt;inline_fail(msg);
4304       else
4305         log-&gt;inline_fail("reason unknown");
4306     }
4307   }
4308 
4309   if (!PrintInlining &amp;&amp; !compilation()-&gt;method()-&gt;has_option("PrintInlining")) {
4310     return;
4311   }
4312   CompileTask::print_inlining(callee, scope()-&gt;level(), bci(), msg);
4313   if (success &amp;&amp; CIPrintMethodCodes) {
4314     callee-&gt;print_codes();
4315   }
4316 }
4317 
4318 bool GraphBuilder::append_unsafe_get_and_set_obj(ciMethod* callee, bool is_add) {
4319   if (InlineUnsafeOps) {
4320     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4321     BasicType t = callee-&gt;return_type()-&gt;basic_type();
4322     null_check(args-&gt;at(0));
4323     Instruction* offset = args-&gt;at(2);
4324 #ifndef _LP64
4325     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4326 #endif
4327     Instruction* op = append(new UnsafeGetAndSetObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_add));
4328     compilation()-&gt;set_has_unsafe_access(true);
4329     kill_all();
4330     push(op-&gt;type(), op);
4331   }
4332   return InlineUnsafeOps;
4333 }
4334 
4335 #ifndef PRODUCT
4336 void GraphBuilder::print_stats() {
4337   vmap()-&gt;print();
4338 }
4339 #endif // PRODUCT
4340 
4341 void GraphBuilder::profile_call(ciMethod* callee, Value recv, ciKlass* known_holder, Values* obj_args, bool inlined) {
4342   assert(known_holder == NULL || (known_holder-&gt;is_instance_klass() &amp;&amp;
4343                                   (!known_holder-&gt;is_interface() ||
4344                                    ((ciInstanceKlass*)known_holder)-&gt;has_default_methods())), "should be default method");
4345   if (known_holder != NULL) {
4346     if (known_holder-&gt;exact_klass() == NULL) {
4347       known_holder = compilation()-&gt;cha_exact_type(known_holder);
4348     }
4349   }
4350 
4351   append(new ProfileCall(method(), bci(), callee, recv, known_holder, obj_args, inlined));
4352 }
4353 
4354 void GraphBuilder::profile_return_type(Value ret, ciMethod* callee, ciMethod* m, int invoke_bci) {
4355   assert((m == NULL) == (invoke_bci &lt; 0), "invalid method and invalid bci together");
4356   if (m == NULL) {
4357     m = method();
4358   }
4359   if (invoke_bci &lt; 0) {
4360     invoke_bci = bci();
4361   }
4362   ciMethodData* md = m-&gt;method_data_or_null();
4363   ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
4364   if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
4365     append(new ProfileReturnType(m , invoke_bci, callee, ret));
4366   }
4367 }
4368 
4369 void GraphBuilder::profile_invocation(ciMethod* callee, ValueStack* state) {
4370   append(new ProfileInvoke(callee, state));
4371 }
</pre></body></html>
