<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/c1/c1_GraphBuilder.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "c1/c1_CFGPrinter.hpp"
  27 #include "c1/c1_Canonicalizer.hpp"
  28 #include "c1/c1_Compilation.hpp"
  29 #include "c1/c1_GraphBuilder.hpp"
  30 #include "c1/c1_InstructionPrinter.hpp"
  31 #include "ci/ciCallSite.hpp"
  32 #include "ci/ciField.hpp"
  33 #include "ci/ciKlass.hpp"
  34 #include "ci/ciMemberName.hpp"
  35 #include "compiler/compileBroker.hpp"
  36 #include "interpreter/bytecode.hpp"
  37 #include "runtime/sharedRuntime.hpp"
  38 #include "runtime/compilationPolicy.hpp"
  39 #include "utilities/bitMap.inline.hpp"
  40 
  41 class BlockListBuilder VALUE_OBJ_CLASS_SPEC {
  42  private:
  43   Compilation* _compilation;
  44   IRScope*     _scope;
  45 
  46   BlockList    _blocks;                // internal list of all blocks
  47   BlockList*   _bci2block;             // mapping from bci to blocks for GraphBuilder
  48 
  49   // fields used by mark_loops
  50   BitMap       _active;                // for iteration of control flow graph
  51   BitMap       _visited;               // for iteration of control flow graph
  52   intArray     _loop_map;              // caches the information if a block is contained in a loop
  53   int          _next_loop_index;       // next free loop number
  54   int          _next_block_number;     // for reverse postorder numbering of blocks
  55 
  56   // accessors
  57   Compilation*  compilation() const              { return _compilation; }
  58   IRScope*      scope() const                    { return _scope; }
  59   ciMethod*     method() const                   { return scope()-&gt;method(); }
  60   XHandlers*    xhandlers() const                { return scope()-&gt;xhandlers(); }
  61 
  62   // unified bailout support
  63   void          bailout(const char* msg) const   { compilation()-&gt;bailout(msg); }
  64   bool          bailed_out() const               { return compilation()-&gt;bailed_out(); }
  65 
  66   // helper functions
  67   BlockBegin* make_block_at(int bci, BlockBegin* predecessor);
  68   void handle_exceptions(BlockBegin* current, int cur_bci);
  69   void handle_jsr(BlockBegin* current, int sr_bci, int next_bci);
  70   void store_one(BlockBegin* current, int local);
  71   void store_two(BlockBegin* current, int local);
  72   void set_entries(int osr_bci);
  73   void set_leaders();
  74 
  75   void make_loop_header(BlockBegin* block);
  76   void mark_loops();
  77   int  mark_loops(BlockBegin* b, bool in_subroutine);
  78 
  79   // debugging
  80 #ifndef PRODUCT
  81   void print();
  82 #endif
  83 
  84  public:
  85   // creation
  86   BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci);
  87 
  88   // accessors for GraphBuilder
  89   BlockList*    bci2block() const                { return _bci2block; }
  90 };
  91 
  92 
  93 // Implementation of BlockListBuilder
  94 
  95 BlockListBuilder::BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci)
  96  : _compilation(compilation)
  97  , _scope(scope)
  98  , _blocks(16)
  99  , _bci2block(new BlockList(scope-&gt;method()-&gt;code_size(), NULL))
 100  , _next_block_number(0)
 101  , _active()         // size not known yet
 102  , _visited()        // size not known yet
 103  , _next_loop_index(0)
 104  , _loop_map() // size not known yet
 105 {
 106   set_entries(osr_bci);
 107   set_leaders();
 108   CHECK_BAILOUT();
 109 
 110   mark_loops();
 111   NOT_PRODUCT(if (PrintInitialBlockList) print());
 112 
 113 #ifndef PRODUCT
 114   if (PrintCFGToFile) {
 115     stringStream title;
 116     title.print("BlockListBuilder ");
 117     scope-&gt;method()-&gt;print_name(&amp;title);
 118     CFGPrinter::print_cfg(_bci2block, title.as_string(), false, false);
 119   }
 120 #endif
 121 }
 122 
 123 
 124 void BlockListBuilder::set_entries(int osr_bci) {
 125   // generate start blocks
 126   BlockBegin* std_entry = make_block_at(0, NULL);
 127   if (scope()-&gt;caller() == NULL) {
 128     std_entry-&gt;set(BlockBegin::std_entry_flag);
 129   }
 130   if (osr_bci != -1) {
 131     BlockBegin* osr_entry = make_block_at(osr_bci, NULL);
 132     osr_entry-&gt;set(BlockBegin::osr_entry_flag);
 133   }
 134 
 135   // generate exception entry blocks
 136   XHandlers* list = xhandlers();
 137   const int n = list-&gt;length();
 138   for (int i = 0; i &lt; n; i++) {
 139     XHandler* h = list-&gt;handler_at(i);
 140     BlockBegin* entry = make_block_at(h-&gt;handler_bci(), NULL);
 141     entry-&gt;set(BlockBegin::exception_entry_flag);
 142     h-&gt;set_entry_block(entry);
 143   }
 144 }
 145 
 146 
 147 BlockBegin* BlockListBuilder::make_block_at(int cur_bci, BlockBegin* predecessor) {
 148   assert(method()-&gt;bci_block_start().at(cur_bci), "wrong block starts of MethodLivenessAnalyzer");
 149 
 150   BlockBegin* block = _bci2block-&gt;at(cur_bci);
 151   if (block == NULL) {
 152     block = new BlockBegin(cur_bci);
 153     block-&gt;init_stores_to_locals(method()-&gt;max_locals());
 154     _bci2block-&gt;at_put(cur_bci, block);
 155     _blocks.append(block);
 156 
 157     assert(predecessor == NULL || predecessor-&gt;bci() &lt; cur_bci, "targets for backward branches must already exist");
 158   }
 159 
 160   if (predecessor != NULL) {
 161     if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 162       BAILOUT_("Exception handler can be reached by both normal and exceptional control flow", block);
 163     }
 164 
 165     predecessor-&gt;add_successor(block);
 166     block-&gt;increment_total_preds();
 167   }
 168 
 169   return block;
 170 }
 171 
 172 
 173 inline void BlockListBuilder::store_one(BlockBegin* current, int local) {
 174   current-&gt;stores_to_locals().set_bit(local);
 175 }
 176 inline void BlockListBuilder::store_two(BlockBegin* current, int local) {
 177   store_one(current, local);
 178   store_one(current, local + 1);
 179 }
 180 
 181 
 182 void BlockListBuilder::handle_exceptions(BlockBegin* current, int cur_bci) {
 183   // Draws edges from a block to its exception handlers
 184   XHandlers* list = xhandlers();
 185   const int n = list-&gt;length();
 186 
 187   for (int i = 0; i &lt; n; i++) {
 188     XHandler* h = list-&gt;handler_at(i);
 189 
 190     if (h-&gt;covers(cur_bci)) {
 191       BlockBegin* entry = h-&gt;entry_block();
 192       assert(entry != NULL &amp;&amp; entry == _bci2block-&gt;at(h-&gt;handler_bci()), "entry must be set");
 193       assert(entry-&gt;is_set(BlockBegin::exception_entry_flag), "flag must be set");
 194 
 195       // add each exception handler only once
 196       if (!current-&gt;is_successor(entry)) {
 197         current-&gt;add_successor(entry);
 198         entry-&gt;increment_total_preds();
 199       }
 200 
 201       // stop when reaching catchall
 202       if (h-&gt;catch_type() == 0) break;
 203     }
 204   }
 205 }
 206 
 207 void BlockListBuilder::handle_jsr(BlockBegin* current, int sr_bci, int next_bci) {
 208   // start a new block after jsr-bytecode and link this block into cfg
 209   make_block_at(next_bci, current);
 210 
 211   // start a new block at the subroutine entry at mark it with special flag
 212   BlockBegin* sr_block = make_block_at(sr_bci, current);
 213   if (!sr_block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 214     sr_block-&gt;set(BlockBegin::subroutine_entry_flag);
 215   }
 216 }
 217 
 218 
 219 void BlockListBuilder::set_leaders() {
 220   bool has_xhandlers = xhandlers()-&gt;has_handlers();
 221   BlockBegin* current = NULL;
 222 
 223   // The information which bci starts a new block simplifies the analysis
 224   // Without it, backward branches could jump to a bci where no block was created
 225   // during bytecode iteration. This would require the creation of a new block at the
 226   // branch target and a modification of the successor lists.
 227   BitMap bci_block_start = method()-&gt;bci_block_start();
 228 
 229   ciBytecodeStream s(method());
 230   while (s.next() != ciBytecodeStream::EOBC()) {
 231     int cur_bci = s.cur_bci();
 232 
 233     if (bci_block_start.at(cur_bci)) {
 234       current = make_block_at(cur_bci, current);
 235     }
 236     assert(current != NULL, "must have current block");
 237 
 238     if (has_xhandlers &amp;&amp; GraphBuilder::can_trap(method(), s.cur_bc())) {
 239       handle_exceptions(current, cur_bci);
 240     }
 241 
 242     switch (s.cur_bc()) {
 243       // track stores to local variables for selective creation of phi functions
 244       case Bytecodes::_iinc:     store_one(current, s.get_index()); break;
 245       case Bytecodes::_istore:   store_one(current, s.get_index()); break;
 246       case Bytecodes::_lstore:   store_two(current, s.get_index()); break;
 247       case Bytecodes::_fstore:   store_one(current, s.get_index()); break;
 248       case Bytecodes::_dstore:   store_two(current, s.get_index()); break;
 249       case Bytecodes::_astore:   store_one(current, s.get_index()); break;
 250       case Bytecodes::_istore_0: store_one(current, 0); break;
 251       case Bytecodes::_istore_1: store_one(current, 1); break;
 252       case Bytecodes::_istore_2: store_one(current, 2); break;
 253       case Bytecodes::_istore_3: store_one(current, 3); break;
 254       case Bytecodes::_lstore_0: store_two(current, 0); break;
 255       case Bytecodes::_lstore_1: store_two(current, 1); break;
 256       case Bytecodes::_lstore_2: store_two(current, 2); break;
 257       case Bytecodes::_lstore_3: store_two(current, 3); break;
 258       case Bytecodes::_fstore_0: store_one(current, 0); break;
 259       case Bytecodes::_fstore_1: store_one(current, 1); break;
 260       case Bytecodes::_fstore_2: store_one(current, 2); break;
 261       case Bytecodes::_fstore_3: store_one(current, 3); break;
 262       case Bytecodes::_dstore_0: store_two(current, 0); break;
 263       case Bytecodes::_dstore_1: store_two(current, 1); break;
 264       case Bytecodes::_dstore_2: store_two(current, 2); break;
 265       case Bytecodes::_dstore_3: store_two(current, 3); break;
 266       case Bytecodes::_astore_0: store_one(current, 0); break;
 267       case Bytecodes::_astore_1: store_one(current, 1); break;
 268       case Bytecodes::_astore_2: store_one(current, 2); break;
 269       case Bytecodes::_astore_3: store_one(current, 3); break;
 270 
 271       // track bytecodes that affect the control flow
 272       case Bytecodes::_athrow:  // fall through
 273       case Bytecodes::_ret:     // fall through
 274       case Bytecodes::_ireturn: // fall through
 275       case Bytecodes::_lreturn: // fall through
 276       case Bytecodes::_freturn: // fall through
 277       case Bytecodes::_dreturn: // fall through
 278       case Bytecodes::_areturn: // fall through
 279       case Bytecodes::_return:
 280         current = NULL;
 281         break;
 282 
 283       case Bytecodes::_ifeq:      // fall through
 284       case Bytecodes::_ifne:      // fall through
 285       case Bytecodes::_iflt:      // fall through
 286       case Bytecodes::_ifge:      // fall through
 287       case Bytecodes::_ifgt:      // fall through
 288       case Bytecodes::_ifle:      // fall through
 289       case Bytecodes::_if_icmpeq: // fall through
 290       case Bytecodes::_if_icmpne: // fall through
 291       case Bytecodes::_if_icmplt: // fall through
 292       case Bytecodes::_if_icmpge: // fall through
 293       case Bytecodes::_if_icmpgt: // fall through
 294       case Bytecodes::_if_icmple: // fall through
 295       case Bytecodes::_if_acmpeq: // fall through
 296       case Bytecodes::_if_acmpne: // fall through
 297       case Bytecodes::_ifnull:    // fall through
 298       case Bytecodes::_ifnonnull:
 299         make_block_at(s.next_bci(), current);
 300         make_block_at(s.get_dest(), current);
 301         current = NULL;
 302         break;
 303 
 304       case Bytecodes::_goto:
 305         make_block_at(s.get_dest(), current);
 306         current = NULL;
 307         break;
 308 
 309       case Bytecodes::_goto_w:
 310         make_block_at(s.get_far_dest(), current);
 311         current = NULL;
 312         break;
 313 
 314       case Bytecodes::_jsr:
 315         handle_jsr(current, s.get_dest(), s.next_bci());
 316         current = NULL;
 317         break;
 318 
 319       case Bytecodes::_jsr_w:
 320         handle_jsr(current, s.get_far_dest(), s.next_bci());
 321         current = NULL;
 322         break;
 323 
 324       case Bytecodes::_tableswitch: {
 325         // set block for each case
 326         Bytecode_tableswitch sw(&amp;s);
 327         int l = sw.length();
 328         for (int i = 0; i &lt; l; i++) {
 329           make_block_at(cur_bci + sw.dest_offset_at(i), current);
 330         }
 331         make_block_at(cur_bci + sw.default_offset(), current);
 332         current = NULL;
 333         break;
 334       }
 335 
 336       case Bytecodes::_lookupswitch: {
 337         // set block for each case
 338         Bytecode_lookupswitch sw(&amp;s);
 339         int l = sw.number_of_pairs();
 340         for (int i = 0; i &lt; l; i++) {
 341           make_block_at(cur_bci + sw.pair_at(i).offset(), current);
 342         }
 343         make_block_at(cur_bci + sw.default_offset(), current);
 344         current = NULL;
 345         break;
 346       }
 347     }
 348   }
 349 }
 350 
 351 
 352 void BlockListBuilder::mark_loops() {
 353   ResourceMark rm;
 354 
 355   _active = BitMap(BlockBegin::number_of_blocks());         _active.clear();
 356   _visited = BitMap(BlockBegin::number_of_blocks());        _visited.clear();
 357   _loop_map = intArray(BlockBegin::number_of_blocks(), 0);
 358   _next_loop_index = 0;
 359   _next_block_number = _blocks.length();
 360 
 361   // recursively iterate the control flow graph
 362   mark_loops(_bci2block-&gt;at(0), false);
 363   assert(_next_block_number &gt;= 0, "invalid block numbers");
 364 }
 365 
 366 void BlockListBuilder::make_loop_header(BlockBegin* block) {
 367   if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 368     // exception edges may look like loops but don't mark them as such
 369     // since it screws up block ordering.
 370     return;
 371   }
 372   if (!block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 373     block-&gt;set(BlockBegin::parser_loop_header_flag);
 374 
 375     assert(_loop_map.at(block-&gt;block_id()) == 0, "must not be set yet");
 376     assert(0 &lt;= _next_loop_index &amp;&amp; _next_loop_index &lt; BitsPerInt, "_next_loop_index is used as a bit-index in integer");
 377     _loop_map.at_put(block-&gt;block_id(), 1 &lt;&lt; _next_loop_index);
 378     if (_next_loop_index &lt; 31) _next_loop_index++;
 379   } else {
 380     // block already marked as loop header
 381     assert(is_power_of_2((unsigned int)_loop_map.at(block-&gt;block_id())), "exactly one bit must be set");
 382   }
 383 }
 384 
 385 int BlockListBuilder::mark_loops(BlockBegin* block, bool in_subroutine) {
 386   int block_id = block-&gt;block_id();
 387 
 388   if (_visited.at(block_id)) {
 389     if (_active.at(block_id)) {
 390       // reached block via backward branch
 391       make_loop_header(block);
 392     }
 393     // return cached loop information for this block
 394     return _loop_map.at(block_id);
 395   }
 396 
 397   if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 398     in_subroutine = true;
 399   }
 400 
 401   // set active and visited bits before successors are processed
 402   _visited.set_bit(block_id);
 403   _active.set_bit(block_id);
 404 
 405   intptr_t loop_state = 0;
 406   for (int i = block-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
 407     // recursively process all successors
 408     loop_state |= mark_loops(block-&gt;sux_at(i), in_subroutine);
 409   }
 410 
 411   // clear active-bit after all successors are processed
 412   _active.clear_bit(block_id);
 413 
 414   // reverse-post-order numbering of all blocks
 415   block-&gt;set_depth_first_number(_next_block_number);
 416   _next_block_number--;
 417 
 418   if (loop_state != 0 || in_subroutine ) {
 419     // block is contained at least in one loop, so phi functions are necessary
 420     // phi functions are also necessary for all locals stored in a subroutine
 421     scope()-&gt;requires_phi_function().set_union(block-&gt;stores_to_locals());
 422   }
 423 
 424   if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 425     int header_loop_state = _loop_map.at(block_id);
 426     assert(is_power_of_2((unsigned)header_loop_state), "exactly one bit must be set");
 427 
 428     // If the highest bit is set (i.e. when integer value is negative), the method
 429     // has 32 or more loops. This bit is never cleared because it is used for multiple loops
 430     if (header_loop_state &gt;= 0) {
 431       clear_bits(loop_state, header_loop_state);
 432     }
 433   }
 434 
 435   // cache and return loop information for this block
 436   _loop_map.at_put(block_id, loop_state);
 437   return loop_state;
 438 }
 439 
 440 
 441 #ifndef PRODUCT
 442 
 443 int compare_depth_first(BlockBegin** a, BlockBegin** b) {
 444   return (*a)-&gt;depth_first_number() - (*b)-&gt;depth_first_number();
 445 }
 446 
 447 void BlockListBuilder::print() {
 448   tty-&gt;print("----- initial block list of BlockListBuilder for method ");
 449   method()-&gt;print_short_name();
 450   tty-&gt;cr();
 451 
 452   // better readability if blocks are sorted in processing order
 453   _blocks.sort(compare_depth_first);
 454 
 455   for (int i = 0; i &lt; _blocks.length(); i++) {
 456     BlockBegin* cur = _blocks.at(i);
 457     tty-&gt;print("%4d: B%-4d bci: %-4d  preds: %-4d ", cur-&gt;depth_first_number(), cur-&gt;block_id(), cur-&gt;bci(), cur-&gt;total_preds());
 458 
 459     tty-&gt;print(cur-&gt;is_set(BlockBegin::std_entry_flag)               ? " std" : "    ");
 460     tty-&gt;print(cur-&gt;is_set(BlockBegin::osr_entry_flag)               ? " osr" : "    ");
 461     tty-&gt;print(cur-&gt;is_set(BlockBegin::exception_entry_flag)         ? " ex" : "   ");
 462     tty-&gt;print(cur-&gt;is_set(BlockBegin::subroutine_entry_flag)        ? " sr" : "   ");
 463     tty-&gt;print(cur-&gt;is_set(BlockBegin::parser_loop_header_flag)      ? " lh" : "   ");
 464 
 465     if (cur-&gt;number_of_sux() &gt; 0) {
 466       tty-&gt;print("    sux: ");
 467       for (int j = 0; j &lt; cur-&gt;number_of_sux(); j++) {
 468         BlockBegin* sux = cur-&gt;sux_at(j);
 469         tty-&gt;print("B%d ", sux-&gt;block_id());
 470       }
 471     }
 472     tty-&gt;cr();
 473   }
 474 }
 475 
 476 #endif
 477 
 478 
 479 // A simple growable array of Values indexed by ciFields
 480 class FieldBuffer: public CompilationResourceObj {
 481  private:
 482   GrowableArray&lt;Value&gt; _values;
 483 
 484  public:
 485   FieldBuffer() {}
 486 
 487   void kill() {
 488     _values.trunc_to(0);
 489   }
 490 
 491   Value at(ciField* field) {
 492     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 493     int offset = field-&gt;offset();
 494     if (offset &lt; _values.length()) {
 495       return _values.at(offset);
 496     } else {
 497       return NULL;
 498     }
 499   }
 500 
 501   void at_put(ciField* field, Value value) {
 502     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 503     int offset = field-&gt;offset();
 504     _values.at_put_grow(offset, value, NULL);
 505   }
 506 
 507 };
 508 
 509 
 510 // MemoryBuffer is fairly simple model of the current state of memory.
 511 // It partitions memory into several pieces.  The first piece is
 512 // generic memory where little is known about the owner of the memory.
 513 // This is conceptually represented by the tuple &lt;O, F, V&gt; which says
 514 // that the field F of object O has value V.  This is flattened so
 515 // that F is represented by the offset of the field and the parallel
 516 // arrays _objects and _values are used for O and V.  Loads of O.F can
 517 // simply use V.  Newly allocated objects are kept in a separate list
 518 // along with a parallel array for each object which represents the
 519 // current value of its fields.  Stores of the default value to fields
 520 // which have never been stored to before are eliminated since they
 521 // are redundant.  Once newly allocated objects are stored into
 522 // another object or they are passed out of the current compile they
 523 // are treated like generic memory.
 524 
 525 class MemoryBuffer: public CompilationResourceObj {
 526  private:
 527   FieldBuffer                 _values;
 528   GrowableArray&lt;Value&gt;        _objects;
 529   GrowableArray&lt;Value&gt;        _newobjects;
 530   GrowableArray&lt;FieldBuffer*&gt; _fields;
 531 
 532  public:
 533   MemoryBuffer() {}
 534 
 535   StoreField* store(StoreField* st) {
 536     if (!EliminateFieldAccess) {
 537       return st;
 538     }
 539 
 540     Value object = st-&gt;obj();
 541     Value value = st-&gt;value();
 542     ciField* field = st-&gt;field();
 543     if (field-&gt;holder()-&gt;is_loaded()) {
 544       int offset = field-&gt;offset();
 545       int index = _newobjects.find(object);
 546       if (index != -1) {
 547         // newly allocated object with no other stores performed on this field
 548         FieldBuffer* buf = _fields.at(index);
 549         if (buf-&gt;at(field) == NULL &amp;&amp; is_default_value(value)) {
 550 #ifndef PRODUCT
 551           if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 552             tty-&gt;print_cr("Eliminated store for object %d:", index);
 553             st-&gt;print_line();
 554           }
 555 #endif
 556           return NULL;
 557         } else {
 558           buf-&gt;at_put(field, value);
 559         }
 560       } else {
 561         _objects.at_put_grow(offset, object, NULL);
 562         _values.at_put(field, value);
 563       }
 564 
 565       store_value(value);
 566     } else {
 567       // if we held onto field names we could alias based on names but
 568       // we don't know what's being stored to so kill it all.
 569       kill();
 570     }
 571     return st;
 572   }
 573 
 574 
 575   // return true if this value correspond to the default value of a field.
 576   bool is_default_value(Value value) {
 577     Constant* con = value-&gt;as_Constant();
 578     if (con) {
 579       switch (con-&gt;type()-&gt;tag()) {
 580         case intTag:    return con-&gt;type()-&gt;as_IntConstant()-&gt;value() == 0;
 581         case longTag:   return con-&gt;type()-&gt;as_LongConstant()-&gt;value() == 0;
 582         case floatTag:  return jint_cast(con-&gt;type()-&gt;as_FloatConstant()-&gt;value()) == 0;
 583         case doubleTag: return jlong_cast(con-&gt;type()-&gt;as_DoubleConstant()-&gt;value()) == jlong_cast(0);
 584         case objectTag: return con-&gt;type() == objectNull;
 585         default:  ShouldNotReachHere();
 586       }
 587     }
 588     return false;
 589   }
 590 
 591 
 592   // return either the actual value of a load or the load itself
 593   Value load(LoadField* load) {
 594     if (!EliminateFieldAccess) {
 595       return load;
 596     }
 597 
 598     if (RoundFPResults &amp;&amp; UseSSE &lt; 2 &amp;&amp; load-&gt;type()-&gt;is_float_kind()) {
 599       // can't skip load since value might get rounded as a side effect
 600       return load;
 601     }
 602 
 603     ciField* field = load-&gt;field();
 604     Value object   = load-&gt;obj();
 605     if (field-&gt;holder()-&gt;is_loaded() &amp;&amp; !field-&gt;is_volatile()) {
 606       int offset = field-&gt;offset();
 607       Value result = NULL;
 608       int index = _newobjects.find(object);
 609       if (index != -1) {
 610         result = _fields.at(index)-&gt;at(field);
 611       } else if (_objects.at_grow(offset, NULL) == object) {
 612         result = _values.at(field);
 613       }
 614       if (result != NULL) {
 615 #ifndef PRODUCT
 616         if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 617           tty-&gt;print_cr("Eliminated load: ");
 618           load-&gt;print_line();
 619         }
 620 #endif
 621         assert(result-&gt;type()-&gt;tag() == load-&gt;type()-&gt;tag(), "wrong types");
 622         return result;
 623       }
 624     }
 625     return load;
 626   }
 627 
 628   // Record this newly allocated object
 629   void new_instance(NewInstance* object) {
 630     int index = _newobjects.length();
 631     _newobjects.append(object);
 632     if (_fields.at_grow(index, NULL) == NULL) {
 633       _fields.at_put(index, new FieldBuffer());
 634     } else {
 635       _fields.at(index)-&gt;kill();
 636     }
 637   }
 638 
 639   void store_value(Value value) {
 640     int index = _newobjects.find(value);
 641     if (index != -1) {
 642       // stored a newly allocated object into another object.
 643       // Assume we've lost track of it as separate slice of memory.
 644       // We could do better by keeping track of whether individual
 645       // fields could alias each other.
 646       _newobjects.remove_at(index);
 647       // pull out the field info and store it at the end up the list
 648       // of field info list to be reused later.
 649       _fields.append(_fields.at(index));
 650       _fields.remove_at(index);
 651     }
 652   }
 653 
 654   void kill() {
 655     _newobjects.trunc_to(0);
 656     _objects.trunc_to(0);
 657     _values.kill();
 658   }
 659 };
 660 
 661 
 662 // Implementation of GraphBuilder's ScopeData
 663 
 664 GraphBuilder::ScopeData::ScopeData(ScopeData* parent)
 665   : _parent(parent)
 666   , _bci2block(NULL)
 667   , _scope(NULL)
 668   , _has_handler(false)
 669   , _stream(NULL)
 670   , _work_list(NULL)
 671   , _parsing_jsr(false)
 672   , _jsr_xhandlers(NULL)
 673   , _caller_stack_size(-1)
 674   , _continuation(NULL)
 675   , _num_returns(0)
 676   , _cleanup_block(NULL)
 677   , _cleanup_return_prev(NULL)
 678   , _cleanup_state(NULL)
 679 {
 680   if (parent != NULL) {
 681     _max_inline_size = (intx) ((float) NestedInliningSizeRatio * (float) parent-&gt;max_inline_size() / 100.0f);
 682   } else {
 683     _max_inline_size = MaxInlineSize;
 684   }
 685   if (_max_inline_size &lt; MaxTrivialSize) {
 686     _max_inline_size = MaxTrivialSize;
 687   }
 688 }
 689 
 690 
 691 void GraphBuilder::kill_all() {
 692   if (UseLocalValueNumbering) {
 693     vmap()-&gt;kill_all();
 694   }
 695   _memory-&gt;kill();
 696 }
 697 
 698 
 699 BlockBegin* GraphBuilder::ScopeData::block_at(int bci) {
 700   if (parsing_jsr()) {
 701     // It is necessary to clone all blocks associated with a
 702     // subroutine, including those for exception handlers in the scope
 703     // of the method containing the jsr (because those exception
 704     // handlers may contain ret instructions in some cases).
 705     BlockBegin* block = bci2block()-&gt;at(bci);
 706     if (block != NULL &amp;&amp; block == parent()-&gt;bci2block()-&gt;at(bci)) {
 707       BlockBegin* new_block = new BlockBegin(block-&gt;bci());
 708 #ifndef PRODUCT
 709       if (PrintInitialBlockList) {
 710         tty-&gt;print_cr("CFG: cloned block %d (bci %d) as block %d for jsr",
 711                       block-&gt;block_id(), block-&gt;bci(), new_block-&gt;block_id());
 712       }
 713 #endif
 714       // copy data from cloned blocked
 715       new_block-&gt;set_depth_first_number(block-&gt;depth_first_number());
 716       if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) new_block-&gt;set(BlockBegin::parser_loop_header_flag);
 717       // Preserve certain flags for assertion checking
 718       if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) new_block-&gt;set(BlockBegin::subroutine_entry_flag);
 719       if (block-&gt;is_set(BlockBegin::exception_entry_flag))  new_block-&gt;set(BlockBegin::exception_entry_flag);
 720 
 721       // copy was_visited_flag to allow early detection of bailouts
 722       // if a block that is used in a jsr has already been visited before,
 723       // it is shared between the normal control flow and a subroutine
 724       // BlockBegin::try_merge returns false when the flag is set, this leads
 725       // to a compilation bailout
 726       if (block-&gt;is_set(BlockBegin::was_visited_flag))  new_block-&gt;set(BlockBegin::was_visited_flag);
 727 
 728       bci2block()-&gt;at_put(bci, new_block);
 729       block = new_block;
 730     }
 731     return block;
 732   } else {
 733     return bci2block()-&gt;at(bci);
 734   }
 735 }
 736 
 737 
 738 XHandlers* GraphBuilder::ScopeData::xhandlers() const {
 739   if (_jsr_xhandlers == NULL) {
 740     assert(!parsing_jsr(), "");
 741     return scope()-&gt;xhandlers();
 742   }
 743   assert(parsing_jsr(), "");
 744   return _jsr_xhandlers;
 745 }
 746 
 747 
 748 void GraphBuilder::ScopeData::set_scope(IRScope* scope) {
 749   _scope = scope;
 750   bool parent_has_handler = false;
 751   if (parent() != NULL) {
 752     parent_has_handler = parent()-&gt;has_handler();
 753   }
 754   _has_handler = parent_has_handler || scope-&gt;xhandlers()-&gt;has_handlers();
 755 }
 756 
 757 
 758 void GraphBuilder::ScopeData::set_inline_cleanup_info(BlockBegin* block,
 759                                                       Instruction* return_prev,
 760                                                       ValueStack* return_state) {
 761   _cleanup_block       = block;
 762   _cleanup_return_prev = return_prev;
 763   _cleanup_state       = return_state;
 764 }
 765 
 766 
 767 void GraphBuilder::ScopeData::add_to_work_list(BlockBegin* block) {
 768   if (_work_list == NULL) {
 769     _work_list = new BlockList();
 770   }
 771 
 772   if (!block-&gt;is_set(BlockBegin::is_on_work_list_flag)) {
 773     // Do not start parsing the continuation block while in a
 774     // sub-scope
 775     if (parsing_jsr()) {
 776       if (block == jsr_continuation()) {
 777         return;
 778       }
 779     } else {
 780       if (block == continuation()) {
 781         return;
 782       }
 783     }
 784     block-&gt;set(BlockBegin::is_on_work_list_flag);
 785     _work_list-&gt;push(block);
 786 
 787     sort_top_into_worklist(_work_list, block);
 788   }
 789 }
 790 
 791 
 792 void GraphBuilder::sort_top_into_worklist(BlockList* worklist, BlockBegin* top) {
 793   assert(worklist-&gt;top() == top, "");
 794   // sort block descending into work list
 795   const int dfn = top-&gt;depth_first_number();
 796   assert(dfn != -1, "unknown depth first number");
 797   int i = worklist-&gt;length()-2;
 798   while (i &gt;= 0) {
 799     BlockBegin* b = worklist-&gt;at(i);
 800     if (b-&gt;depth_first_number() &lt; dfn) {
 801       worklist-&gt;at_put(i+1, b);
 802     } else {
 803       break;
 804     }
 805     i --;
 806   }
 807   if (i &gt;= -1) worklist-&gt;at_put(i + 1, top);
 808 }
 809 
 810 
 811 BlockBegin* GraphBuilder::ScopeData::remove_from_work_list() {
 812   if (is_work_list_empty()) {
 813     return NULL;
 814   }
 815   return _work_list-&gt;pop();
 816 }
 817 
 818 
 819 bool GraphBuilder::ScopeData::is_work_list_empty() const {
 820   return (_work_list == NULL || _work_list-&gt;length() == 0);
 821 }
 822 
 823 
 824 void GraphBuilder::ScopeData::setup_jsr_xhandlers() {
 825   assert(parsing_jsr(), "");
 826   // clone all the exception handlers from the scope
 827   XHandlers* handlers = new XHandlers(scope()-&gt;xhandlers());
 828   const int n = handlers-&gt;length();
 829   for (int i = 0; i &lt; n; i++) {
 830     // The XHandlers need to be adjusted to dispatch to the cloned
 831     // handler block instead of the default one but the synthetic
 832     // unlocker needs to be handled specially.  The synthetic unlocker
 833     // should be left alone since there can be only one and all code
 834     // should dispatch to the same one.
 835     XHandler* h = handlers-&gt;handler_at(i);
 836     assert(h-&gt;handler_bci() != SynchronizationEntryBCI, "must be real");
 837     h-&gt;set_entry_block(block_at(h-&gt;handler_bci()));
 838   }
 839   _jsr_xhandlers = handlers;
 840 }
 841 
 842 
 843 int GraphBuilder::ScopeData::num_returns() {
 844   if (parsing_jsr()) {
 845     return parent()-&gt;num_returns();
 846   }
 847   return _num_returns;
 848 }
 849 
 850 
 851 void GraphBuilder::ScopeData::incr_num_returns() {
 852   if (parsing_jsr()) {
 853     parent()-&gt;incr_num_returns();
 854   } else {
 855     ++_num_returns;
 856   }
 857 }
 858 
 859 
 860 // Implementation of GraphBuilder
 861 
 862 #define INLINE_BAILOUT(msg)        { inline_bailout(msg); return false; }
 863 
 864 
 865 void GraphBuilder::load_constant() {
 866   ciConstant con = stream()-&gt;get_constant();
 867   if (con.basic_type() == T_ILLEGAL) {
 868     BAILOUT("could not resolve a constant");
 869   } else {
 870     ValueType* t = illegalType;
 871     ValueStack* patch_state = NULL;
 872     switch (con.basic_type()) {
 873       case T_BOOLEAN: t = new IntConstant     (con.as_boolean()); break;
 874       case T_BYTE   : t = new IntConstant     (con.as_byte   ()); break;
 875       case T_CHAR   : t = new IntConstant     (con.as_char   ()); break;
 876       case T_SHORT  : t = new IntConstant     (con.as_short  ()); break;
 877       case T_INT    : t = new IntConstant     (con.as_int    ()); break;
 878       case T_LONG   : t = new LongConstant    (con.as_long   ()); break;
 879       case T_FLOAT  : t = new FloatConstant   (con.as_float  ()); break;
 880       case T_DOUBLE : t = new DoubleConstant  (con.as_double ()); break;
 881       case T_ARRAY  : t = new ArrayConstant   (con.as_object ()-&gt;as_array   ()); break;
 882       case T_OBJECT :
 883        {
 884         ciObject* obj = con.as_object();
 885         if (!obj-&gt;is_loaded()
 886             || (PatchALot &amp;&amp; obj-&gt;klass() != ciEnv::current()-&gt;String_klass())) {
 887           patch_state = copy_state_before();
 888           t = new ObjectConstant(obj);
 889         } else {
 890           assert(obj-&gt;is_instance(), "must be java_mirror of klass");
 891           t = new InstanceConstant(obj-&gt;as_instance());
 892         }
 893         break;
 894        }
 895       default       : ShouldNotReachHere();
 896     }
 897     Value x;
 898     if (patch_state != NULL) {
 899       x = new Constant(t, patch_state);
 900     } else {
 901       x = new Constant(t);
 902     }
 903     push(t, append(x));
 904   }
 905 }
 906 
 907 
 908 void GraphBuilder::load_local(ValueType* type, int index) {
 909   Value x = state()-&gt;local_at(index);
 910   assert(x != NULL &amp;&amp; !x-&gt;type()-&gt;is_illegal(), "access of illegal local variable");
 911   push(type, x);
 912 }
 913 
 914 
 915 void GraphBuilder::store_local(ValueType* type, int index) {
 916   Value x = pop(type);
 917   store_local(state(), x, index);
 918 }
 919 
 920 
 921 void GraphBuilder::store_local(ValueStack* state, Value x, int index) {
 922   if (parsing_jsr()) {
 923     // We need to do additional tracking of the location of the return
 924     // address for jsrs since we don't handle arbitrary jsr/ret
 925     // constructs. Here we are figuring out in which circumstances we
 926     // need to bail out.
 927     if (x-&gt;type()-&gt;is_address()) {
 928       scope_data()-&gt;set_jsr_return_address_local(index);
 929 
 930       // Also check parent jsrs (if any) at this time to see whether
 931       // they are using this local. We don't handle skipping over a
 932       // ret.
 933       for (ScopeData* cur_scope_data = scope_data()-&gt;parent();
 934            cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
 935            cur_scope_data = cur_scope_data-&gt;parent()) {
 936         if (cur_scope_data-&gt;jsr_return_address_local() == index) {
 937           BAILOUT("subroutine overwrites return address from previous subroutine");
 938         }
 939       }
 940     } else if (index == scope_data()-&gt;jsr_return_address_local()) {
 941       scope_data()-&gt;set_jsr_return_address_local(-1);
 942     }
 943   }
 944 
 945   state-&gt;store_local(index, round_fp(x));
 946 }
 947 
 948 
 949 void GraphBuilder::load_indexed(BasicType type) {
 950   // In case of in block code motion in range check elimination
 951   ValueStack* state_before = copy_state_indexed_access();
 952   compilation()-&gt;set_has_access_indexed(true);
 953   Value index = ipop();
 954   Value array = apop();
 955   Value length = NULL;
 956   if (CSEArrayLength ||
 957       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 958       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 959     length = append(new ArrayLength(array, state_before));
 960   }
 961   push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));
 962 }
 963 
 964 
 965 void GraphBuilder::store_indexed(BasicType type) {
 966   // In case of in block code motion in range check elimination
 967   ValueStack* state_before = copy_state_indexed_access();
 968   compilation()-&gt;set_has_access_indexed(true);
 969   Value value = pop(as_ValueType(type));
 970   Value index = ipop();
 971   Value array = apop();
 972   Value length = NULL;
 973   if (CSEArrayLength ||
 974       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 975       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 976     length = append(new ArrayLength(array, state_before));
 977   }
 978   StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before);
 979   append(result);
 980   _memory-&gt;store_value(value);
 981 
 982   if (type == T_OBJECT &amp;&amp; is_profiling()) {
 983     // Note that we'd collect profile data in this method if we wanted it.
 984     compilation()-&gt;set_would_profile(true);
 985 
 986     if (profile_checkcasts()) {
 987       result-&gt;set_profiled_method(method());
 988       result-&gt;set_profiled_bci(bci());
 989       result-&gt;set_should_profile(true);
 990     }
 991   }
 992 }
 993 
 994 
 995 void GraphBuilder::stack_op(Bytecodes::Code code) {
 996   switch (code) {
 997     case Bytecodes::_pop:
 998       { state()-&gt;raw_pop();
 999       }
1000       break;
1001     case Bytecodes::_pop2:
1002       { state()-&gt;raw_pop();
1003         state()-&gt;raw_pop();
1004       }
1005       break;
1006     case Bytecodes::_dup:
1007       { Value w = state()-&gt;raw_pop();
1008         state()-&gt;raw_push(w);
1009         state()-&gt;raw_push(w);
1010       }
1011       break;
1012     case Bytecodes::_dup_x1:
1013       { Value w1 = state()-&gt;raw_pop();
1014         Value w2 = state()-&gt;raw_pop();
1015         state()-&gt;raw_push(w1);
1016         state()-&gt;raw_push(w2);
1017         state()-&gt;raw_push(w1);
1018       }
1019       break;
1020     case Bytecodes::_dup_x2:
1021       { Value w1 = state()-&gt;raw_pop();
1022         Value w2 = state()-&gt;raw_pop();
1023         Value w3 = state()-&gt;raw_pop();
1024         state()-&gt;raw_push(w1);
1025         state()-&gt;raw_push(w3);
1026         state()-&gt;raw_push(w2);
1027         state()-&gt;raw_push(w1);
1028       }
1029       break;
1030     case Bytecodes::_dup2:
1031       { Value w1 = state()-&gt;raw_pop();
1032         Value w2 = state()-&gt;raw_pop();
1033         state()-&gt;raw_push(w2);
1034         state()-&gt;raw_push(w1);
1035         state()-&gt;raw_push(w2);
1036         state()-&gt;raw_push(w1);
1037       }
1038       break;
1039     case Bytecodes::_dup2_x1:
1040       { Value w1 = state()-&gt;raw_pop();
1041         Value w2 = state()-&gt;raw_pop();
1042         Value w3 = state()-&gt;raw_pop();
1043         state()-&gt;raw_push(w2);
1044         state()-&gt;raw_push(w1);
1045         state()-&gt;raw_push(w3);
1046         state()-&gt;raw_push(w2);
1047         state()-&gt;raw_push(w1);
1048       }
1049       break;
1050     case Bytecodes::_dup2_x2:
1051       { Value w1 = state()-&gt;raw_pop();
1052         Value w2 = state()-&gt;raw_pop();
1053         Value w3 = state()-&gt;raw_pop();
1054         Value w4 = state()-&gt;raw_pop();
1055         state()-&gt;raw_push(w2);
1056         state()-&gt;raw_push(w1);
1057         state()-&gt;raw_push(w4);
1058         state()-&gt;raw_push(w3);
1059         state()-&gt;raw_push(w2);
1060         state()-&gt;raw_push(w1);
1061       }
1062       break;
1063     case Bytecodes::_swap:
1064       { Value w1 = state()-&gt;raw_pop();
1065         Value w2 = state()-&gt;raw_pop();
1066         state()-&gt;raw_push(w1);
1067         state()-&gt;raw_push(w2);
1068       }
1069       break;
1070     default:
1071       ShouldNotReachHere();
1072       break;
1073   }
1074 }
1075 
1076 
1077 void GraphBuilder::arithmetic_op(ValueType* type, Bytecodes::Code code, ValueStack* state_before) {
1078   Value y = pop(type);
1079   Value x = pop(type);
1080   // NOTE: strictfp can be queried from current method since we don't
1081   // inline methods with differing strictfp bits
1082   Value res = new ArithmeticOp(code, x, y, method()-&gt;is_strict(), state_before);
1083   // Note: currently single-precision floating-point rounding on Intel is handled at the LIRGenerator level
1084   res = append(res);
1085   if (method()-&gt;is_strict()) {
1086     res = round_fp(res);
1087   }
1088   push(type, res);
1089 }
1090 
1091 
1092 void GraphBuilder::negate_op(ValueType* type) {
1093   push(type, append(new NegateOp(pop(type))));
1094 }
1095 
1096 
1097 void GraphBuilder::shift_op(ValueType* type, Bytecodes::Code code) {
1098   Value s = ipop();
1099   Value x = pop(type);
1100   // try to simplify
1101   // Note: This code should go into the canonicalizer as soon as it can
1102   //       can handle canonicalized forms that contain more than one node.
1103   if (CanonicalizeNodes &amp;&amp; code == Bytecodes::_iushr) {
1104     // pattern: x &gt;&gt;&gt; s
1105     IntConstant* s1 = s-&gt;type()-&gt;as_IntConstant();
1106     if (s1 != NULL) {
1107       // pattern: x &gt;&gt;&gt; s1, with s1 constant
1108       ShiftOp* l = x-&gt;as_ShiftOp();
1109       if (l != NULL &amp;&amp; l-&gt;op() == Bytecodes::_ishl) {
1110         // pattern: (a &lt;&lt; b) &gt;&gt;&gt; s1
1111         IntConstant* s0 = l-&gt;y()-&gt;type()-&gt;as_IntConstant();
1112         if (s0 != NULL) {
1113           // pattern: (a &lt;&lt; s0) &gt;&gt;&gt; s1
1114           const int s0c = s0-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1115           const int s1c = s1-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1116           if (s0c == s1c) {
1117             if (s0c == 0) {
1118               // pattern: (a &lt;&lt; 0) &gt;&gt;&gt; 0 =&gt; simplify to: a
1119               ipush(l-&gt;x());
1120             } else {
1121               // pattern: (a &lt;&lt; s0c) &gt;&gt;&gt; s0c =&gt; simplify to: a &amp; m, with m constant
1122               assert(0 &lt; s0c &amp;&amp; s0c &lt; BitsPerInt, "adjust code below to handle corner cases");
1123               const int m = (1 &lt;&lt; (BitsPerInt - s0c)) - 1;
1124               Value s = append(new Constant(new IntConstant(m)));
1125               ipush(append(new LogicOp(Bytecodes::_iand, l-&gt;x(), s)));
1126             }
1127             return;
1128           }
1129         }
1130       }
1131     }
1132   }
1133   // could not simplify
1134   push(type, append(new ShiftOp(code, x, s)));
1135 }
1136 
1137 
1138 void GraphBuilder::logic_op(ValueType* type, Bytecodes::Code code) {
1139   Value y = pop(type);
1140   Value x = pop(type);
1141   push(type, append(new LogicOp(code, x, y)));
1142 }
1143 
1144 
1145 void GraphBuilder::compare_op(ValueType* type, Bytecodes::Code code) {
1146   ValueStack* state_before = copy_state_before();
1147   Value y = pop(type);
1148   Value x = pop(type);
1149   ipush(append(new CompareOp(code, x, y, state_before)));
1150 }
1151 
1152 
1153 void GraphBuilder::convert(Bytecodes::Code op, BasicType from, BasicType to) {
1154   push(as_ValueType(to), append(new Convert(op, pop(as_ValueType(from)), as_ValueType(to))));
1155 }
1156 
1157 
1158 void GraphBuilder::increment() {
1159   int index = stream()-&gt;get_index();
1160   int delta = stream()-&gt;is_wide() ? (signed short)Bytes::get_Java_u2(stream()-&gt;cur_bcp() + 4) : (signed char)(stream()-&gt;cur_bcp()[2]);
1161   load_local(intType, index);
1162   ipush(append(new Constant(new IntConstant(delta))));
1163   arithmetic_op(intType, Bytecodes::_iadd);
1164   store_local(intType, index);
1165 }
1166 
1167 
1168 void GraphBuilder::_goto(int from_bci, int to_bci) {
1169   Goto *x = new Goto(block_at(to_bci), to_bci &lt;= from_bci);
1170   if (is_profiling()) {
1171     compilation()-&gt;set_would_profile(true);
1172     x-&gt;set_profiled_bci(bci());
1173     if (profile_branches()) {
1174       x-&gt;set_profiled_method(method());
1175       x-&gt;set_should_profile(true);
1176     }
1177   }
1178   append(x);
1179 }
1180 
1181 
1182 void GraphBuilder::if_node(Value x, If::Condition cond, Value y, ValueStack* state_before) {
1183   BlockBegin* tsux = block_at(stream()-&gt;get_dest());
1184   BlockBegin* fsux = block_at(stream()-&gt;next_bci());
1185   bool is_bb = tsux-&gt;bci() &lt; stream()-&gt;cur_bci() || fsux-&gt;bci() &lt; stream()-&gt;cur_bci();
1186   // In case of loop invariant code motion or predicate insertion
1187   // before the body of a loop the state is needed
1188   Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()-&gt;is_optimistic()) ? state_before : NULL, is_bb));
1189 
1190   assert(i-&gt;as_Goto() == NULL ||
1191          (i-&gt;as_Goto()-&gt;sux_at(0) == tsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == tsux-&gt;bci() &lt; stream()-&gt;cur_bci()) ||
1192          (i-&gt;as_Goto()-&gt;sux_at(0) == fsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == fsux-&gt;bci() &lt; stream()-&gt;cur_bci()),
1193          "safepoint state of Goto returned by canonicalizer incorrect");
1194 
1195   if (is_profiling()) {
1196     If* if_node = i-&gt;as_If();
1197     if (if_node != NULL) {
1198       // Note that we'd collect profile data in this method if we wanted it.
1199       compilation()-&gt;set_would_profile(true);
1200       // At level 2 we need the proper bci to count backedges
1201       if_node-&gt;set_profiled_bci(bci());
1202       if (profile_branches()) {
1203         // Successors can be rotated by the canonicalizer, check for this case.
1204         if_node-&gt;set_profiled_method(method());
1205         if_node-&gt;set_should_profile(true);
1206         if (if_node-&gt;tsux() == fsux) {
1207           if_node-&gt;set_swapped(true);
1208         }
1209       }
1210       return;
1211     }
1212 
1213     // Check if this If was reduced to Goto.
1214     Goto *goto_node = i-&gt;as_Goto();
1215     if (goto_node != NULL) {
1216       compilation()-&gt;set_would_profile(true);
1217       goto_node-&gt;set_profiled_bci(bci());
1218       if (profile_branches()) {
1219         goto_node-&gt;set_profiled_method(method());
1220         goto_node-&gt;set_should_profile(true);
1221         // Find out which successor is used.
1222         if (goto_node-&gt;default_sux() == tsux) {
1223           goto_node-&gt;set_direction(Goto::taken);
1224         } else if (goto_node-&gt;default_sux() == fsux) {
1225           goto_node-&gt;set_direction(Goto::not_taken);
1226         } else {
1227           ShouldNotReachHere();
1228         }
1229       }
1230       return;
1231     }
1232   }
1233 }
1234 
1235 
1236 void GraphBuilder::if_zero(ValueType* type, If::Condition cond) {
1237   Value y = append(new Constant(intZero));
1238   ValueStack* state_before = copy_state_before();
1239   Value x = ipop();
1240   if_node(x, cond, y, state_before);
1241 }
1242 
1243 
1244 void GraphBuilder::if_null(ValueType* type, If::Condition cond) {
1245   Value y = append(new Constant(objectNull));
1246   ValueStack* state_before = copy_state_before();
1247   Value x = apop();
1248   if_node(x, cond, y, state_before);
1249 }
1250 
1251 
1252 void GraphBuilder::if_same(ValueType* type, If::Condition cond) {
1253   ValueStack* state_before = copy_state_before();
1254   Value y = pop(type);
1255   Value x = pop(type);
1256   if_node(x, cond, y, state_before);
1257 }
1258 
1259 
1260 void GraphBuilder::jsr(int dest) {
1261   // We only handle well-formed jsrs (those which are "block-structured").
1262   // If the bytecodes are strange (jumping out of a jsr block) then we
1263   // might end up trying to re-parse a block containing a jsr which
1264   // has already been activated. Watch for this case and bail out.
1265   for (ScopeData* cur_scope_data = scope_data();
1266        cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
1267        cur_scope_data = cur_scope_data-&gt;parent()) {
1268     if (cur_scope_data-&gt;jsr_entry_bci() == dest) {
1269       BAILOUT("too-complicated jsr/ret structure");
1270     }
1271   }
1272 
1273   push(addressType, append(new Constant(new AddressConstant(next_bci()))));
1274   if (!try_inline_jsr(dest)) {
1275     return; // bailed out while parsing and inlining subroutine
1276   }
1277 }
1278 
1279 
1280 void GraphBuilder::ret(int local_index) {
1281   if (!parsing_jsr()) BAILOUT("ret encountered while not parsing subroutine");
1282 
1283   if (local_index != scope_data()-&gt;jsr_return_address_local()) {
1284     BAILOUT("can not handle complicated jsr/ret constructs");
1285   }
1286 
1287   // Rets simply become (NON-SAFEPOINT) gotos to the jsr continuation
1288   append(new Goto(scope_data()-&gt;jsr_continuation(), false));
1289 }
1290 
1291 
1292 void GraphBuilder::table_switch() {
1293   Bytecode_tableswitch sw(stream());
1294   const int l = sw.length();
1295   if (CanonicalizeNodes &amp;&amp; l == 1) {
1296     // total of 2 successors =&gt; use If instead of switch
1297     // Note: This code should go into the canonicalizer as soon as it can
1298     //       can handle canonicalized forms that contain more than one node.
1299     Value key = append(new Constant(new IntConstant(sw.low_key())));
1300     BlockBegin* tsux = block_at(bci() + sw.dest_offset_at(0));
1301     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1302     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1303     // In case of loop invariant code motion or predicate insertion
1304     // before the body of a loop the state is needed
1305     ValueStack* state_before = copy_state_if_bb(is_bb);
1306     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1307   } else {
1308     // collect successors
1309     BlockList* sux = new BlockList(l + 1, NULL);
1310     int i;
1311     bool has_bb = false;
1312     for (i = 0; i &lt; l; i++) {
1313       sux-&gt;at_put(i, block_at(bci() + sw.dest_offset_at(i)));
1314       if (sw.dest_offset_at(i) &lt; 0) has_bb = true;
1315     }
1316     // add default successor
1317     if (sw.default_offset() &lt; 0) has_bb = true;
1318     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1319     // In case of loop invariant code motion or predicate insertion
1320     // before the body of a loop the state is needed
1321     ValueStack* state_before = copy_state_if_bb(has_bb);
1322     Instruction* res = append(new TableSwitch(ipop(), sux, sw.low_key(), state_before, has_bb));
1323 #ifdef ASSERT
1324     if (res-&gt;as_Goto()) {
1325       for (i = 0; i &lt; l; i++) {
1326         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1327           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.dest_offset_at(i) &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1328         }
1329       }
1330     }
1331 #endif
1332   }
1333 }
1334 
1335 
1336 void GraphBuilder::lookup_switch() {
1337   Bytecode_lookupswitch sw(stream());
1338   const int l = sw.number_of_pairs();
1339   if (CanonicalizeNodes &amp;&amp; l == 1) {
1340     // total of 2 successors =&gt; use If instead of switch
1341     // Note: This code should go into the canonicalizer as soon as it can
1342     //       can handle canonicalized forms that contain more than one node.
1343     // simplify to If
1344     LookupswitchPair pair = sw.pair_at(0);
1345     Value key = append(new Constant(new IntConstant(pair.match())));
1346     BlockBegin* tsux = block_at(bci() + pair.offset());
1347     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1348     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1349     // In case of loop invariant code motion or predicate insertion
1350     // before the body of a loop the state is needed
1351     ValueStack* state_before = copy_state_if_bb(is_bb);;
1352     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1353   } else {
1354     // collect successors &amp; keys
1355     BlockList* sux = new BlockList(l + 1, NULL);
1356     intArray* keys = new intArray(l, 0);
1357     int i;
1358     bool has_bb = false;
1359     for (i = 0; i &lt; l; i++) {
1360       LookupswitchPair pair = sw.pair_at(i);
1361       if (pair.offset() &lt; 0) has_bb = true;
1362       sux-&gt;at_put(i, block_at(bci() + pair.offset()));
1363       keys-&gt;at_put(i, pair.match());
1364     }
1365     // add default successor
1366     if (sw.default_offset() &lt; 0) has_bb = true;
1367     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1368     // In case of loop invariant code motion or predicate insertion
1369     // before the body of a loop the state is needed
1370     ValueStack* state_before = copy_state_if_bb(has_bb);
1371     Instruction* res = append(new LookupSwitch(ipop(), sux, keys, state_before, has_bb));
1372 #ifdef ASSERT
1373     if (res-&gt;as_Goto()) {
1374       for (i = 0; i &lt; l; i++) {
1375         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1376           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.pair_at(i).offset() &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1377         }
1378       }
1379     }
1380 #endif
1381   }
1382 }
1383 
1384 void GraphBuilder::call_register_finalizer() {
1385   // If the receiver requires finalization then emit code to perform
1386   // the registration on return.
1387 
1388   // Gather some type information about the receiver
1389   Value receiver = state()-&gt;local_at(0);
1390   assert(receiver != NULL, "must have a receiver");
1391   ciType* declared_type = receiver-&gt;declared_type();
1392   ciType* exact_type = receiver-&gt;exact_type();
1393   if (exact_type == NULL &amp;&amp;
1394       receiver-&gt;as_Local() &amp;&amp;
1395       receiver-&gt;as_Local()-&gt;java_index() == 0) {
1396     ciInstanceKlass* ik = compilation()-&gt;method()-&gt;holder();
1397     if (ik-&gt;is_final()) {
1398       exact_type = ik;
1399     } else if (UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
1400       // test class is leaf class
1401       compilation()-&gt;dependency_recorder()-&gt;assert_leaf_type(ik);
1402       exact_type = ik;
1403     } else {
1404       declared_type = ik;
1405     }
1406   }
1407 
1408   // see if we know statically that registration isn't required
1409   bool needs_check = true;
1410   if (exact_type != NULL) {
1411     needs_check = exact_type-&gt;as_instance_klass()-&gt;has_finalizer();
1412   } else if (declared_type != NULL) {
1413     ciInstanceKlass* ik = declared_type-&gt;as_instance_klass();
1414     if (!Dependencies::has_finalizable_subclass(ik)) {
1415       compilation()-&gt;dependency_recorder()-&gt;assert_has_no_finalizable_subclasses(ik);
1416       needs_check = false;
1417     }
1418   }
1419 
1420   if (needs_check) {
1421     // Perform the registration of finalizable objects.
1422     ValueStack* state_before = copy_state_for_exception();
1423     load_local(objectType, 0);
1424     append_split(new Intrinsic(voidType, vmIntrinsics::_Object_init,
1425                                state()-&gt;pop_arguments(1),
1426                                true, state_before, true));
1427   }
1428 }
1429 
1430 
1431 void GraphBuilder::method_return(Value x) {
1432   if (RegisterFinalizersAtInit &amp;&amp;
1433       method()-&gt;intrinsic_id() == vmIntrinsics::_Object_init) {
1434     call_register_finalizer();
1435   }
1436 
1437   bool need_mem_bar = false;
1438   if (method()-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
1439       scope()-&gt;wrote_final()) {
1440     need_mem_bar = true;
1441   }
1442 
1443   // Check to see whether we are inlining. If so, Return
1444   // instructions become Gotos to the continuation point.
1445   if (continuation() != NULL) {
1446     assert(!method()-&gt;is_synchronized() || InlineSynchronizedMethods, "can not inline synchronized methods yet");
1447 
1448     if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
1449       // Report exit from inline methods
1450       Values* args = new Values(1);
1451       args-&gt;push(append(new Constant(new MethodConstant(method()))));
1452       append(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args));
1453     }
1454 
1455     // If the inlined method is synchronized, the monitor must be
1456     // released before we jump to the continuation block.
1457     if (method()-&gt;is_synchronized()) {
1458       assert(state()-&gt;locks_size() == 1, "receiver must be locked here");
1459       monitorexit(state()-&gt;lock_at(0), SynchronizationEntryBCI);
1460     }
1461 
1462     if (need_mem_bar) {
1463       append(new MemBar(lir_membar_storestore));
1464     }
1465 
1466     // State at end of inlined method is the state of the caller
1467     // without the method parameters on stack, including the
1468     // return value, if any, of the inlined method on operand stack.
1469     int invoke_bci = state()-&gt;caller_state()-&gt;bci();
1470     set_state(state()-&gt;caller_state()-&gt;copy_for_parsing());
1471     if (x != NULL) {
1472       state()-&gt;push(x-&gt;type(), x);
1473       if (profile_return() &amp;&amp; x-&gt;type()-&gt;is_object_kind()) {
1474         ciMethod* caller = state()-&gt;scope()-&gt;method();
1475         ciMethodData* md = caller-&gt;method_data_or_null();
1476         ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
1477         if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1478           bool has_return = data-&gt;is_CallTypeData() ? ((ciCallTypeData*)data)-&gt;has_return() : ((ciVirtualCallTypeData*)data)-&gt;has_return();
1479           // May not be true in case of an inlined call through a method handle intrinsic.
1480           if (has_return) {
1481             profile_return_type(x, method(), caller, invoke_bci);
1482           }
1483         }
1484       }
1485     }
1486     Goto* goto_callee = new Goto(continuation(), false);
1487 
1488     // See whether this is the first return; if so, store off some
1489     // of the state for later examination
1490     if (num_returns() == 0) {
1491       set_inline_cleanup_info();
1492     }
1493 
1494     // The current bci() is in the wrong scope, so use the bci() of
1495     // the continuation point.
1496     append_with_bci(goto_callee, scope_data()-&gt;continuation()-&gt;bci());
1497     incr_num_returns();
1498     return;
1499   }
1500 
1501   state()-&gt;truncate_stack(0);
1502   if (method()-&gt;is_synchronized()) {
1503     // perform the unlocking before exiting the method
1504     Value receiver;
1505     if (!method()-&gt;is_static()) {
1506       receiver = _initial_state-&gt;local_at(0);
1507     } else {
1508       receiver = append(new Constant(new ClassConstant(method()-&gt;holder())));
1509     }
1510     append_split(new MonitorExit(receiver, state()-&gt;unlock()));
1511   }
1512 
1513   if (need_mem_bar) {
1514       append(new MemBar(lir_membar_storestore));
1515   }
1516 
1517   append(new Return(x));
1518 }
1519 
1520 
1521 void GraphBuilder::access_field(Bytecodes::Code code) {
1522   bool will_link;
1523   ciField* field = stream()-&gt;get_field(will_link);
1524   ciInstanceKlass* holder = field-&gt;holder();
1525   BasicType field_type = field-&gt;type()-&gt;basic_type();
1526   ValueType* type = as_ValueType(field_type);
1527   // call will_link again to determine if the field is valid.
1528   const bool needs_patching = !holder-&gt;is_loaded() ||
1529                               !field-&gt;will_link(method()-&gt;holder(), code) ||
1530                               PatchALot;
1531 
1532   ValueStack* state_before = NULL;
1533   if (!holder-&gt;is_initialized() || needs_patching) {
1534     // save state before instruction for debug info when
1535     // deoptimization happens during patching
1536     state_before = copy_state_before();
1537   }
1538 
1539   Value obj = NULL;
1540   if (code == Bytecodes::_getstatic || code == Bytecodes::_putstatic) {
1541     if (state_before != NULL) {
1542       // build a patching constant
1543       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()), state_before);
1544     } else {
1545       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()));
1546     }
1547   }
1548 
1549   if (field-&gt;is_final() &amp;&amp; (code == Bytecodes::_putfield)) {
1550     scope()-&gt;set_wrote_final();
1551   }
1552 
1553   const int offset = !needs_patching ? field-&gt;offset() : -1;
1554   switch (code) {
1555     case Bytecodes::_getstatic: {
1556       // check for compile-time constants, i.e., initialized static final fields
1557       Instruction* constant = NULL;
1558       if (field-&gt;is_constant() &amp;&amp; !PatchALot) {
1559         ciConstant field_val = field-&gt;constant_value();
1560         BasicType field_type = field_val.basic_type();
1561         switch (field_type) {
1562         case T_ARRAY:
1563         case T_OBJECT:
1564           if (field_val.as_object()-&gt;should_be_constant()) {
1565             constant = new Constant(as_ValueType(field_val));
1566           }
1567           break;
1568 
1569         default:
1570           constant = new Constant(as_ValueType(field_val));
1571         }
1572       }
1573       if (constant != NULL) {
1574         push(type, append(constant));
1575       } else {
1576         if (state_before == NULL) {
1577           state_before = copy_state_for_exception();
1578         }
1579         push(type, append(new LoadField(append(obj), offset, field, true,
1580                                         state_before, needs_patching)));
1581       }
1582       break;
1583     }
1584     case Bytecodes::_putstatic:
1585       { Value val = pop(type);
1586         if (state_before == NULL) {
1587           state_before = copy_state_for_exception();
1588         }
1589         append(new StoreField(append(obj), offset, field, val, true, state_before, needs_patching));
1590       }
1591       break;
1592     case Bytecodes::_getfield: {
1593       // Check for compile-time constants, i.e., trusted final non-static fields.
1594       Instruction* constant = NULL;
1595       obj = apop();
1596       ObjectType* obj_type = obj-&gt;type()-&gt;as_ObjectType();
1597       if (obj_type-&gt;is_constant() &amp;&amp; !PatchALot) {
1598         ciObject* const_oop = obj_type-&gt;constant_value();
1599         if (!const_oop-&gt;is_null_object() &amp;&amp; const_oop-&gt;is_loaded()) {
1600           if (field-&gt;is_constant()) {
1601             ciConstant field_val = field-&gt;constant_value_of(const_oop);
1602             BasicType field_type = field_val.basic_type();
1603             switch (field_type) {
1604             case T_ARRAY:
1605             case T_OBJECT:
1606               if (field_val.as_object()-&gt;should_be_constant()) {
1607                 constant = new Constant(as_ValueType(field_val));
1608               }
1609               break;
1610             default:
1611               constant = new Constant(as_ValueType(field_val));
1612             }
1613           } else {
1614             // For CallSite objects treat the target field as a compile time constant.
1615             if (const_oop-&gt;is_call_site()) {
1616               ciCallSite* call_site = const_oop-&gt;as_call_site();
1617               if (field-&gt;is_call_site_target()) {
1618                 ciMethodHandle* target = call_site-&gt;get_target();
1619                 if (target != NULL) {  // just in case
1620                   ciConstant field_val(T_OBJECT, target);
1621                   constant = new Constant(as_ValueType(field_val));
1622                   // Add a dependence for invalidation of the optimization.
1623                   if (!call_site-&gt;is_constant_call_site()) {
1624                     dependency_recorder()-&gt;assert_call_site_target_value(call_site, target);
1625                   }
1626                 }
1627               }
1628             }
1629           }
1630         }
1631       }
1632       if (constant != NULL) {
1633         push(type, append(constant));
1634       } else {
1635         if (state_before == NULL) {
1636           state_before = copy_state_for_exception();
1637         }
1638         LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);
1639         Value replacement = !needs_patching ? _memory-&gt;load(load) : load;
1640         if (replacement != load) {
1641           assert(replacement-&gt;is_linked() || !replacement-&gt;can_be_linked(), "should already by linked");
1642           push(type, replacement);
1643         } else {
1644           push(type, append(load));
1645         }
1646       }
1647       break;
1648     }
1649     case Bytecodes::_putfield: {
1650       Value val = pop(type);
1651       obj = apop();
1652       if (state_before == NULL) {
1653         state_before = copy_state_for_exception();
1654       }
1655       StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);
1656       if (!needs_patching) store = _memory-&gt;store(store);
1657       if (store != NULL) {
1658         append(store);
1659       }
1660       break;
1661     }
1662     default:
1663       ShouldNotReachHere();
1664       break;
1665   }
1666 }
1667 
1668 
1669 Dependencies* GraphBuilder::dependency_recorder() const {
1670   assert(DeoptC1, "need debug information");
1671   return compilation()-&gt;dependency_recorder();
1672 }
1673 
1674 // How many arguments do we want to profile?
1675 Values* GraphBuilder::args_list_for_profiling(ciMethod* target, int&amp; start, bool may_have_receiver) {
1676   int n = 0;
1677   bool has_receiver = may_have_receiver &amp;&amp; Bytecodes::has_receiver(method()-&gt;java_code_at_bci(bci()));
1678   start = has_receiver ? 1 : 0;
1679   if (profile_arguments()) {
1680     ciProfileData* data = method()-&gt;method_data()-&gt;bci_to_data(bci());
1681     if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1682       n = data-&gt;is_CallTypeData() ? data-&gt;as_CallTypeData()-&gt;number_of_arguments() : data-&gt;as_VirtualCallTypeData()-&gt;number_of_arguments();
1683     }
1684   }
1685   // If we are inlining then we need to collect arguments to profile parameters for the target
1686   if (profile_parameters() &amp;&amp; target != NULL) {
1687     if (target-&gt;method_data() != NULL &amp;&amp; target-&gt;method_data()-&gt;parameters_type_data() != NULL) {
1688       // The receiver is profiled on method entry so it's included in
1689       // the number of parameters but here we're only interested in
1690       // actual arguments.
1691       n = MAX2(n, target-&gt;method_data()-&gt;parameters_type_data()-&gt;number_of_parameters() - start);
1692     }
1693   }
1694   if (n &gt; 0) {
1695     return new Values(n);
1696   }
1697   return NULL;
1698 }
1699 
1700 // Collect arguments that we want to profile in a list
1701 Values* GraphBuilder::collect_args_for_profiling(Values* args, ciMethod* target, bool may_have_receiver) {
1702   int start = 0;
1703   Values* obj_args = args_list_for_profiling(target, start, may_have_receiver);
1704   if (obj_args == NULL) {
1705     return NULL;
1706   }
1707   int s = obj_args-&gt;size();
1708   for (int i = start, j = 0; j &lt; s; i++) {
1709     if (args-&gt;at(i)-&gt;type()-&gt;is_object_kind()) {
1710       obj_args-&gt;push(args-&gt;at(i));
1711       j++;
1712     }
1713   }
1714   assert(s == obj_args-&gt;length(), "missed on arg?");
1715   return obj_args;
1716 }
1717 
1718 
1719 void GraphBuilder::invoke(Bytecodes::Code code) {
1720   bool will_link;
1721   ciSignature* declared_signature = NULL;
1722   ciMethod*             target = stream()-&gt;get_method(will_link, &amp;declared_signature);
1723   ciKlass*              holder = stream()-&gt;get_declared_method_holder();
1724   const Bytecodes::Code bc_raw = stream()-&gt;cur_bc_raw();
1725   assert(declared_signature != NULL, "cannot be null");
1726 
1727   if (!C1PatchInvokeDynamic &amp;&amp; Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; !will_link) {
1728     BAILOUT("unlinked call site (C1PatchInvokeDynamic is off)");
1729   }
1730 
1731   // we have to make sure the argument size (incl. the receiver)
1732   // is correct for compilation (the call would fail later during
1733   // linkage anyway) - was bug (gri 7/28/99)
1734   {
1735     // Use raw to get rewritten bytecode.
1736     const bool is_invokestatic = bc_raw == Bytecodes::_invokestatic;
1737     const bool allow_static =
1738           is_invokestatic ||
1739           bc_raw == Bytecodes::_invokehandle ||
1740           bc_raw == Bytecodes::_invokedynamic;
1741     if (target-&gt;is_loaded()) {
1742       if (( target-&gt;is_static() &amp;&amp; !allow_static) ||
1743           (!target-&gt;is_static() &amp;&amp;  is_invokestatic)) {
1744         BAILOUT("will cause link error");
1745       }
1746     }
1747   }
1748   ciInstanceKlass* klass = target-&gt;holder();
1749 
1750   // check if CHA possible: if so, change the code to invoke_special
1751   ciInstanceKlass* calling_klass = method()-&gt;holder();
1752   ciInstanceKlass* callee_holder = ciEnv::get_instance_klass_for_declared_method_holder(holder);
1753   ciInstanceKlass* actual_recv = callee_holder;
1754 
1755   CompileLog* log = compilation()-&gt;log();
1756   if (log != NULL)
1757       log-&gt;elem("call method='%d' instr='%s'",
1758                 log-&gt;identify(target),
1759                 Bytecodes::name(code));
1760 
1761   // Some methods are obviously bindable without any type checks so
1762   // convert them directly to an invokespecial or invokestatic.
1763   if (target-&gt;is_loaded() &amp;&amp; !target-&gt;is_abstract() &amp;&amp; target-&gt;can_be_statically_bound()) {
1764     switch (bc_raw) {
1765     case Bytecodes::_invokevirtual:
1766       code = Bytecodes::_invokespecial;
1767       break;
1768     case Bytecodes::_invokehandle:
1769       code = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokespecial;
1770       break;
1771     }
1772   } else {
1773     if (bc_raw == Bytecodes::_invokehandle) {
1774       assert(!will_link, "should come here only for unlinked call");
1775       code = Bytecodes::_invokespecial;
1776     }
1777   }
1778 
1779   // Push appendix argument (MethodType, CallSite, etc.), if one.
1780   bool patch_for_appendix = false;
1781   int patching_appendix_arg = 0;
1782   if (C1PatchInvokeDynamic &amp;&amp;
1783       (Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; (!will_link || PatchALot))) {
1784     Value arg = append(new Constant(new ObjectConstant(compilation()-&gt;env()-&gt;unloaded_ciinstance()), copy_state_before()));
1785     apush(arg);
1786     patch_for_appendix = true;
1787     patching_appendix_arg = (will_link &amp;&amp; stream()-&gt;has_appendix()) ? 0 : 1;
1788   } else if (stream()-&gt;has_appendix()) {
1789     ciObject* appendix = stream()-&gt;get_appendix();
1790     Value arg = append(new Constant(new ObjectConstant(appendix)));
1791     apush(arg);
1792   }
1793 
1794   // NEEDS_CLEANUP
1795   // I've added the target-&gt;is_loaded() test below but I don't really understand
1796   // how klass-&gt;is_loaded() can be true and yet target-&gt;is_loaded() is false.
1797   // this happened while running the JCK invokevirtual tests under doit.  TKR
1798   ciMethod* cha_monomorphic_target = NULL;
1799   ciMethod* exact_target = NULL;
1800   Value better_receiver = NULL;
1801   if (UseCHA &amp;&amp; DeoptC1 &amp;&amp; klass-&gt;is_loaded() &amp;&amp; target-&gt;is_loaded() &amp;&amp;
1802       !(// %%% FIXME: Are both of these relevant?
1803         target-&gt;is_method_handle_intrinsic() ||
1804         target-&gt;is_compiled_lambda_form()) &amp;&amp;
1805       !patch_for_appendix) {
1806     Value receiver = NULL;
1807     ciInstanceKlass* receiver_klass = NULL;
1808     bool type_is_exact = false;
1809     // try to find a precise receiver type
1810     if (will_link &amp;&amp; !target-&gt;is_static()) {
1811       int index = state()-&gt;stack_size() - (target-&gt;arg_size_no_receiver() + 1);
1812       receiver = state()-&gt;stack_at(index);
1813       ciType* type = receiver-&gt;exact_type();
1814       if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1815           type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1816         receiver_klass = (ciInstanceKlass*) type;
1817         type_is_exact = true;
1818       }
1819       if (type == NULL) {
1820         type = receiver-&gt;declared_type();
1821         if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1822             type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1823           receiver_klass = (ciInstanceKlass*) type;
1824           if (receiver_klass-&gt;is_leaf_type() &amp;&amp; !receiver_klass-&gt;is_final()) {
1825             // Insert a dependency on this type since
1826             // find_monomorphic_target may assume it's already done.
1827             dependency_recorder()-&gt;assert_leaf_type(receiver_klass);
1828             type_is_exact = true;
1829           }
1830         }
1831       }
1832     }
1833     if (receiver_klass != NULL &amp;&amp; type_is_exact &amp;&amp;
1834         receiver_klass-&gt;is_loaded() &amp;&amp; code != Bytecodes::_invokespecial) {
1835       // If we have the exact receiver type we can bind directly to
1836       // the method to call.
1837       exact_target = target-&gt;resolve_invoke(calling_klass, receiver_klass);
1838       if (exact_target != NULL) {
1839         target = exact_target;
1840         code = Bytecodes::_invokespecial;
1841       }
1842     }
1843     if (receiver_klass != NULL &amp;&amp;
1844         receiver_klass-&gt;is_subtype_of(actual_recv) &amp;&amp;
1845         actual_recv-&gt;is_initialized()) {
1846       actual_recv = receiver_klass;
1847     }
1848 
1849     if ((code == Bytecodes::_invokevirtual &amp;&amp; callee_holder-&gt;is_initialized()) ||
1850         (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_initialized() &amp;&amp; !actual_recv-&gt;is_interface())) {
1851       // Use CHA on the receiver to select a more precise method.
1852       cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, callee_holder, actual_recv);
1853     } else if (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_loaded() &amp;&amp; receiver != NULL) {
1854       // if there is only one implementor of this interface then we
1855       // may be able bind this invoke directly to the implementing
1856       // klass but we need both a dependence on the single interface
1857       // and on the method we bind to.  Additionally since all we know
1858       // about the receiver type is the it's supposed to implement the
1859       // interface we have to insert a check that it's the class we
1860       // expect.  Interface types are not checked by the verifier so
1861       // they are roughly equivalent to Object.
1862       ciInstanceKlass* singleton = NULL;
1863       if (target-&gt;holder()-&gt;nof_implementors() == 1) {
1864         singleton = target-&gt;holder()-&gt;implementor();
1865         assert(singleton != NULL &amp;&amp; singleton != target-&gt;holder(),
1866                "just checking");
1867 
1868         assert(holder-&gt;is_interface(), "invokeinterface to non interface?");
1869         ciInstanceKlass* decl_interface = (ciInstanceKlass*)holder;
1870         // the number of implementors for decl_interface is less or
1871         // equal to the number of implementors for target-&gt;holder() so
1872         // if number of implementors of target-&gt;holder() == 1 then
1873         // number of implementors for decl_interface is 0 or 1. If
1874         // it's 0 then no class implements decl_interface and there's
1875         // no point in inlining.
1876         if (!holder-&gt;is_loaded() || decl_interface-&gt;nof_implementors() != 1 || decl_interface-&gt;has_default_methods()) {
1877           singleton = NULL;
1878         }
1879       }
1880       if (singleton) {
1881         cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, target-&gt;holder(), singleton);
1882         if (cha_monomorphic_target != NULL) {
1883           // If CHA is able to bind this invoke then update the class
1884           // to match that class, otherwise klass will refer to the
1885           // interface.
1886           klass = cha_monomorphic_target-&gt;holder();
1887           actual_recv = target-&gt;holder();
1888 
1889           // insert a check it's really the expected class.
1890           CheckCast* c = new CheckCast(klass, receiver, copy_state_for_exception());
1891           c-&gt;set_incompatible_class_change_check();
1892           c-&gt;set_direct_compare(klass-&gt;is_final());
1893           // pass the result of the checkcast so that the compiler has
1894           // more accurate type info in the inlinee
1895           better_receiver = append_split(c);
1896         }
1897       }
1898     }
1899   }
1900 
1901   if (cha_monomorphic_target != NULL) {
1902     if (cha_monomorphic_target-&gt;is_abstract()) {
1903       // Do not optimize for abstract methods
1904       cha_monomorphic_target = NULL;
1905     }
1906   }
1907 
1908   if (cha_monomorphic_target != NULL) {
1909     if (!(target-&gt;is_final_method())) {
1910       // If we inlined because CHA revealed only a single target method,
1911       // then we are dependent on that target method not getting overridden
1912       // by dynamic class loading.  Be sure to test the "static" receiver
1913       // dest_method here, as opposed to the actual receiver, which may
1914       // falsely lead us to believe that the receiver is final or private.
1915       dependency_recorder()-&gt;assert_unique_concrete_method(actual_recv, cha_monomorphic_target);
1916     }
1917     code = Bytecodes::_invokespecial;
1918   }
1919 
1920   // check if we could do inlining
1921   if (!PatchALot &amp;&amp; Inline &amp;&amp; klass-&gt;is_loaded() &amp;&amp;
1922       (klass-&gt;is_initialized() || klass-&gt;is_interface() &amp;&amp; target-&gt;holder()-&gt;is_initialized())
1923       &amp;&amp; target-&gt;is_loaded()
1924       &amp;&amp; !patch_for_appendix) {
1925     // callee is known =&gt; check if we have static binding
1926     assert(target-&gt;is_loaded(), "callee must be known");
1927     if (code == Bytecodes::_invokestatic  ||
1928         code == Bytecodes::_invokespecial ||
1929         code == Bytecodes::_invokevirtual &amp;&amp; target-&gt;is_final_method() ||
1930         code == Bytecodes::_invokedynamic) {
1931       ciMethod* inline_target = (cha_monomorphic_target != NULL) ? cha_monomorphic_target : target;
1932       // static binding =&gt; check if callee is ok
1933       bool success = try_inline(inline_target, (cha_monomorphic_target != NULL) || (exact_target != NULL), code, better_receiver);
1934 
1935       CHECK_BAILOUT();
1936       clear_inline_bailout();
1937 
1938       if (success) {
1939         // Register dependence if JVMTI has either breakpoint
1940         // setting or hotswapping of methods capabilities since they may
1941         // cause deoptimization.
1942         if (compilation()-&gt;env()-&gt;jvmti_can_hotswap_or_post_breakpoint()) {
1943           dependency_recorder()-&gt;assert_evol_method(inline_target);
1944         }
1945         return;
1946       }
1947     } else {
1948       print_inlining(target, "no static binding", /*success*/ false);
1949     }
1950   } else {
1951     print_inlining(target, "not inlineable", /*success*/ false);
1952   }
1953 
1954   // If we attempted an inline which did not succeed because of a
1955   // bailout during construction of the callee graph, the entire
1956   // compilation has to be aborted. This is fairly rare and currently
1957   // seems to only occur for jasm-generated classes which contain
1958   // jsr/ret pairs which are not associated with finally clauses and
1959   // do not have exception handlers in the containing method, and are
1960   // therefore not caught early enough to abort the inlining without
1961   // corrupting the graph. (We currently bail out with a non-empty
1962   // stack at a ret in these situations.)
1963   CHECK_BAILOUT();
1964 
1965   // inlining not successful =&gt; standard invoke
1966   bool is_loaded = target-&gt;is_loaded();
1967   ValueType* result_type = as_ValueType(declared_signature-&gt;return_type());
1968   ValueStack* state_before = copy_state_exhandling();
1969 
1970   // The bytecode (code) might change in this method so we are checking this very late.
1971   const bool has_receiver =
1972     code == Bytecodes::_invokespecial   ||
1973     code == Bytecodes::_invokevirtual   ||
1974     code == Bytecodes::_invokeinterface;
1975   Values* args = state()-&gt;pop_arguments(target-&gt;arg_size_no_receiver() + patching_appendix_arg);
1976   Value recv = has_receiver ? apop() : NULL;
1977   int vtable_index = Method::invalid_vtable_index;
1978 
1979 #ifdef SPARC
1980   // Currently only supported on Sparc.
1981   // The UseInlineCaches only controls dispatch to invokevirtuals for
1982   // loaded classes which we weren't able to statically bind.
1983   if (!UseInlineCaches &amp;&amp; is_loaded &amp;&amp; code == Bytecodes::_invokevirtual
1984       &amp;&amp; !target-&gt;can_be_statically_bound()) {
1985     // Find a vtable index if one is available
1986     vtable_index = target-&gt;resolve_vtable_index(calling_klass, callee_holder);
1987   }
1988 #endif
1989 
1990   if (recv != NULL &amp;&amp;
1991       (code == Bytecodes::_invokespecial ||
1992        !is_loaded || target-&gt;is_final())) {
1993     // invokespecial always needs a NULL check.  invokevirtual where
1994     // the target is final or where it's not known that whether the
1995     // target is final requires a NULL check.  Otherwise normal
1996     // invokevirtual will perform the null check during the lookup
1997     // logic or the unverified entry point.  Profiling of calls
1998     // requires that the null check is performed in all cases.
1999     null_check(recv);
2000   }
2001 
2002   if (is_profiling()) {
2003     if (recv != NULL &amp;&amp; profile_calls()) {
2004       null_check(recv);
2005     }
2006     // Note that we'd collect profile data in this method if we wanted it.
2007     compilation()-&gt;set_would_profile(true);
2008 
2009     if (profile_calls()) {
2010       assert(cha_monomorphic_target == NULL || exact_target == NULL, "both can not be set");
2011       ciKlass* target_klass = NULL;
2012       if (cha_monomorphic_target != NULL) {
2013         target_klass = cha_monomorphic_target-&gt;holder();
2014       } else if (exact_target != NULL) {
2015         target_klass = exact_target-&gt;holder();
2016       }
2017       profile_call(target, recv, target_klass, collect_args_for_profiling(args, NULL, false), false);
2018     }
2019   }
2020 
2021   Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);
2022   // push result
2023   append_split(result);
2024 
2025   if (result_type != voidType) {
2026     if (method()-&gt;is_strict()) {
2027       push(result_type, round_fp(result));
2028     } else {
2029       push(result_type, result);
2030     }
2031   }
2032   if (profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
2033     profile_return_type(result, target);
2034   }
2035 }
2036 
2037 
2038 void GraphBuilder::new_instance(int klass_index) {
2039   ValueStack* state_before = copy_state_exhandling();
2040   bool will_link;
2041   ciKlass* klass = stream()-&gt;get_klass(will_link);
2042   assert(klass-&gt;is_instance_klass(), "must be an instance klass");
2043   NewInstance* new_instance = new NewInstance(klass-&gt;as_instance_klass(), state_before);
2044   _memory-&gt;new_instance(new_instance);
2045   apush(append_split(new_instance));
2046 }
2047 
2048 
2049 void GraphBuilder::new_type_array() {
2050   ValueStack* state_before = copy_state_exhandling();
2051   apush(append_split(new NewTypeArray(ipop(), (BasicType)stream()-&gt;get_index(), state_before)));
2052 }
2053 
2054 
2055 void GraphBuilder::new_object_array() {
2056   bool will_link;
2057   ciKlass* klass = stream()-&gt;get_klass(will_link);
2058   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2059   NewArray* n = new NewObjectArray(klass, ipop(), state_before);
2060   apush(append_split(n));
2061 }
2062 
2063 
2064 bool GraphBuilder::direct_compare(ciKlass* k) {
2065   if (k-&gt;is_loaded() &amp;&amp; k-&gt;is_instance_klass() &amp;&amp; !UseSlowPath) {
2066     ciInstanceKlass* ik = k-&gt;as_instance_klass();
2067     if (ik-&gt;is_final()) {
2068       return true;
2069     } else {
2070       if (DeoptC1 &amp;&amp; UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
2071         // test class is leaf class
2072         dependency_recorder()-&gt;assert_leaf_type(ik);
2073         return true;
2074       }
2075     }
2076   }
2077   return false;
2078 }
2079 
2080 
2081 void GraphBuilder::check_cast(int klass_index) {
2082   bool will_link;
2083   ciKlass* klass = stream()-&gt;get_klass(will_link);
2084   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
2085   CheckCast* c = new CheckCast(klass, apop(), state_before);
2086   apush(append_split(c));
2087   c-&gt;set_direct_compare(direct_compare(klass));
2088 
2089   if (is_profiling()) {
2090     // Note that we'd collect profile data in this method if we wanted it.
2091     compilation()-&gt;set_would_profile(true);
2092 
2093     if (profile_checkcasts()) {
2094       c-&gt;set_profiled_method(method());
2095       c-&gt;set_profiled_bci(bci());
2096       c-&gt;set_should_profile(true);
2097     }
2098   }
2099 }
2100 
2101 
2102 void GraphBuilder::instance_of(int klass_index) {
2103   bool will_link;
2104   ciKlass* klass = stream()-&gt;get_klass(will_link);
2105   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2106   InstanceOf* i = new InstanceOf(klass, apop(), state_before);
2107   ipush(append_split(i));
2108   i-&gt;set_direct_compare(direct_compare(klass));
2109 
2110   if (is_profiling()) {
2111     // Note that we'd collect profile data in this method if we wanted it.
2112     compilation()-&gt;set_would_profile(true);
2113 
2114     if (profile_checkcasts()) {
2115       i-&gt;set_profiled_method(method());
2116       i-&gt;set_profiled_bci(bci());
2117       i-&gt;set_should_profile(true);
2118     }
2119   }
2120 }
2121 
2122 
2123 void GraphBuilder::monitorenter(Value x, int bci) {
2124   // save state before locking in case of deoptimization after a NullPointerException
2125   ValueStack* state_before = copy_state_for_exception_with_bci(bci);
2126   append_with_bci(new MonitorEnter(x, state()-&gt;lock(x), state_before), bci);
2127   kill_all();
2128 }
2129 
2130 
2131 void GraphBuilder::monitorexit(Value x, int bci) {
2132   append_with_bci(new MonitorExit(x, state()-&gt;unlock()), bci);
2133   kill_all();
2134 }
2135 
2136 
2137 void GraphBuilder::new_multi_array(int dimensions) {
2138   bool will_link;
2139   ciKlass* klass = stream()-&gt;get_klass(will_link);
2140   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2141 
2142   Values* dims = new Values(dimensions, NULL);
2143   // fill in all dimensions
2144   int i = dimensions;
2145   while (i-- &gt; 0) dims-&gt;at_put(i, ipop());
2146   // create array
2147   NewArray* n = new NewMultiArray(klass, dims, state_before);
2148   apush(append_split(n));
2149 }
2150 
2151 
2152 void GraphBuilder::throw_op(int bci) {
2153   // We require that the debug info for a Throw be the "state before"
2154   // the Throw (i.e., exception oop is still on TOS)
2155   ValueStack* state_before = copy_state_before_with_bci(bci);
2156   Throw* t = new Throw(apop(), state_before);
2157   // operand stack not needed after a throw
2158   state()-&gt;truncate_stack(0);
2159   append_with_bci(t, bci);
2160 }
2161 
2162 
2163 Value GraphBuilder::round_fp(Value fp_value) {
2164   // no rounding needed if SSE2 is used
2165   if (RoundFPResults &amp;&amp; UseSSE &lt; 2) {
2166     // Must currently insert rounding node for doubleword values that
2167     // are results of expressions (i.e., not loads from memory or
2168     // constants)
2169     if (fp_value-&gt;type()-&gt;tag() == doubleTag &amp;&amp;
2170         fp_value-&gt;as_Constant() == NULL &amp;&amp;
2171         fp_value-&gt;as_Local() == NULL &amp;&amp;       // method parameters need no rounding
2172         fp_value-&gt;as_RoundFP() == NULL) {
2173       return append(new RoundFP(fp_value));
2174     }
2175   }
2176   return fp_value;
2177 }
2178 
2179 
2180 Instruction* GraphBuilder::append_with_bci(Instruction* instr, int bci) {
2181   Canonicalizer canon(compilation(), instr, bci);
2182   Instruction* i1 = canon.canonical();
2183   if (i1-&gt;is_linked() || !i1-&gt;can_be_linked()) {
2184     // Canonicalizer returned an instruction which was already
2185     // appended so simply return it.
2186     return i1;
2187   }
2188 
2189   if (UseLocalValueNumbering) {
2190     // Lookup the instruction in the ValueMap and add it to the map if
2191     // it's not found.
2192     Instruction* i2 = vmap()-&gt;find_insert(i1);
2193     if (i2 != i1) {
2194       // found an entry in the value map, so just return it.
2195       assert(i2-&gt;is_linked(), "should already be linked");
2196       return i2;
2197     }
2198     ValueNumberingEffects vne(vmap());
2199     i1-&gt;visit(&amp;vne);
2200   }
2201 
2202   // i1 was not eliminated =&gt; append it
2203   assert(i1-&gt;next() == NULL, "shouldn't already be linked");
2204   _last = _last-&gt;set_next(i1, canon.bci());
2205 
2206   if (++_instruction_count &gt;= InstructionCountCutoff &amp;&amp; !bailed_out()) {
2207     // set the bailout state but complete normal processing.  We
2208     // might do a little more work before noticing the bailout so we
2209     // want processing to continue normally until it's noticed.
2210     bailout("Method and/or inlining is too large");
2211   }
2212 
2213 #ifndef PRODUCT
2214   if (PrintIRDuringConstruction) {
2215     InstructionPrinter ip;
2216     ip.print_line(i1);
2217     if (Verbose) {
2218       state()-&gt;print();
2219     }
2220   }
2221 #endif
2222 
2223   // save state after modification of operand stack for StateSplit instructions
2224   StateSplit* s = i1-&gt;as_StateSplit();
2225   if (s != NULL) {
2226     if (EliminateFieldAccess) {
2227       Intrinsic* intrinsic = s-&gt;as_Intrinsic();
2228       if (s-&gt;as_Invoke() != NULL || (intrinsic &amp;&amp; !intrinsic-&gt;preserves_state())) {
2229         _memory-&gt;kill();
2230       }
2231     }
2232     s-&gt;set_state(state()-&gt;copy(ValueStack::StateAfter, canon.bci()));
2233   }
2234 
2235   // set up exception handlers for this instruction if necessary
2236   if (i1-&gt;can_trap()) {
2237     i1-&gt;set_exception_handlers(handle_exception(i1));
2238     assert(i1-&gt;exception_state() != NULL || !i1-&gt;needs_exception_state() || bailed_out(), "handle_exception must set exception state");
2239   }
2240   return i1;
2241 }
2242 
2243 
2244 Instruction* GraphBuilder::append(Instruction* instr) {
2245   assert(instr-&gt;as_StateSplit() == NULL || instr-&gt;as_BlockEnd() != NULL, "wrong append used");
2246   return append_with_bci(instr, bci());
2247 }
2248 
2249 
2250 Instruction* GraphBuilder::append_split(StateSplit* instr) {
2251   return append_with_bci(instr, bci());
2252 }
2253 
2254 
2255 void GraphBuilder::null_check(Value value) {
2256   if (value-&gt;as_NewArray() != NULL || value-&gt;as_NewInstance() != NULL) {
2257     return;
2258   } else {
2259     Constant* con = value-&gt;as_Constant();
2260     if (con) {
2261       ObjectType* c = con-&gt;type()-&gt;as_ObjectType();
2262       if (c &amp;&amp; c-&gt;is_loaded()) {
2263         ObjectConstant* oc = c-&gt;as_ObjectConstant();
2264         if (!oc || !oc-&gt;value()-&gt;is_null_object()) {
2265           return;
2266         }
2267       }
2268     }
2269   }
2270   append(new NullCheck(value, copy_state_for_exception()));
2271 }
2272 
2273 
2274 
2275 XHandlers* GraphBuilder::handle_exception(Instruction* instruction) {
2276   if (!has_handler() &amp;&amp; (!instruction-&gt;needs_exception_state() || instruction-&gt;exception_state() != NULL)) {
2277     assert(instruction-&gt;exception_state() == NULL
2278            || instruction-&gt;exception_state()-&gt;kind() == ValueStack::EmptyExceptionState
2279            || (instruction-&gt;exception_state()-&gt;kind() == ValueStack::ExceptionState &amp;&amp; _compilation-&gt;env()-&gt;jvmti_can_access_local_variables()),
2280            "exception_state should be of exception kind");
2281     return new XHandlers();
2282   }
2283 
2284   XHandlers*  exception_handlers = new XHandlers();
2285   ScopeData*  cur_scope_data = scope_data();
2286   ValueStack* cur_state = instruction-&gt;state_before();
2287   ValueStack* prev_state = NULL;
2288   int scope_count = 0;
2289 
2290   assert(cur_state != NULL, "state_before must be set");
2291   do {
2292     int cur_bci = cur_state-&gt;bci();
2293     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2294     assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data-&gt;stream()-&gt;cur_bci(), "invalid bci");
2295 
2296     // join with all potential exception handlers
2297     XHandlers* list = cur_scope_data-&gt;xhandlers();
2298     const int n = list-&gt;length();
2299     for (int i = 0; i &lt; n; i++) {
2300       XHandler* h = list-&gt;handler_at(i);
2301       if (h-&gt;covers(cur_bci)) {
2302         // h is a potential exception handler =&gt; join it
2303         compilation()-&gt;set_has_exception_handlers(true);
2304 
2305         BlockBegin* entry = h-&gt;entry_block();
2306         if (entry == block()) {
2307           // It's acceptable for an exception handler to cover itself
2308           // but we don't handle that in the parser currently.  It's
2309           // very rare so we bailout instead of trying to handle it.
2310           BAILOUT_("exception handler covers itself", exception_handlers);
2311         }
2312         assert(entry-&gt;bci() == h-&gt;handler_bci(), "must match");
2313         assert(entry-&gt;bci() == -1 || entry == cur_scope_data-&gt;block_at(entry-&gt;bci()), "blocks must correspond");
2314 
2315         // previously this was a BAILOUT, but this is not necessary
2316         // now because asynchronous exceptions are not handled this way.
2317         assert(entry-&gt;state() == NULL || cur_state-&gt;total_locks_size() == entry-&gt;state()-&gt;total_locks_size(), "locks do not match");
2318 
2319         // xhandler start with an empty expression stack
2320         if (cur_state-&gt;stack_size() != 0) {
2321           cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2322         }
2323         if (instruction-&gt;exception_state() == NULL) {
2324           instruction-&gt;set_exception_state(cur_state);
2325         }
2326 
2327         // Note: Usually this join must work. However, very
2328         // complicated jsr-ret structures where we don't ret from
2329         // the subroutine can cause the objects on the monitor
2330         // stacks to not match because blocks can be parsed twice.
2331         // The only test case we've seen so far which exhibits this
2332         // problem is caught by the infinite recursion test in
2333         // GraphBuilder::jsr() if the join doesn't work.
2334         if (!entry-&gt;try_merge(cur_state)) {
2335           BAILOUT_("error while joining with exception handler, prob. due to complicated jsr/rets", exception_handlers);
2336         }
2337 
2338         // add current state for correct handling of phi functions at begin of xhandler
2339         int phi_operand = entry-&gt;add_exception_state(cur_state);
2340 
2341         // add entry to the list of xhandlers of this block
2342         _block-&gt;add_exception_handler(entry);
2343 
2344         // add back-edge from xhandler entry to this block
2345         if (!entry-&gt;is_predecessor(_block)) {
2346           entry-&gt;add_predecessor(_block);
2347         }
2348 
2349         // clone XHandler because phi_operand and scope_count can not be shared
2350         XHandler* new_xhandler = new XHandler(h);
2351         new_xhandler-&gt;set_phi_operand(phi_operand);
2352         new_xhandler-&gt;set_scope_count(scope_count);
2353         exception_handlers-&gt;append(new_xhandler);
2354 
2355         // fill in exception handler subgraph lazily
2356         assert(!entry-&gt;is_set(BlockBegin::was_visited_flag), "entry must not be visited yet");
2357         cur_scope_data-&gt;add_to_work_list(entry);
2358 
2359         // stop when reaching catchall
2360         if (h-&gt;catch_type() == 0) {
2361           return exception_handlers;
2362         }
2363       }
2364     }
2365 
2366     if (exception_handlers-&gt;length() == 0) {
2367       // This scope and all callees do not handle exceptions, so the local
2368       // variables of this scope are not needed. However, the scope itself is
2369       // required for a correct exception stack trace -&gt; clear out the locals.
2370       if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
2371         cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2372       } else {
2373         cur_state = cur_state-&gt;copy(ValueStack::EmptyExceptionState, cur_state-&gt;bci());
2374       }
2375       if (prev_state != NULL) {
2376         prev_state-&gt;set_caller_state(cur_state);
2377       }
2378       if (instruction-&gt;exception_state() == NULL) {
2379         instruction-&gt;set_exception_state(cur_state);
2380       }
2381     }
2382 
2383     // Set up iteration for next time.
2384     // If parsing a jsr, do not grab exception handlers from the
2385     // parent scopes for this method (already got them, and they
2386     // needed to be cloned)
2387 
2388     while (cur_scope_data-&gt;parsing_jsr()) {
2389       cur_scope_data = cur_scope_data-&gt;parent();
2390     }
2391 
2392     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2393     assert(cur_state-&gt;locks_size() == 0 || cur_state-&gt;locks_size() == 1, "unlocking must be done in a catchall exception handler");
2394 
2395     prev_state = cur_state;
2396     cur_state = cur_state-&gt;caller_state();
2397     cur_scope_data = cur_scope_data-&gt;parent();
2398     scope_count++;
2399   } while (cur_scope_data != NULL);
2400 
2401   return exception_handlers;
2402 }
2403 
2404 
2405 // Helper class for simplifying Phis.
2406 class PhiSimplifier : public BlockClosure {
2407  private:
2408   bool _has_substitutions;
2409   Value simplify(Value v);
2410 
2411  public:
2412   PhiSimplifier(BlockBegin* start) : _has_substitutions(false) {
2413     start-&gt;iterate_preorder(this);
2414     if (_has_substitutions) {
2415       SubstitutionResolver sr(start);
2416     }
2417   }
2418   void block_do(BlockBegin* b);
2419   bool has_substitutions() const { return _has_substitutions; }
2420 };
2421 
2422 
2423 Value PhiSimplifier::simplify(Value v) {
2424   Phi* phi = v-&gt;as_Phi();
2425 
2426   if (phi == NULL) {
2427     // no phi function
2428     return v;
2429   } else if (v-&gt;has_subst()) {
2430     // already substituted; subst can be phi itself -&gt; simplify
2431     return simplify(v-&gt;subst());
2432   } else if (phi-&gt;is_set(Phi::cannot_simplify)) {
2433     // already tried to simplify phi before
2434     return phi;
2435   } else if (phi-&gt;is_set(Phi::visited)) {
2436     // break cycles in phi functions
2437     return phi;
2438   } else if (phi-&gt;type()-&gt;is_illegal()) {
2439     // illegal phi functions are ignored anyway
2440     return phi;
2441 
2442   } else {
2443     // mark phi function as processed to break cycles in phi functions
2444     phi-&gt;set(Phi::visited);
2445 
2446     // simplify x = [y, x] and x = [y, y] to y
2447     Value subst = NULL;
2448     int opd_count = phi-&gt;operand_count();
2449     for (int i = 0; i &lt; opd_count; i++) {
2450       Value opd = phi-&gt;operand_at(i);
2451       assert(opd != NULL, "Operand must exist!");
2452 
2453       if (opd-&gt;type()-&gt;is_illegal()) {
2454         // if one operand is illegal, the entire phi function is illegal
2455         phi-&gt;make_illegal();
2456         phi-&gt;clear(Phi::visited);
2457         return phi;
2458       }
2459 
2460       Value new_opd = simplify(opd);
2461       assert(new_opd != NULL, "Simplified operand must exist!");
2462 
2463       if (new_opd != phi &amp;&amp; new_opd != subst) {
2464         if (subst == NULL) {
2465           subst = new_opd;
2466         } else {
2467           // no simplification possible
2468           phi-&gt;set(Phi::cannot_simplify);
2469           phi-&gt;clear(Phi::visited);
2470           return phi;
2471         }
2472       }
2473     }
2474 
2475     // sucessfully simplified phi function
2476     assert(subst != NULL, "illegal phi function");
2477     _has_substitutions = true;
2478     phi-&gt;clear(Phi::visited);
2479     phi-&gt;set_subst(subst);
2480 
2481 #ifndef PRODUCT
2482     if (PrintPhiFunctions) {
2483       tty-&gt;print_cr("simplified phi function %c%d to %c%d (Block B%d)", phi-&gt;type()-&gt;tchar(), phi-&gt;id(), subst-&gt;type()-&gt;tchar(), subst-&gt;id(), phi-&gt;block()-&gt;block_id());
2484     }
2485 #endif
2486 
2487     return subst;
2488   }
2489 }
2490 
2491 
2492 void PhiSimplifier::block_do(BlockBegin* b) {
2493   for_each_phi_fun(b, phi,
2494     simplify(phi);
2495   );
2496 
2497 #ifdef ASSERT
2498   for_each_phi_fun(b, phi,
2499                    assert(phi-&gt;operand_count() != 1 || phi-&gt;subst() != phi, "missed trivial simplification");
2500   );
2501 
2502   ValueStack* state = b-&gt;state()-&gt;caller_state();
2503   for_each_state_value(state, value,
2504     Phi* phi = value-&gt;as_Phi();
2505     assert(phi == NULL || phi-&gt;block() != b, "must not have phi function to simplify in caller state");
2506   );
2507 #endif
2508 }
2509 
2510 // This method is called after all blocks are filled with HIR instructions
2511 // It eliminates all Phi functions of the form x = [y, y] and x = [y, x]
2512 void GraphBuilder::eliminate_redundant_phis(BlockBegin* start) {
2513   PhiSimplifier simplifier(start);
2514 }
2515 
2516 
2517 void GraphBuilder::connect_to_end(BlockBegin* beg) {
2518   // setup iteration
2519   kill_all();
2520   _block = beg;
2521   _state = beg-&gt;state()-&gt;copy_for_parsing();
2522   _last  = beg;
2523   iterate_bytecodes_for_block(beg-&gt;bci());
2524 }
2525 
2526 
2527 BlockEnd* GraphBuilder::iterate_bytecodes_for_block(int bci) {
2528 #ifndef PRODUCT
2529   if (PrintIRDuringConstruction) {
2530     tty-&gt;cr();
2531     InstructionPrinter ip;
2532     ip.print_instr(_block); tty-&gt;cr();
2533     ip.print_stack(_block-&gt;state()); tty-&gt;cr();
2534     ip.print_inline_level(_block);
2535     ip.print_head();
2536     tty-&gt;print_cr("locals size: %d stack size: %d", state()-&gt;locals_size(), state()-&gt;stack_size());
2537   }
2538 #endif
2539   _skip_block = false;
2540   assert(state() != NULL, "ValueStack missing!");
2541   CompileLog* log = compilation()-&gt;log();
2542   ciBytecodeStream s(method());
2543   s.reset_to_bci(bci);
2544   int prev_bci = bci;
2545   scope_data()-&gt;set_stream(&amp;s);
2546   // iterate
2547   Bytecodes::Code code = Bytecodes::_illegal;
2548   bool push_exception = false;
2549 
2550   if (block()-&gt;is_set(BlockBegin::exception_entry_flag) &amp;&amp; block()-&gt;next() == NULL) {
2551     // first thing in the exception entry block should be the exception object.
2552     push_exception = true;
2553   }
2554 
2555   while (!bailed_out() &amp;&amp; last()-&gt;as_BlockEnd() == NULL &amp;&amp;
2556          (code = stream()-&gt;next()) != ciBytecodeStream::EOBC() &amp;&amp;
2557          (block_at(s.cur_bci()) == NULL || block_at(s.cur_bci()) == block())) {
2558     assert(state()-&gt;kind() == ValueStack::Parsing, "invalid state kind");
2559 
2560     if (log != NULL)
2561       log-&gt;set_context("bc code='%d' bci='%d'", (int)code, s.cur_bci());
2562 
2563     // Check for active jsr during OSR compilation
2564     if (compilation()-&gt;is_osr_compile()
2565         &amp;&amp; scope()-&gt;is_top_scope()
2566         &amp;&amp; parsing_jsr()
2567         &amp;&amp; s.cur_bci() == compilation()-&gt;osr_bci()) {
2568       bailout("OSR not supported while a jsr is active");
2569     }
2570 
2571     if (push_exception) {
2572       apush(append(new ExceptionObject()));
2573       push_exception = false;
2574     }
2575 
2576     // handle bytecode
2577     switch (code) {
2578       case Bytecodes::_nop            : /* nothing to do */ break;
2579       case Bytecodes::_aconst_null    : apush(append(new Constant(objectNull            ))); break;
2580       case Bytecodes::_iconst_m1      : ipush(append(new Constant(new IntConstant   (-1)))); break;
2581       case Bytecodes::_iconst_0       : ipush(append(new Constant(intZero               ))); break;
2582       case Bytecodes::_iconst_1       : ipush(append(new Constant(intOne                ))); break;
2583       case Bytecodes::_iconst_2       : ipush(append(new Constant(new IntConstant   ( 2)))); break;
2584       case Bytecodes::_iconst_3       : ipush(append(new Constant(new IntConstant   ( 3)))); break;
2585       case Bytecodes::_iconst_4       : ipush(append(new Constant(new IntConstant   ( 4)))); break;
2586       case Bytecodes::_iconst_5       : ipush(append(new Constant(new IntConstant   ( 5)))); break;
2587       case Bytecodes::_lconst_0       : lpush(append(new Constant(new LongConstant  ( 0)))); break;
2588       case Bytecodes::_lconst_1       : lpush(append(new Constant(new LongConstant  ( 1)))); break;
2589       case Bytecodes::_fconst_0       : fpush(append(new Constant(new FloatConstant ( 0)))); break;
2590       case Bytecodes::_fconst_1       : fpush(append(new Constant(new FloatConstant ( 1)))); break;
2591       case Bytecodes::_fconst_2       : fpush(append(new Constant(new FloatConstant ( 2)))); break;
2592       case Bytecodes::_dconst_0       : dpush(append(new Constant(new DoubleConstant( 0)))); break;
2593       case Bytecodes::_dconst_1       : dpush(append(new Constant(new DoubleConstant( 1)))); break;
2594       case Bytecodes::_bipush         : ipush(append(new Constant(new IntConstant(((signed char*)s.cur_bcp())[1])))); break;
2595       case Bytecodes::_sipush         : ipush(append(new Constant(new IntConstant((short)Bytes::get_Java_u2(s.cur_bcp()+1))))); break;
2596       case Bytecodes::_ldc            : // fall through
2597       case Bytecodes::_ldc_w          : // fall through
2598       case Bytecodes::_ldc2_w         : load_constant(); break;
2599       case Bytecodes::_iload          : load_local(intType     , s.get_index()); break;
2600       case Bytecodes::_lload          : load_local(longType    , s.get_index()); break;
2601       case Bytecodes::_fload          : load_local(floatType   , s.get_index()); break;
2602       case Bytecodes::_dload          : load_local(doubleType  , s.get_index()); break;
2603       case Bytecodes::_aload          : load_local(instanceType, s.get_index()); break;
2604       case Bytecodes::_iload_0        : load_local(intType   , 0); break;
2605       case Bytecodes::_iload_1        : load_local(intType   , 1); break;
2606       case Bytecodes::_iload_2        : load_local(intType   , 2); break;
2607       case Bytecodes::_iload_3        : load_local(intType   , 3); break;
2608       case Bytecodes::_lload_0        : load_local(longType  , 0); break;
2609       case Bytecodes::_lload_1        : load_local(longType  , 1); break;
2610       case Bytecodes::_lload_2        : load_local(longType  , 2); break;
2611       case Bytecodes::_lload_3        : load_local(longType  , 3); break;
2612       case Bytecodes::_fload_0        : load_local(floatType , 0); break;
2613       case Bytecodes::_fload_1        : load_local(floatType , 1); break;
2614       case Bytecodes::_fload_2        : load_local(floatType , 2); break;
2615       case Bytecodes::_fload_3        : load_local(floatType , 3); break;
2616       case Bytecodes::_dload_0        : load_local(doubleType, 0); break;
2617       case Bytecodes::_dload_1        : load_local(doubleType, 1); break;
2618       case Bytecodes::_dload_2        : load_local(doubleType, 2); break;
2619       case Bytecodes::_dload_3        : load_local(doubleType, 3); break;
2620       case Bytecodes::_aload_0        : load_local(objectType, 0); break;
2621       case Bytecodes::_aload_1        : load_local(objectType, 1); break;
2622       case Bytecodes::_aload_2        : load_local(objectType, 2); break;
2623       case Bytecodes::_aload_3        : load_local(objectType, 3); break;
2624       case Bytecodes::_iaload         : load_indexed(T_INT   ); break;
2625       case Bytecodes::_laload         : load_indexed(T_LONG  ); break;
2626       case Bytecodes::_faload         : load_indexed(T_FLOAT ); break;
2627       case Bytecodes::_daload         : load_indexed(T_DOUBLE); break;
2628       case Bytecodes::_aaload         : load_indexed(T_OBJECT); break;
2629       case Bytecodes::_baload         : load_indexed(T_BYTE  ); break;
2630       case Bytecodes::_caload         : load_indexed(T_CHAR  ); break;
2631       case Bytecodes::_saload         : load_indexed(T_SHORT ); break;
2632       case Bytecodes::_istore         : store_local(intType   , s.get_index()); break;
2633       case Bytecodes::_lstore         : store_local(longType  , s.get_index()); break;
2634       case Bytecodes::_fstore         : store_local(floatType , s.get_index()); break;
2635       case Bytecodes::_dstore         : store_local(doubleType, s.get_index()); break;
2636       case Bytecodes::_astore         : store_local(objectType, s.get_index()); break;
2637       case Bytecodes::_istore_0       : store_local(intType   , 0); break;
2638       case Bytecodes::_istore_1       : store_local(intType   , 1); break;
2639       case Bytecodes::_istore_2       : store_local(intType   , 2); break;
2640       case Bytecodes::_istore_3       : store_local(intType   , 3); break;
2641       case Bytecodes::_lstore_0       : store_local(longType  , 0); break;
2642       case Bytecodes::_lstore_1       : store_local(longType  , 1); break;
2643       case Bytecodes::_lstore_2       : store_local(longType  , 2); break;
2644       case Bytecodes::_lstore_3       : store_local(longType  , 3); break;
2645       case Bytecodes::_fstore_0       : store_local(floatType , 0); break;
2646       case Bytecodes::_fstore_1       : store_local(floatType , 1); break;
2647       case Bytecodes::_fstore_2       : store_local(floatType , 2); break;
2648       case Bytecodes::_fstore_3       : store_local(floatType , 3); break;
2649       case Bytecodes::_dstore_0       : store_local(doubleType, 0); break;
2650       case Bytecodes::_dstore_1       : store_local(doubleType, 1); break;
2651       case Bytecodes::_dstore_2       : store_local(doubleType, 2); break;
2652       case Bytecodes::_dstore_3       : store_local(doubleType, 3); break;
2653       case Bytecodes::_astore_0       : store_local(objectType, 0); break;
2654       case Bytecodes::_astore_1       : store_local(objectType, 1); break;
2655       case Bytecodes::_astore_2       : store_local(objectType, 2); break;
2656       case Bytecodes::_astore_3       : store_local(objectType, 3); break;
2657       case Bytecodes::_iastore        : store_indexed(T_INT   ); break;
2658       case Bytecodes::_lastore        : store_indexed(T_LONG  ); break;
2659       case Bytecodes::_fastore        : store_indexed(T_FLOAT ); break;
2660       case Bytecodes::_dastore        : store_indexed(T_DOUBLE); break;
2661       case Bytecodes::_aastore        : store_indexed(T_OBJECT); break;
2662       case Bytecodes::_bastore        : store_indexed(T_BYTE  ); break;
2663       case Bytecodes::_castore        : store_indexed(T_CHAR  ); break;
2664       case Bytecodes::_sastore        : store_indexed(T_SHORT ); break;
2665       case Bytecodes::_pop            : // fall through
2666       case Bytecodes::_pop2           : // fall through
2667       case Bytecodes::_dup            : // fall through
2668       case Bytecodes::_dup_x1         : // fall through
2669       case Bytecodes::_dup_x2         : // fall through
2670       case Bytecodes::_dup2           : // fall through
2671       case Bytecodes::_dup2_x1        : // fall through
2672       case Bytecodes::_dup2_x2        : // fall through
2673       case Bytecodes::_swap           : stack_op(code); break;
2674       case Bytecodes::_iadd           : arithmetic_op(intType   , code); break;
2675       case Bytecodes::_ladd           : arithmetic_op(longType  , code); break;
2676       case Bytecodes::_fadd           : arithmetic_op(floatType , code); break;
2677       case Bytecodes::_dadd           : arithmetic_op(doubleType, code); break;
2678       case Bytecodes::_isub           : arithmetic_op(intType   , code); break;
2679       case Bytecodes::_lsub           : arithmetic_op(longType  , code); break;
2680       case Bytecodes::_fsub           : arithmetic_op(floatType , code); break;
2681       case Bytecodes::_dsub           : arithmetic_op(doubleType, code); break;
2682       case Bytecodes::_imul           : arithmetic_op(intType   , code); break;
2683       case Bytecodes::_lmul           : arithmetic_op(longType  , code); break;
2684       case Bytecodes::_fmul           : arithmetic_op(floatType , code); break;
2685       case Bytecodes::_dmul           : arithmetic_op(doubleType, code); break;
2686       case Bytecodes::_idiv           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2687       case Bytecodes::_ldiv           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2688       case Bytecodes::_fdiv           : arithmetic_op(floatType , code); break;
2689       case Bytecodes::_ddiv           : arithmetic_op(doubleType, code); break;
2690       case Bytecodes::_irem           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2691       case Bytecodes::_lrem           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2692       case Bytecodes::_frem           : arithmetic_op(floatType , code); break;
2693       case Bytecodes::_drem           : arithmetic_op(doubleType, code); break;
2694       case Bytecodes::_ineg           : negate_op(intType   ); break;
2695       case Bytecodes::_lneg           : negate_op(longType  ); break;
2696       case Bytecodes::_fneg           : negate_op(floatType ); break;
2697       case Bytecodes::_dneg           : negate_op(doubleType); break;
2698       case Bytecodes::_ishl           : shift_op(intType , code); break;
2699       case Bytecodes::_lshl           : shift_op(longType, code); break;
2700       case Bytecodes::_ishr           : shift_op(intType , code); break;
2701       case Bytecodes::_lshr           : shift_op(longType, code); break;
2702       case Bytecodes::_iushr          : shift_op(intType , code); break;
2703       case Bytecodes::_lushr          : shift_op(longType, code); break;
2704       case Bytecodes::_iand           : logic_op(intType , code); break;
2705       case Bytecodes::_land           : logic_op(longType, code); break;
2706       case Bytecodes::_ior            : logic_op(intType , code); break;
2707       case Bytecodes::_lor            : logic_op(longType, code); break;
2708       case Bytecodes::_ixor           : logic_op(intType , code); break;
2709       case Bytecodes::_lxor           : logic_op(longType, code); break;
2710       case Bytecodes::_iinc           : increment(); break;
2711       case Bytecodes::_i2l            : convert(code, T_INT   , T_LONG  ); break;
2712       case Bytecodes::_i2f            : convert(code, T_INT   , T_FLOAT ); break;
2713       case Bytecodes::_i2d            : convert(code, T_INT   , T_DOUBLE); break;
2714       case Bytecodes::_l2i            : convert(code, T_LONG  , T_INT   ); break;
2715       case Bytecodes::_l2f            : convert(code, T_LONG  , T_FLOAT ); break;
2716       case Bytecodes::_l2d            : convert(code, T_LONG  , T_DOUBLE); break;
2717       case Bytecodes::_f2i            : convert(code, T_FLOAT , T_INT   ); break;
2718       case Bytecodes::_f2l            : convert(code, T_FLOAT , T_LONG  ); break;
2719       case Bytecodes::_f2d            : convert(code, T_FLOAT , T_DOUBLE); break;
2720       case Bytecodes::_d2i            : convert(code, T_DOUBLE, T_INT   ); break;
2721       case Bytecodes::_d2l            : convert(code, T_DOUBLE, T_LONG  ); break;
2722       case Bytecodes::_d2f            : convert(code, T_DOUBLE, T_FLOAT ); break;
2723       case Bytecodes::_i2b            : convert(code, T_INT   , T_BYTE  ); break;
2724       case Bytecodes::_i2c            : convert(code, T_INT   , T_CHAR  ); break;
2725       case Bytecodes::_i2s            : convert(code, T_INT   , T_SHORT ); break;
2726       case Bytecodes::_lcmp           : compare_op(longType  , code); break;
2727       case Bytecodes::_fcmpl          : compare_op(floatType , code); break;
2728       case Bytecodes::_fcmpg          : compare_op(floatType , code); break;
2729       case Bytecodes::_dcmpl          : compare_op(doubleType, code); break;
2730       case Bytecodes::_dcmpg          : compare_op(doubleType, code); break;
2731       case Bytecodes::_ifeq           : if_zero(intType   , If::eql); break;
2732       case Bytecodes::_ifne           : if_zero(intType   , If::neq); break;
2733       case Bytecodes::_iflt           : if_zero(intType   , If::lss); break;
2734       case Bytecodes::_ifge           : if_zero(intType   , If::geq); break;
2735       case Bytecodes::_ifgt           : if_zero(intType   , If::gtr); break;
2736       case Bytecodes::_ifle           : if_zero(intType   , If::leq); break;
2737       case Bytecodes::_if_icmpeq      : if_same(intType   , If::eql); break;
2738       case Bytecodes::_if_icmpne      : if_same(intType   , If::neq); break;
2739       case Bytecodes::_if_icmplt      : if_same(intType   , If::lss); break;
2740       case Bytecodes::_if_icmpge      : if_same(intType   , If::geq); break;
2741       case Bytecodes::_if_icmpgt      : if_same(intType   , If::gtr); break;
2742       case Bytecodes::_if_icmple      : if_same(intType   , If::leq); break;
2743       case Bytecodes::_if_acmpeq      : if_same(objectType, If::eql); break;
2744       case Bytecodes::_if_acmpne      : if_same(objectType, If::neq); break;
2745       case Bytecodes::_goto           : _goto(s.cur_bci(), s.get_dest()); break;
2746       case Bytecodes::_jsr            : jsr(s.get_dest()); break;
2747       case Bytecodes::_ret            : ret(s.get_index()); break;
2748       case Bytecodes::_tableswitch    : table_switch(); break;
2749       case Bytecodes::_lookupswitch   : lookup_switch(); break;
2750       case Bytecodes::_ireturn        : method_return(ipop()); break;
2751       case Bytecodes::_lreturn        : method_return(lpop()); break;
2752       case Bytecodes::_freturn        : method_return(fpop()); break;
2753       case Bytecodes::_dreturn        : method_return(dpop()); break;
2754       case Bytecodes::_areturn        : method_return(apop()); break;
2755       case Bytecodes::_return         : method_return(NULL  ); break;
2756       case Bytecodes::_getstatic      : // fall through
2757       case Bytecodes::_putstatic      : // fall through
2758       case Bytecodes::_getfield       : // fall through
2759       case Bytecodes::_putfield       : access_field(code); break;
2760       case Bytecodes::_invokevirtual  : // fall through
2761       case Bytecodes::_invokespecial  : // fall through
2762       case Bytecodes::_invokestatic   : // fall through
2763       case Bytecodes::_invokedynamic  : // fall through
2764       case Bytecodes::_invokeinterface: invoke(code); break;
2765       case Bytecodes::_new            : new_instance(s.get_index_u2()); break;
2766       case Bytecodes::_newarray       : new_type_array(); break;
2767       case Bytecodes::_anewarray      : new_object_array(); break;
2768       case Bytecodes::_arraylength    : { ValueStack* state_before = copy_state_for_exception(); ipush(append(new ArrayLength(apop(), state_before))); break; }
2769       case Bytecodes::_athrow         : throw_op(s.cur_bci()); break;
2770       case Bytecodes::_checkcast      : check_cast(s.get_index_u2()); break;
2771       case Bytecodes::_instanceof     : instance_of(s.get_index_u2()); break;
2772       case Bytecodes::_monitorenter   : monitorenter(apop(), s.cur_bci()); break;
2773       case Bytecodes::_monitorexit    : monitorexit (apop(), s.cur_bci()); break;
2774       case Bytecodes::_wide           : ShouldNotReachHere(); break;
2775       case Bytecodes::_multianewarray : new_multi_array(s.cur_bcp()[3]); break;
2776       case Bytecodes::_ifnull         : if_null(objectType, If::eql); break;
2777       case Bytecodes::_ifnonnull      : if_null(objectType, If::neq); break;
2778       case Bytecodes::_goto_w         : _goto(s.cur_bci(), s.get_far_dest()); break;
2779       case Bytecodes::_jsr_w          : jsr(s.get_far_dest()); break;
2780       case Bytecodes::_breakpoint     : BAILOUT_("concurrent setting of breakpoint", NULL);
2781       default                         : ShouldNotReachHere(); break;
2782     }
2783 
2784     if (log != NULL)
2785       log-&gt;clear_context(); // skip marker if nothing was printed
2786 
2787     // save current bci to setup Goto at the end
2788     prev_bci = s.cur_bci();
2789 
2790   }
2791   CHECK_BAILOUT_(NULL);
2792   // stop processing of this block (see try_inline_full)
2793   if (_skip_block) {
2794     _skip_block = false;
2795     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
2796     return _last-&gt;as_BlockEnd();
2797   }
2798   // if there are any, check if last instruction is a BlockEnd instruction
2799   BlockEnd* end = last()-&gt;as_BlockEnd();
2800   if (end == NULL) {
2801     // all blocks must end with a BlockEnd instruction =&gt; add a Goto
2802     end = new Goto(block_at(s.cur_bci()), false);
2803     append(end);
2804   }
2805   assert(end == last()-&gt;as_BlockEnd(), "inconsistency");
2806 
2807   assert(end-&gt;state() != NULL, "state must already be present");
2808   assert(end-&gt;as_Return() == NULL || end-&gt;as_Throw() == NULL || end-&gt;state()-&gt;stack_size() == 0, "stack not needed for return and throw");
2809 
2810   // connect to begin &amp; set state
2811   // NOTE that inlining may have changed the block we are parsing
2812   block()-&gt;set_end(end);
2813   // propagate state
2814   for (int i = end-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
2815     BlockBegin* sux = end-&gt;sux_at(i);
2816     assert(sux-&gt;is_predecessor(block()), "predecessor missing");
2817     // be careful, bailout if bytecodes are strange
2818     if (!sux-&gt;try_merge(end-&gt;state())) BAILOUT_("block join failed", NULL);
2819     scope_data()-&gt;add_to_work_list(end-&gt;sux_at(i));
2820   }
2821 
2822   scope_data()-&gt;set_stream(NULL);
2823 
2824   // done
2825   return end;
2826 }
2827 
2828 
2829 void GraphBuilder::iterate_all_blocks(bool start_in_current_block_for_inlining) {
2830   do {
2831     if (start_in_current_block_for_inlining &amp;&amp; !bailed_out()) {
2832       iterate_bytecodes_for_block(0);
2833       start_in_current_block_for_inlining = false;
2834     } else {
2835       BlockBegin* b;
2836       while ((b = scope_data()-&gt;remove_from_work_list()) != NULL) {
2837         if (!b-&gt;is_set(BlockBegin::was_visited_flag)) {
2838           if (b-&gt;is_set(BlockBegin::osr_entry_flag)) {
2839             // we're about to parse the osr entry block, so make sure
2840             // we setup the OSR edge leading into this block so that
2841             // Phis get setup correctly.
2842             setup_osr_entry_block();
2843             // this is no longer the osr entry block, so clear it.
2844             b-&gt;clear(BlockBegin::osr_entry_flag);
2845           }
2846           b-&gt;set(BlockBegin::was_visited_flag);
2847           connect_to_end(b);
2848         }
2849       }
2850     }
2851   } while (!bailed_out() &amp;&amp; !scope_data()-&gt;is_work_list_empty());
2852 }
2853 
2854 
2855 bool GraphBuilder::_can_trap      [Bytecodes::number_of_java_codes];
2856 
2857 void GraphBuilder::initialize() {
2858   // the following bytecodes are assumed to potentially
2859   // throw exceptions in compiled code - note that e.g.
2860   // monitorexit &amp; the return bytecodes do not throw
2861   // exceptions since monitor pairing proved that they
2862   // succeed (if monitor pairing succeeded)
2863   Bytecodes::Code can_trap_list[] =
2864     { Bytecodes::_ldc
2865     , Bytecodes::_ldc_w
2866     , Bytecodes::_ldc2_w
2867     , Bytecodes::_iaload
2868     , Bytecodes::_laload
2869     , Bytecodes::_faload
2870     , Bytecodes::_daload
2871     , Bytecodes::_aaload
2872     , Bytecodes::_baload
2873     , Bytecodes::_caload
2874     , Bytecodes::_saload
2875     , Bytecodes::_iastore
2876     , Bytecodes::_lastore
2877     , Bytecodes::_fastore
2878     , Bytecodes::_dastore
2879     , Bytecodes::_aastore
2880     , Bytecodes::_bastore
2881     , Bytecodes::_castore
2882     , Bytecodes::_sastore
2883     , Bytecodes::_idiv
2884     , Bytecodes::_ldiv
2885     , Bytecodes::_irem
2886     , Bytecodes::_lrem
2887     , Bytecodes::_getstatic
2888     , Bytecodes::_putstatic
2889     , Bytecodes::_getfield
2890     , Bytecodes::_putfield
2891     , Bytecodes::_invokevirtual
2892     , Bytecodes::_invokespecial
2893     , Bytecodes::_invokestatic
2894     , Bytecodes::_invokedynamic
2895     , Bytecodes::_invokeinterface
2896     , Bytecodes::_new
2897     , Bytecodes::_newarray
2898     , Bytecodes::_anewarray
2899     , Bytecodes::_arraylength
2900     , Bytecodes::_athrow
2901     , Bytecodes::_checkcast
2902     , Bytecodes::_instanceof
2903     , Bytecodes::_monitorenter
2904     , Bytecodes::_multianewarray
2905     };
2906 
2907   // inititialize trap tables
2908   for (int i = 0; i &lt; Bytecodes::number_of_java_codes; i++) {
2909     _can_trap[i] = false;
2910   }
2911   // set standard trap info
2912   for (uint j = 0; j &lt; ARRAY_SIZE(can_trap_list); j++) {
2913     _can_trap[can_trap_list[j]] = true;
2914   }
2915 }
2916 
2917 
2918 BlockBegin* GraphBuilder::header_block(BlockBegin* entry, BlockBegin::Flag f, ValueStack* state) {
2919   assert(entry-&gt;is_set(f), "entry/flag mismatch");
2920   // create header block
2921   BlockBegin* h = new BlockBegin(entry-&gt;bci());
2922   h-&gt;set_depth_first_number(0);
2923 
2924   Value l = h;
2925   BlockEnd* g = new Goto(entry, false);
2926   l-&gt;set_next(g, entry-&gt;bci());
2927   h-&gt;set_end(g);
2928   h-&gt;set(f);
2929   // setup header block end state
2930   ValueStack* s = state-&gt;copy(ValueStack::StateAfter, entry-&gt;bci()); // can use copy since stack is empty (=&gt; no phis)
2931   assert(s-&gt;stack_is_empty(), "must have empty stack at entry point");
2932   g-&gt;set_state(s);
2933   return h;
2934 }
2935 
2936 
2937 
2938 BlockBegin* GraphBuilder::setup_start_block(int osr_bci, BlockBegin* std_entry, BlockBegin* osr_entry, ValueStack* state) {
2939   BlockBegin* start = new BlockBegin(0);
2940 
2941   // This code eliminates the empty start block at the beginning of
2942   // each method.  Previously, each method started with the
2943   // start-block created below, and this block was followed by the
2944   // header block that was always empty.  This header block is only
2945   // necesary if std_entry is also a backward branch target because
2946   // then phi functions may be necessary in the header block.  It's
2947   // also necessary when profiling so that there's a single block that
2948   // can increment the interpreter_invocation_count.
2949   BlockBegin* new_header_block;
2950   if (std_entry-&gt;number_of_preds() &gt; 0 || count_invocations() || count_backedges()) {
2951     new_header_block = header_block(std_entry, BlockBegin::std_entry_flag, state);
2952   } else {
2953     new_header_block = std_entry;
2954   }
2955 
2956   // setup start block (root for the IR graph)
2957   Base* base =
2958     new Base(
2959       new_header_block,
2960       osr_entry
2961     );
2962   start-&gt;set_next(base, 0);
2963   start-&gt;set_end(base);
2964   // create &amp; setup state for start block
2965   start-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2966   base-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2967 
2968   if (base-&gt;std_entry()-&gt;state() == NULL) {
2969     // setup states for header blocks
2970     base-&gt;std_entry()-&gt;merge(state);
2971   }
2972 
2973   assert(base-&gt;std_entry()-&gt;state() != NULL, "");
2974   return start;
2975 }
2976 
2977 
2978 void GraphBuilder::setup_osr_entry_block() {
2979   assert(compilation()-&gt;is_osr_compile(), "only for osrs");
2980 
2981   int osr_bci = compilation()-&gt;osr_bci();
2982   ciBytecodeStream s(method());
2983   s.reset_to_bci(osr_bci);
2984   s.next();
2985   scope_data()-&gt;set_stream(&amp;s);
2986 
2987   // create a new block to be the osr setup code
2988   _osr_entry = new BlockBegin(osr_bci);
2989   _osr_entry-&gt;set(BlockBegin::osr_entry_flag);
2990   _osr_entry-&gt;set_depth_first_number(0);
2991   BlockBegin* target = bci2block()-&gt;at(osr_bci);
2992   assert(target != NULL &amp;&amp; target-&gt;is_set(BlockBegin::osr_entry_flag), "must be there");
2993   // the osr entry has no values for locals
2994   ValueStack* state = target-&gt;state()-&gt;copy();
2995   _osr_entry-&gt;set_state(state);
2996 
2997   kill_all();
2998   _block = _osr_entry;
2999   _state = _osr_entry-&gt;state()-&gt;copy();
3000   assert(_state-&gt;bci() == osr_bci, "mismatch");
3001   _last  = _osr_entry;
3002   Value e = append(new OsrEntry());
3003   e-&gt;set_needs_null_check(false);
3004 
3005   // OSR buffer is
3006   //
3007   // locals[nlocals-1..0]
3008   // monitors[number_of_locks-1..0]
3009   //
3010   // locals is a direct copy of the interpreter frame so in the osr buffer
3011   // so first slot in the local array is the last local from the interpreter
3012   // and last slot is local[0] (receiver) from the interpreter
3013   //
3014   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
3015   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
3016   // in the interpreter frame (the method lock if a sync method)
3017 
3018   // Initialize monitors in the compiled activation.
3019 
3020   int index;
3021   Value local;
3022 
3023   // find all the locals that the interpreter thinks contain live oops
3024   const BitMap live_oops = method()-&gt;live_local_oops_at_bci(osr_bci);
3025 
3026   // compute the offset into the locals so that we can treat the buffer
3027   // as if the locals were still in the interpreter frame
3028   int locals_offset = BytesPerWord * (method()-&gt;max_locals() - 1);
3029   for_each_local_value(state, index, local) {
3030     int offset = locals_offset - (index + local-&gt;type()-&gt;size() - 1) * BytesPerWord;
3031     Value get;
3032     if (local-&gt;type()-&gt;is_object_kind() &amp;&amp; !live_oops.at(index)) {
3033       // The interpreter thinks this local is dead but the compiler
3034       // doesn't so pretend that the interpreter passed in null.
3035       get = append(new Constant(objectNull));
3036     } else {
3037       get = append(new UnsafeGetRaw(as_BasicType(local-&gt;type()), e,
3038                                     append(new Constant(new IntConstant(offset))),
3039                                     0,
3040                                     true /*unaligned*/, true /*wide*/));
3041     }
3042     _state-&gt;store_local(index, get);
3043   }
3044 
3045   // the storage for the OSR buffer is freed manually in the LIRGenerator.
3046 
3047   assert(state-&gt;caller_state() == NULL, "should be top scope");
3048   state-&gt;clear_locals();
3049   Goto* g = new Goto(target, false);
3050   append(g);
3051   _osr_entry-&gt;set_end(g);
3052   target-&gt;merge(_osr_entry-&gt;end()-&gt;state());
3053 
3054   scope_data()-&gt;set_stream(NULL);
3055 }
3056 
3057 
3058 ValueStack* GraphBuilder::state_at_entry() {
3059   ValueStack* state = new ValueStack(scope(), NULL);
3060 
3061   // Set up locals for receiver
3062   int idx = 0;
3063   if (!method()-&gt;is_static()) {
3064     // we should always see the receiver
3065     state-&gt;store_local(idx, new Local(method()-&gt;holder(), objectType, idx));
3066     idx = 1;
3067   }
3068 
3069   // Set up locals for incoming arguments
3070   ciSignature* sig = method()-&gt;signature();
3071   for (int i = 0; i &lt; sig-&gt;count(); i++) {
3072     ciType* type = sig-&gt;type_at(i);
3073     BasicType basic_type = type-&gt;basic_type();
3074     // don't allow T_ARRAY to propagate into locals types
3075     if (basic_type == T_ARRAY) basic_type = T_OBJECT;
3076     ValueType* vt = as_ValueType(basic_type);
3077     state-&gt;store_local(idx, new Local(type, vt, idx));
3078     idx += type-&gt;size();
3079   }
3080 
3081   // lock synchronized method
3082   if (method()-&gt;is_synchronized()) {
3083     state-&gt;lock(NULL);
3084   }
3085 
3086   return state;
3087 }
3088 
3089 
3090 GraphBuilder::GraphBuilder(Compilation* compilation, IRScope* scope)
3091   : _scope_data(NULL)
3092   , _instruction_count(0)
3093   , _osr_entry(NULL)
3094   , _memory(new MemoryBuffer())
3095   , _compilation(compilation)
3096   , _inline_bailout_msg(NULL)
3097 {
3098   int osr_bci = compilation-&gt;osr_bci();
3099 
3100   // determine entry points and bci2block mapping
3101   BlockListBuilder blm(compilation, scope, osr_bci);
3102   CHECK_BAILOUT();
3103 
3104   BlockList* bci2block = blm.bci2block();
3105   BlockBegin* start_block = bci2block-&gt;at(0);
3106 
3107   push_root_scope(scope, bci2block, start_block);
3108 
3109   // setup state for std entry
3110   _initial_state = state_at_entry();
3111   start_block-&gt;merge(_initial_state);
3112 
3113   // complete graph
3114   _vmap        = new ValueMap();
3115   switch (scope-&gt;method()-&gt;intrinsic_id()) {
3116   case vmIntrinsics::_dabs          : // fall through
3117   case vmIntrinsics::_dsqrt         : // fall through
3118   case vmIntrinsics::_dsin          : // fall through
3119   case vmIntrinsics::_dcos          : // fall through
3120   case vmIntrinsics::_dtan          : // fall through
3121   case vmIntrinsics::_dlog          : // fall through
3122   case vmIntrinsics::_dlog10        : // fall through
3123   case vmIntrinsics::_dexp          : // fall through
3124   case vmIntrinsics::_dpow          : // fall through
3125     {
3126       // Compiles where the root method is an intrinsic need a special
3127       // compilation environment because the bytecodes for the method
3128       // shouldn't be parsed during the compilation, only the special
3129       // Intrinsic node should be emitted.  If this isn't done the the
3130       // code for the inlined version will be different than the root
3131       // compiled version which could lead to monotonicity problems on
3132       // intel.
3133 
3134       // Set up a stream so that appending instructions works properly.
3135       ciBytecodeStream s(scope-&gt;method());
3136       s.reset_to_bci(0);
3137       scope_data()-&gt;set_stream(&amp;s);
3138       s.next();
3139 
3140       // setup the initial block state
3141       _block = start_block;
3142       _state = start_block-&gt;state()-&gt;copy_for_parsing();
3143       _last  = start_block;
3144       load_local(doubleType, 0);
3145       if (scope-&gt;method()-&gt;intrinsic_id() == vmIntrinsics::_dpow) {
3146         load_local(doubleType, 2);
3147       }
3148 
3149       // Emit the intrinsic node.
3150       bool result = try_inline_intrinsics(scope-&gt;method());
3151       if (!result) BAILOUT("failed to inline intrinsic");
3152       method_return(dpop());
3153 
3154       // connect the begin and end blocks and we're all done.
3155       BlockEnd* end = last()-&gt;as_BlockEnd();
3156       block()-&gt;set_end(end);
3157       break;
3158     }
3159 
3160   case vmIntrinsics::_Reference_get:
3161     {
3162       {
3163         // With java.lang.ref.reference.get() we must go through the
3164         // intrinsic - when G1 is enabled - even when get() is the root
3165         // method of the compile so that, if necessary, the value in
3166         // the referent field of the reference object gets recorded by
3167         // the pre-barrier code.
3168         // Specifically, if G1 is enabled, the value in the referent
3169         // field is recorded by the G1 SATB pre barrier. This will
3170         // result in the referent being marked live and the reference
3171         // object removed from the list of discovered references during
3172         // reference processing.
3173 
3174         // Also we need intrinsic to prevent commoning reads from this field
3175         // across safepoint since GC can change its value.
3176 
3177         // Set up a stream so that appending instructions works properly.
3178         ciBytecodeStream s(scope-&gt;method());
3179         s.reset_to_bci(0);
3180         scope_data()-&gt;set_stream(&amp;s);
3181         s.next();
3182 
3183         // setup the initial block state
3184         _block = start_block;
3185         _state = start_block-&gt;state()-&gt;copy_for_parsing();
3186         _last  = start_block;
3187         load_local(objectType, 0);
3188 
3189         // Emit the intrinsic node.
3190         bool result = try_inline_intrinsics(scope-&gt;method());
3191         if (!result) BAILOUT("failed to inline intrinsic");
3192         method_return(apop());
3193 
3194         // connect the begin and end blocks and we're all done.
3195         BlockEnd* end = last()-&gt;as_BlockEnd();
3196         block()-&gt;set_end(end);
3197         break;
3198       }
3199       // Otherwise, fall thru
3200     }
3201 
3202   default:
3203     scope_data()-&gt;add_to_work_list(start_block);
3204     iterate_all_blocks();
3205     break;
3206   }
3207   CHECK_BAILOUT();
3208 
3209   _start = setup_start_block(osr_bci, start_block, _osr_entry, _initial_state);
3210 
3211   eliminate_redundant_phis(_start);
3212 
3213   NOT_PRODUCT(if (PrintValueNumbering &amp;&amp; Verbose) print_stats());
3214   // for osr compile, bailout if some requirements are not fulfilled
3215   if (osr_bci != -1) {
3216     BlockBegin* osr_block = blm.bci2block()-&gt;at(osr_bci);
3217     assert(osr_block-&gt;is_set(BlockBegin::was_visited_flag),"osr entry must have been visited for osr compile");
3218 
3219     // check if osr entry point has empty stack - we cannot handle non-empty stacks at osr entry points
3220     if (!osr_block-&gt;state()-&gt;stack_is_empty()) {
3221       BAILOUT("stack not empty at OSR entry point");
3222     }
3223   }
3224 #ifndef PRODUCT
3225   if (PrintCompilation &amp;&amp; Verbose) tty-&gt;print_cr("Created %d Instructions", _instruction_count);
3226 #endif
3227 }
3228 
3229 
3230 ValueStack* GraphBuilder::copy_state_before() {
3231   return copy_state_before_with_bci(bci());
3232 }
3233 
3234 ValueStack* GraphBuilder::copy_state_exhandling() {
3235   return copy_state_exhandling_with_bci(bci());
3236 }
3237 
3238 ValueStack* GraphBuilder::copy_state_for_exception() {
3239   return copy_state_for_exception_with_bci(bci());
3240 }
3241 
3242 ValueStack* GraphBuilder::copy_state_before_with_bci(int bci) {
3243   return state()-&gt;copy(ValueStack::StateBefore, bci);
3244 }
3245 
3246 ValueStack* GraphBuilder::copy_state_exhandling_with_bci(int bci) {
3247   if (!has_handler()) return NULL;
3248   return state()-&gt;copy(ValueStack::StateBefore, bci);
3249 }
3250 
3251 ValueStack* GraphBuilder::copy_state_for_exception_with_bci(int bci) {
3252   ValueStack* s = copy_state_exhandling_with_bci(bci);
3253   if (s == NULL) {
3254     if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
3255       s = state()-&gt;copy(ValueStack::ExceptionState, bci);
3256     } else {
3257       s = state()-&gt;copy(ValueStack::EmptyExceptionState, bci);
3258     }
3259   }
3260   return s;
3261 }
3262 
3263 int GraphBuilder::recursive_inline_level(ciMethod* cur_callee) const {
3264   int recur_level = 0;
3265   for (IRScope* s = scope(); s != NULL; s = s-&gt;caller()) {
3266     if (s-&gt;method() == cur_callee) {
3267       ++recur_level;
3268     }
3269   }
3270   return recur_level;
3271 }
3272 
3273 
3274 bool GraphBuilder::try_inline(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3275   const char* msg = NULL;
3276 
3277   // clear out any existing inline bailout condition
3278   clear_inline_bailout();
3279 
3280   // exclude methods we don't want to inline
3281   msg = should_not_inline(callee);
3282   if (msg != NULL) {
3283     print_inlining(callee, msg, /*success*/ false);
3284     return false;
3285   }
3286 
3287   // method handle invokes
3288   if (callee-&gt;is_method_handle_intrinsic()) {
3289     return try_method_handle_inline(callee);
3290   }
3291 
3292   // handle intrinsics
3293   if (callee-&gt;intrinsic_id() != vmIntrinsics::_none) {
3294     if (try_inline_intrinsics(callee)) {
3295       print_inlining(callee, "intrinsic");
3296       return true;
3297     }
3298     // try normal inlining
3299   }
3300 
3301   // certain methods cannot be parsed at all
3302   msg = check_can_parse(callee);
3303   if (msg != NULL) {
3304     print_inlining(callee, msg, /*success*/ false);
3305     return false;
3306   }
3307 
3308   // If bytecode not set use the current one.
3309   if (bc == Bytecodes::_illegal) {
3310     bc = code();
3311   }
3312   if (try_inline_full(callee, holder_known, bc, receiver))
3313     return true;
3314 
3315   // Entire compilation could fail during try_inline_full call.
3316   // In that case printing inlining decision info is useless.
3317   if (!bailed_out())
3318     print_inlining(callee, _inline_bailout_msg, /*success*/ false);
3319 
3320   return false;
3321 }
3322 
3323 
3324 const char* GraphBuilder::check_can_parse(ciMethod* callee) const {
3325   // Certain methods cannot be parsed at all:
3326   if ( callee-&gt;is_native())            return "native method";
3327   if ( callee-&gt;is_abstract())          return "abstract method";
3328   if (!callee-&gt;can_be_compiled())      return "not compilable (disabled)";
3329   return NULL;
3330 }
3331 
3332 
3333 // negative filter: should callee NOT be inlined?  returns NULL, ok to inline, or rejection msg
3334 const char* GraphBuilder::should_not_inline(ciMethod* callee) const {
3335   if ( callee-&gt;should_exclude())       return "excluded by CompilerOracle";
3336   if ( callee-&gt;should_not_inline())    return "disallowed by CompilerOracle";
3337   if ( callee-&gt;dont_inline())          return "don't inline by annotation";
3338   return NULL;
3339 }
3340 
3341 
3342 bool GraphBuilder::try_inline_intrinsics(ciMethod* callee) {
3343   if (callee-&gt;is_synchronized()) {
3344     // We don't currently support any synchronized intrinsics
3345     return false;
3346   }
3347 
3348   // callee seems like a good candidate
3349   // determine id
3350   vmIntrinsics::ID id = callee-&gt;intrinsic_id();
3351   if (!InlineNatives &amp;&amp; id != vmIntrinsics::_Reference_get) {
3352     // InlineNatives does not control Reference.get
3353     INLINE_BAILOUT("intrinsic method inlining disabled");
3354   }
3355   bool preserves_state = false;
3356   bool cantrap = true;
3357   switch (id) {
3358     case vmIntrinsics::_arraycopy:
3359       if (!InlineArrayCopy) return false;
3360       break;
3361 
3362 #ifdef TRACE_HAVE_INTRINSICS
3363     case vmIntrinsics::_classID:
3364     case vmIntrinsics::_threadID:
3365       preserves_state = true;
3366       cantrap = true;
3367       break;
3368 
3369     case vmIntrinsics::_counterTime:
3370       preserves_state = true;
3371       cantrap = false;
3372       break;
3373 #endif
3374 
3375     case vmIntrinsics::_currentTimeMillis:
3376     case vmIntrinsics::_nanoTime:
3377       preserves_state = true;
3378       cantrap = false;
3379       break;
3380 
3381     case vmIntrinsics::_floatToRawIntBits   :
3382     case vmIntrinsics::_intBitsToFloat      :
3383     case vmIntrinsics::_doubleToRawLongBits :
3384     case vmIntrinsics::_longBitsToDouble    :
3385       if (!InlineMathNatives) return false;
3386       preserves_state = true;
3387       cantrap = false;
3388       break;
3389 
3390     case vmIntrinsics::_getClass      :
3391     case vmIntrinsics::_isInstance    :
3392       if (!InlineClassNatives) return false;
3393       preserves_state = true;
3394       break;
3395 
3396     case vmIntrinsics::_currentThread :
3397       if (!InlineThreadNatives) return false;
3398       preserves_state = true;
3399       cantrap = false;
3400       break;
3401 
3402     case vmIntrinsics::_dabs          : // fall through
3403     case vmIntrinsics::_dsqrt         : // fall through
3404     case vmIntrinsics::_dsin          : // fall through
3405     case vmIntrinsics::_dcos          : // fall through
3406     case vmIntrinsics::_dtan          : // fall through
3407     case vmIntrinsics::_dlog          : // fall through
3408     case vmIntrinsics::_dlog10        : // fall through
3409     case vmIntrinsics::_dexp          : // fall through
3410     case vmIntrinsics::_dpow          : // fall through
3411       if (!InlineMathNatives) return false;
3412       cantrap = false;
3413       preserves_state = true;
3414       break;
3415 
3416     // Use special nodes for Unsafe instructions so we can more easily
3417     // perform an address-mode optimization on the raw variants
3418     case vmIntrinsics::_getObject : return append_unsafe_get_obj(callee, T_OBJECT,  false);
3419     case vmIntrinsics::_getBoolean: return append_unsafe_get_obj(callee, T_BOOLEAN, false);
3420     case vmIntrinsics::_getByte   : return append_unsafe_get_obj(callee, T_BYTE,    false);
3421     case vmIntrinsics::_getShort  : return append_unsafe_get_obj(callee, T_SHORT,   false);
3422     case vmIntrinsics::_getChar   : return append_unsafe_get_obj(callee, T_CHAR,    false);
3423     case vmIntrinsics::_getInt    : return append_unsafe_get_obj(callee, T_INT,     false);
3424     case vmIntrinsics::_getLong   : return append_unsafe_get_obj(callee, T_LONG,    false);
3425     case vmIntrinsics::_getFloat  : return append_unsafe_get_obj(callee, T_FLOAT,   false);
3426     case vmIntrinsics::_getDouble : return append_unsafe_get_obj(callee, T_DOUBLE,  false);
3427 
3428     case vmIntrinsics::_putObject : return append_unsafe_put_obj(callee, T_OBJECT,  false);
3429     case vmIntrinsics::_putBoolean: return append_unsafe_put_obj(callee, T_BOOLEAN, false);
3430     case vmIntrinsics::_putByte   : return append_unsafe_put_obj(callee, T_BYTE,    false);
3431     case vmIntrinsics::_putShort  : return append_unsafe_put_obj(callee, T_SHORT,   false);
3432     case vmIntrinsics::_putChar   : return append_unsafe_put_obj(callee, T_CHAR,    false);
3433     case vmIntrinsics::_putInt    : return append_unsafe_put_obj(callee, T_INT,     false);
3434     case vmIntrinsics::_putLong   : return append_unsafe_put_obj(callee, T_LONG,    false);
3435     case vmIntrinsics::_putFloat  : return append_unsafe_put_obj(callee, T_FLOAT,   false);
3436     case vmIntrinsics::_putDouble : return append_unsafe_put_obj(callee, T_DOUBLE,  false);
3437 
3438     case vmIntrinsics::_getObjectVolatile : return append_unsafe_get_obj(callee, T_OBJECT,  true);
3439     case vmIntrinsics::_getBooleanVolatile: return append_unsafe_get_obj(callee, T_BOOLEAN, true);
3440     case vmIntrinsics::_getByteVolatile   : return append_unsafe_get_obj(callee, T_BYTE,    true);
3441     case vmIntrinsics::_getShortVolatile  : return append_unsafe_get_obj(callee, T_SHORT,   true);
3442     case vmIntrinsics::_getCharVolatile   : return append_unsafe_get_obj(callee, T_CHAR,    true);
3443     case vmIntrinsics::_getIntVolatile    : return append_unsafe_get_obj(callee, T_INT,     true);
3444     case vmIntrinsics::_getLongVolatile   : return append_unsafe_get_obj(callee, T_LONG,    true);
3445     case vmIntrinsics::_getFloatVolatile  : return append_unsafe_get_obj(callee, T_FLOAT,   true);
3446     case vmIntrinsics::_getDoubleVolatile : return append_unsafe_get_obj(callee, T_DOUBLE,  true);
3447 
3448     case vmIntrinsics::_putObjectVolatile : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3449     case vmIntrinsics::_putBooleanVolatile: return append_unsafe_put_obj(callee, T_BOOLEAN, true);
3450     case vmIntrinsics::_putByteVolatile   : return append_unsafe_put_obj(callee, T_BYTE,    true);
3451     case vmIntrinsics::_putShortVolatile  : return append_unsafe_put_obj(callee, T_SHORT,   true);
3452     case vmIntrinsics::_putCharVolatile   : return append_unsafe_put_obj(callee, T_CHAR,    true);
3453     case vmIntrinsics::_putIntVolatile    : return append_unsafe_put_obj(callee, T_INT,     true);
3454     case vmIntrinsics::_putLongVolatile   : return append_unsafe_put_obj(callee, T_LONG,    true);
3455     case vmIntrinsics::_putFloatVolatile  : return append_unsafe_put_obj(callee, T_FLOAT,   true);
3456     case vmIntrinsics::_putDoubleVolatile : return append_unsafe_put_obj(callee, T_DOUBLE,  true);
3457 
3458     case vmIntrinsics::_getByte_raw   : return append_unsafe_get_raw(callee, T_BYTE);
3459     case vmIntrinsics::_getShort_raw  : return append_unsafe_get_raw(callee, T_SHORT);
3460     case vmIntrinsics::_getChar_raw   : return append_unsafe_get_raw(callee, T_CHAR);
3461     case vmIntrinsics::_getInt_raw    : return append_unsafe_get_raw(callee, T_INT);
3462     case vmIntrinsics::_getLong_raw   : return append_unsafe_get_raw(callee, T_LONG);
3463     case vmIntrinsics::_getFloat_raw  : return append_unsafe_get_raw(callee, T_FLOAT);
3464     case vmIntrinsics::_getDouble_raw : return append_unsafe_get_raw(callee, T_DOUBLE);
3465 
3466     case vmIntrinsics::_putByte_raw   : return append_unsafe_put_raw(callee, T_BYTE);
3467     case vmIntrinsics::_putShort_raw  : return append_unsafe_put_raw(callee, T_SHORT);
3468     case vmIntrinsics::_putChar_raw   : return append_unsafe_put_raw(callee, T_CHAR);
3469     case vmIntrinsics::_putInt_raw    : return append_unsafe_put_raw(callee, T_INT);
3470     case vmIntrinsics::_putLong_raw   : return append_unsafe_put_raw(callee, T_LONG);
3471     case vmIntrinsics::_putFloat_raw  : return append_unsafe_put_raw(callee, T_FLOAT);
3472     case vmIntrinsics::_putDouble_raw : return append_unsafe_put_raw(callee, T_DOUBLE);
3473 
3474     case vmIntrinsics::_prefetchRead        : return append_unsafe_prefetch(callee, false, false);
3475     case vmIntrinsics::_prefetchWrite       : return append_unsafe_prefetch(callee, false, true);
3476     case vmIntrinsics::_prefetchReadStatic  : return append_unsafe_prefetch(callee, true,  false);
3477     case vmIntrinsics::_prefetchWriteStatic : return append_unsafe_prefetch(callee, true,  true);
3478 
3479     case vmIntrinsics::_checkIndex    :
3480       if (!InlineNIOCheckIndex) return false;
3481       preserves_state = true;
3482       break;
3483     case vmIntrinsics::_putOrderedObject : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3484     case vmIntrinsics::_putOrderedInt    : return append_unsafe_put_obj(callee, T_INT,     true);
3485     case vmIntrinsics::_putOrderedLong   : return append_unsafe_put_obj(callee, T_LONG,    true);
3486 
3487     case vmIntrinsics::_compareAndSwapLong:
3488       if (!VM_Version::supports_cx8()) return false;
3489       // fall through
3490     case vmIntrinsics::_compareAndSwapInt:
3491     case vmIntrinsics::_compareAndSwapObject:
3492       append_unsafe_CAS(callee);
3493       return true;
3494 
3495     case vmIntrinsics::_getAndAddInt:
3496       if (!VM_Version::supports_atomic_getadd4()) {
3497         return false;
3498       }
3499       return append_unsafe_get_and_set_obj(callee, true);
3500     case vmIntrinsics::_getAndAddLong:
3501       if (!VM_Version::supports_atomic_getadd8()) {
3502         return false;
3503       }
3504       return append_unsafe_get_and_set_obj(callee, true);
3505     case vmIntrinsics::_getAndSetInt:
3506       if (!VM_Version::supports_atomic_getset4()) {
3507         return false;
3508       }
3509       return append_unsafe_get_and_set_obj(callee, false);
3510     case vmIntrinsics::_getAndSetLong:
3511       if (!VM_Version::supports_atomic_getset8()) {
3512         return false;
3513       }
3514       return append_unsafe_get_and_set_obj(callee, false);
3515     case vmIntrinsics::_getAndSetObject:
3516 #ifdef _LP64
3517       if (!UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset8()) {
3518         return false;
3519       }
3520       if (UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset4()) {
3521         return false;
3522       }
3523 #else
3524       if (!VM_Version::supports_atomic_getset4()) {
3525         return false;
3526       }
3527 #endif
3528       return append_unsafe_get_and_set_obj(callee, false);
3529 
3530     case vmIntrinsics::_Reference_get:
3531       // Use the intrinsic version of Reference.get() so that the value in
3532       // the referent field can be registered by the G1 pre-barrier code.
3533       // Also to prevent commoning reads from this field across safepoint
3534       // since GC can change its value.
3535       preserves_state = true;
3536       break;
3537 
3538     case vmIntrinsics::_updateCRC32:
3539     case vmIntrinsics::_updateBytesCRC32:
3540     case vmIntrinsics::_updateByteBufferCRC32:
3541       if (!UseCRC32Intrinsics) return false;
3542       cantrap = false;
3543       preserves_state = true;
3544       break;
3545 
3546     case vmIntrinsics::_loadFence :
3547     case vmIntrinsics::_storeFence:
3548     case vmIntrinsics::_fullFence :
3549       break;
3550 
3551     default                       : return false; // do not inline
3552   }
3553   // create intrinsic node
3554   const bool has_receiver = !callee-&gt;is_static();
3555   ValueType* result_type = as_ValueType(callee-&gt;return_type());
3556   ValueStack* state_before = copy_state_for_exception();
3557 
3558   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
3559 
3560   if (is_profiling()) {
3561     // Don't profile in the special case where the root method
3562     // is the intrinsic
3563     if (callee != method()) {
3564       // Note that we'd collect profile data in this method if we wanted it.
3565       compilation()-&gt;set_would_profile(true);
3566       if (profile_calls()) {
3567         Value recv = NULL;
3568         if (has_receiver) {
3569           recv = args-&gt;at(0);
3570           null_check(recv);
3571         }
3572         profile_call(callee, recv, NULL, collect_args_for_profiling(args, callee, true), true);
3573       }
3574     }
3575   }
3576 
3577   Intrinsic* result = new Intrinsic(result_type, id, args, has_receiver, state_before,
3578                                     preserves_state, cantrap);
3579   // append instruction &amp; push result
3580   Value value = append_split(result);
3581   if (result_type != voidType) push(result_type, value);
3582 
3583   if (callee != method() &amp;&amp; profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
3584     profile_return_type(result, callee);
3585   }
3586 
3587   // done
3588   return true;
3589 }
3590 
3591 
3592 bool GraphBuilder::try_inline_jsr(int jsr_dest_bci) {
3593   // Introduce a new callee continuation point - all Ret instructions
3594   // will be replaced with Gotos to this point.
3595   BlockBegin* cont = block_at(next_bci());
3596   assert(cont != NULL, "continuation must exist (BlockListBuilder starts a new block after a jsr");
3597 
3598   // Note: can not assign state to continuation yet, as we have to
3599   // pick up the state from the Ret instructions.
3600 
3601   // Push callee scope
3602   push_scope_for_jsr(cont, jsr_dest_bci);
3603 
3604   // Temporarily set up bytecode stream so we can append instructions
3605   // (only using the bci of this stream)
3606   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3607 
3608   BlockBegin* jsr_start_block = block_at(jsr_dest_bci);
3609   assert(jsr_start_block != NULL, "jsr start block must exist");
3610   assert(!jsr_start_block-&gt;is_set(BlockBegin::was_visited_flag), "should not have visited jsr yet");
3611   Goto* goto_sub = new Goto(jsr_start_block, false);
3612   // Must copy state to avoid wrong sharing when parsing bytecodes
3613   assert(jsr_start_block-&gt;state() == NULL, "should have fresh jsr starting block");
3614   jsr_start_block-&gt;set_state(copy_state_before_with_bci(jsr_dest_bci));
3615   append(goto_sub);
3616   _block-&gt;set_end(goto_sub);
3617   _last = _block = jsr_start_block;
3618 
3619   // Clear out bytecode stream
3620   scope_data()-&gt;set_stream(NULL);
3621 
3622   scope_data()-&gt;add_to_work_list(jsr_start_block);
3623 
3624   // Ready to resume parsing in subroutine
3625   iterate_all_blocks();
3626 
3627   // If we bailed out during parsing, return immediately (this is bad news)
3628   CHECK_BAILOUT_(false);
3629 
3630   // Detect whether the continuation can actually be reached. If not,
3631   // it has not had state set by the join() operations in
3632   // iterate_bytecodes_for_block()/ret() and we should not touch the
3633   // iteration state. The calling activation of
3634   // iterate_bytecodes_for_block will then complete normally.
3635   if (cont-&gt;state() != NULL) {
3636     if (!cont-&gt;is_set(BlockBegin::was_visited_flag)) {
3637       // add continuation to work list instead of parsing it immediately
3638       scope_data()-&gt;parent()-&gt;add_to_work_list(cont);
3639     }
3640   }
3641 
3642   assert(jsr_continuation() == cont, "continuation must not have changed");
3643   assert(!jsr_continuation()-&gt;is_set(BlockBegin::was_visited_flag) ||
3644          jsr_continuation()-&gt;is_set(BlockBegin::parser_loop_header_flag),
3645          "continuation can only be visited in case of backward branches");
3646   assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "block must have end");
3647 
3648   // continuation is in work list, so end iteration of current block
3649   _skip_block = true;
3650   pop_scope_for_jsr();
3651 
3652   return true;
3653 }
3654 
3655 
3656 // Inline the entry of a synchronized method as a monitor enter and
3657 // register the exception handler which releases the monitor if an
3658 // exception is thrown within the callee. Note that the monitor enter
3659 // cannot throw an exception itself, because the receiver is
3660 // guaranteed to be non-null by the explicit null check at the
3661 // beginning of inlining.
3662 void GraphBuilder::inline_sync_entry(Value lock, BlockBegin* sync_handler) {
3663   assert(lock != NULL &amp;&amp; sync_handler != NULL, "lock or handler missing");
3664 
3665   monitorenter(lock, SynchronizationEntryBCI);
3666   assert(_last-&gt;as_MonitorEnter() != NULL, "monitor enter expected");
3667   _last-&gt;set_needs_null_check(false);
3668 
3669   sync_handler-&gt;set(BlockBegin::exception_entry_flag);
3670   sync_handler-&gt;set(BlockBegin::is_on_work_list_flag);
3671 
3672   ciExceptionHandler* desc = new ciExceptionHandler(method()-&gt;holder(), 0, method()-&gt;code_size(), -1, 0);
3673   XHandler* h = new XHandler(desc);
3674   h-&gt;set_entry_block(sync_handler);
3675   scope_data()-&gt;xhandlers()-&gt;append(h);
3676   scope_data()-&gt;set_has_handler();
3677 }
3678 
3679 
3680 // If an exception is thrown and not handled within an inlined
3681 // synchronized method, the monitor must be released before the
3682 // exception is rethrown in the outer scope. Generate the appropriate
3683 // instructions here.
3684 void GraphBuilder::fill_sync_handler(Value lock, BlockBegin* sync_handler, bool default_handler) {
3685   BlockBegin* orig_block = _block;
3686   ValueStack* orig_state = _state;
3687   Instruction* orig_last = _last;
3688   _last = _block = sync_handler;
3689   _state = sync_handler-&gt;state()-&gt;copy();
3690 
3691   assert(sync_handler != NULL, "handler missing");
3692   assert(!sync_handler-&gt;is_set(BlockBegin::was_visited_flag), "is visited here");
3693 
3694   assert(lock != NULL || default_handler, "lock or handler missing");
3695 
3696   XHandler* h = scope_data()-&gt;xhandlers()-&gt;remove_last();
3697   assert(h-&gt;entry_block() == sync_handler, "corrupt list of handlers");
3698 
3699   block()-&gt;set(BlockBegin::was_visited_flag);
3700   Value exception = append_with_bci(new ExceptionObject(), SynchronizationEntryBCI);
3701   assert(exception-&gt;is_pinned(), "must be");
3702 
3703   int bci = SynchronizationEntryBCI;
3704   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3705     // Report exit from inline methods.  We don't have a stream here
3706     // so pass an explicit bci of SynchronizationEntryBCI.
3707     Values* args = new Values(1);
3708     args-&gt;push(append_with_bci(new Constant(new MethodConstant(method())), bci));
3709     append_with_bci(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args), bci);
3710   }
3711 
3712   if (lock) {
3713     assert(state()-&gt;locks_size() &gt; 0 &amp;&amp; state()-&gt;lock_at(state()-&gt;locks_size() - 1) == lock, "lock is missing");
3714     if (!lock-&gt;is_linked()) {
3715       lock = append_with_bci(lock, bci);
3716     }
3717 
3718     // exit the monitor in the context of the synchronized method
3719     monitorexit(lock, bci);
3720 
3721     // exit the context of the synchronized method
3722     if (!default_handler) {
3723       pop_scope();
3724       bci = _state-&gt;caller_state()-&gt;bci();
3725       _state = _state-&gt;caller_state()-&gt;copy_for_parsing();
3726     }
3727   }
3728 
3729   // perform the throw as if at the the call site
3730   apush(exception);
3731   throw_op(bci);
3732 
3733   BlockEnd* end = last()-&gt;as_BlockEnd();
3734   block()-&gt;set_end(end);
3735 
3736   _block = orig_block;
3737   _state = orig_state;
3738   _last = orig_last;
3739 }
3740 
3741 
3742 bool GraphBuilder::try_inline_full(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3743   assert(!callee-&gt;is_native(), "callee must not be native");
3744   if (CompilationPolicy::policy()-&gt;should_not_inline(compilation()-&gt;env(), callee)) {
3745     INLINE_BAILOUT("inlining prohibited by policy");
3746   }
3747   // first perform tests of things it's not possible to inline
3748   if (callee-&gt;has_exception_handlers() &amp;&amp;
3749       !InlineMethodsWithExceptionHandlers) INLINE_BAILOUT("callee has exception handlers");
3750   if (callee-&gt;is_synchronized() &amp;&amp;
3751       !InlineSynchronizedMethods         ) INLINE_BAILOUT("callee is synchronized");
3752   if (!callee-&gt;holder()-&gt;is_initialized()) INLINE_BAILOUT("callee's klass not initialized yet");
3753   if (!callee-&gt;has_balanced_monitors())    INLINE_BAILOUT("callee's monitors do not match");
3754 
3755   // Proper inlining of methods with jsrs requires a little more work.
3756   if (callee-&gt;has_jsrs()                 ) INLINE_BAILOUT("jsrs not handled properly by inliner yet");
3757 
3758   // When SSE2 is used on intel, then no special handling is needed
3759   // for strictfp because the enum-constant is fixed at compile time,
3760   // the check for UseSSE2 is needed here
3761   if (strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt; 2 &amp;&amp; method()-&gt;is_strict() != callee-&gt;is_strict()) {
3762     INLINE_BAILOUT("caller and callee have different strict fp requirements");
3763   }
3764 
3765   if (is_profiling() &amp;&amp; !callee-&gt;ensure_method_data()) {
3766     INLINE_BAILOUT("mdo allocation failed");
3767   }
3768 
3769   // now perform tests that are based on flag settings
3770   if (callee-&gt;force_inline()) {
3771     if (inline_level() &gt; MaxForceInlineLevel) INLINE_BAILOUT("MaxForceInlineLevel");
3772     print_inlining(callee, "force inline by annotation");
3773   } else if (callee-&gt;should_inline()) {
3774     print_inlining(callee, "force inline by CompileOracle");
3775   } else {
3776     // use heuristic controls on inlining
3777     if (inline_level() &gt; MaxInlineLevel                         ) INLINE_BAILOUT("inlining too deep");
3778     if (recursive_inline_level(callee) &gt; MaxRecursiveInlineLevel) INLINE_BAILOUT("recursive inlining too deep");
3779     if (callee-&gt;code_size_for_inlining() &gt; max_inline_size()    ) INLINE_BAILOUT("callee is too large");
3780 
3781     // don't inline throwable methods unless the inlining tree is rooted in a throwable class
3782     if (callee-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
3783         callee-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3784       // Throwable constructor call
3785       IRScope* top = scope();
3786       while (top-&gt;caller() != NULL) {
3787         top = top-&gt;caller();
3788       }
3789       if (!top-&gt;method()-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3790         INLINE_BAILOUT("don't inline Throwable constructors");
3791       }
3792     }
3793 
3794     if (compilation()-&gt;env()-&gt;num_inlined_bytecodes() &gt; DesiredMethodLimit) {
3795       INLINE_BAILOUT("total inlining greater than DesiredMethodLimit");
3796     }
3797     // printing
3798     print_inlining(callee);
3799   }
3800 
3801   // NOTE: Bailouts from this point on, which occur at the
3802   // GraphBuilder level, do not cause bailout just of the inlining but
3803   // in fact of the entire compilation.
3804 
3805   BlockBegin* orig_block = block();
3806 
3807   const bool is_invokedynamic = bc == Bytecodes::_invokedynamic;
3808   const bool has_receiver = (bc != Bytecodes::_invokestatic &amp;&amp; !is_invokedynamic);
3809 
3810   const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
3811   assert(args_base &gt;= 0, "stack underflow during inlining");
3812 
3813   // Insert null check if necessary
3814   Value recv = NULL;
3815   if (has_receiver) {
3816     // note: null check must happen even if first instruction of callee does
3817     //       an implicit null check since the callee is in a different scope
3818     //       and we must make sure exception handling does the right thing
3819     assert(!callee-&gt;is_static(), "callee must not be static");
3820     assert(callee-&gt;arg_size() &gt; 0, "must have at least a receiver");
3821     recv = state()-&gt;stack_at(args_base);
3822     null_check(recv);
3823   }
3824 
3825   if (is_profiling()) {
3826     // Note that we'd collect profile data in this method if we wanted it.
3827     // this may be redundant here...
3828     compilation()-&gt;set_would_profile(true);
3829 
3830     if (profile_calls()) {
3831       int start = 0;
3832       Values* obj_args = args_list_for_profiling(callee, start, has_receiver);
3833       if (obj_args != NULL) {
3834         int s = obj_args-&gt;size();
3835         // if called through method handle invoke, some arguments may have been popped
3836         for (int i = args_base+start, j = 0; j &lt; obj_args-&gt;size() &amp;&amp; i &lt; state()-&gt;stack_size(); ) {
3837           Value v = state()-&gt;stack_at_inc(i);
3838           if (v-&gt;type()-&gt;is_object_kind()) {
3839             obj_args-&gt;push(v);
3840             j++;
3841           }
3842         }
3843 #ifdef ASSERT
3844         {
3845           bool ignored_will_link;
3846           ciSignature* declared_signature = NULL;
3847           ciMethod* real_target = method()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
3848           assert(s == obj_args-&gt;length() || real_target-&gt;is_method_handle_intrinsic(), "missed on arg?");
3849         }
3850 #endif
3851       }
3852       profile_call(callee, recv, holder_known ? callee-&gt;holder() : NULL, obj_args, true);
3853     }
3854   }
3855 
3856   // Introduce a new callee continuation point - if the callee has
3857   // more than one return instruction or the return does not allow
3858   // fall-through of control flow, all return instructions of the
3859   // callee will need to be replaced by Goto's pointing to this
3860   // continuation point.
3861   BlockBegin* cont = block_at(next_bci());
3862   bool continuation_existed = true;
3863   if (cont == NULL) {
3864     cont = new BlockBegin(next_bci());
3865     // low number so that continuation gets parsed as early as possible
3866     cont-&gt;set_depth_first_number(0);
3867 #ifndef PRODUCT
3868     if (PrintInitialBlockList) {
3869       tty-&gt;print_cr("CFG: created block %d (bci %d) as continuation for inline at bci %d",
3870                     cont-&gt;block_id(), cont-&gt;bci(), bci());
3871     }
3872 #endif
3873     continuation_existed = false;
3874   }
3875   // Record number of predecessors of continuation block before
3876   // inlining, to detect if inlined method has edges to its
3877   // continuation after inlining.
3878   int continuation_preds = cont-&gt;number_of_preds();
3879 
3880   // Push callee scope
3881   push_scope(callee, cont);
3882 
3883   // the BlockListBuilder for the callee could have bailed out
3884   if (bailed_out())
3885       return false;
3886 
3887   // Temporarily set up bytecode stream so we can append instructions
3888   // (only using the bci of this stream)
3889   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3890 
3891   // Pass parameters into callee state: add assignments
3892   // note: this will also ensure that all arguments are computed before being passed
3893   ValueStack* callee_state = state();
3894   ValueStack* caller_state = state()-&gt;caller_state();
3895   for (int i = args_base; i &lt; caller_state-&gt;stack_size(); ) {
3896     const int arg_no = i - args_base;
3897     Value arg = caller_state-&gt;stack_at_inc(i);
3898     store_local(callee_state, arg, arg_no);
3899   }
3900 
3901   // Remove args from stack.
3902   // Note that we preserve locals state in case we can use it later
3903   // (see use of pop_scope() below)
3904   caller_state-&gt;truncate_stack(args_base);
3905   assert(callee_state-&gt;stack_size() == 0, "callee stack must be empty");
3906 
3907   Value lock;
3908   BlockBegin* sync_handler;
3909 
3910   // Inline the locking of the receiver if the callee is synchronized
3911   if (callee-&gt;is_synchronized()) {
3912     lock = callee-&gt;is_static() ? append(new Constant(new InstanceConstant(callee-&gt;holder()-&gt;java_mirror())))
3913                                : state()-&gt;local_at(0);
3914     sync_handler = new BlockBegin(SynchronizationEntryBCI);
3915     inline_sync_entry(lock, sync_handler);
3916   }
3917 
3918   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3919     Values* args = new Values(1);
3920     args-&gt;push(append(new Constant(new MethodConstant(method()))));
3921     append(new RuntimeCall(voidType, "dtrace_method_entry", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), args));
3922   }
3923 
3924   if (profile_inlined_calls()) {
3925     profile_invocation(callee, copy_state_before_with_bci(SynchronizationEntryBCI));
3926   }
3927 
3928   BlockBegin* callee_start_block = block_at(0);
3929   if (callee_start_block != NULL) {
3930     assert(callee_start_block-&gt;is_set(BlockBegin::parser_loop_header_flag), "must be loop header");
3931     Goto* goto_callee = new Goto(callee_start_block, false);
3932     // The state for this goto is in the scope of the callee, so use
3933     // the entry bci for the callee instead of the call site bci.
3934     append_with_bci(goto_callee, 0);
3935     _block-&gt;set_end(goto_callee);
3936     callee_start_block-&gt;merge(callee_state);
3937 
3938     _last = _block = callee_start_block;
3939 
3940     scope_data()-&gt;add_to_work_list(callee_start_block);
3941   }
3942 
3943   // Clear out bytecode stream
3944   scope_data()-&gt;set_stream(NULL);
3945 
3946   // Ready to resume parsing in callee (either in the same block we
3947   // were in before or in the callee's start block)
3948   iterate_all_blocks(callee_start_block == NULL);
3949 
3950   // If we bailed out during parsing, return immediately (this is bad news)
3951   if (bailed_out())
3952       return false;
3953 
3954   // iterate_all_blocks theoretically traverses in random order; in
3955   // practice, we have only traversed the continuation if we are
3956   // inlining into a subroutine
3957   assert(continuation_existed ||
3958          !continuation()-&gt;is_set(BlockBegin::was_visited_flag),
3959          "continuation should not have been parsed yet if we created it");
3960 
3961   // At this point we are almost ready to return and resume parsing of
3962   // the caller back in the GraphBuilder. The only thing we want to do
3963   // first is an optimization: during parsing of the callee we
3964   // generated at least one Goto to the continuation block. If we
3965   // generated exactly one, and if the inlined method spanned exactly
3966   // one block (and we didn't have to Goto its entry), then we snip
3967   // off the Goto to the continuation, allowing control to fall
3968   // through back into the caller block and effectively performing
3969   // block merging. This allows load elimination and CSE to take place
3970   // across multiple callee scopes if they are relatively simple, and
3971   // is currently essential to making inlining profitable.
3972   if (num_returns() == 1
3973       &amp;&amp; block() == orig_block
3974       &amp;&amp; block() == inline_cleanup_block()) {
3975     _last  = inline_cleanup_return_prev();
3976     _state = inline_cleanup_state();
3977   } else if (continuation_preds == cont-&gt;number_of_preds()) {
3978     // Inlining caused that the instructions after the invoke in the
3979     // caller are not reachable any more. So skip filling this block
3980     // with instructions!
3981     assert(cont == continuation(), "");
3982     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
3983     _skip_block = true;
3984   } else {
3985     // Resume parsing in continuation block unless it was already parsed.
3986     // Note that if we don't change _last here, iteration in
3987     // iterate_bytecodes_for_block will stop when we return.
3988     if (!continuation()-&gt;is_set(BlockBegin::was_visited_flag)) {
3989       // add continuation to work list instead of parsing it immediately
3990       assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
3991       scope_data()-&gt;parent()-&gt;add_to_work_list(continuation());
3992       _skip_block = true;
3993     }
3994   }
3995 
3996   // Fill the exception handler for synchronized methods with instructions
3997   if (callee-&gt;is_synchronized() &amp;&amp; sync_handler-&gt;state() != NULL) {
3998     fill_sync_handler(lock, sync_handler);
3999   } else {
4000     pop_scope();
4001   }
4002 
4003   compilation()-&gt;notice_inlined_method(callee);
4004 
4005   return true;
4006 }
4007 
4008 
4009 bool GraphBuilder::try_method_handle_inline(ciMethod* callee) {
4010   ValueStack* state_before = state()-&gt;copy_for_parsing();
4011   vmIntrinsics::ID iid = callee-&gt;intrinsic_id();
4012   switch (iid) {
4013   case vmIntrinsics::_invokeBasic:
4014     {
4015       // get MethodHandle receiver
4016       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4017       ValueType* type = state()-&gt;stack_at(args_base)-&gt;type();
4018       if (type-&gt;is_constant()) {
4019         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_method_handle()-&gt;get_vmtarget();
4020         // We don't do CHA here so only inline static and statically bindable methods.
4021         if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4022           Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4023           if (try_inline(target, /*holder_known*/ true, bc)) {
4024             return true;
4025           }
4026         } else {
4027           print_inlining(target, "not static or statically bindable", /*success*/ false);
4028         }
4029       } else {
4030         print_inlining(callee, "receiver not constant", /*success*/ false);
4031       }
4032     }
4033     break;
4034 
4035   case vmIntrinsics::_linkToVirtual:
4036   case vmIntrinsics::_linkToStatic:
4037   case vmIntrinsics::_linkToSpecial:
4038   case vmIntrinsics::_linkToInterface:
4039     {
4040       // pop MemberName argument
4041       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4042       ValueType* type = apop()-&gt;type();
4043       if (type-&gt;is_constant()) {
4044         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_member_name()-&gt;get_vmtarget();
4045         // If the target is another method handle invoke try recursivly to get
4046         // a better target.
4047         if (target-&gt;is_method_handle_intrinsic()) {
4048           if (try_method_handle_inline(target)) {
4049             return true;
4050           }
4051         } else {
4052           ciSignature* signature = target-&gt;signature();
4053           const int receiver_skip = target-&gt;is_static() ? 0 : 1;
4054           // Cast receiver to its type.
4055           if (!target-&gt;is_static()) {
4056             ciKlass* tk = signature-&gt;accessing_klass();
4057             Value obj = state()-&gt;stack_at(args_base);
4058             if (obj-&gt;exact_type() == NULL &amp;&amp;
4059                 obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4060               TypeCast* c = new TypeCast(tk, obj, state_before);
4061               append(c);
4062               state()-&gt;stack_at_put(args_base, c);
4063             }
4064           }
4065           // Cast reference arguments to its type.
4066           for (int i = 0, j = 0; i &lt; signature-&gt;count(); i++) {
4067             ciType* t = signature-&gt;type_at(i);
4068             if (t-&gt;is_klass()) {
4069               ciKlass* tk = t-&gt;as_klass();
4070               Value obj = state()-&gt;stack_at(args_base + receiver_skip + j);
4071               if (obj-&gt;exact_type() == NULL &amp;&amp;
4072                   obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4073                 TypeCast* c = new TypeCast(t, obj, state_before);
4074                 append(c);
4075                 state()-&gt;stack_at_put(args_base + receiver_skip + j, c);
4076               }
4077             }
4078             j += t-&gt;size();  // long and double take two slots
4079           }
4080           // We don't do CHA here so only inline static and statically bindable methods.
4081           if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4082             Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4083             if (try_inline(target, /*holder_known*/ true, bc)) {
4084               return true;
4085             }
4086           } else {
4087             print_inlining(target, "not static or statically bindable", /*success*/ false);
4088           }
4089         }
4090       } else {
4091         print_inlining(callee, "MemberName not constant", /*success*/ false);
4092       }
4093     }
4094     break;
4095 
4096   default:
4097     fatal(err_msg("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
4098     break;
4099   }
4100   set_state(state_before);
4101   return false;
4102 }
4103 
4104 
4105 void GraphBuilder::inline_bailout(const char* msg) {
4106   assert(msg != NULL, "inline bailout msg must exist");
4107   _inline_bailout_msg = msg;
4108 }
4109 
4110 
4111 void GraphBuilder::clear_inline_bailout() {
4112   _inline_bailout_msg = NULL;
4113 }
4114 
4115 
4116 void GraphBuilder::push_root_scope(IRScope* scope, BlockList* bci2block, BlockBegin* start) {
4117   ScopeData* data = new ScopeData(NULL);
4118   data-&gt;set_scope(scope);
4119   data-&gt;set_bci2block(bci2block);
4120   _scope_data = data;
4121   _block = start;
4122 }
4123 
4124 
4125 void GraphBuilder::push_scope(ciMethod* callee, BlockBegin* continuation) {
4126   IRScope* callee_scope = new IRScope(compilation(), scope(), bci(), callee, -1, false);
4127   scope()-&gt;add_callee(callee_scope);
4128 
4129   BlockListBuilder blb(compilation(), callee_scope, -1);
4130   CHECK_BAILOUT();
4131 
4132   if (!blb.bci2block()-&gt;at(0)-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
4133     // this scope can be inlined directly into the caller so remove
4134     // the block at bci 0.
4135     blb.bci2block()-&gt;at_put(0, NULL);
4136   }
4137 
4138   set_state(new ValueStack(callee_scope, state()-&gt;copy(ValueStack::CallerState, bci())));
4139 
4140   ScopeData* data = new ScopeData(scope_data());
4141   data-&gt;set_scope(callee_scope);
4142   data-&gt;set_bci2block(blb.bci2block());
4143   data-&gt;set_continuation(continuation);
4144   _scope_data = data;
4145 }
4146 
4147 
4148 void GraphBuilder::push_scope_for_jsr(BlockBegin* jsr_continuation, int jsr_dest_bci) {
4149   ScopeData* data = new ScopeData(scope_data());
4150   data-&gt;set_parsing_jsr();
4151   data-&gt;set_jsr_entry_bci(jsr_dest_bci);
4152   data-&gt;set_jsr_return_address_local(-1);
4153   // Must clone bci2block list as we will be mutating it in order to
4154   // properly clone all blocks in jsr region as well as exception
4155   // handlers containing rets
4156   BlockList* new_bci2block = new BlockList(bci2block()-&gt;length());
4157   new_bci2block-&gt;push_all(bci2block());
4158   data-&gt;set_bci2block(new_bci2block);
4159   data-&gt;set_scope(scope());
4160   data-&gt;setup_jsr_xhandlers();
4161   data-&gt;set_continuation(continuation());
4162   data-&gt;set_jsr_continuation(jsr_continuation);
4163   _scope_data = data;
4164 }
4165 
4166 
4167 void GraphBuilder::pop_scope() {
4168   int number_of_locks = scope()-&gt;number_of_locks();
4169   _scope_data = scope_data()-&gt;parent();
4170   // accumulate minimum number of monitor slots to be reserved
4171   scope()-&gt;set_min_number_of_locks(number_of_locks);
4172 }
4173 
4174 
4175 void GraphBuilder::pop_scope_for_jsr() {
4176   _scope_data = scope_data()-&gt;parent();
4177 }
4178 
4179 bool GraphBuilder::append_unsafe_get_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4180   if (InlineUnsafeOps) {
4181     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4182     null_check(args-&gt;at(0));
4183     Instruction* offset = args-&gt;at(2);
4184 #ifndef _LP64
4185     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4186 #endif
4187     Instruction* op = append(new UnsafeGetObject(t, args-&gt;at(1), offset, is_volatile));
4188     push(op-&gt;type(), op);
4189     compilation()-&gt;set_has_unsafe_access(true);
4190   }
4191   return InlineUnsafeOps;
4192 }
4193 
4194 
4195 bool GraphBuilder::append_unsafe_put_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4196   if (InlineUnsafeOps) {
4197     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4198     null_check(args-&gt;at(0));
4199     Instruction* offset = args-&gt;at(2);
4200 #ifndef _LP64
4201     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4202 #endif
4203     Instruction* op = append(new UnsafePutObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_volatile));
4204     compilation()-&gt;set_has_unsafe_access(true);
4205     kill_all();
4206   }
4207   return InlineUnsafeOps;
4208 }
4209 
4210 
4211 bool GraphBuilder::append_unsafe_get_raw(ciMethod* callee, BasicType t) {
4212   if (InlineUnsafeOps) {
4213     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4214     null_check(args-&gt;at(0));
4215     Instruction* op = append(new UnsafeGetRaw(t, args-&gt;at(1), false));
4216     push(op-&gt;type(), op);
4217     compilation()-&gt;set_has_unsafe_access(true);
4218   }
4219   return InlineUnsafeOps;
4220 }
4221 
4222 
4223 bool GraphBuilder::append_unsafe_put_raw(ciMethod* callee, BasicType t) {
4224   if (InlineUnsafeOps) {
4225     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4226     null_check(args-&gt;at(0));
4227     Instruction* op = append(new UnsafePutRaw(t, args-&gt;at(1), args-&gt;at(2)));
4228     compilation()-&gt;set_has_unsafe_access(true);
4229   }
4230   return InlineUnsafeOps;
4231 }
4232 
4233 
4234 bool GraphBuilder::append_unsafe_prefetch(ciMethod* callee, bool is_static, bool is_store) {
4235   if (InlineUnsafeOps) {
4236     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4237     int obj_arg_index = 1; // Assume non-static case
4238     if (is_static) {
4239       obj_arg_index = 0;
4240     } else {
4241       null_check(args-&gt;at(0));
4242     }
4243     Instruction* offset = args-&gt;at(obj_arg_index + 1);
4244 #ifndef _LP64
4245     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4246 #endif
4247     Instruction* op = is_store ? append(new UnsafePrefetchWrite(args-&gt;at(obj_arg_index), offset))
4248                                : append(new UnsafePrefetchRead (args-&gt;at(obj_arg_index), offset));
4249     compilation()-&gt;set_has_unsafe_access(true);
4250   }
4251   return InlineUnsafeOps;
4252 }
4253 
4254 
4255 void GraphBuilder::append_unsafe_CAS(ciMethod* callee) {
4256   ValueStack* state_before = copy_state_for_exception();
4257   ValueType* result_type = as_ValueType(callee-&gt;return_type());
4258   assert(result_type-&gt;is_int(), "int result");
4259   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4260 
4261   // Pop off some args to speically handle, then push back
4262   Value newval = args-&gt;pop();
4263   Value cmpval = args-&gt;pop();
4264   Value offset = args-&gt;pop();
4265   Value src = args-&gt;pop();
4266   Value unsafe_obj = args-&gt;pop();
4267 
4268   // Separately handle the unsafe arg. It is not needed for code
4269   // generation, but must be null checked
4270   null_check(unsafe_obj);
4271 
4272 #ifndef _LP64
4273   offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4274 #endif
4275 
4276   args-&gt;push(src);
4277   args-&gt;push(offset);
4278   args-&gt;push(cmpval);
4279   args-&gt;push(newval);
4280 
4281   // An unsafe CAS can alias with other field accesses, but we don't
4282   // know which ones so mark the state as no preserved.  This will
4283   // cause CSE to invalidate memory across it.
4284   bool preserves_state = false;
4285   Intrinsic* result = new Intrinsic(result_type, callee-&gt;intrinsic_id(), args, false, state_before, preserves_state);
4286   append_split(result);
4287   push(result_type, result);
4288   compilation()-&gt;set_has_unsafe_access(true);
4289 }
4290 
4291 
4292 void GraphBuilder::print_inlining(ciMethod* callee, const char* msg, bool success) {
4293   CompileLog* log = compilation()-&gt;log();
4294   if (log != NULL) {
4295     if (success) {
4296       if (msg != NULL)
4297         log-&gt;inline_success(msg);
4298       else
4299         log-&gt;inline_success("receiver is statically known");
4300     } else {
4301       if (msg != NULL)
4302         log-&gt;inline_fail(msg);
4303       else
4304         log-&gt;inline_fail("reason unknown");
4305     }
4306   }
4307 
4308   if (!PrintInlining &amp;&amp; !compilation()-&gt;method()-&gt;has_option("PrintInlining")) {
4309     return;
4310   }
4311   CompileTask::print_inlining(callee, scope()-&gt;level(), bci(), msg);
4312   if (success &amp;&amp; CIPrintMethodCodes) {
4313     callee-&gt;print_codes();
4314   }
4315 }
4316 
4317 bool GraphBuilder::append_unsafe_get_and_set_obj(ciMethod* callee, bool is_add) {
4318   if (InlineUnsafeOps) {
4319     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4320     BasicType t = callee-&gt;return_type()-&gt;basic_type();
4321     null_check(args-&gt;at(0));
4322     Instruction* offset = args-&gt;at(2);
4323 #ifndef _LP64
4324     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4325 #endif
4326     Instruction* op = append(new UnsafeGetAndSetObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_add));
4327     compilation()-&gt;set_has_unsafe_access(true);
4328     kill_all();
4329     push(op-&gt;type(), op);
4330   }
4331   return InlineUnsafeOps;
4332 }
4333 
4334 #ifndef PRODUCT
4335 void GraphBuilder::print_stats() {
4336   vmap()-&gt;print();
4337 }
4338 #endif // PRODUCT
4339 
4340 void GraphBuilder::profile_call(ciMethod* callee, Value recv, ciKlass* known_holder, Values* obj_args, bool inlined) {
4341   assert(known_holder == NULL || (known_holder-&gt;is_instance_klass() &amp;&amp;
4342                                   (!known_holder-&gt;is_interface() ||
4343                                    ((ciInstanceKlass*)known_holder)-&gt;has_default_methods())), "should be default method");
4344   if (known_holder != NULL) {
4345     if (known_holder-&gt;exact_klass() == NULL) {
4346       known_holder = compilation()-&gt;cha_exact_type(known_holder);
4347     }
4348   }
4349 
4350   append(new ProfileCall(method(), bci(), callee, recv, known_holder, obj_args, inlined));
4351 }
4352 
4353 void GraphBuilder::profile_return_type(Value ret, ciMethod* callee, ciMethod* m, int invoke_bci) {
4354   assert((m == NULL) == (invoke_bci &lt; 0), "invalid method and invalid bci together");
4355   if (m == NULL) {
4356     m = method();
4357   }
4358   if (invoke_bci &lt; 0) {
4359     invoke_bci = bci();
4360   }
4361   ciMethodData* md = m-&gt;method_data_or_null();
4362   ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
4363   if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
4364     append(new ProfileReturnType(m , invoke_bci, callee, ret));
4365   }
4366 }
4367 
4368 void GraphBuilder::profile_invocation(ciMethod* callee, ValueStack* state) {
4369   append(new ProfileInvoke(callee, state));
4370 }
</pre></body></html>
