<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/cfgnode.hpp"
  32 #include "opto/compile.hpp"
  33 #include "opto/connode.hpp"
  34 #include "opto/loopnode.hpp"
  35 #include "opto/machnode.hpp"
  36 #include "opto/matcher.hpp"
  37 #include "opto/memnode.hpp"
  38 #include "opto/mulnode.hpp"
  39 #include "opto/phaseX.hpp"
  40 #include "opto/regmask.hpp"
  41 
  42 // Portions of code courtesy of Clifford Click
  43 
  44 // Optimization - Graph Style
  45 
  46 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  47 
  48 //=============================================================================
  49 uint MemNode::size_of() const { return sizeof(*this); }
  50 
  51 const TypePtr *MemNode::adr_type() const {
  52   Node* adr = in(Address);
  53   const TypePtr* cross_check = NULL;
  54   DEBUG_ONLY(cross_check = _adr_type);
  55   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  56 }
  57 
  58 #ifndef PRODUCT
  59 void MemNode::dump_spec(outputStream *st) const {
  60   if (in(Address) == NULL)  return; // node is dead
  61 #ifndef ASSERT
  62   // fake the missing field
  63   const TypePtr* _adr_type = NULL;
  64   if (in(Address) != NULL)
  65     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  66 #endif
  67   dump_adr_type(this, _adr_type, st);
  68 
  69   Compile* C = Compile::current();
  70   if( C-&gt;alias_type(_adr_type)-&gt;is_volatile() )
  71     st-&gt;print(" Volatile!");
  72 }
  73 
  74 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  75   st-&gt;print(" @");
  76   if (adr_type == NULL) {
  77     st-&gt;print("NULL");
  78   } else {
  79     adr_type-&gt;dump_on(st);
  80     Compile* C = Compile::current();
  81     Compile::AliasType* atp = NULL;
  82     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  83     if (atp == NULL)
  84       st-&gt;print(", idx=?\?;");
  85     else if (atp-&gt;index() == Compile::AliasIdxBot)
  86       st-&gt;print(", idx=Bot;");
  87     else if (atp-&gt;index() == Compile::AliasIdxTop)
  88       st-&gt;print(", idx=Top;");
  89     else if (atp-&gt;index() == Compile::AliasIdxRaw)
  90       st-&gt;print(", idx=Raw;");
  91     else {
  92       ciField* field = atp-&gt;field();
  93       if (field) {
  94         st-&gt;print(", name=");
  95         field-&gt;print_name_on(st);
  96       }
  97       st-&gt;print(", idx=%d;", atp-&gt;index());
  98     }
  99   }
 100 }
 101 
 102 extern void print_alias_types();
 103 
 104 #endif
 105 
 106 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 107   assert((t_oop != NULL), "sanity");
 108   bool is_instance = t_oop-&gt;is_known_instance_field();
 109   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 110                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 111                              (phase-&gt;is_IterGVN() != NULL);
 112   if (!(is_instance || is_boxed_value_load))
 113     return mchain;  // don't try to optimize non-instance types
 114   uint instance_id = t_oop-&gt;instance_id();
 115   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 116   Node *prev = NULL;
 117   Node *result = mchain;
 118   while (prev != result) {
 119     prev = result;
 120     if (result == start_mem)
 121       break;  // hit one of our sentinels
 122     // skip over a call which does not affect this memory slice
 123     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 124       Node *proj_in = result-&gt;in(0);
 125       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 126         break;  // hit one of our sentinels
 127       } else if (proj_in-&gt;is_Call()) {
 128         CallNode *call = proj_in-&gt;as_Call();
 129         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 130           result = call-&gt;in(TypeFunc::Memory);
 131         }
 132       } else if (proj_in-&gt;is_Initialize()) {
 133         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 134         // Stop if this is the initialization for the object instance which
 135         // which contains this memory slice, otherwise skip over it.
 136         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 137           break;
 138         }
 139         if (is_instance) {
 140           result = proj_in-&gt;in(TypeFunc::Memory);
 141         } else if (is_boxed_value_load) {
 142           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 143           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 144           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 145             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 146           }
 147         }
 148       } else if (proj_in-&gt;is_MemBar()) {
 149         result = proj_in-&gt;in(TypeFunc::Memory);
 150       } else {
 151         assert(false, "unexpected projection");
 152       }
 153     } else if (result-&gt;is_ClearArray()) {
 154       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 155         // Can not bypass initialization of the instance
 156         // we are looking for.
 157         break;
 158       }
 159       // Otherwise skip it (the call updated 'result' value).
 160     } else if (result-&gt;is_MergeMem()) {
 161       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 162     }
 163   }
 164   return result;
 165 }
 166 
 167 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 168   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 169   if (t_oop == NULL)
 170     return mchain;  // don't try to optimize non-oop types
 171   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 172   bool is_instance = t_oop-&gt;is_known_instance_field();
 173   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 174   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 175     PhiNode *mphi = result-&gt;as_Phi();
 176     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 177     const TypePtr *t = mphi-&gt;adr_type();
 178     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 179         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 180         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 181          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 182          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 183       // clone the Phi with our address type
 184       result = mphi-&gt;split_out_instance(t_adr, igvn);
 185     } else {
 186       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 187     }
 188   }
 189   return result;
 190 }
 191 
 192 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 193   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 194   Node *mem = mmem;
 195 #ifdef ASSERT
 196   {
 197     // Check that current type is consistent with the alias index used during graph construction
 198     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 199     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 200                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 201     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 202     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 203                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 204         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 205         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 206           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 207           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 208       // don't assert if it is dead code.
 209       consistent = true;
 210     }
 211     if( !consistent ) {
 212       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 213       if( adr_check == NULL ) {
 214         st-&gt;print("NULL");
 215       } else {
 216         adr_check-&gt;dump();
 217       }
 218       st-&gt;cr();
 219       print_alias_types();
 220       assert(consistent, "adr_check must match alias idx");
 221     }
 222   }
 223 #endif
 224   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 225   // means an array I have not precisely typed yet.  Do not do any
 226   // alias stuff with it any time soon.
 227   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 228   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 229       !(toop &amp;&amp;
 230         toop-&gt;klass() != NULL &amp;&amp;
 231         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 232         toop-&gt;offset() == Type::OffsetBot) ) {
 233     // compress paths and change unreachable cycles to TOP
 234     // If not, we can update the input infinitely along a MergeMem cycle
 235     // Equivalent code in PhiNode::Ideal
 236     Node* m  = phase-&gt;transform(mmem);
 237     // If transformed to a MergeMem, get the desired slice
 238     // Otherwise the returned node represents memory for every slice
 239     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 240     // Update input if it is progress over what we have now
 241   }
 242   return mem;
 243 }
 244 
 245 //--------------------------Ideal_common---------------------------------------
 246 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 247 // Unhook non-raw memories from complete (macro-expanded) initializations.
 248 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 249   // If our control input is a dead region, kill all below the region
 250   Node *ctl = in(MemNode::Control);
 251   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 252     return this;
 253   ctl = in(MemNode::Control);
 254   // Don't bother trying to transform a dead node
 255   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 256 
 257   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 258   // Wait if control on the worklist.
 259   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 260     Node* bol = NULL;
 261     Node* cmp = NULL;
 262     if (ctl-&gt;in(0)-&gt;is_If()) {
 263       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 264       bol = ctl-&gt;in(0)-&gt;in(1);
 265       if (bol-&gt;is_Bool())
 266         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 267     }
 268     if (igvn-&gt;_worklist.member(ctl) ||
 269         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 270         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 271       // This control path may be dead.
 272       // Delay this memory node transformation until the control is processed.
 273       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 274       return NodeSentinel; // caller will return NULL
 275     }
 276   }
 277   // Ignore if memory is dead, or self-loop
 278   Node *mem = in(MemNode::Memory);
 279   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 280   assert(mem != this, "dead loop in MemNode::Ideal");
 281 
 282   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 283     // This memory slice may be dead.
 284     // Delay this mem node transformation until the memory is processed.
 285     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 286     return NodeSentinel; // caller will return NULL
 287   }
 288 
 289   Node *address = in(MemNode::Address);
 290   const Type *t_adr = phase-&gt;type(address);
 291   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 292 
 293   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 294       (igvn-&gt;_worklist.member(address) ||
 295        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 296     // The address's base and type may change when the address is processed.
 297     // Delay this mem node transformation until the address is processed.
 298     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 299     return NodeSentinel; // caller will return NULL
 300   }
 301 
 302   // Do NOT remove or optimize the next lines: ensure a new alias index
 303   // is allocated for an oop pointer type before Escape Analysis.
 304   // Note: C++ will not remove it since the call has side effect.
 305   if (t_adr-&gt;isa_oopptr()) {
 306     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 307   }
 308 
 309 #ifdef ASSERT
 310   Node* base = NULL;
 311   if (address-&gt;is_AddP())
 312     base = address-&gt;in(AddPNode::Base);
 313   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 314       !t_adr-&gt;isa_rawptr()) {
 315     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 316     Compile* C = phase-&gt;C;
 317     tty-&gt;cr();
 318     tty-&gt;print_cr("===== NULL+offs not RAW address =====");
 319     if (C-&gt;is_dead_node(this-&gt;_idx))    tty-&gt;print_cr("'this' is dead");
 320     if ((ctl != NULL) &amp;&amp; C-&gt;is_dead_node(ctl-&gt;_idx)) tty-&gt;print_cr("'ctl' is dead");
 321     if (C-&gt;is_dead_node(mem-&gt;_idx))     tty-&gt;print_cr("'mem' is dead");
 322     if (C-&gt;is_dead_node(address-&gt;_idx)) tty-&gt;print_cr("'address' is dead");
 323     if (C-&gt;is_dead_node(base-&gt;_idx))    tty-&gt;print_cr("'base' is dead");
 324     tty-&gt;cr();
 325     base-&gt;dump(1);
 326     tty-&gt;cr();
 327     this-&gt;dump(2);
 328     tty-&gt;print("this-&gt;adr_type():     "); adr_type()-&gt;dump(); tty-&gt;cr();
 329     tty-&gt;print("phase-&gt;type(address): "); t_adr-&gt;dump(); tty-&gt;cr();
 330     tty-&gt;print("phase-&gt;type(base):    "); phase-&gt;type(address)-&gt;dump(); tty-&gt;cr();
 331     tty-&gt;cr();
 332   }
 333   assert(base == NULL || t_adr-&gt;isa_rawptr() ||
 334         !phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR), "NULL+offs not RAW address?");
 335 #endif
 336 
 337   // Avoid independent memory operations
 338   Node* old_mem = mem;
 339 
 340   // The code which unhooks non-raw memories from complete (macro-expanded)
 341   // initializations was removed. After macro-expansion all stores catched
 342   // by Initialize node became raw stores and there is no information
 343   // which memory slices they modify. So it is unsafe to move any memory
 344   // operation above these stores. Also in most cases hooked non-raw memories
 345   // were already unhooked by using information from detect_ptr_independence()
 346   // and find_previous_store().
 347 
 348   if (mem-&gt;is_MergeMem()) {
 349     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 350     const TypePtr *tp = t_adr-&gt;is_ptr();
 351 
 352     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 353   }
 354 
 355   if (mem != old_mem) {
 356     set_req(MemNode::Memory, mem);
 357     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 358         igvn-&gt;_worklist.push(old_mem);
 359     }
 360     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 361     return this;
 362   }
 363 
 364   // let the subclass continue analyzing...
 365   return NULL;
 366 }
 367 
 368 // Helper function for proving some simple control dominations.
 369 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 370 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 371 // is not a constant (dominated by the method's StartNode).
 372 // Used by MemNode::find_previous_store to prove that the
 373 // control input of a memory operation predates (dominates)
 374 // an allocation it wants to look past.
 375 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 376   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 377     return false; // Conservative answer for dead code
 378 
 379   // Check 'dom'. Skip Proj and CatchProj nodes.
 380   dom = dom-&gt;find_exact_control(dom);
 381   if (dom == NULL || dom-&gt;is_top())
 382     return false; // Conservative answer for dead code
 383 
 384   if (dom == sub) {
 385     // For the case when, for example, 'sub' is Initialize and the original
 386     // 'dom' is Proj node of the 'sub'.
 387     return false;
 388   }
 389 
 390   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 391     return true;
 392 
 393   // 'dom' dominates 'sub' if its control edge and control edges
 394   // of all its inputs dominate or equal to sub's control edge.
 395 
 396   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 397   // Or Region for the check in LoadNode::Ideal();
 398   // 'sub' should have sub-&gt;in(0) != NULL.
 399   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 400          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 401 
 402   // Get control edge of 'sub'.
 403   Node* orig_sub = sub;
 404   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 405   if (sub == NULL || sub-&gt;is_top())
 406     return false; // Conservative answer for dead code
 407 
 408   assert(sub-&gt;is_CFG(), "expecting control");
 409 
 410   if (sub == dom)
 411     return true;
 412 
 413   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 414     return false;
 415 
 416   {
 417     // Check all control edges of 'dom'.
 418 
 419     ResourceMark rm;
 420     Arena* arena = Thread::current()-&gt;resource_area();
 421     Node_List nlist(arena);
 422     Unique_Node_List dom_list(arena);
 423 
 424     dom_list.push(dom);
 425     bool only_dominating_controls = false;
 426 
 427     for (uint next = 0; next &lt; dom_list.size(); next++) {
 428       Node* n = dom_list.at(next);
 429       if (n == orig_sub)
 430         return false; // One of dom's inputs dominated by sub.
 431       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 432         // Check only own control edge for pinned non-control nodes.
 433         n = n-&gt;find_exact_control(n-&gt;in(0));
 434         if (n == NULL || n-&gt;is_top())
 435           return false; // Conservative answer for dead code
 436         assert(n-&gt;is_CFG(), "expecting control");
 437         dom_list.push(n);
 438       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 439         only_dominating_controls = true;
 440       } else if (n-&gt;is_CFG()) {
 441         if (n-&gt;dominates(sub, nlist))
 442           only_dominating_controls = true;
 443         else
 444           return false;
 445       } else {
 446         // First, own control edge.
 447         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 448         if (m != NULL) {
 449           if (m-&gt;is_top())
 450             return false; // Conservative answer for dead code
 451           dom_list.push(m);
 452         }
 453         // Now, the rest of edges.
 454         uint cnt = n-&gt;req();
 455         for (uint i = 1; i &lt; cnt; i++) {
 456           m = n-&gt;find_exact_control(n-&gt;in(i));
 457           if (m == NULL || m-&gt;is_top())
 458             continue;
 459           dom_list.push(m);
 460         }
 461       }
 462     }
 463     return only_dominating_controls;
 464   }
 465 }
 466 
 467 //---------------------detect_ptr_independence---------------------------------
 468 // Used by MemNode::find_previous_store to prove that two base
 469 // pointers are never equal.
 470 // The pointers are accompanied by their associated allocations,
 471 // if any, which have been previously discovered by the caller.
 472 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 473                                       Node* p2, AllocateNode* a2,
 474                                       PhaseTransform* phase) {
 475   // Attempt to prove that these two pointers cannot be aliased.
 476   // They may both manifestly be allocations, and they should differ.
 477   // Or, if they are not both allocations, they can be distinct constants.
 478   // Otherwise, one is an allocation and the other a pre-existing value.
 479   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 480     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 481   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 482     return (a1 != a2);
 483   } else if (a1 != NULL) {                  // one allocation a1
 484     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 485     return all_controls_dominate(p2, a1);
 486   } else { //(a2 != NULL)                   // one allocation a2
 487     return all_controls_dominate(p1, a2);
 488   }
 489   return false;
 490 }
 491 
 492 
 493 // The logic for reordering loads and stores uses four steps:
 494 // (a) Walk carefully past stores and initializations which we
 495 //     can prove are independent of this load.
 496 // (b) Observe that the next memory state makes an exact match
 497 //     with self (load or store), and locate the relevant store.
 498 // (c) Ensure that, if we were to wire self directly to the store,
 499 //     the optimizer would fold it up somehow.
 500 // (d) Do the rewiring, and return, depending on some other part of
 501 //     the optimizer to fold up the load.
 502 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 503 // specific to loads and stores, so they are handled by the callers.
 504 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 505 //
 506 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 507   Node*         ctrl   = in(MemNode::Control);
 508   Node*         adr    = in(MemNode::Address);
 509   intptr_t      offset = 0;
 510   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 511   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 512 
 513   if (offset == Type::OffsetBot)
 514     return NULL;            // cannot unalias unless there are precise offsets
 515 
 516   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 517 
 518   intptr_t size_in_bytes = memory_size();
 519 
 520   Node* mem = in(MemNode::Memory);   // start searching here...
 521 
 522   int cnt = 50;             // Cycle limiter
 523   for (;;) {                // While we can dance past unrelated stores...
 524     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 525 
 526     if (mem-&gt;is_Store()) {
 527       Node* st_adr = mem-&gt;in(MemNode::Address);
 528       intptr_t st_offset = 0;
 529       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 530       if (st_base == NULL)
 531         break;              // inscrutable pointer
 532       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 533         const int MAX_STORE = BytesPerLong;
 534         if (st_offset &gt;= offset + size_in_bytes ||
 535             st_offset &lt;= offset - MAX_STORE ||
 536             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 537           // Success:  The offsets are provably independent.
 538           // (You may ask, why not just test st_offset != offset and be done?
 539           // The answer is that stores of different sizes can co-exist
 540           // in the same sequence of RawMem effects.  We sometimes initialize
 541           // a whole 'tile' of array elements with a single jint or jlong.)
 542           mem = mem-&gt;in(MemNode::Memory);
 543           continue;           // (a) advance through independent store memory
 544         }
 545       }
 546       if (st_base != base &amp;&amp;
 547           detect_ptr_independence(base, alloc,
 548                                   st_base,
 549                                   AllocateNode::Ideal_allocation(st_base, phase),
 550                                   phase)) {
 551         // Success:  The bases are provably independent.
 552         mem = mem-&gt;in(MemNode::Memory);
 553         continue;           // (a) advance through independent store memory
 554       }
 555 
 556       // (b) At this point, if the bases or offsets do not agree, we lose,
 557       // since we have not managed to prove 'this' and 'mem' independent.
 558       if (st_base == base &amp;&amp; st_offset == offset) {
 559         return mem;         // let caller handle steps (c), (d)
 560       }
 561 
 562     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 563       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 564       AllocateNode*  st_alloc = st_init-&gt;allocation();
 565       if (st_alloc == NULL)
 566         break;              // something degenerated
 567       bool known_identical = false;
 568       bool known_independent = false;
 569       if (alloc == st_alloc)
 570         known_identical = true;
 571       else if (alloc != NULL)
 572         known_independent = true;
 573       else if (all_controls_dominate(this, st_alloc))
 574         known_independent = true;
 575 
 576       if (known_independent) {
 577         // The bases are provably independent: Either they are
 578         // manifestly distinct allocations, or else the control
 579         // of this load dominates the store's allocation.
 580         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 581         if (alias_idx == Compile::AliasIdxRaw) {
 582           mem = st_alloc-&gt;in(TypeFunc::Memory);
 583         } else {
 584           mem = st_init-&gt;memory(alias_idx);
 585         }
 586         continue;           // (a) advance through independent store memory
 587       }
 588 
 589       // (b) at this point, if we are not looking at a store initializing
 590       // the same allocation we are loading from, we lose.
 591       if (known_identical) {
 592         // From caller, can_see_stored_value will consult find_captured_store.
 593         return mem;         // let caller handle steps (c), (d)
 594       }
 595 
 596     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 597       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 598       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 599         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 600         if (!call-&gt;may_modify(addr_t, phase)) {
 601           mem = call-&gt;in(TypeFunc::Memory);
 602           continue;         // (a) advance through independent call memory
 603         }
 604       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 605         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 606         continue;           // (a) advance through independent MemBar memory
 607       } else if (mem-&gt;is_ClearArray()) {
 608         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 609           // (the call updated 'mem' value)
 610           continue;         // (a) advance through independent allocation memory
 611         } else {
 612           // Can not bypass initialization of the instance
 613           // we are looking for.
 614           return mem;
 615         }
 616       } else if (mem-&gt;is_MergeMem()) {
 617         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 618         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 619         continue;           // (a) advance through independent MergeMem memory
 620       }
 621     }
 622 
 623     // Unless there is an explicit 'continue', we must bail out here,
 624     // because 'mem' is an inscrutable memory state (e.g., a call).
 625     break;
 626   }
 627 
 628   return NULL;              // bail out
 629 }
 630 
 631 //----------------------calculate_adr_type-------------------------------------
 632 // Helper function.  Notices when the given type of address hits top or bottom.
 633 // Also, asserts a cross-check of the type against the expected address type.
 634 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 635   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 636   #ifdef PRODUCT
 637   cross_check = NULL;
 638   #else
 639   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 640   #endif
 641   const TypePtr* tp = t-&gt;isa_ptr();
 642   if (tp == NULL) {
 643     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 644     return TypePtr::BOTTOM;           // touches lots of memory
 645   } else {
 646     #ifdef ASSERT
 647     // %%%% [phh] We don't check the alias index if cross_check is
 648     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 649     if (cross_check != NULL &amp;&amp;
 650         cross_check != TypePtr::BOTTOM &amp;&amp;
 651         cross_check != TypeRawPtr::BOTTOM) {
 652       // Recheck the alias index, to see if it has changed (due to a bug).
 653       Compile* C = Compile::current();
 654       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 655              "must stay in the original alias category");
 656       // The type of the address must be contained in the adr_type,
 657       // disregarding "null"-ness.
 658       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 659       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 660       assert(cross_check-&gt;meet(tp_notnull) == cross_check,
 661              "real address must not escape from expected memory type");
 662     }
 663     #endif
 664     return tp;
 665   }
 666 }
 667 
 668 //------------------------adr_phi_is_loop_invariant----------------------------
 669 // A helper function for Ideal_DU_postCCP to check if a Phi in a counted
 670 // loop is loop invariant. Make a quick traversal of Phi and associated
 671 // CastPP nodes, looking to see if they are a closed group within the loop.
 672 bool MemNode::adr_phi_is_loop_invariant(Node* adr_phi, Node* cast) {
 673   // The idea is that the phi-nest must boil down to only CastPP nodes
 674   // with the same data. This implies that any path into the loop already
 675   // includes such a CastPP, and so the original cast, whatever its input,
 676   // must be covered by an equivalent cast, with an earlier control input.
 677   ResourceMark rm;
 678 
 679   // The loop entry input of the phi should be the unique dominating
 680   // node for every Phi/CastPP in the loop.
 681   Unique_Node_List closure;
 682   closure.push(adr_phi-&gt;in(LoopNode::EntryControl));
 683 
 684   // Add the phi node and the cast to the worklist.
 685   Unique_Node_List worklist;
 686   worklist.push(adr_phi);
 687   if( cast != NULL ){
 688     if( !cast-&gt;is_ConstraintCast() ) return false;
 689     worklist.push(cast);
 690   }
 691 
 692   // Begin recursive walk of phi nodes.
 693   while( worklist.size() ){
 694     // Take a node off the worklist
 695     Node *n = worklist.pop();
 696     if( !closure.member(n) ){
 697       // Add it to the closure.
 698       closure.push(n);
 699       // Make a sanity check to ensure we don't waste too much time here.
 700       if( closure.size() &gt; 20) return false;
 701       // This node is OK if:
 702       //  - it is a cast of an identical value
 703       //  - or it is a phi node (then we add its inputs to the worklist)
 704       // Otherwise, the node is not OK, and we presume the cast is not invariant
 705       if( n-&gt;is_ConstraintCast() ){
 706         worklist.push(n-&gt;in(1));
 707       } else if( n-&gt;is_Phi() ) {
 708         for( uint i = 1; i &lt; n-&gt;req(); i++ ) {
 709           worklist.push(n-&gt;in(i));
 710         }
 711       } else {
 712         return false;
 713       }
 714     }
 715   }
 716 
 717   // Quit when the worklist is empty, and we've found no offending nodes.
 718   return true;
 719 }
 720 
 721 //------------------------------Ideal_DU_postCCP-------------------------------
 722 // Find any cast-away of null-ness and keep its control.  Null cast-aways are
 723 // going away in this pass and we need to make this memory op depend on the
 724 // gating null check.
 725 Node *MemNode::Ideal_DU_postCCP( PhaseCCP *ccp ) {
 726   return Ideal_common_DU_postCCP(ccp, this, in(MemNode::Address));
 727 }
 728 
 729 // I tried to leave the CastPP's in.  This makes the graph more accurate in
 730 // some sense; we get to keep around the knowledge that an oop is not-null
 731 // after some test.  Alas, the CastPP's interfere with GVN (some values are
 732 // the regular oop, some are the CastPP of the oop, all merge at Phi's which
 733 // cannot collapse, etc).  This cost us 10% on SpecJVM, even when I removed
 734 // some of the more trivial cases in the optimizer.  Removing more useless
 735 // Phi's started allowing Loads to illegally float above null checks.  I gave
 736 // up on this approach.  CNC 10/20/2000
 737 // This static method may be called not from MemNode (EncodePNode calls it).
 738 // Only the control edge of the node 'n' might be updated.
 739 Node *MemNode::Ideal_common_DU_postCCP( PhaseCCP *ccp, Node* n, Node* adr ) {
 740   Node *skipped_cast = NULL;
 741   // Need a null check?  Regular static accesses do not because they are
 742   // from constant addresses.  Array ops are gated by the range check (which
 743   // always includes a NULL check).  Just check field ops.
 744   if( n-&gt;in(MemNode::Control) == NULL ) {
 745     // Scan upwards for the highest location we can place this memory op.
 746     while( true ) {
 747       switch( adr-&gt;Opcode() ) {
 748 
 749       case Op_AddP:             // No change to NULL-ness, so peek thru AddP's
 750         adr = adr-&gt;in(AddPNode::Base);
 751         continue;
 752 
 753       case Op_DecodeN:         // No change to NULL-ness, so peek thru
 754       case Op_DecodeNKlass:
 755         adr = adr-&gt;in(1);
 756         continue;
 757 
 758       case Op_EncodeP:
 759       case Op_EncodePKlass:
 760         // EncodeP node's control edge could be set by this method
 761         // when EncodeP node depends on CastPP node.
 762         //
 763         // Use its control edge for memory op because EncodeP may go away
 764         // later when it is folded with following or preceding DecodeN node.
 765         if (adr-&gt;in(0) == NULL) {
 766           // Keep looking for cast nodes.
 767           adr = adr-&gt;in(1);
 768           continue;
 769         }
 770         ccp-&gt;hash_delete(n);
 771         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 772         ccp-&gt;hash_insert(n);
 773         return n;
 774 
 775       case Op_CastPP:
 776         // If the CastPP is useless, just peek on through it.
 777         if( ccp-&gt;type(adr) == ccp-&gt;type(adr-&gt;in(1)) ) {
 778           // Remember the cast that we've peeked though. If we peek
 779           // through more than one, then we end up remembering the highest
 780           // one, that is, if in a loop, the one closest to the top.
 781           skipped_cast = adr;
 782           adr = adr-&gt;in(1);
 783           continue;
 784         }
 785         // CastPP is going away in this pass!  We need this memory op to be
 786         // control-dependent on the test that is guarding the CastPP.
 787         ccp-&gt;hash_delete(n);
 788         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 789         ccp-&gt;hash_insert(n);
 790         return n;
 791 
 792       case Op_Phi:
 793         // Attempt to float above a Phi to some dominating point.
 794         if (adr-&gt;in(0) != NULL &amp;&amp; adr-&gt;in(0)-&gt;is_CountedLoop()) {
 795           // If we've already peeked through a Cast (which could have set the
 796           // control), we can't float above a Phi, because the skipped Cast
 797           // may not be loop invariant.
 798           if (adr_phi_is_loop_invariant(adr, skipped_cast)) {
 799             adr = adr-&gt;in(1);
 800             continue;
 801           }
 802         }
 803 
 804         // Intentional fallthrough!
 805 
 806         // No obvious dominating point.  The mem op is pinned below the Phi
 807         // by the Phi itself.  If the Phi goes away (no true value is merged)
 808         // then the mem op can float, but not indefinitely.  It must be pinned
 809         // behind the controls leading to the Phi.
 810       case Op_CheckCastPP:
 811         // These usually stick around to change address type, however a
 812         // useless one can be elided and we still need to pick up a control edge
 813         if (adr-&gt;in(0) == NULL) {
 814           // This CheckCastPP node has NO control and is likely useless. But we
 815           // need check further up the ancestor chain for a control input to keep
 816           // the node in place. 4959717.
 817           skipped_cast = adr;
 818           adr = adr-&gt;in(1);
 819           continue;
 820         }
 821         ccp-&gt;hash_delete(n);
 822         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 823         ccp-&gt;hash_insert(n);
 824         return n;
 825 
 826         // List of "safe" opcodes; those that implicitly block the memory
 827         // op below any null check.
 828       case Op_CastX2P:          // no null checks on native pointers
 829       case Op_Parm:             // 'this' pointer is not null
 830       case Op_LoadP:            // Loading from within a klass
 831       case Op_LoadN:            // Loading from within a klass
 832       case Op_LoadKlass:        // Loading from within a klass
 833       case Op_LoadNKlass:       // Loading from within a klass
 834       case Op_ConP:             // Loading from a klass
 835       case Op_ConN:             // Loading from a klass
 836       case Op_ConNKlass:        // Loading from a klass
 837       case Op_CreateEx:         // Sucking up the guts of an exception oop
 838       case Op_Con:              // Reading from TLS
 839       case Op_CMoveP:           // CMoveP is pinned
 840       case Op_CMoveN:           // CMoveN is pinned
 841         break;                  // No progress
 842 
 843       case Op_Proj:             // Direct call to an allocation routine
 844       case Op_SCMemProj:        // Memory state from store conditional ops
 845 #ifdef ASSERT
 846         {
 847           assert(adr-&gt;as_Proj()-&gt;_con == TypeFunc::Parms, "must be return value");
 848           const Node* call = adr-&gt;in(0);
 849           if (call-&gt;is_CallJava()) {
 850             const CallJavaNode* call_java = call-&gt;as_CallJava();
 851             const TypeTuple *r = call_java-&gt;tf()-&gt;range();
 852             assert(r-&gt;cnt() &gt; TypeFunc::Parms, "must return value");
 853             const Type* ret_type = r-&gt;field_at(TypeFunc::Parms);
 854             assert(ret_type &amp;&amp; ret_type-&gt;isa_ptr(), "must return pointer");
 855             // We further presume that this is one of
 856             // new_instance_Java, new_array_Java, or
 857             // the like, but do not assert for this.
 858           } else if (call-&gt;is_Allocate()) {
 859             // similar case to new_instance_Java, etc.
 860           } else if (!call-&gt;is_CallLeaf()) {
 861             // Projections from fetch_oop (OSR) are allowed as well.
 862             ShouldNotReachHere();
 863           }
 864         }
 865 #endif
 866         break;
 867       default:
 868         ShouldNotReachHere();
 869       }
 870       break;
 871     }
 872   }
 873 
 874   return  NULL;               // No progress
 875 }
 876 
 877 
 878 //=============================================================================
 879 uint LoadNode::size_of() const { return sizeof(*this); }
 880 uint LoadNode::cmp( const Node &amp;n ) const
 881 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 882 const Type *LoadNode::bottom_type() const { return _type; }
 883 uint LoadNode::ideal_reg() const {
 884   return _type-&gt;ideal_reg();
 885 }
 886 
 887 #ifndef PRODUCT
 888 void LoadNode::dump_spec(outputStream *st) const {
 889   MemNode::dump_spec(st);
 890   if( !Verbose &amp;&amp; !WizardMode ) {
 891     // standard dump does this in Verbose and WizardMode
 892     st-&gt;print(" #"); _type-&gt;dump_on(st);
 893   }
 894 }
 895 #endif
 896 
 897 #ifdef ASSERT
 898 //----------------------------is_immutable_value-------------------------------
 899 // Helper function to allow a raw load without control edge for some cases
 900 bool LoadNode::is_immutable_value(Node* adr) {
 901   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 902           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 903           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 904            in_bytes(JavaThread::osthread_offset())));
 905 }
 906 #endif
 907 
 908 //----------------------------LoadNode::make-----------------------------------
 909 // Polymorphic factory method:
 910 Node *LoadNode::make( PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt ) {
 911   Compile* C = gvn.C;
 912 
 913   // sanity check the alias category against the created node type
 914   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 915            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 916          "use LoadKlassNode instead");
 917   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 918            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 919          "use LoadRangeNode instead");
 920   // Check control edge of raw loads
 921   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 922           // oop will be recorded in oop map if load crosses safepoint
 923           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 924           "raw memory operations should have control edge");
 925   switch (bt) {
 926   case T_BOOLEAN: return new (C) LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 927   case T_BYTE:    return new (C) LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 928   case T_INT:     return new (C) LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 929   case T_CHAR:    return new (C) LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 930   case T_SHORT:   return new (C) LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 931   case T_LONG:    return new (C) LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long()   );
 932   case T_FLOAT:   return new (C) LoadFNode (ctl, mem, adr, adr_type, rt              );
 933   case T_DOUBLE:  return new (C) LoadDNode (ctl, mem, adr, adr_type, rt              );
 934   case T_ADDRESS: return new (C) LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr()    );
 935   case T_OBJECT:
 936 #ifdef _LP64
 937     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 938       Node* load  = gvn.transform(new (C) LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop()));
 939       return new (C) DecodeNNode(load, load-&gt;bottom_type()-&gt;make_ptr());
 940     } else
 941 #endif
 942     {
 943       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 944       return new (C) LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr());
 945     }
 946   }
 947   ShouldNotReachHere();
 948   return (LoadNode*)NULL;
 949 }
 950 
 951 LoadLNode* LoadLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt) {
 952   bool require_atomic = true;
 953   return new (C) LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), require_atomic);
 954 }
 955 
 956 
 957 
 958 
 959 //------------------------------hash-------------------------------------------
 960 uint LoadNode::hash() const {
 961   // unroll addition of interesting fields
 962   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 963 }
 964 
 965 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 966   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 967     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 968     bool is_stable_ary = FoldStableValues &amp;&amp;
 969                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 970                          tp-&gt;isa_aryptr()-&gt;is_stable();
 971 
 972     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 973   }
 974 
 975   return false;
 976 }
 977 
 978 //---------------------------can_see_stored_value------------------------------
 979 // This routine exists to make sure this set of tests is done the same
 980 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 981 // will change the graph shape in a way which makes memory alive twice at the
 982 // same time (uses the Oracle model of aliasing), then some
 983 // LoadXNode::Identity will fold things back to the equivalence-class model
 984 // of aliasing.
 985 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 986   Node* ld_adr = in(MemNode::Address);
 987   intptr_t ld_off = 0;
 988   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 989   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 990   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 991   // This is more general than load from boxing objects.
 992   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 993     uint alias_idx = atp-&gt;index();
 994     bool final = !atp-&gt;is_rewritable();
 995     Node* result = NULL;
 996     Node* current = st;
 997     // Skip through chains of MemBarNodes checking the MergeMems for
 998     // new states for the slice of this load.  Stop once any other
 999     // kind of node is encountered.  Loads from final memory can skip
1000     // through any kind of MemBar but normal loads shouldn't skip
1001     // through MemBarAcquire since the could allow them to move out of
1002     // a synchronized region.
1003     while (current-&gt;is_Proj()) {
1004       int opc = current-&gt;in(0)-&gt;Opcode();
1005       if ((final &amp;&amp; (opc == Op_MemBarAcquire || opc == Op_MemBarAcquireLock)) ||
1006           opc == Op_MemBarRelease || opc == Op_MemBarCPUOrder ||
1007           opc == Op_MemBarReleaseLock) {
1008         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
1009         if (mem-&gt;is_MergeMem()) {
1010           MergeMemNode* merge = mem-&gt;as_MergeMem();
1011           Node* new_st = merge-&gt;memory_at(alias_idx);
1012           if (new_st == merge-&gt;base_memory()) {
1013             // Keep searching
1014             current = new_st;
1015             continue;
1016           }
1017           // Save the new memory state for the slice and fall through
1018           // to exit.
1019           result = new_st;
1020         }
1021       }
1022       break;
1023     }
1024     if (result != NULL) {
1025       st = result;
1026     }
1027   }
1028 
1029   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1030   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1031   for (int trip = 0; trip &lt;= 1; trip++) {
1032 
1033     if (st-&gt;is_Store()) {
1034       Node* st_adr = st-&gt;in(MemNode::Address);
1035       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1036         // Try harder before giving up...  Match raw and non-raw pointers.
1037         intptr_t st_off = 0;
1038         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
1039         if (alloc == NULL)       return NULL;
1040         if (alloc != ld_alloc)   return NULL;
1041         if (ld_off != st_off)    return NULL;
1042         // At this point we have proven something like this setup:
1043         //  A = Allocate(...)
1044         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
1045         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
1046         // (Actually, we haven't yet proven the Q's are the same.)
1047         // In other words, we are loading from a casted version of
1048         // the same pointer-and-offset that we stored to.
1049         // Thus, we are able to replace L by V.
1050       }
1051       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1052       if (store_Opcode() != st-&gt;Opcode())
1053         return NULL;
1054       return st-&gt;in(MemNode::ValueIn);
1055     }
1056 
1057     // A load from a freshly-created object always returns zero.
1058     // (This can happen after LoadNode::Ideal resets the load's memory input
1059     // to find_captured_store, which returned InitializeNode::zero_memory.)
1060     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1061         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1062         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1063       // return a zero value for the load's basic type
1064       // (This is one of the few places where a generic PhaseTransform
1065       // can create new nodes.  Think of it as lazily manifesting
1066       // virtually pre-existing constants.)
1067       return phase-&gt;zerocon(memory_type());
1068     }
1069 
1070     // A load from an initialization barrier can match a captured store.
1071     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1072       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1073       AllocateNode* alloc = init-&gt;allocation();
1074       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1075         // examine a captured store value
1076         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1077         if (st != NULL)
1078           continue;             // take one more trip around
1079       }
1080     }
1081 
1082     // Load boxed value from result of valueOf() call is input parameter.
1083     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1084         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1085       intptr_t ignore = 0;
1086       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1087       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1088           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1089           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1090           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1091         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1092       }
1093     }
1094 
1095     break;
1096   }
1097 
1098   return NULL;
1099 }
1100 
1101 //----------------------is_instance_field_load_with_local_phi------------------
1102 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1103   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1104       in(Address)-&gt;is_AddP() ) {
1105     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1106     // Only instances and boxed values.
1107     if( t_oop != NULL &amp;&amp;
1108         (t_oop-&gt;is_ptr_to_boxed_value() ||
1109          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1110         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1111         t_oop-&gt;offset() != Type::OffsetTop) {
1112       return true;
1113     }
1114   }
1115   return false;
1116 }
1117 
1118 //------------------------------Identity---------------------------------------
1119 // Loads are identity if previous store is to same address
1120 Node *LoadNode::Identity( PhaseTransform *phase ) {
1121   // If the previous store-maker is the right kind of Store, and the store is
1122   // to the same address, then we are equal to the value stored.
1123   Node* mem = in(Memory);
1124   Node* value = can_see_stored_value(mem, phase);
1125   if( value ) {
1126     // byte, short &amp; char stores truncate naturally.
1127     // A load has to load the truncated value which requires
1128     // some sort of masking operation and that requires an
1129     // Ideal call instead of an Identity call.
1130     if (memory_size() &lt; BytesPerInt) {
1131       // If the input to the store does not fit with the load's result type,
1132       // it must be truncated via an Ideal call.
1133       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1134         return this;
1135     }
1136     // (This works even when value is a Con, but LoadNode::Value
1137     // usually runs first, producing the singleton type of the Con.)
1138     return value;
1139   }
1140 
1141   // Search for an existing data phi which was generated before for the same
1142   // instance's field to avoid infinite generation of phis in a loop.
1143   Node *region = mem-&gt;in(0);
1144   if (is_instance_field_load_with_local_phi(region)) {
1145     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1146     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1147     int this_offset = addr_t-&gt;offset();
1148     int this_iid    = addr_t-&gt;instance_id();
1149     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1150          addr_t-&gt;is_ptr_to_boxed_value()) {
1151       // Use _idx of address base (could be Phi node) for boxed values.
1152       intptr_t   ignore = 0;
1153       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1154       this_iid = base-&gt;_idx;
1155     }
1156     const Type* this_type = bottom_type();
1157     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1158       Node* phi = region-&gt;fast_out(i);
1159       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1160           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1161         return phi;
1162       }
1163     }
1164   }
1165 
1166   return this;
1167 }
1168 
1169 // We're loading from an object which has autobox behaviour.
1170 // If this object is result of a valueOf call we'll have a phi
1171 // merging a newly allocated object and a load from the cache.
1172 // We want to replace this load with the original incoming
1173 // argument to the valueOf call.
1174 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1175   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1176   intptr_t ignore = 0;
1177   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1178   if ((base == NULL) || base-&gt;is_Phi()) {
1179     // Push the loads from the phi that comes from valueOf up
1180     // through it to allow elimination of the loads and the recovery
1181     // of the original value. It is done in split_through_phi().
1182     return NULL;
1183   } else if (base-&gt;is_Load() ||
1184              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1185     // Eliminate the load of boxed value for integer types from the cache
1186     // array by deriving the value from the index into the array.
1187     // Capture the offset of the load and then reverse the computation.
1188 
1189     // Get LoadN node which loads a boxing object from 'cache' array.
1190     if (base-&gt;is_DecodeN()) {
1191       base = base-&gt;in(1);
1192     }
1193     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1194       return NULL; // Complex address
1195     }
1196     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1197     Node* cache_base = address-&gt;in(AddPNode::Base);
1198     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1199       // Get ConP node which is static 'cache' field.
1200       cache_base = cache_base-&gt;in(1);
1201     }
1202     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1203       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1204       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1205         Node* elements[4];
1206         int shift = exact_log2(type2aelembytes(T_OBJECT));
1207         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1208         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1209             ((count == 1) ||
1210              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1211                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1212           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1213           // Fetch the box object cache[0] at the base of the array and get its value
1214           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1215           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1216           assert(ik-&gt;is_box_klass(), "sanity");
1217           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1218           if (ik-&gt;nof_nonstatic_fields() == 1) {
1219             // This should be true nonstatic_field_at requires calling
1220             // nof_nonstatic_fields so check it anyway
1221             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1222             BasicType bt = c.basic_type();
1223             // Only integer types have boxing cache.
1224             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1225                    bt == T_BYTE    || bt == T_SHORT ||
1226                    bt == T_INT     || bt == T_LONG, err_msg_res("wrong type = %s", type2name(bt)));
1227             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1228             if (cache_low != (int)cache_low) {
1229               return NULL; // should not happen since cache is array indexed by value
1230             }
1231             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1232             if (offset != (int)offset) {
1233               return NULL; // should not happen since cache is array indexed by value
1234             }
1235            // Add up all the offsets making of the address of the load
1236             Node* result = elements[0];
1237             for (int i = 1; i &lt; count; i++) {
1238               result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, elements[i]));
1239             }
1240             // Remove the constant offset from the address and then
1241             result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1242             // remove the scaling of the offset to recover the original index.
1243             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1244               // Peel the shift off directly but wrap it in a dummy node
1245               // since Ideal can't return existing nodes
1246               result = new (phase-&gt;C) RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1247             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1248                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1249                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1250               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1251               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1252               // (there is range check) so we do this optimizatrion by hand here.
1253               Node* add_con = new (phase-&gt;C) RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1254               result = new (phase-&gt;C) AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1255             } else {
1256               result = new (phase-&gt;C) RShiftXNode(result, phase-&gt;intcon(shift));
1257             }
1258 #ifdef _LP64
1259             if (bt != T_LONG) {
1260               result = new (phase-&gt;C) ConvL2INode(phase-&gt;transform(result));
1261             }
1262 #else
1263             if (bt == T_LONG) {
1264               result = new (phase-&gt;C) ConvI2LNode(phase-&gt;transform(result));
1265             }
1266 #endif
1267             return result;
1268           }
1269         }
1270       }
1271     }
1272   }
1273   return NULL;
1274 }
1275 
1276 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1277   Node* region = phi-&gt;in(0);
1278   if (region == NULL) {
1279     return false; // Wait stable graph
1280   }
1281   uint cnt = phi-&gt;req();
1282   for (uint i = 1; i &lt; cnt; i++) {
1283     Node* rc = region-&gt;in(i);
1284     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1285       return false; // Wait stable graph
1286     Node* in = phi-&gt;in(i);
1287     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1288       return false; // Wait stable graph
1289   }
1290   return true;
1291 }
1292 //------------------------------split_through_phi------------------------------
1293 // Split instance or boxed field load through Phi.
1294 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1295   Node* mem     = in(Memory);
1296   Node* address = in(Address);
1297   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1298 
1299   assert((t_oop != NULL) &amp;&amp;
1300          (t_oop-&gt;is_known_instance_field() ||
1301           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1302 
1303   Compile* C = phase-&gt;C;
1304   intptr_t ignore = 0;
1305   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1306   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1307   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1308                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1309                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1310 
1311   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1312         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1313     return NULL; // memory is not Phi
1314   }
1315 
1316   if (mem-&gt;is_Phi()) {
1317     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1318       return NULL; // Wait stable graph
1319     }
1320     uint cnt = mem-&gt;req();
1321     // Check for loop invariant memory.
1322     if (cnt == 3) {
1323       for (uint i = 1; i &lt; cnt; i++) {
1324         Node* in = mem-&gt;in(i);
1325         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1326         if (m == mem) {
1327           set_req(Memory, mem-&gt;in(cnt - i));
1328           return this; // made change
1329         }
1330       }
1331     }
1332   }
1333   if (base_is_phi) {
1334     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1335       return NULL; // Wait stable graph
1336     }
1337     uint cnt = base-&gt;req();
1338     // Check for loop invariant memory.
1339     if (cnt == 3) {
1340       for (uint i = 1; i &lt; cnt; i++) {
1341         if (base-&gt;in(i) == base) {
1342           return NULL; // Wait stable graph
1343         }
1344       }
1345     }
1346   }
1347 
1348   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1349 
1350   // Split through Phi (see original code in loopopts.cpp).
1351   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1352 
1353   // Do nothing here if Identity will find a value
1354   // (to avoid infinite chain of value phis generation).
1355   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1356     return NULL;
1357 
1358   // Select Region to split through.
1359   Node* region;
1360   if (!base_is_phi) {
1361     assert(mem-&gt;is_Phi(), "sanity");
1362     region = mem-&gt;in(0);
1363     // Skip if the region dominates some control edge of the address.
1364     if (!MemNode::all_controls_dominate(address, region))
1365       return NULL;
1366   } else if (!mem-&gt;is_Phi()) {
1367     assert(base_is_phi, "sanity");
1368     region = base-&gt;in(0);
1369     // Skip if the region dominates some control edge of the memory.
1370     if (!MemNode::all_controls_dominate(mem, region))
1371       return NULL;
1372   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1373     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1374     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1375       region = base-&gt;in(0);
1376     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1377       region = mem-&gt;in(0);
1378     } else {
1379       return NULL; // complex graph
1380     }
1381   } else {
1382     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1383     region = mem-&gt;in(0);
1384   }
1385 
1386   const Type* this_type = this-&gt;bottom_type();
1387   int this_index  = C-&gt;get_alias_index(t_oop);
1388   int this_offset = t_oop-&gt;offset();
1389   int this_iid    = t_oop-&gt;instance_id();
1390   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1391     // Use _idx of address base for boxed values.
1392     this_iid = base-&gt;_idx;
1393   }
1394   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1395   Node* phi = new (C) PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1396   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1397     Node* x;
1398     Node* the_clone = NULL;
1399     if (region-&gt;in(i) == C-&gt;top()) {
1400       x = C-&gt;top();      // Dead path?  Use a dead data op
1401     } else {
1402       x = this-&gt;clone();        // Else clone up the data op
1403       the_clone = x;            // Remember for possible deletion.
1404       // Alter data node to use pre-phi inputs
1405       if (this-&gt;in(0) == region) {
1406         x-&gt;set_req(0, region-&gt;in(i));
1407       } else {
1408         x-&gt;set_req(0, NULL);
1409       }
1410       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1411         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1412       }
1413       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1414         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1415       }
1416       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1417         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1418         Node* adr_x = phase-&gt;transform(new (C) AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1419         x-&gt;set_req(Address, adr_x);
1420       }
1421     }
1422     // Check for a 'win' on some paths
1423     const Type *t = x-&gt;Value(igvn);
1424 
1425     bool singleton = t-&gt;singleton();
1426 
1427     // See comments in PhaseIdealLoop::split_thru_phi().
1428     if (singleton &amp;&amp; t == Type::TOP) {
1429       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1430     }
1431 
1432     if (singleton) {
1433       x = igvn-&gt;makecon(t);
1434     } else {
1435       // We now call Identity to try to simplify the cloned node.
1436       // Note that some Identity methods call phase-&gt;type(this).
1437       // Make sure that the type array is big enough for
1438       // our new node, even though we may throw the node away.
1439       // (This tweaking with igvn only works because x is a new node.)
1440       igvn-&gt;set_type(x, t);
1441       // If x is a TypeNode, capture any more-precise type permanently into Node
1442       // otherwise it will be not updated during igvn-&gt;transform since
1443       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1444       x-&gt;raise_bottom_type(t);
1445       Node *y = x-&gt;Identity(igvn);
1446       if (y != x) {
1447         x = y;
1448       } else {
1449         y = igvn-&gt;hash_find_insert(x);
1450         if (y) {
1451           x = y;
1452         } else {
1453           // Else x is a new node we are keeping
1454           // We do not need register_new_node_with_optimizer
1455           // because set_type has already been called.
1456           igvn-&gt;_worklist.push(x);
1457         }
1458       }
1459     }
1460     if (x != the_clone &amp;&amp; the_clone != NULL) {
1461       igvn-&gt;remove_dead_node(the_clone);
1462     }
1463     phi-&gt;set_req(i, x);
1464   }
1465   // Record Phi
1466   igvn-&gt;register_new_node_with_optimizer(phi);
1467   return phi;
1468 }
1469 
1470 //------------------------------Ideal------------------------------------------
1471 // If the load is from Field memory and the pointer is non-null, we can
1472 // zero out the control input.
1473 // If the offset is constant and the base is an object allocation,
1474 // try to hook me up to the exact initializing store.
1475 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1476   Node* p = MemNode::Ideal_common(phase, can_reshape);
1477   if (p)  return (p == NodeSentinel) ? NULL : p;
1478 
1479   Node* ctrl    = in(MemNode::Control);
1480   Node* address = in(MemNode::Address);
1481 
1482   // Skip up past a SafePoint control.  Cannot do this for Stores because
1483   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1484   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1485       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1486     ctrl = ctrl-&gt;in(0);
1487     set_req(MemNode::Control,ctrl);
1488   }
1489 
1490   intptr_t ignore = 0;
1491   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1492   if (base != NULL
1493       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1494     // Check for useless control edge in some common special cases
1495     if (in(MemNode::Control) != NULL
1496         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1497         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1498       // A method-invariant, non-null address (constant or 'this' argument).
1499       set_req(MemNode::Control, NULL);
1500     }
1501   }
1502 
1503   Node* mem = in(MemNode::Memory);
1504   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1505 
1506   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1507     // try to optimize our memory input
1508     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1509     if (opt_mem != mem) {
1510       set_req(MemNode::Memory, opt_mem);
1511       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1512       return this;
1513     }
1514     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1515     if ((t_oop != NULL) &amp;&amp;
1516         (t_oop-&gt;is_known_instance_field() ||
1517          t_oop-&gt;is_ptr_to_boxed_value())) {
1518       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1519       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1520         // Delay this transformation until memory Phi is processed.
1521         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1522         return NULL;
1523       }
1524       // Split instance field load through Phi.
1525       Node* result = split_through_phi(phase);
1526       if (result != NULL) return result;
1527 
1528       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1529         Node* result = eliminate_autobox(phase);
1530         if (result != NULL) return result;
1531       }
1532     }
1533   }
1534 
1535   // Check for prior store with a different base or offset; make Load
1536   // independent.  Skip through any number of them.  Bail out if the stores
1537   // are in an endless dead cycle and report no progress.  This is a key
1538   // transform for Reflection.  However, if after skipping through the Stores
1539   // we can't then fold up against a prior store do NOT do the transform as
1540   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1541   // array memory alive twice: once for the hoisted Load and again after the
1542   // bypassed Store.  This situation only works if EVERYBODY who does
1543   // anti-dependence work knows how to bypass.  I.e. we need all
1544   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1545   // the alias index stuff.  So instead, peek through Stores and IFF we can
1546   // fold up, do so.
1547   Node* prev_mem = find_previous_store(phase);
1548   // Steps (a), (b):  Walk past independent stores to find an exact match.
1549   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1550     // (c) See if we can fold up on the spot, but don't fold up here.
1551     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1552     // just return a prior value, which is done by Identity calls.
1553     if (can_see_stored_value(prev_mem, phase)) {
1554       // Make ready for step (d):
1555       set_req(MemNode::Memory, prev_mem);
1556       return this;
1557     }
1558   }
1559 
1560   return NULL;                  // No further progress
1561 }
1562 
1563 // Helper to recognize certain Klass fields which are invariant across
1564 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1565 const Type*
1566 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1567                                  ciKlass* klass) const {
1568   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1569     // The field is Klass::_modifier_flags.  Return its (constant) value.
1570     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1571     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1572     return TypeInt::make(klass-&gt;modifier_flags());
1573   }
1574   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1575     // The field is Klass::_access_flags.  Return its (constant) value.
1576     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1577     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1578     return TypeInt::make(klass-&gt;access_flags());
1579   }
1580   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1581     // The field is Klass::_layout_helper.  Return its constant value if known.
1582     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1583     return TypeInt::make(klass-&gt;layout_helper());
1584   }
1585 
1586   // No match.
1587   return NULL;
1588 }
1589 
1590 // Try to constant-fold a stable array element.
1591 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1592   assert(ary-&gt;is_stable(), "array should be stable");
1593 
1594   if (ary-&gt;const_oop() != NULL) {
1595     // Decode the results of GraphKit::array_element_address.
1596     ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1597     ciConstant con = aobj-&gt;element_value_by_offset(off);
1598 
1599     if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1600       const Type* con_type = Type::make_from_constant(con);
1601       if (con_type != NULL) {
1602         if (con_type-&gt;isa_aryptr()) {
1603           // Join with the array element type, in case it is also stable.
1604           int dim = ary-&gt;stable_dimension();
1605           con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1606         }
1607         if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1608           con_type = con_type-&gt;make_narrowoop();
1609         }
1610 #ifndef PRODUCT
1611         if (TraceIterativeGVN) {
1612           tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1613           con_type-&gt;dump(); tty-&gt;cr();
1614         }
1615 #endif //PRODUCT
1616         return con_type;
1617       }
1618     }
1619   }
1620 
1621   return NULL;
1622 }
1623 
1624 //------------------------------Value-----------------------------------------
1625 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1626   // Either input is TOP ==&gt; the result is TOP
1627   Node* mem = in(MemNode::Memory);
1628   const Type *t1 = phase-&gt;type(mem);
1629   if (t1 == Type::TOP)  return Type::TOP;
1630   Node* adr = in(MemNode::Address);
1631   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1632   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1633   int off = tp-&gt;offset();
1634   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1635   Compile* C = phase-&gt;C;
1636 
1637   // Try to guess loaded type from pointer type
1638   if (tp-&gt;isa_aryptr()) {
1639     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1640     const Type *t = ary-&gt;elem();
1641 
1642     // Determine whether the reference is beyond the header or not, by comparing
1643     // the offset against the offset of the start of the array's data.
1644     // Different array types begin at slightly different offsets (12 vs. 16).
1645     // We choose T_BYTE as an example base type that is least restrictive
1646     // as to alignment, which will therefore produce the smallest
1647     // possible base offset.
1648     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1649     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1650 
1651     // Try to constant-fold a stable array element.
1652     if (FoldStableValues &amp;&amp; ary-&gt;is_stable()) {
1653       // Make sure the reference is not into the header
1654       if (off_beyond_header &amp;&amp; off != Type::OffsetBot) {
1655         assert(adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Offset)-&gt;is_Con(), "offset is a constant");
1656         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1657         if (con_type != NULL) {
1658           return con_type;
1659         }
1660       }
1661     }
1662 
1663     // Don't do this for integer types. There is only potential profit if
1664     // the element type t is lower than _type; that is, for int types, if _type is
1665     // more restrictive than t.  This only happens here if one is short and the other
1666     // char (both 16 bits), and in those cases we've made an intentional decision
1667     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1668     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1669     //
1670     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1671     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1672     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1673     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1674     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1675     // In fact, that could have been the original type of p1, and p1 could have
1676     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1677     // expression (LShiftL quux 3) independently optimized to the constant 8.
1678     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1679         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1680         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1681       // t might actually be lower than _type, if _type is a unique
1682       // concrete subclass of abstract class t.
1683       if (off_beyond_header) {  // is the offset beyond the header?
1684         const Type* jt = t-&gt;join(_type);
1685         // In any case, do not allow the join, per se, to empty out the type.
1686         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1687           // This can happen if a interface-typed array narrows to a class type.
1688           jt = _type;
1689         }
1690 #ifdef ASSERT
1691         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1692           // The pointers in the autobox arrays are always non-null
1693           Node* base = adr-&gt;in(AddPNode::Base);
1694           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1695             // Get LoadN node which loads IntegerCache.cache field
1696             base = base-&gt;in(1);
1697           }
1698           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1699             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1700             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1701               // It could be narrow oop
1702               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1703             }
1704           }
1705         }
1706 #endif
1707         return jt;
1708       }
1709     }
1710   } else if (tp-&gt;base() == Type::InstPtr) {
1711     ciEnv* env = C-&gt;env();
1712     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1713     ciKlass* klass = tinst-&gt;klass();
1714     assert( off != Type::OffsetBot ||
1715             // arrays can be cast to Objects
1716             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1717             // unsafe field access may not have a constant offset
1718             C-&gt;has_unsafe_access(),
1719             "Field accesses must be precise" );
1720     // For oop loads, we expect the _type to be precise
1721     if (klass == env-&gt;String_klass() &amp;&amp;
1722         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1723       // For constant Strings treat the final fields as compile time constants.
1724       Node* base = adr-&gt;in(AddPNode::Base);
1725       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1726       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1727         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1728         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1729           ciObject* string = t-&gt;const_oop();
1730           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1731           if (constant.basic_type() == T_INT) {
1732             return TypeInt::make(constant.as_int());
1733           } else if (constant.basic_type() == T_ARRAY) {
1734             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1735               return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1736             } else {
1737               return TypeOopPtr::make_from_constant(constant.as_object(), true);
1738             }
1739           }
1740         }
1741       }
1742     }
1743     // Optimizations for constant objects
1744     ciObject* const_oop = tinst-&gt;const_oop();
1745     if (const_oop != NULL) {
1746       // For constant Boxed value treat the target field as a compile time constant.
1747       if (tinst-&gt;is_ptr_to_boxed_value()) {
1748         return tinst-&gt;get_const_boxed_value();
1749       } else
1750       // For constant CallSites treat the target field as a compile time constant.
1751       if (const_oop-&gt;is_call_site()) {
1752         ciCallSite* call_site = const_oop-&gt;as_call_site();
1753         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1754         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1755           ciMethodHandle* target = call_site-&gt;get_target();
1756           if (target != NULL) {  // just in case
1757             ciConstant constant(T_OBJECT, target);
1758             const Type* t;
1759             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1760               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1761             } else {
1762               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1763             }
1764             // Add a dependence for invalidation of the optimization.
1765             if (!call_site-&gt;is_constant_call_site()) {
<a name="1" id="anc1"></a>
1766               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1767             }
1768             return t;
1769           }
1770         }
1771       }
1772     }
1773   } else if (tp-&gt;base() == Type::KlassPtr) {
1774     assert( off != Type::OffsetBot ||
1775             // arrays can be cast to Objects
1776             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1777             // also allow array-loading from the primary supertype
1778             // array during subtype checks
1779             Opcode() == Op_LoadKlass,
1780             "Field accesses must be precise" );
1781     // For klass/static loads, we expect the _type to be precise
1782   }
1783 
1784   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1785   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1786     ciKlass* klass = tkls-&gt;klass();
1787     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1788       // We are loading a field from a Klass metaobject whose identity
1789       // is known at compile time (the type is "exact" or "precise").
1790       // Check for fields we know are maintained as constants by the VM.
1791       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1792         // The field is Klass::_super_check_offset.  Return its (constant) value.
1793         // (Folds up type checking code.)
1794         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1795         return TypeInt::make(klass-&gt;super_check_offset());
1796       }
1797       // Compute index into primary_supers array
1798       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1799       // Check for overflowing; use unsigned compare to handle the negative case.
1800       if( depth &lt; ciKlass::primary_super_limit() ) {
1801         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1802         // (Folds up type checking code.)
1803         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1804         ciKlass *ss = klass-&gt;super_of_depth(depth);
1805         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1806       }
1807       const Type* aift = load_array_final_field(tkls, klass);
1808       if (aift != NULL)  return aift;
1809       if (tkls-&gt;offset() == in_bytes(ArrayKlass::component_mirror_offset())
1810           &amp;&amp; klass-&gt;is_array_klass()) {
1811         // The field is ArrayKlass::_component_mirror.  Return its (constant) value.
1812         // (Folds up aClassConstant.getComponentType, common in Arrays.copyOf.)
1813         assert(Opcode() == Op_LoadP, "must load an oop from _component_mirror");
1814         return TypeInstPtr::make(klass-&gt;as_array_klass()-&gt;component_mirror());
1815       }
1816       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1817         // The field is Klass::_java_mirror.  Return its (constant) value.
1818         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1819         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1820         return TypeInstPtr::make(klass-&gt;java_mirror());
1821       }
1822     }
1823 
1824     // We can still check if we are loading from the primary_supers array at a
1825     // shallow enough depth.  Even though the klass is not exact, entries less
1826     // than or equal to its super depth are correct.
1827     if (klass-&gt;is_loaded() ) {
1828       ciType *inner = klass;
1829       while( inner-&gt;is_obj_array_klass() )
1830         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1831       if( inner-&gt;is_instance_klass() &amp;&amp;
1832           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1833         // Compute index into primary_supers array
1834         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1835         // Check for overflowing; use unsigned compare to handle the negative case.
1836         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1837             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1838           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1839           // (Folds up type checking code.)
1840           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1841           ciKlass *ss = klass-&gt;super_of_depth(depth);
1842           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1843         }
1844       }
1845     }
1846 
1847     // If the type is enough to determine that the thing is not an array,
1848     // we can give the layout_helper a positive interval type.
1849     // This will help short-circuit some reflective code.
1850     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1851         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1852         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1853         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1854         ) {
1855       // Note:  When interfaces are reliable, we can narrow the interface
1856       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1857       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1858       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1859       // The key property of this type is that it folds up tests
1860       // for array-ness, since it proves that the layout_helper is positive.
1861       // Thus, a generic value like the basic object layout helper works fine.
1862       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1863     }
1864   }
1865 
1866   // If we are loading from a freshly-allocated object, produce a zero,
1867   // if the load is provably beyond the header of the object.
1868   // (Also allow a variable load from a fresh array to produce zero.)
1869   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1870   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1871   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1872   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1873     Node* value = can_see_stored_value(mem,phase);
1874     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1875       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1876       return value-&gt;bottom_type();
1877     }
1878   }
1879 
1880   if (is_instance) {
1881     // If we have an instance type and our memory input is the
1882     // programs's initial memory state, there is no matching store,
1883     // so just return a zero of the appropriate type
1884     Node *mem = in(MemNode::Memory);
1885     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1886       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1887       return Type::get_zero_type(_type-&gt;basic_type());
1888     }
1889   }
1890   return _type;
1891 }
1892 
1893 //------------------------------match_edge-------------------------------------
1894 // Do we Match on this edge index or not?  Match only the address.
1895 uint LoadNode::match_edge(uint idx) const {
1896   return idx == MemNode::Address;
1897 }
1898 
1899 //--------------------------LoadBNode::Ideal--------------------------------------
1900 //
1901 //  If the previous store is to the same address as this load,
1902 //  and the value stored was larger than a byte, replace this load
1903 //  with the value stored truncated to a byte.  If no truncation is
1904 //  needed, the replacement is done in LoadNode::Identity().
1905 //
1906 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1907   Node* mem = in(MemNode::Memory);
1908   Node* value = can_see_stored_value(mem,phase);
1909   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1910     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(24)) );
1911     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(24));
1912   }
1913   // Identity call will handle the case where truncation is not needed.
1914   return LoadNode::Ideal(phase, can_reshape);
1915 }
1916 
1917 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1918   Node* mem = in(MemNode::Memory);
1919   Node* value = can_see_stored_value(mem,phase);
1920   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1921       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1922     // If the input to the store does not fit with the load's result type,
1923     // it must be truncated. We can't delay until Ideal call since
1924     // a singleton Value is needed for split_thru_phi optimization.
1925     int con = value-&gt;get_int();
1926     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1927   }
1928   return LoadNode::Value(phase);
1929 }
1930 
1931 //--------------------------LoadUBNode::Ideal-------------------------------------
1932 //
1933 //  If the previous store is to the same address as this load,
1934 //  and the value stored was larger than a byte, replace this load
1935 //  with the value stored truncated to a byte.  If no truncation is
1936 //  needed, the replacement is done in LoadNode::Identity().
1937 //
1938 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1939   Node* mem = in(MemNode::Memory);
1940   Node* value = can_see_stored_value(mem, phase);
1941   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1942     return new (phase-&gt;C) AndINode(value, phase-&gt;intcon(0xFF));
1943   // Identity call will handle the case where truncation is not needed.
1944   return LoadNode::Ideal(phase, can_reshape);
1945 }
1946 
1947 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1948   Node* mem = in(MemNode::Memory);
1949   Node* value = can_see_stored_value(mem,phase);
1950   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1951       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1952     // If the input to the store does not fit with the load's result type,
1953     // it must be truncated. We can't delay until Ideal call since
1954     // a singleton Value is needed for split_thru_phi optimization.
1955     int con = value-&gt;get_int();
1956     return TypeInt::make(con &amp; 0xFF);
1957   }
1958   return LoadNode::Value(phase);
1959 }
1960 
1961 //--------------------------LoadUSNode::Ideal-------------------------------------
1962 //
1963 //  If the previous store is to the same address as this load,
1964 //  and the value stored was larger than a char, replace this load
1965 //  with the value stored truncated to a char.  If no truncation is
1966 //  needed, the replacement is done in LoadNode::Identity().
1967 //
1968 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1969   Node* mem = in(MemNode::Memory);
1970   Node* value = can_see_stored_value(mem,phase);
1971   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1972     return new (phase-&gt;C) AndINode(value,phase-&gt;intcon(0xFFFF));
1973   // Identity call will handle the case where truncation is not needed.
1974   return LoadNode::Ideal(phase, can_reshape);
1975 }
1976 
1977 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1978   Node* mem = in(MemNode::Memory);
1979   Node* value = can_see_stored_value(mem,phase);
1980   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1981       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1982     // If the input to the store does not fit with the load's result type,
1983     // it must be truncated. We can't delay until Ideal call since
1984     // a singleton Value is needed for split_thru_phi optimization.
1985     int con = value-&gt;get_int();
1986     return TypeInt::make(con &amp; 0xFFFF);
1987   }
1988   return LoadNode::Value(phase);
1989 }
1990 
1991 //--------------------------LoadSNode::Ideal--------------------------------------
1992 //
1993 //  If the previous store is to the same address as this load,
1994 //  and the value stored was larger than a short, replace this load
1995 //  with the value stored truncated to a short.  If no truncation is
1996 //  needed, the replacement is done in LoadNode::Identity().
1997 //
1998 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1999   Node* mem = in(MemNode::Memory);
2000   Node* value = can_see_stored_value(mem,phase);
2001   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
2002     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(16)) );
2003     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(16));
2004   }
2005   // Identity call will handle the case where truncation is not needed.
2006   return LoadNode::Ideal(phase, can_reshape);
2007 }
2008 
2009 const Type* LoadSNode::Value(PhaseTransform *phase) const {
2010   Node* mem = in(MemNode::Memory);
2011   Node* value = can_see_stored_value(mem,phase);
2012   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2013       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2014     // If the input to the store does not fit with the load's result type,
2015     // it must be truncated. We can't delay until Ideal call since
2016     // a singleton Value is needed for split_thru_phi optimization.
2017     int con = value-&gt;get_int();
2018     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2019   }
2020   return LoadNode::Value(phase);
2021 }
2022 
2023 //=============================================================================
2024 //----------------------------LoadKlassNode::make------------------------------
2025 // Polymorphic factory method:
2026 Node *LoadKlassNode::make( PhaseGVN&amp; gvn, Node *mem, Node *adr, const TypePtr* at, const TypeKlassPtr *tk ) {
2027   Compile* C = gvn.C;
2028   Node *ctl = NULL;
2029   // sanity check the alias category against the created node type
2030   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2031   assert(adr_type != NULL, "expecting TypeKlassPtr");
2032 #ifdef _LP64
2033   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2034     assert(UseCompressedClassPointers, "no compressed klasses");
2035     Node* load_klass = gvn.transform(new (C) LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass()));
2036     return new (C) DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2037   }
2038 #endif
2039   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2040   return new (C) LoadKlassNode(ctl, mem, adr, at, tk);
2041 }
2042 
2043 //------------------------------Value------------------------------------------
2044 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2045   return klass_value_common(phase);
2046 }
2047 
2048 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2049   // Either input is TOP ==&gt; the result is TOP
2050   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2051   if (t1 == Type::TOP)  return Type::TOP;
2052   Node *adr = in(MemNode::Address);
2053   const Type *t2 = phase-&gt;type( adr );
2054   if (t2 == Type::TOP)  return Type::TOP;
2055   const TypePtr *tp = t2-&gt;is_ptr();
2056   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2057       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2058 
2059   // Return a more precise klass, if possible
2060   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2061   if (tinst != NULL) {
2062     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2063     int offset = tinst-&gt;offset();
2064     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2065         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2066             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2067       // We are loading a special hidden field from a Class mirror object,
2068       // the field which points to the VM's Klass metaobject.
2069       ciType* t = tinst-&gt;java_mirror_type();
2070       // java_mirror_type returns non-null for compile-time Class constants.
2071       if (t != NULL) {
2072         // constant oop =&gt; constant klass
2073         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2074           if (t-&gt;is_void()) {
2075             // We cannot create a void array.  Since void is a primitive type return null
2076             // klass.  Users of this result need to do a null check on the returned klass.
2077             return TypePtr::NULL_PTR;
2078           }
2079           return TypeKlassPtr::make(ciArrayKlass::make(t));
2080         }
2081         if (!t-&gt;is_klass()) {
2082           // a primitive Class (e.g., int.class) has NULL for a klass field
2083           return TypePtr::NULL_PTR;
2084         }
2085         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2086         return TypeKlassPtr::make(t-&gt;as_klass());
2087       }
2088       // non-constant mirror, so we can't tell what's going on
2089     }
2090     if( !ik-&gt;is_loaded() )
2091       return _type;             // Bail out if not loaded
2092     if (offset == oopDesc::klass_offset_in_bytes()) {
2093       if (tinst-&gt;klass_is_exact()) {
2094         return TypeKlassPtr::make(ik);
2095       }
2096       // See if we can become precise: no subklasses and no interface
2097       // (Note:  We need to support verified interfaces.)
2098       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2099         //assert(!UseExactTypes, "this code should be useless with exact types");
2100         // Add a dependence; if any subclass added we need to recompile
2101         if (!ik-&gt;is_final()) {
2102           // %%% should use stronger assert_unique_concrete_subtype instead
2103           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2104         }
2105         // Return precise klass
2106         return TypeKlassPtr::make(ik);
2107       }
2108 
2109       // Return root of possible klass
2110       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2111     }
2112   }
2113 
2114   // Check for loading klass from an array
2115   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2116   if( tary != NULL ) {
2117     ciKlass *tary_klass = tary-&gt;klass();
2118     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2119         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2120       if (tary-&gt;klass_is_exact()) {
2121         return TypeKlassPtr::make(tary_klass);
2122       }
2123       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2124       // If the klass is an object array, we defer the question to the
2125       // array component klass.
2126       if( ak-&gt;is_obj_array_klass() ) {
2127         assert( ak-&gt;is_loaded(), "" );
2128         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2129         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2130           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2131           // See if we can become precise: no subklasses and no interface
2132           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2133             //assert(!UseExactTypes, "this code should be useless with exact types");
2134             // Add a dependence; if any subclass added we need to recompile
2135             if (!ik-&gt;is_final()) {
2136               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2137             }
2138             // Return precise array klass
2139             return TypeKlassPtr::make(ak);
2140           }
2141         }
2142         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2143       } else {                  // Found a type-array?
2144         //assert(!UseExactTypes, "this code should be useless with exact types");
2145         assert( ak-&gt;is_type_array_klass(), "" );
2146         return TypeKlassPtr::make(ak); // These are always precise
2147       }
2148     }
2149   }
2150 
2151   // Check for loading klass from an array klass
2152   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2153   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2154     ciKlass* klass = tkls-&gt;klass();
2155     if( !klass-&gt;is_loaded() )
2156       return _type;             // Bail out if not loaded
2157     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2158         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2159       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2160       // // Always returning precise element type is incorrect,
2161       // // e.g., element type could be object and array may contain strings
2162       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2163 
2164       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2165       // according to the element type's subclassing.
2166       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2167     }
2168     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2169         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2170       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2171       // The field is Klass::_super.  Return its (constant) value.
2172       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2173       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2174     }
2175   }
2176 
2177   // Bailout case
2178   return LoadNode::Value(phase);
2179 }
2180 
2181 //------------------------------Identity---------------------------------------
2182 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2183 // Also feed through the klass in Allocate(...klass...)._klass.
2184 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2185   return klass_identity_common(phase);
2186 }
2187 
2188 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2189   Node* x = LoadNode::Identity(phase);
2190   if (x != this)  return x;
2191 
2192   // Take apart the address into an oop and and offset.
2193   // Return 'this' if we cannot.
2194   Node*    adr    = in(MemNode::Address);
2195   intptr_t offset = 0;
2196   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2197   if (base == NULL)     return this;
2198   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2199   if (toop == NULL)     return this;
2200 
2201   // We can fetch the klass directly through an AllocateNode.
2202   // This works even if the klass is not constant (clone or newArray).
2203   if (offset == oopDesc::klass_offset_in_bytes()) {
2204     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2205     if (allocated_klass != NULL) {
2206       return allocated_klass;
2207     }
2208   }
2209 
2210   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2211   // Simplify ak.component_mirror.array_klass to plain ak, ak an ArrayKlass.
2212   // See inline_native_Class_query for occurrences of these patterns.
2213   // Java Example:  x.getClass().isAssignableFrom(y)
2214   // Java Example:  Array.newInstance(x.getClass().getComponentType(), n)
2215   //
2216   // This improves reflective code, often making the Class
2217   // mirror go completely dead.  (Current exception:  Class
2218   // mirrors may appear in debug info, but we could clean them out by
2219   // introducing a new debug info operator for Klass*.java_mirror).
2220   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2221       &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2222           offset == java_lang_Class::array_klass_offset_in_bytes())) {
2223     // We are loading a special hidden field from a Class mirror,
2224     // the field which points to its Klass or ArrayKlass metaobject.
2225     if (base-&gt;is_Load()) {
2226       Node* adr2 = base-&gt;in(MemNode::Address);
2227       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2228       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2229           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2230               tkls-&gt;klass()-&gt;is_array_klass())
2231           &amp;&amp; adr2-&gt;is_AddP()
2232           ) {
2233         int mirror_field = in_bytes(Klass::java_mirror_offset());
2234         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2235           mirror_field = in_bytes(ArrayKlass::component_mirror_offset());
2236         }
2237         if (tkls-&gt;offset() == mirror_field) {
2238           return adr2-&gt;in(AddPNode::Base);
2239         }
2240       }
2241     }
2242   }
2243 
2244   return this;
2245 }
2246 
2247 
2248 //------------------------------Value------------------------------------------
2249 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2250   const Type *t = klass_value_common(phase);
2251   if (t == Type::TOP)
2252     return t;
2253 
2254   return t-&gt;make_narrowklass();
2255 }
2256 
2257 //------------------------------Identity---------------------------------------
2258 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2259 // Also feed through the klass in Allocate(...klass...)._klass.
2260 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2261   Node *x = klass_identity_common(phase);
2262 
2263   const Type *t = phase-&gt;type( x );
2264   if( t == Type::TOP ) return x;
2265   if( t-&gt;isa_narrowklass()) return x;
2266   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2267 
2268   return phase-&gt;transform(new (phase-&gt;C) EncodePKlassNode(x, t-&gt;make_narrowklass()));
2269 }
2270 
2271 //------------------------------Value-----------------------------------------
2272 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2273   // Either input is TOP ==&gt; the result is TOP
2274   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2275   if( t1 == Type::TOP ) return Type::TOP;
2276   Node *adr = in(MemNode::Address);
2277   const Type *t2 = phase-&gt;type( adr );
2278   if( t2 == Type::TOP ) return Type::TOP;
2279   const TypePtr *tp = t2-&gt;is_ptr();
2280   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2281   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2282   if( !tap ) return _type;
2283   return tap-&gt;size();
2284 }
2285 
2286 //-------------------------------Ideal---------------------------------------
2287 // Feed through the length in AllocateArray(...length...)._length.
2288 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2289   Node* p = MemNode::Ideal_common(phase, can_reshape);
2290   if (p)  return (p == NodeSentinel) ? NULL : p;
2291 
2292   // Take apart the address into an oop and and offset.
2293   // Return 'this' if we cannot.
2294   Node*    adr    = in(MemNode::Address);
2295   intptr_t offset = 0;
2296   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2297   if (base == NULL)     return NULL;
2298   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2299   if (tary == NULL)     return NULL;
2300 
2301   // We can fetch the length directly through an AllocateArrayNode.
2302   // This works even if the length is not constant (clone or newArray).
2303   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2304     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2305     if (alloc != NULL) {
2306       Node* allocated_length = alloc-&gt;Ideal_length();
2307       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2308       if (allocated_length != len) {
2309         // New CastII improves on this.
2310         return len;
2311       }
2312     }
2313   }
2314 
2315   return NULL;
2316 }
2317 
2318 //------------------------------Identity---------------------------------------
2319 // Feed through the length in AllocateArray(...length...)._length.
2320 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2321   Node* x = LoadINode::Identity(phase);
2322   if (x != this)  return x;
2323 
2324   // Take apart the address into an oop and and offset.
2325   // Return 'this' if we cannot.
2326   Node*    adr    = in(MemNode::Address);
2327   intptr_t offset = 0;
2328   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2329   if (base == NULL)     return this;
2330   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2331   if (tary == NULL)     return this;
2332 
2333   // We can fetch the length directly through an AllocateArrayNode.
2334   // This works even if the length is not constant (clone or newArray).
2335   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2336     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2337     if (alloc != NULL) {
2338       Node* allocated_length = alloc-&gt;Ideal_length();
2339       // Do not allow make_ideal_length to allocate a CastII node.
2340       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2341       if (allocated_length == len) {
2342         // Return allocated_length only if it would not be improved by a CastII.
2343         return allocated_length;
2344       }
2345     }
2346   }
2347 
2348   return this;
2349 
2350 }
2351 
2352 //=============================================================================
2353 //---------------------------StoreNode::make-----------------------------------
2354 // Polymorphic factory method:
2355 StoreNode* StoreNode::make( PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt ) {
2356   Compile* C = gvn.C;
2357   assert( C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2358           ctl != NULL, "raw memory operations should have control edge");
2359 
2360   switch (bt) {
2361   case T_BOOLEAN:
2362   case T_BYTE:    return new (C) StoreBNode(ctl, mem, adr, adr_type, val);
2363   case T_INT:     return new (C) StoreINode(ctl, mem, adr, adr_type, val);
2364   case T_CHAR:
2365   case T_SHORT:   return new (C) StoreCNode(ctl, mem, adr, adr_type, val);
2366   case T_LONG:    return new (C) StoreLNode(ctl, mem, adr, adr_type, val);
2367   case T_FLOAT:   return new (C) StoreFNode(ctl, mem, adr, adr_type, val);
2368   case T_DOUBLE:  return new (C) StoreDNode(ctl, mem, adr, adr_type, val);
2369   case T_METADATA:
2370   case T_ADDRESS:
2371   case T_OBJECT:
2372 #ifdef _LP64
2373     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2374       val = gvn.transform(new (C) EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2375       return new (C) StoreNNode(ctl, mem, adr, adr_type, val);
2376     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2377                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2378                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2379       val = gvn.transform(new (C) EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2380       return new (C) StoreNKlassNode(ctl, mem, adr, adr_type, val);
2381     }
2382 #endif
2383     {
2384       return new (C) StorePNode(ctl, mem, adr, adr_type, val);
2385     }
2386   }
2387   ShouldNotReachHere();
2388   return (StoreNode*)NULL;
2389 }
2390 
2391 StoreLNode* StoreLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val) {
2392   bool require_atomic = true;
2393   return new (C) StoreLNode(ctl, mem, adr, adr_type, val, require_atomic);
2394 }
2395 
2396 
2397 //--------------------------bottom_type----------------------------------------
2398 const Type *StoreNode::bottom_type() const {
2399   return Type::MEMORY;
2400 }
2401 
2402 //------------------------------hash-------------------------------------------
2403 uint StoreNode::hash() const {
2404   // unroll addition of interesting fields
2405   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2406 
2407   // Since they are not commoned, do not hash them:
2408   return NO_HASH;
2409 }
2410 
2411 //------------------------------Ideal------------------------------------------
2412 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2413 // When a store immediately follows a relevant allocation/initialization,
2414 // try to capture it into the initialization, or hoist it above.
2415 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2416   Node* p = MemNode::Ideal_common(phase, can_reshape);
2417   if (p)  return (p == NodeSentinel) ? NULL : p;
2418 
2419   Node* mem     = in(MemNode::Memory);
2420   Node* address = in(MemNode::Address);
2421 
2422   // Back-to-back stores to same address?  Fold em up.  Generally
2423   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2424   // since they must follow each StoreP operation.  Redundant StoreCMs
2425   // are eliminated just before matching in final_graph_reshape.
2426   if (mem-&gt;is_Store() &amp;&amp; mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2427       mem-&gt;Opcode() != Op_StoreCM) {
2428     // Looking at a dead closed cycle of memory?
2429     assert(mem != mem-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2430 
2431     assert(Opcode() == mem-&gt;Opcode() ||
2432            phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw,
2433            "no mismatched stores, except on raw memory");
2434 
2435     if (mem-&gt;outcnt() == 1 &amp;&amp;           // check for intervening uses
2436         mem-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2437       // If anybody other than 'this' uses 'mem', we cannot fold 'mem' away.
2438       // For example, 'mem' might be the final state at a conditional return.
2439       // Or, 'mem' might be used by some node which is live at the same time
2440       // 'this' is live, which might be unschedulable.  So, require exactly
2441       // ONE user, the 'this' store, until such time as we clone 'mem' for
2442       // each of 'mem's uses (thus making the exactly-1-user-rule hold true).
2443       if (can_reshape) {  // (%%% is this an anachronism?)
2444         set_req_X(MemNode::Memory, mem-&gt;in(MemNode::Memory),
2445                   phase-&gt;is_IterGVN());
2446       } else {
2447         // It's OK to do this in the parser, since DU info is always accurate,
2448         // and the parser always refers to nodes via SafePointNode maps.
2449         set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2450       }
2451       return this;
2452     }
2453   }
2454 
2455   // Capture an unaliased, unconditional, simple store into an initializer.
2456   // Or, if it is independent of the allocation, hoist it above the allocation.
2457   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2458       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2459     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2460     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2461     if (offset &gt; 0) {
2462       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2463       // If the InitializeNode captured me, it made a raw copy of me,
2464       // and I need to disappear.
2465       if (moved != NULL) {
2466         // %%% hack to ensure that Ideal returns a new node:
2467         mem = MergeMemNode::make(phase-&gt;C, mem);
2468         return mem;             // fold me away
2469       }
2470     }
2471   }
2472 
2473   return NULL;                  // No further progress
2474 }
2475 
2476 //------------------------------Value-----------------------------------------
2477 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2478   // Either input is TOP ==&gt; the result is TOP
2479   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2480   if( t1 == Type::TOP ) return Type::TOP;
2481   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2482   if( t2 == Type::TOP ) return Type::TOP;
2483   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2484   if( t3 == Type::TOP ) return Type::TOP;
2485   return Type::MEMORY;
2486 }
2487 
2488 //------------------------------Identity---------------------------------------
2489 // Remove redundant stores:
2490 //   Store(m, p, Load(m, p)) changes to m.
2491 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2492 Node *StoreNode::Identity( PhaseTransform *phase ) {
2493   Node* mem = in(MemNode::Memory);
2494   Node* adr = in(MemNode::Address);
2495   Node* val = in(MemNode::ValueIn);
2496 
2497   // Load then Store?  Then the Store is useless
2498   if (val-&gt;is_Load() &amp;&amp;
2499       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2500       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2501       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2502     return mem;
2503   }
2504 
2505   // Two stores in a row of the same value?
2506   if (mem-&gt;is_Store() &amp;&amp;
2507       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2508       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2509       mem-&gt;Opcode() == Opcode()) {
2510     return mem;
2511   }
2512 
2513   // Store of zero anywhere into a freshly-allocated object?
2514   // Then the store is useless.
2515   // (It must already have been captured by the InitializeNode.)
2516   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2517     // a newly allocated object is already all-zeroes everywhere
2518     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2519       return mem;
2520     }
2521 
2522     // the store may also apply to zero-bits in an earlier object
2523     Node* prev_mem = find_previous_store(phase);
2524     // Steps (a), (b):  Walk past independent stores to find an exact match.
2525     if (prev_mem != NULL) {
2526       Node* prev_val = can_see_stored_value(prev_mem, phase);
2527       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2528         // prev_val and val might differ by a cast; it would be good
2529         // to keep the more informative of the two.
2530         return mem;
2531       }
2532     }
2533   }
2534 
2535   return this;
2536 }
2537 
2538 //------------------------------match_edge-------------------------------------
2539 // Do we Match on this edge index or not?  Match only memory &amp; value
2540 uint StoreNode::match_edge(uint idx) const {
2541   return idx == MemNode::Address || idx == MemNode::ValueIn;
2542 }
2543 
2544 //------------------------------cmp--------------------------------------------
2545 // Do not common stores up together.  They generally have to be split
2546 // back up anyways, so do not bother.
2547 uint StoreNode::cmp( const Node &amp;n ) const {
2548   return (&amp;n == this);          // Always fail except on self
2549 }
2550 
2551 //------------------------------Ideal_masked_input-----------------------------
2552 // Check for a useless mask before a partial-word store
2553 // (StoreB ... (AndI valIn conIa) )
2554 // If (conIa &amp; mask == mask) this simplifies to
2555 // (StoreB ... (valIn) )
2556 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2557   Node *val = in(MemNode::ValueIn);
2558   if( val-&gt;Opcode() == Op_AndI ) {
2559     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2560     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2561       set_req(MemNode::ValueIn, val-&gt;in(1));
2562       return this;
2563     }
2564   }
2565   return NULL;
2566 }
2567 
2568 
2569 //------------------------------Ideal_sign_extended_input----------------------
2570 // Check for useless sign-extension before a partial-word store
2571 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2572 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2573 // (StoreB ... (valIn) )
2574 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2575   Node *val = in(MemNode::ValueIn);
2576   if( val-&gt;Opcode() == Op_RShiftI ) {
2577     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2578     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2579       Node *shl = val-&gt;in(1);
2580       if( shl-&gt;Opcode() == Op_LShiftI ) {
2581         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2582         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2583           set_req(MemNode::ValueIn, shl-&gt;in(1));
2584           return this;
2585         }
2586       }
2587     }
2588   }
2589   return NULL;
2590 }
2591 
2592 //------------------------------value_never_loaded-----------------------------------
2593 // Determine whether there are any possible loads of the value stored.
2594 // For simplicity, we actually check if there are any loads from the
2595 // address stored to, not just for loads of the value stored by this node.
2596 //
2597 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2598   Node *adr = in(Address);
2599   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2600   if (adr_oop == NULL)
2601     return false;
2602   if (!adr_oop-&gt;is_known_instance_field())
2603     return false; // if not a distinct instance, there may be aliases of the address
2604   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2605     Node *use = adr-&gt;fast_out(i);
2606     int opc = use-&gt;Opcode();
2607     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2608       return false;
2609     }
2610   }
2611   return true;
2612 }
2613 
2614 //=============================================================================
2615 //------------------------------Ideal------------------------------------------
2616 // If the store is from an AND mask that leaves the low bits untouched, then
2617 // we can skip the AND operation.  If the store is from a sign-extension
2618 // (a left shift, then right shift) we can skip both.
2619 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2620   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2621   if( progress != NULL ) return progress;
2622 
2623   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2624   if( progress != NULL ) return progress;
2625 
2626   // Finally check the default case
2627   return StoreNode::Ideal(phase, can_reshape);
2628 }
2629 
2630 //=============================================================================
2631 //------------------------------Ideal------------------------------------------
2632 // If the store is from an AND mask that leaves the low bits untouched, then
2633 // we can skip the AND operation
2634 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2635   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2636   if( progress != NULL ) return progress;
2637 
2638   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2639   if( progress != NULL ) return progress;
2640 
2641   // Finally check the default case
2642   return StoreNode::Ideal(phase, can_reshape);
2643 }
2644 
2645 //=============================================================================
2646 //------------------------------Identity---------------------------------------
2647 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2648   // No need to card mark when storing a null ptr
2649   Node* my_store = in(MemNode::OopStore);
2650   if (my_store-&gt;is_Store()) {
2651     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2652     if( t1 == TypePtr::NULL_PTR ) {
2653       return in(MemNode::Memory);
2654     }
2655   }
2656   return this;
2657 }
2658 
2659 //=============================================================================
2660 //------------------------------Ideal---------------------------------------
2661 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2662   Node* progress = StoreNode::Ideal(phase, can_reshape);
2663   if (progress != NULL) return progress;
2664 
2665   Node* my_store = in(MemNode::OopStore);
2666   if (my_store-&gt;is_MergeMem()) {
2667     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2668     set_req(MemNode::OopStore, mem);
2669     return this;
2670   }
2671 
2672   return NULL;
2673 }
2674 
2675 //------------------------------Value-----------------------------------------
2676 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2677   // Either input is TOP ==&gt; the result is TOP
2678   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2679   if( t == Type::TOP ) return Type::TOP;
2680   t = phase-&gt;type( in(MemNode::Address) );
2681   if( t == Type::TOP ) return Type::TOP;
2682   t = phase-&gt;type( in(MemNode::ValueIn) );
2683   if( t == Type::TOP ) return Type::TOP;
2684   // If extra input is TOP ==&gt; the result is TOP
2685   t = phase-&gt;type( in(MemNode::OopStore) );
2686   if( t == Type::TOP ) return Type::TOP;
2687 
2688   return StoreNode::Value( phase );
2689 }
2690 
2691 
2692 //=============================================================================
2693 //----------------------------------SCMemProjNode------------------------------
2694 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2695 {
2696   return bottom_type();
2697 }
2698 
2699 //=============================================================================
2700 //----------------------------------LoadStoreNode------------------------------
2701 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2702   : Node(required),
2703     _type(rt),
2704     _adr_type(at)
2705 {
2706   init_req(MemNode::Control, c  );
2707   init_req(MemNode::Memory , mem);
2708   init_req(MemNode::Address, adr);
2709   init_req(MemNode::ValueIn, val);
2710   init_class_id(Class_LoadStore);
2711 }
2712 
2713 uint LoadStoreNode::ideal_reg() const {
2714   return _type-&gt;ideal_reg();
2715 }
2716 
2717 bool LoadStoreNode::result_not_used() const {
2718   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2719     Node *x = fast_out(i);
2720     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2721     return false;
2722   }
2723   return true;
2724 }
2725 
2726 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2727 
2728 //=============================================================================
2729 //----------------------------------LoadStoreConditionalNode--------------------
2730 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2731   init_req(ExpectedIn, ex );
2732 }
2733 
2734 //=============================================================================
2735 //-------------------------------adr_type--------------------------------------
2736 // Do we Match on this edge index or not?  Do not match memory
2737 const TypePtr* ClearArrayNode::adr_type() const {
2738   Node *adr = in(3);
2739   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2740 }
2741 
2742 //------------------------------match_edge-------------------------------------
2743 // Do we Match on this edge index or not?  Do not match memory
2744 uint ClearArrayNode::match_edge(uint idx) const {
2745   return idx &gt; 1;
2746 }
2747 
2748 //------------------------------Identity---------------------------------------
2749 // Clearing a zero length array does nothing
2750 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2751   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2752 }
2753 
2754 //------------------------------Idealize---------------------------------------
2755 // Clearing a short array is faster with stores
2756 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2757   const int unit = BytesPerLong;
2758   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2759   if (!t)  return NULL;
2760   if (!t-&gt;is_con())  return NULL;
2761   intptr_t raw_count = t-&gt;get_con();
2762   intptr_t size = raw_count;
2763   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2764   // Clearing nothing uses the Identity call.
2765   // Negative clears are possible on dead ClearArrays
2766   // (see jck test stmt114.stmt11402.val).
2767   if (size &lt;= 0 || size % unit != 0)  return NULL;
2768   intptr_t count = size / unit;
2769   // Length too long; use fast hardware clear
2770   if (size &gt; Matcher::init_array_short_size)  return NULL;
2771   Node *mem = in(1);
2772   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2773   Node *adr = in(3);
2774   const Type* at = phase-&gt;type(adr);
2775   if( at==Type::TOP ) return NULL;
2776   const TypePtr* atp = at-&gt;isa_ptr();
2777   // adjust atp to be the correct array element address type
2778   if (atp == NULL)  atp = TypePtr::BOTTOM;
2779   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2780   // Get base for derived pointer purposes
2781   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2782   Node *base = adr-&gt;in(1);
2783 
2784   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2785   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2786   mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero);
2787   count--;
2788   while( count-- ) {
2789     mem = phase-&gt;transform(mem);
2790     adr = phase-&gt;transform(new (phase-&gt;C) AddPNode(base,adr,off));
2791     mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero);
2792   }
2793   return mem;
2794 }
2795 
2796 //----------------------------step_through----------------------------------
2797 // Return allocation input memory edge if it is different instance
2798 // or itself if it is the one we are looking for.
2799 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2800   Node* n = *np;
2801   assert(n-&gt;is_ClearArray(), "sanity");
2802   intptr_t offset;
2803   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2804   // This method is called only before Allocate nodes are expanded during
2805   // macro nodes expansion. Before that ClearArray nodes are only generated
2806   // in LibraryCallKit::generate_arraycopy() which follows allocations.
2807   assert(alloc != NULL, "should have allocation");
2808   if (alloc-&gt;_idx == instance_id) {
2809     // Can not bypass initialization of the instance we are looking for.
2810     return false;
2811   }
2812   // Otherwise skip it.
2813   InitializeNode* init = alloc-&gt;initialization();
2814   if (init != NULL)
2815     *np = init-&gt;in(TypeFunc::Memory);
2816   else
2817     *np = alloc-&gt;in(TypeFunc::Memory);
2818   return true;
2819 }
2820 
2821 //----------------------------clear_memory-------------------------------------
2822 // Generate code to initialize object storage to zero.
2823 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2824                                    intptr_t start_offset,
2825                                    Node* end_offset,
2826                                    PhaseGVN* phase) {
2827   Compile* C = phase-&gt;C;
2828   intptr_t offset = start_offset;
2829 
2830   int unit = BytesPerLong;
2831   if ((offset % unit) != 0) {
2832     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2833     adr = phase-&gt;transform(adr);
2834     const TypePtr* atp = TypeRawPtr::BOTTOM;
2835     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT);
2836     mem = phase-&gt;transform(mem);
2837     offset += BytesPerInt;
2838   }
2839   assert((offset % unit) == 0, "");
2840 
2841   // Initialize the remaining stuff, if any, with a ClearArray.
2842   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2843 }
2844 
2845 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2846                                    Node* start_offset,
2847                                    Node* end_offset,
2848                                    PhaseGVN* phase) {
2849   if (start_offset == end_offset) {
2850     // nothing to do
2851     return mem;
2852   }
2853 
2854   Compile* C = phase-&gt;C;
2855   int unit = BytesPerLong;
2856   Node* zbase = start_offset;
2857   Node* zend  = end_offset;
2858 
2859   // Scale to the unit required by the CPU:
2860   if (!Matcher::init_array_count_is_in_bytes) {
2861     Node* shift = phase-&gt;intcon(exact_log2(unit));
2862     zbase = phase-&gt;transform( new(C) URShiftXNode(zbase, shift) );
2863     zend  = phase-&gt;transform( new(C) URShiftXNode(zend,  shift) );
2864   }
2865 
2866   // Bulk clear double-words
2867   Node* zsize = phase-&gt;transform( new(C) SubXNode(zend, zbase) );
2868   Node* adr = phase-&gt;transform( new(C) AddPNode(dest, dest, start_offset) );
2869   mem = new (C) ClearArrayNode(ctl, mem, zsize, adr);
2870   return phase-&gt;transform(mem);
2871 }
2872 
2873 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2874                                    intptr_t start_offset,
2875                                    intptr_t end_offset,
2876                                    PhaseGVN* phase) {
2877   if (start_offset == end_offset) {
2878     // nothing to do
2879     return mem;
2880   }
2881 
2882   Compile* C = phase-&gt;C;
2883   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2884   intptr_t done_offset = end_offset;
2885   if ((done_offset % BytesPerLong) != 0) {
2886     done_offset -= BytesPerInt;
2887   }
2888   if (done_offset &gt; start_offset) {
2889     mem = clear_memory(ctl, mem, dest,
2890                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2891   }
2892   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2893     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2894     adr = phase-&gt;transform(adr);
2895     const TypePtr* atp = TypeRawPtr::BOTTOM;
2896     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT);
2897     mem = phase-&gt;transform(mem);
2898     done_offset += BytesPerInt;
2899   }
2900   assert(done_offset == end_offset, "");
2901   return mem;
2902 }
2903 
2904 //=============================================================================
2905 // Do not match memory edge.
2906 uint StrIntrinsicNode::match_edge(uint idx) const {
2907   return idx == 2 || idx == 3;
2908 }
2909 
2910 //------------------------------Ideal------------------------------------------
2911 // Return a node which is more "ideal" than the current node.  Strip out
2912 // control copies
2913 Node *StrIntrinsicNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2914   if (remove_dead_region(phase, can_reshape)) return this;
2915   // Don't bother trying to transform a dead node
2916   if (in(0) &amp;&amp; in(0)-&gt;is_top())  return NULL;
2917 
2918   if (can_reshape) {
2919     Node* mem = phase-&gt;transform(in(MemNode::Memory));
2920     // If transformed to a MergeMem, get the desired slice
2921     uint alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
2922     mem = mem-&gt;is_MergeMem() ? mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : mem;
2923     if (mem != in(MemNode::Memory)) {
2924       set_req(MemNode::Memory, mem);
2925       return this;
2926     }
2927   }
2928   return NULL;
2929 }
2930 
2931 //------------------------------Value------------------------------------------
2932 const Type *StrIntrinsicNode::Value( PhaseTransform *phase ) const {
2933   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2934   return bottom_type();
2935 }
2936 
2937 //=============================================================================
2938 //------------------------------match_edge-------------------------------------
2939 // Do not match memory edge
2940 uint EncodeISOArrayNode::match_edge(uint idx) const {
2941   return idx == 2 || idx == 3; // EncodeISOArray src (Binary dst len)
2942 }
2943 
2944 //------------------------------Ideal------------------------------------------
2945 // Return a node which is more "ideal" than the current node.  Strip out
2946 // control copies
2947 Node *EncodeISOArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2948   return remove_dead_region(phase, can_reshape) ? this : NULL;
2949 }
2950 
2951 //------------------------------Value------------------------------------------
2952 const Type *EncodeISOArrayNode::Value(PhaseTransform *phase) const {
2953   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2954   return bottom_type();
2955 }
2956 
2957 //=============================================================================
2958 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2959   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2960     _adr_type(C-&gt;get_adr_type(alias_idx))
2961 {
2962   init_class_id(Class_MemBar);
2963   Node* top = C-&gt;top();
2964   init_req(TypeFunc::I_O,top);
2965   init_req(TypeFunc::FramePtr,top);
2966   init_req(TypeFunc::ReturnAdr,top);
2967   if (precedent != NULL)
2968     init_req(TypeFunc::Parms, precedent);
2969 }
2970 
2971 //------------------------------cmp--------------------------------------------
2972 uint MemBarNode::hash() const { return NO_HASH; }
2973 uint MemBarNode::cmp( const Node &amp;n ) const {
2974   return (&amp;n == this);          // Always fail except on self
2975 }
2976 
2977 //------------------------------make-------------------------------------------
2978 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2979   switch (opcode) {
2980   case Op_MemBarAcquire:   return new(C) MemBarAcquireNode(C,  atp, pn);
2981   case Op_MemBarRelease:   return new(C) MemBarReleaseNode(C,  atp, pn);
2982   case Op_MemBarAcquireLock: return new(C) MemBarAcquireLockNode(C,  atp, pn);
2983   case Op_MemBarReleaseLock: return new(C) MemBarReleaseLockNode(C,  atp, pn);
2984   case Op_MemBarVolatile:  return new(C) MemBarVolatileNode(C, atp, pn);
2985   case Op_MemBarCPUOrder:  return new(C) MemBarCPUOrderNode(C, atp, pn);
2986   case Op_Initialize:      return new(C) InitializeNode(C,     atp, pn);
2987   case Op_MemBarStoreStore: return new(C) MemBarStoreStoreNode(C,  atp, pn);
2988   default:                 ShouldNotReachHere(); return NULL;
2989   }
2990 }
2991 
2992 //------------------------------Ideal------------------------------------------
2993 // Return a node which is more "ideal" than the current node.  Strip out
2994 // control copies
2995 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2996   if (remove_dead_region(phase, can_reshape)) return this;
2997   // Don't bother trying to transform a dead node
2998   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
2999     return NULL;
3000   }
3001 
3002   // Eliminate volatile MemBars for scalar replaced objects.
3003   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3004     bool eliminate = false;
3005     int opc = Opcode();
3006     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3007       // Volatile field loads and stores.
3008       Node* my_mem = in(MemBarNode::Precedent);
3009       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3010       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3011         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3012         // replace this Precedent (decodeN) with the Load instead.
3013         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3014           Node* load_node = my_mem-&gt;in(1);
3015           set_req(MemBarNode::Precedent, load_node);
3016           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3017           my_mem = load_node;
3018         } else {
3019           assert(my_mem-&gt;unique_out() == this, "sanity");
3020           del_req(Precedent);
3021           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
3022           my_mem = NULL;
3023         }
3024       }
3025       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
3026         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
3027         // Check for scalar replaced object reference.
3028         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
3029             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
3030             t_oop-&gt;offset() != Type::OffsetTop) {
3031           eliminate = true;
3032         }
3033       }
3034     } else if (opc == Op_MemBarRelease) {
3035       // Final field stores.
3036       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
3037       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
3038           alloc-&gt;as_Allocate()-&gt;_is_non_escaping) {
3039         // The allocated object does not escape.
3040         eliminate = true;
3041       }
3042     }
3043     if (eliminate) {
3044       // Replace MemBar projections by its inputs.
3045       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3046       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3047       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3048       // Must return either the original node (now dead) or a new node
3049       // (Do not return a top here, since that would break the uniqueness of top.)
3050       return new (phase-&gt;C) ConINode(TypeInt::ZERO);
3051     }
3052   }
3053   return NULL;
3054 }
3055 
3056 //------------------------------Value------------------------------------------
3057 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3058   if( !in(0) ) return Type::TOP;
3059   if( phase-&gt;type(in(0)) == Type::TOP )
3060     return Type::TOP;
3061   return TypeTuple::MEMBAR;
3062 }
3063 
3064 //------------------------------match------------------------------------------
3065 // Construct projections for memory.
3066 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3067   switch (proj-&gt;_con) {
3068   case TypeFunc::Control:
3069   case TypeFunc::Memory:
3070     return new (m-&gt;C) MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3071   }
3072   ShouldNotReachHere();
3073   return NULL;
3074 }
3075 
3076 //===========================InitializeNode====================================
3077 // SUMMARY:
3078 // This node acts as a memory barrier on raw memory, after some raw stores.
3079 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3080 // The Initialize can 'capture' suitably constrained stores as raw inits.
3081 // It can coalesce related raw stores into larger units (called 'tiles').
3082 // It can avoid zeroing new storage for memory units which have raw inits.
3083 // At macro-expansion, it is marked 'complete', and does not optimize further.
3084 //
3085 // EXAMPLE:
3086 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3087 //   ctl = incoming control; mem* = incoming memory
3088 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3089 // First allocate uninitialized memory and fill in the header:
3090 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3091 //   ctl := alloc.Control; mem* := alloc.Memory*
3092 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3093 // Then initialize to zero the non-header parts of the raw memory block:
3094 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3095 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3096 // After the initialize node executes, the object is ready for service:
3097 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3098 // Suppose its body is immediately initialized as {1,2}:
3099 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3100 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3101 //   mem.SLICE(#short[*]) := store2
3102 //
3103 // DETAILS:
3104 // An InitializeNode collects and isolates object initialization after
3105 // an AllocateNode and before the next possible safepoint.  As a
3106 // memory barrier (MemBarNode), it keeps critical stores from drifting
3107 // down past any safepoint or any publication of the allocation.
3108 // Before this barrier, a newly-allocated object may have uninitialized bits.
3109 // After this barrier, it may be treated as a real oop, and GC is allowed.
3110 //
3111 // The semantics of the InitializeNode include an implicit zeroing of
3112 // the new object from object header to the end of the object.
3113 // (The object header and end are determined by the AllocateNode.)
3114 //
3115 // Certain stores may be added as direct inputs to the InitializeNode.
3116 // These stores must update raw memory, and they must be to addresses
3117 // derived from the raw address produced by AllocateNode, and with
3118 // a constant offset.  They must be ordered by increasing offset.
3119 // The first one is at in(RawStores), the last at in(req()-1).
3120 // Unlike most memory operations, they are not linked in a chain,
3121 // but are displayed in parallel as users of the rawmem output of
3122 // the allocation.
3123 //
3124 // (See comments in InitializeNode::capture_store, which continue
3125 // the example given above.)
3126 //
3127 // When the associated Allocate is macro-expanded, the InitializeNode
3128 // may be rewritten to optimize collected stores.  A ClearArrayNode
3129 // may also be created at that point to represent any required zeroing.
3130 // The InitializeNode is then marked 'complete', prohibiting further
3131 // capturing of nearby memory operations.
3132 //
3133 // During macro-expansion, all captured initializations which store
3134 // constant values of 32 bits or smaller are coalesced (if advantageous)
3135 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3136 // initialized in fewer memory operations.  Memory words which are
3137 // covered by neither tiles nor non-constant stores are pre-zeroed
3138 // by explicit stores of zero.  (The code shape happens to do all
3139 // zeroing first, then all other stores, with both sequences occurring
3140 // in order of ascending offsets.)
3141 //
3142 // Alternatively, code may be inserted between an AllocateNode and its
3143 // InitializeNode, to perform arbitrary initialization of the new object.
3144 // E.g., the object copying intrinsics insert complex data transfers here.
3145 // The initialization must then be marked as 'complete' disable the
3146 // built-in zeroing semantics and the collection of initializing stores.
3147 //
3148 // While an InitializeNode is incomplete, reads from the memory state
3149 // produced by it are optimizable if they match the control edge and
3150 // new oop address associated with the allocation/initialization.
3151 // They return a stored value (if the offset matches) or else zero.
3152 // A write to the memory state, if it matches control and address,
3153 // and if it is to a constant offset, may be 'captured' by the
3154 // InitializeNode.  It is cloned as a raw memory operation and rewired
3155 // inside the initialization, to the raw oop produced by the allocation.
3156 // Operations on addresses which are provably distinct (e.g., to
3157 // other AllocateNodes) are allowed to bypass the initialization.
3158 //
3159 // The effect of all this is to consolidate object initialization
3160 // (both arrays and non-arrays, both piecewise and bulk) into a
3161 // single location, where it can be optimized as a unit.
3162 //
3163 // Only stores with an offset less than TrackedInitializationLimit words
3164 // will be considered for capture by an InitializeNode.  This puts a
3165 // reasonable limit on the complexity of optimized initializations.
3166 
3167 //---------------------------InitializeNode------------------------------------
3168 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3169   : _is_complete(Incomplete), _does_not_escape(false),
3170     MemBarNode(C, adr_type, rawoop)
3171 {
3172   init_class_id(Class_Initialize);
3173 
3174   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3175   assert(in(RawAddress) == rawoop, "proper init");
3176   // Note:  allocation() can be NULL, for secondary initialization barriers
3177 }
3178 
3179 // Since this node is not matched, it will be processed by the
3180 // register allocator.  Declare that there are no constraints
3181 // on the allocation of the RawAddress edge.
3182 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3183   // This edge should be set to top, by the set_complete.  But be conservative.
3184   if (idx == InitializeNode::RawAddress)
3185     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3186   return RegMask::Empty;
3187 }
3188 
3189 Node* InitializeNode::memory(uint alias_idx) {
3190   Node* mem = in(Memory);
3191   if (mem-&gt;is_MergeMem()) {
3192     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3193   } else {
3194     // incoming raw memory is not split
3195     return mem;
3196   }
3197 }
3198 
3199 bool InitializeNode::is_non_zero() {
3200   if (is_complete())  return false;
3201   remove_extra_zeroes();
3202   return (req() &gt; RawStores);
3203 }
3204 
3205 void InitializeNode::set_complete(PhaseGVN* phase) {
3206   assert(!is_complete(), "caller responsibility");
3207   _is_complete = Complete;
3208 
3209   // After this node is complete, it contains a bunch of
3210   // raw-memory initializations.  There is no need for
3211   // it to have anything to do with non-raw memory effects.
3212   // Therefore, tell all non-raw users to re-optimize themselves,
3213   // after skipping the memory effects of this initialization.
3214   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3215   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3216 }
3217 
3218 // convenience function
3219 // return false if the init contains any stores already
3220 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3221   InitializeNode* init = initialization();
3222   if (init == NULL || init-&gt;is_complete())  return false;
3223   init-&gt;remove_extra_zeroes();
3224   // for now, if this allocation has already collected any inits, bail:
3225   if (init-&gt;is_non_zero())  return false;
3226   init-&gt;set_complete(phase);
3227   return true;
3228 }
3229 
3230 void InitializeNode::remove_extra_zeroes() {
3231   if (req() == RawStores)  return;
3232   Node* zmem = zero_memory();
3233   uint fill = RawStores;
3234   for (uint i = fill; i &lt; req(); i++) {
3235     Node* n = in(i);
3236     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3237     if (fill &lt; i)  set_req(fill, n);          // compact
3238     ++fill;
3239   }
3240   // delete any empty spaces created:
3241   while (fill &lt; req()) {
3242     del_req(fill);
3243   }
3244 }
3245 
3246 // Helper for remembering which stores go with which offsets.
3247 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3248   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3249   intptr_t offset = -1;
3250   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3251                                                phase, offset);
3252   if (base == NULL)     return -1;  // something is dead,
3253   if (offset &lt; 0)       return -1;  //        dead, dead
3254   return offset;
3255 }
3256 
3257 // Helper for proving that an initialization expression is
3258 // "simple enough" to be folded into an object initialization.
3259 // Attempts to prove that a store's initial value 'n' can be captured
3260 // within the initialization without creating a vicious cycle, such as:
3261 //     { Foo p = new Foo(); p.next = p; }
3262 // True for constants and parameters and small combinations thereof.
3263 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3264   if (n == NULL)      return true;   // (can this really happen?)
3265   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3266   if (n == this)      return false;  // found a cycle
3267   if (n-&gt;is_Con())    return true;
3268   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3269   if (n-&gt;is_Root())   return true;   // even better
3270 
3271   Node* ctl = n-&gt;in(0);
3272   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3273     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3274     if (ctl == this)  return false;
3275 
3276     // If we already know that the enclosing memory op is pinned right after
3277     // the init, then any control flow that the store has picked up
3278     // must have preceded the init, or else be equal to the init.
3279     // Even after loop optimizations (which might change control edges)
3280     // a store is never pinned *before* the availability of its inputs.
3281     if (!MemNode::all_controls_dominate(n, this))
3282       return false;                  // failed to prove a good control
3283   }
3284 
3285   // Check data edges for possible dependencies on 'this'.
3286   if ((count += 1) &gt; 20)  return false;  // complexity limit
3287   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3288     Node* m = n-&gt;in(i);
3289     if (m == NULL || m == n || m-&gt;is_top())  continue;
3290     uint first_i = n-&gt;find_edge(m);
3291     if (i != first_i)  continue;  // process duplicate edge just once
3292     if (!detect_init_independence(m, count)) {
3293       return false;
3294     }
3295   }
3296 
3297   return true;
3298 }
3299 
3300 // Here are all the checks a Store must pass before it can be moved into
3301 // an initialization.  Returns zero if a check fails.
3302 // On success, returns the (constant) offset to which the store applies,
3303 // within the initialized memory.
3304 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3305   const int FAIL = 0;
3306   if (st-&gt;req() != MemNode::ValueIn + 1)
3307     return FAIL;                // an inscrutable StoreNode (card mark?)
3308   Node* ctl = st-&gt;in(MemNode::Control);
3309   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3310     return FAIL;                // must be unconditional after the initialization
3311   Node* mem = st-&gt;in(MemNode::Memory);
3312   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3313     return FAIL;                // must not be preceded by other stores
3314   Node* adr = st-&gt;in(MemNode::Address);
3315   intptr_t offset;
3316   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3317   if (alloc == NULL)
3318     return FAIL;                // inscrutable address
3319   if (alloc != allocation())
3320     return FAIL;                // wrong allocation!  (store needs to float up)
3321   Node* val = st-&gt;in(MemNode::ValueIn);
3322   int complexity_count = 0;
3323   if (!detect_init_independence(val, complexity_count))
3324     return FAIL;                // stored value must be 'simple enough'
3325 
3326   // The Store can be captured only if nothing after the allocation
3327   // and before the Store is using the memory location that the store
3328   // overwrites.
3329   bool failed = false;
3330   // If is_complete_with_arraycopy() is true the shape of the graph is
3331   // well defined and is safe so no need for extra checks.
3332   if (!is_complete_with_arraycopy()) {
3333     // We are going to look at each use of the memory state following
3334     // the allocation to make sure nothing reads the memory that the
3335     // Store writes.
3336     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3337     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3338     ResourceMark rm;
3339     Unique_Node_List mems;
3340     mems.push(mem);
3341     Node* unique_merge = NULL;
3342     for (uint next = 0; next &lt; mems.size(); ++next) {
3343       Node *m  = mems.at(next);
3344       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3345         Node *n = m-&gt;fast_out(j);
3346         if (n-&gt;outcnt() == 0) {
3347           continue;
3348         }
3349         if (n == st) {
3350           continue;
3351         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3352           // If the control of this use is different from the control
3353           // of the Store which is right after the InitializeNode then
3354           // this node cannot be between the InitializeNode and the
3355           // Store.
3356           continue;
3357         } else if (n-&gt;is_MergeMem()) {
3358           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3359             // We can hit a MergeMemNode (that will likely go away
3360             // later) that is a direct use of the memory state
3361             // following the InitializeNode on the same slice as the
3362             // store node that we'd like to capture. We need to check
3363             // the uses of the MergeMemNode.
3364             mems.push(n);
3365           }
3366         } else if (n-&gt;is_Mem()) {
3367           Node* other_adr = n-&gt;in(MemNode::Address);
3368           if (other_adr == adr) {
3369             failed = true;
3370             break;
3371           } else {
3372             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3373             if (other_t_adr != NULL) {
3374               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3375               if (other_alias_idx == alias_idx) {
3376                 // A load from the same memory slice as the store right
3377                 // after the InitializeNode. We check the control of the
3378                 // object/array that is loaded from. If it's the same as
3379                 // the store control then we cannot capture the store.
3380                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3381                 Node* base = other_adr;
3382                 assert(base-&gt;is_AddP(), err_msg_res("should be addp but is %s", base-&gt;Name()));
3383                 base = base-&gt;in(AddPNode::Base);
3384                 if (base != NULL) {
3385                   base = base-&gt;uncast();
3386                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3387                     failed = true;
3388                     break;
3389                   }
3390                 }
3391               }
3392             }
3393           }
3394         } else {
3395           failed = true;
3396           break;
3397         }
3398       }
3399     }
3400   }
3401   if (failed) {
3402     if (!can_reshape) {
3403       // We decided we couldn't capture the store during parsing. We
3404       // should try again during the next IGVN once the graph is
3405       // cleaner.
3406       phase-&gt;C-&gt;record_for_igvn(st);
3407     }
3408     return FAIL;
3409   }
3410 
3411   return offset;                // success
3412 }
3413 
3414 // Find the captured store in(i) which corresponds to the range
3415 // [start..start+size) in the initialized object.
3416 // If there is one, return its index i.  If there isn't, return the
3417 // negative of the index where it should be inserted.
3418 // Return 0 if the queried range overlaps an initialization boundary
3419 // or if dead code is encountered.
3420 // If size_in_bytes is zero, do not bother with overlap checks.
3421 int InitializeNode::captured_store_insertion_point(intptr_t start,
3422                                                    int size_in_bytes,
3423                                                    PhaseTransform* phase) {
3424   const int FAIL = 0, MAX_STORE = BytesPerLong;
3425 
3426   if (is_complete())
3427     return FAIL;                // arraycopy got here first; punt
3428 
3429   assert(allocation() != NULL, "must be present");
3430 
3431   // no negatives, no header fields:
3432   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3433 
3434   // after a certain size, we bail out on tracking all the stores:
3435   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3436   if (start &gt;= ti_limit)  return FAIL;
3437 
3438   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3439     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3440 
3441     Node*    st     = in(i);
3442     intptr_t st_off = get_store_offset(st, phase);
3443     if (st_off &lt; 0) {
3444       if (st != zero_memory()) {
3445         return FAIL;            // bail out if there is dead garbage
3446       }
3447     } else if (st_off &gt; start) {
3448       // ...we are done, since stores are ordered
3449       if (st_off &lt; start + size_in_bytes) {
3450         return FAIL;            // the next store overlaps
3451       }
3452       return -(int)i;           // not found; here is where to put it
3453     } else if (st_off &lt; start) {
3454       if (size_in_bytes != 0 &amp;&amp;
3455           start &lt; st_off + MAX_STORE &amp;&amp;
3456           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3457         return FAIL;            // the previous store overlaps
3458       }
3459     } else {
3460       if (size_in_bytes != 0 &amp;&amp;
3461           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3462         return FAIL;            // mismatched store size
3463       }
3464       return i;
3465     }
3466 
3467     ++i;
3468   }
3469 }
3470 
3471 // Look for a captured store which initializes at the offset 'start'
3472 // with the given size.  If there is no such store, and no other
3473 // initialization interferes, then return zero_memory (the memory
3474 // projection of the AllocateNode).
3475 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3476                                           PhaseTransform* phase) {
3477   assert(stores_are_sane(phase), "");
3478   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3479   if (i == 0) {
3480     return NULL;                // something is dead
3481   } else if (i &lt; 0) {
3482     return zero_memory();       // just primordial zero bits here
3483   } else {
3484     Node* st = in(i);           // here is the store at this position
3485     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3486     return st;
3487   }
3488 }
3489 
3490 // Create, as a raw pointer, an address within my new object at 'offset'.
3491 Node* InitializeNode::make_raw_address(intptr_t offset,
3492                                        PhaseTransform* phase) {
3493   Node* addr = in(RawAddress);
3494   if (offset != 0) {
3495     Compile* C = phase-&gt;C;
3496     addr = phase-&gt;transform( new (C) AddPNode(C-&gt;top(), addr,
3497                                                  phase-&gt;MakeConX(offset)) );
3498   }
3499   return addr;
3500 }
3501 
3502 // Clone the given store, converting it into a raw store
3503 // initializing a field or element of my new object.
3504 // Caller is responsible for retiring the original store,
3505 // with subsume_node or the like.
3506 //
3507 // From the example above InitializeNode::InitializeNode,
3508 // here are the old stores to be captured:
3509 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3510 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3511 //
3512 // Here is the changed code; note the extra edges on init:
3513 //   alloc = (Allocate ...)
3514 //   rawoop = alloc.RawAddress
3515 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3516 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3517 //   init = (Initialize alloc.Control alloc.Memory rawoop
3518 //                      rawstore1 rawstore2)
3519 //
3520 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3521                                     PhaseTransform* phase, bool can_reshape) {
3522   assert(stores_are_sane(phase), "");
3523 
3524   if (start &lt; 0)  return NULL;
3525   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3526 
3527   Compile* C = phase-&gt;C;
3528   int size_in_bytes = st-&gt;memory_size();
3529   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3530   if (i == 0)  return NULL;     // bail out
3531   Node* prev_mem = NULL;        // raw memory for the captured store
3532   if (i &gt; 0) {
3533     prev_mem = in(i);           // there is a pre-existing store under this one
3534     set_req(i, C-&gt;top());       // temporarily disconnect it
3535     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3536   } else {
3537     i = -i;                     // no pre-existing store
3538     prev_mem = zero_memory();   // a slice of the newly allocated object
3539     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3540       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3541     else
3542       ins_req(i, C-&gt;top());     // build a new edge
3543   }
3544   Node* new_st = st-&gt;clone();
3545   new_st-&gt;set_req(MemNode::Control, in(Control));
3546   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3547   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3548   new_st = phase-&gt;transform(new_st);
3549 
3550   // At this point, new_st might have swallowed a pre-existing store
3551   // at the same offset, or perhaps new_st might have disappeared,
3552   // if it redundantly stored the same value (or zero to fresh memory).
3553 
3554   // In any case, wire it in:
3555   set_req(i, new_st);
3556 
3557   // The caller may now kill the old guy.
3558   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3559   assert(check_st == new_st || check_st == NULL, "must be findable");
3560   assert(!is_complete(), "");
3561   return new_st;
3562 }
3563 
3564 static bool store_constant(jlong* tiles, int num_tiles,
3565                            intptr_t st_off, int st_size,
3566                            jlong con) {
3567   if ((st_off &amp; (st_size-1)) != 0)
3568     return false;               // strange store offset (assume size==2**N)
3569   address addr = (address)tiles + st_off;
3570   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3571   switch (st_size) {
3572   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3573   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3574   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3575   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3576   default: return false;        // strange store size (detect size!=2**N here)
3577   }
3578   return true;                  // return success to caller
3579 }
3580 
3581 // Coalesce subword constants into int constants and possibly
3582 // into long constants.  The goal, if the CPU permits,
3583 // is to initialize the object with a small number of 64-bit tiles.
3584 // Also, convert floating-point constants to bit patterns.
3585 // Non-constants are not relevant to this pass.
3586 //
3587 // In terms of the running example on InitializeNode::InitializeNode
3588 // and InitializeNode::capture_store, here is the transformation
3589 // of rawstore1 and rawstore2 into rawstore12:
3590 //   alloc = (Allocate ...)
3591 //   rawoop = alloc.RawAddress
3592 //   tile12 = 0x00010002
3593 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3594 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3595 //
3596 void
3597 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3598                                         Node* size_in_bytes,
3599                                         PhaseGVN* phase) {
3600   Compile* C = phase-&gt;C;
3601 
3602   assert(stores_are_sane(phase), "");
3603   // Note:  After this pass, they are not completely sane,
3604   // since there may be some overlaps.
3605 
3606   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3607 
3608   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3609   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3610   size_limit = MIN2(size_limit, ti_limit);
3611   size_limit = align_size_up(size_limit, BytesPerLong);
3612   int num_tiles = size_limit / BytesPerLong;
3613 
3614   // allocate space for the tile map:
3615   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3616   jlong  tiles_buf[small_len];
3617   Node*  nodes_buf[small_len];
3618   jlong  inits_buf[small_len];
3619   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3620                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3621   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3622                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3623   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3624                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3625   // tiles: exact bitwise model of all primitive constants
3626   // nodes: last constant-storing node subsumed into the tiles model
3627   // inits: which bytes (in each tile) are touched by any initializations
3628 
3629   //// Pass A: Fill in the tile model with any relevant stores.
3630 
3631   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3632   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3633   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3634   Node* zmem = zero_memory(); // initially zero memory state
3635   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3636     Node* st = in(i);
3637     intptr_t st_off = get_store_offset(st, phase);
3638 
3639     // Figure out the store's offset and constant value:
3640     if (st_off &lt; header_size)             continue; //skip (ignore header)
3641     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3642     int st_size = st-&gt;as_Store()-&gt;memory_size();
3643     if (st_off + st_size &gt; size_limit)    break;
3644 
3645     // Record which bytes are touched, whether by constant or not.
3646     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3647       continue;                 // skip (strange store size)
3648 
3649     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3650     if (!val-&gt;singleton())                continue; //skip (non-con store)
3651     BasicType type = val-&gt;basic_type();
3652 
3653     jlong con = 0;
3654     switch (type) {
3655     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3656     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3657     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3658     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3659     default:                              continue; //skip (odd store type)
3660     }
3661 
3662     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3663         st-&gt;Opcode() == Op_StoreL) {
3664       continue;                 // This StoreL is already optimal.
3665     }
3666 
3667     // Store down the constant.
3668     store_constant(tiles, num_tiles, st_off, st_size, con);
3669 
3670     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3671 
3672     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3673         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3674       jlong lcon = tiles[j];
3675       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3676           st-&gt;Opcode() == Op_StoreI) {
3677         // This StoreI is already optimal by itself.
3678         jint* intcon = (jint*) &amp;tiles[j];
3679         intcon[1] = 0;  // undo the store_constant()
3680 
3681         // If the previous store is also optimal by itself, back up and
3682         // undo the action of the previous loop iteration... if we can.
3683         // But if we can't, just let the previous half take care of itself.
3684         st = nodes[j];
3685         st_off -= BytesPerInt;
3686         con = intcon[0];
3687         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3688           assert(st_off &gt;= header_size, "still ignoring header");
3689           assert(get_store_offset(st, phase) == st_off, "must be");
3690           assert(in(i-1) == zmem, "must be");
3691           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3692           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3693           // Undo the effects of the previous loop trip, which swallowed st:
3694           intcon[0] = 0;        // undo store_constant()
3695           set_req(i-1, st);     // undo set_req(i, zmem)
3696           nodes[j] = NULL;      // undo nodes[j] = st
3697           --old_subword;        // undo ++old_subword
3698         }
3699         continue;               // This StoreI is already optimal.
3700       }
3701     }
3702 
3703     // This store is not needed.
3704     set_req(i, zmem);
3705     nodes[j] = st;              // record for the moment
3706     if (st_size &lt; BytesPerLong) // something has changed
3707           ++old_subword;        // includes int/float, but who's counting...
3708     else  ++old_long;
3709   }
3710 
3711   if ((old_subword + old_long) == 0)
3712     return;                     // nothing more to do
3713 
3714   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3715   // Be sure to insert them before overlapping non-constant stores.
3716   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3717   for (int j = 0; j &lt; num_tiles; j++) {
3718     jlong con  = tiles[j];
3719     jlong init = inits[j];
3720     if (con == 0)  continue;
3721     jint con0,  con1;           // split the constant, address-wise
3722     jint init0, init1;          // split the init map, address-wise
3723     { union { jlong con; jint intcon[2]; } u;
3724       u.con = con;
3725       con0  = u.intcon[0];
3726       con1  = u.intcon[1];
3727       u.con = init;
3728       init0 = u.intcon[0];
3729       init1 = u.intcon[1];
3730     }
3731 
3732     Node* old = nodes[j];
3733     assert(old != NULL, "need the prior store");
3734     intptr_t offset = (j * BytesPerLong);
3735 
3736     bool split = !Matcher::isSimpleConstant64(con);
3737 
3738     if (offset &lt; header_size) {
3739       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3740       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3741       split = true;             // only the second word counts
3742       // Example:  int a[] = { 42 ... }
3743     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3744       split = true;             // first word is covered by full inits
3745       // Example:  int a[] = { ... foo(), 42 ... }
3746     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3747       split = true;             // second word is covered by full inits
3748       // Example:  int a[] = { ... 42, foo() ... }
3749     }
3750 
3751     // Here's a case where init0 is neither 0 nor -1:
3752     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3753     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3754     // In this case the tile is not split; it is (jlong)42.
3755     // The big tile is stored down, and then the foo() value is inserted.
3756     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3757 
3758     Node* ctl = old-&gt;in(MemNode::Control);
3759     Node* adr = make_raw_address(offset, phase);
3760     const TypePtr* atp = TypeRawPtr::BOTTOM;
3761 
3762     // One or two coalesced stores to plop down.
3763     Node*    st[2];
3764     intptr_t off[2];
3765     int  nst = 0;
3766     if (!split) {
3767       ++new_long;
3768       off[nst] = offset;
3769       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3770                                   phase-&gt;longcon(con), T_LONG);
3771     } else {
3772       // Omit either if it is a zero.
3773       if (con0 != 0) {
3774         ++new_int;
3775         off[nst]  = offset;
3776         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3777                                     phase-&gt;intcon(con0), T_INT);
3778       }
3779       if (con1 != 0) {
3780         ++new_int;
3781         offset += BytesPerInt;
3782         adr = make_raw_address(offset, phase);
3783         off[nst]  = offset;
3784         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3785                                     phase-&gt;intcon(con1), T_INT);
3786       }
3787     }
3788 
3789     // Insert second store first, then the first before the second.
3790     // Insert each one just before any overlapping non-constant stores.
3791     while (nst &gt; 0) {
3792       Node* st1 = st[--nst];
3793       C-&gt;copy_node_notes_to(st1, old);
3794       st1 = phase-&gt;transform(st1);
3795       offset = off[nst];
3796       assert(offset &gt;= header_size, "do not smash header");
3797       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3798       guarantee(ins_idx != 0, "must re-insert constant store");
3799       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3800       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3801         set_req(--ins_idx, st1);
3802       else
3803         ins_req(ins_idx, st1);
3804     }
3805   }
3806 
3807   if (PrintCompilation &amp;&amp; WizardMode)
3808     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3809                   old_subword, old_long, new_int, new_long);
3810   if (C-&gt;log() != NULL)
3811     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3812                    old_subword, old_long, new_int, new_long);
3813 
3814   // Clean up any remaining occurrences of zmem:
3815   remove_extra_zeroes();
3816 }
3817 
3818 // Explore forward from in(start) to find the first fully initialized
3819 // word, and return its offset.  Skip groups of subword stores which
3820 // together initialize full words.  If in(start) is itself part of a
3821 // fully initialized word, return the offset of in(start).  If there
3822 // are no following full-word stores, or if something is fishy, return
3823 // a negative value.
3824 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3825   int       int_map = 0;
3826   intptr_t  int_map_off = 0;
3827   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3828 
3829   for (uint i = start, limit = req(); i &lt; limit; i++) {
3830     Node* st = in(i);
3831 
3832     intptr_t st_off = get_store_offset(st, phase);
3833     if (st_off &lt; 0)  break;  // return conservative answer
3834 
3835     int st_size = st-&gt;as_Store()-&gt;memory_size();
3836     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3837       return st_off;            // we found a complete word init
3838     }
3839 
3840     // update the map:
3841 
3842     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3843     if (this_int_off != int_map_off) {
3844       // reset the map:
3845       int_map = 0;
3846       int_map_off = this_int_off;
3847     }
3848 
3849     int subword_off = st_off - this_int_off;
3850     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3851     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3852       return this_int_off;      // we found a complete word init
3853     }
3854 
3855     // Did this store hit or cross the word boundary?
3856     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3857     if (next_int_off == this_int_off + BytesPerInt) {
3858       // We passed the current int, without fully initializing it.
3859       int_map_off = next_int_off;
3860       int_map &gt;&gt;= BytesPerInt;
3861     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3862       // We passed the current and next int.
3863       return this_int_off + BytesPerInt;
3864     }
3865   }
3866 
3867   return -1;
3868 }
3869 
3870 
3871 // Called when the associated AllocateNode is expanded into CFG.
3872 // At this point, we may perform additional optimizations.
3873 // Linearize the stores by ascending offset, to make memory
3874 // activity as coherent as possible.
3875 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3876                                       intptr_t header_size,
3877                                       Node* size_in_bytes,
3878                                       PhaseGVN* phase) {
3879   assert(!is_complete(), "not already complete");
3880   assert(stores_are_sane(phase), "");
3881   assert(allocation() != NULL, "must be present");
3882 
3883   remove_extra_zeroes();
3884 
3885   if (ReduceFieldZeroing || ReduceBulkZeroing)
3886     // reduce instruction count for common initialization patterns
3887     coalesce_subword_stores(header_size, size_in_bytes, phase);
3888 
3889   Node* zmem = zero_memory();   // initially zero memory state
3890   Node* inits = zmem;           // accumulating a linearized chain of inits
3891   #ifdef ASSERT
3892   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3893   intptr_t last_init_off = first_offset;  // previous init offset
3894   intptr_t last_init_end = first_offset;  // previous init offset+size
3895   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3896   #endif
3897   intptr_t zeroes_done = header_size;
3898 
3899   bool do_zeroing = true;       // we might give up if inits are very sparse
3900   int  big_init_gaps = 0;       // how many large gaps have we seen?
3901 
3902   if (ZeroTLAB)  do_zeroing = false;
3903   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3904 
3905   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3906     Node* st = in(i);
3907     intptr_t st_off = get_store_offset(st, phase);
3908     if (st_off &lt; 0)
3909       break;                    // unknown junk in the inits
3910     if (st-&gt;in(MemNode::Memory) != zmem)
3911       break;                    // complicated store chains somehow in list
3912 
3913     int st_size = st-&gt;as_Store()-&gt;memory_size();
3914     intptr_t next_init_off = st_off + st_size;
3915 
3916     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3917       // See if this store needs a zero before it or under it.
3918       intptr_t zeroes_needed = st_off;
3919 
3920       if (st_size &lt; BytesPerInt) {
3921         // Look for subword stores which only partially initialize words.
3922         // If we find some, we must lay down some word-level zeroes first,
3923         // underneath the subword stores.
3924         //
3925         // Examples:
3926         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3927         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3928         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3929         //
3930         // Note:  coalesce_subword_stores may have already done this,
3931         // if it was prompted by constant non-zero subword initializers.
3932         // But this case can still arise with non-constant stores.
3933 
3934         intptr_t next_full_store = find_next_fullword_store(i, phase);
3935 
3936         // In the examples above:
3937         //   in(i)          p   q   r   s     x   y     z
3938         //   st_off        12  13  14  15    12  13    14
3939         //   st_size        1   1   1   1     1   1     1
3940         //   next_full_s.  12  16  16  16    16  16    16
3941         //   z's_done      12  16  16  16    12  16    12
3942         //   z's_needed    12  16  16  16    16  16    16
3943         //   zsize          0   0   0   0     4   0     4
3944         if (next_full_store &lt; 0) {
3945           // Conservative tack:  Zero to end of current word.
3946           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3947         } else {
3948           // Zero to beginning of next fully initialized word.
3949           // Or, don't zero at all, if we are already in that word.
3950           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3951           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3952           zeroes_needed = next_full_store;
3953         }
3954       }
3955 
3956       if (zeroes_needed &gt; zeroes_done) {
3957         intptr_t zsize = zeroes_needed - zeroes_done;
3958         // Do some incremental zeroing on rawmem, in parallel with inits.
3959         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3960         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3961                                               zeroes_done, zeroes_needed,
3962                                               phase);
3963         zeroes_done = zeroes_needed;
3964         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3965           do_zeroing = false;   // leave the hole, next time
3966       }
3967     }
3968 
3969     // Collect the store and move on:
3970     st-&gt;set_req(MemNode::Memory, inits);
3971     inits = st;                 // put it on the linearized chain
3972     set_req(i, zmem);           // unhook from previous position
3973 
3974     if (zeroes_done == st_off)
3975       zeroes_done = next_init_off;
3976 
3977     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3978 
3979     #ifdef ASSERT
3980     // Various order invariants.  Weaker than stores_are_sane because
3981     // a large constant tile can be filled in by smaller non-constant stores.
3982     assert(st_off &gt;= last_init_off, "inits do not reverse");
3983     last_init_off = st_off;
3984     const Type* val = NULL;
3985     if (st_size &gt;= BytesPerInt &amp;&amp;
3986         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3987         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3988       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3989       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3990       last_tile_end = MAX2(last_tile_end, next_init_off);
3991     } else {
3992       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
3993       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
3994       assert(st_off      &gt;= last_init_end, "inits do not overlap");
3995       last_init_end = next_init_off;  // it's a non-tile
3996     }
3997     #endif //ASSERT
3998   }
3999 
4000   remove_extra_zeroes();        // clear out all the zmems left over
4001   add_req(inits);
4002 
4003   if (!ZeroTLAB) {
4004     // If anything remains to be zeroed, zero it all now.
4005     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
4006     // if it is the last unused 4 bytes of an instance, forget about it
4007     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4008     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4009       assert(allocation() != NULL, "");
4010       if (allocation()-&gt;Opcode() == Op_Allocate) {
4011         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
4012         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4013         if (zeroes_done == k-&gt;layout_helper())
4014           zeroes_done = size_limit;
4015       }
4016     }
4017     if (zeroes_done &lt; size_limit) {
4018       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4019                                             zeroes_done, size_in_bytes, phase);
4020     }
4021   }
4022 
4023   set_complete(phase);
4024   return rawmem;
4025 }
4026 
4027 
4028 #ifdef ASSERT
4029 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4030   if (is_complete())
4031     return true;                // stores could be anything at this point
4032   assert(allocation() != NULL, "must be present");
4033   intptr_t last_off = allocation()-&gt;minimum_header_size();
4034   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4035     Node* st = in(i);
4036     intptr_t st_off = get_store_offset(st, phase);
4037     if (st_off &lt; 0)  continue;  // ignore dead garbage
4038     if (last_off &gt; st_off) {
4039       tty-&gt;print_cr("*** bad store offset at %d: %d &gt; %d", i, last_off, st_off);
4040       this-&gt;dump(2);
4041       assert(false, "ascending store offsets");
4042       return false;
4043     }
4044     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
4045   }
4046   return true;
4047 }
4048 #endif //ASSERT
4049 
4050 
4051 
4052 
4053 //============================MergeMemNode=====================================
4054 //
4055 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4056 // contributing store or call operations.  Each contributor provides the memory
4057 // state for a particular "alias type" (see Compile::alias_type).  For example,
4058 // if a MergeMem has an input X for alias category #6, then any memory reference
4059 // to alias category #6 may use X as its memory state input, as an exact equivalent
4060 // to using the MergeMem as a whole.
4061 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4062 //
4063 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4064 //
4065 // In one special case (and more cases in the future), alias categories overlap.
4066 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4067 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4068 // it is exactly equivalent to that state W:
4069 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4070 //
4071 // Usually, the merge has more than one input.  In that case, where inputs
4072 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4073 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4074 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4075 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4076 //
4077 // A merge can take a "wide" memory state as one of its narrow inputs.
4078 // This simply means that the merge observes out only the relevant parts of
4079 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4080 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4081 //
4082 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4083 // and that memory slices "leak through":
4084 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4085 //
4086 // But, in such a cascade, repeated memory slices can "block the leak":
4087 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4088 //
4089 // In the last example, Y is not part of the combined memory state of the
4090 // outermost MergeMem.  The system must, of course, prevent unschedulable
4091 // memory states from arising, so you can be sure that the state Y is somehow
4092 // a precursor to state Y'.
4093 //
4094 //
4095 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4096 // of each MergeMemNode array are exactly the numerical alias indexes, including
4097 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4098 // Compile::alias_type (and kin) produce and manage these indexes.
4099 //
4100 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4101 // (Note that this provides quick access to the top node inside MergeMem methods,
4102 // without the need to reach out via TLS to Compile::current.)
4103 //
4104 // As a consequence of what was just described, a MergeMem that represents a full
4105 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4106 // containing all alias categories.
4107 //
4108 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4109 //
4110 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4111 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4112 // there is no particular input for that alias type.  Note that the length of
4113 // a MergeMem is variable, and may be extended at any time to accommodate new
4114 // memory states at larger alias indexes.  When merges grow, they are of course
4115 // filled with "top" in the unused in() positions.
4116 //
4117 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4118 // (Top was chosen because it works smoothly with passes like GCM.)
4119 //
4120 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4121 // the type of random VM bits like TLS references.)  Since it is always the
4122 // first non-Bot memory slice, some low-level loops use it to initialize an
4123 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4124 //
4125 //
4126 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4127 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4128 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4129 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4130 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4131 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4132 //
4133 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4134 // really that different from the other memory inputs.  An abbreviation called
4135 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4136 //
4137 //
4138 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4139 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4140 // that "emerges though" the base memory will be marked as excluding the alias types
4141 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4142 //
4143 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4144 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4145 //
4146 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4147 // (It is currently unimplemented.)  As you can see, the resulting merge is
4148 // actually a disjoint union of memory states, rather than an overlay.
4149 //
4150 
4151 //------------------------------MergeMemNode-----------------------------------
4152 Node* MergeMemNode::make_empty_memory() {
4153   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4154   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4155   return empty_memory;
4156 }
4157 
4158 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4159   init_class_id(Class_MergeMem);
4160   // all inputs are nullified in Node::Node(int)
4161   // set_input(0, NULL);  // no control input
4162 
4163   // Initialize the edges uniformly to top, for starters.
4164   Node* empty_mem = make_empty_memory();
4165   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4166     init_req(i,empty_mem);
4167   }
4168   assert(empty_memory() == empty_mem, "");
4169 
4170   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4171     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4172     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4173     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4174       mms.set_memory(mms.memory2());
4175     }
4176     assert(base_memory() == mdef-&gt;base_memory(), "");
4177   } else {
4178     set_base_memory(new_base);
4179   }
4180 }
4181 
4182 // Make a new, untransformed MergeMem with the same base as 'mem'.
4183 // If mem is itself a MergeMem, populate the result with the same edges.
4184 MergeMemNode* MergeMemNode::make(Compile* C, Node* mem) {
4185   return new(C) MergeMemNode(mem);
4186 }
4187 
4188 //------------------------------cmp--------------------------------------------
4189 uint MergeMemNode::hash() const { return NO_HASH; }
4190 uint MergeMemNode::cmp( const Node &amp;n ) const {
4191   return (&amp;n == this);          // Always fail except on self
4192 }
4193 
4194 //------------------------------Identity---------------------------------------
4195 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4196   // Identity if this merge point does not record any interesting memory
4197   // disambiguations.
4198   Node* base_mem = base_memory();
4199   Node* empty_mem = empty_memory();
4200   if (base_mem != empty_mem) {  // Memory path is not dead?
4201     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4202       Node* mem = in(i);
4203       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4204         return this;            // Many memory splits; no change
4205       }
4206     }
4207   }
4208   return base_mem;              // No memory splits; ID on the one true input
4209 }
4210 
4211 //------------------------------Ideal------------------------------------------
4212 // This method is invoked recursively on chains of MergeMem nodes
4213 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4214   // Remove chain'd MergeMems
4215   //
4216   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4217   // relative to the "in(Bot)".  Since we are patching both at the same time,
4218   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4219   // but rewrite each "in(i)" relative to the new "in(Bot)".
4220   Node *progress = NULL;
4221 
4222 
4223   Node* old_base = base_memory();
4224   Node* empty_mem = empty_memory();
4225   if (old_base == empty_mem)
4226     return NULL; // Dead memory path.
4227 
4228   MergeMemNode* old_mbase;
4229   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4230     old_mbase = old_base-&gt;as_MergeMem();
4231   else
4232     old_mbase = NULL;
4233   Node* new_base = old_base;
4234 
4235   // simplify stacked MergeMems in base memory
4236   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4237 
4238   // the base memory might contribute new slices beyond my req()
4239   if (old_mbase)  grow_to_match(old_mbase);
4240 
4241   // Look carefully at the base node if it is a phi.
4242   PhiNode* phi_base;
4243   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4244     phi_base = new_base-&gt;as_Phi();
4245   else
4246     phi_base = NULL;
4247 
4248   Node*    phi_reg = NULL;
4249   uint     phi_len = (uint)-1;
4250   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4251     // do not examine phi if degraded to a copy
4252     phi_reg = phi_base-&gt;region();
4253     phi_len = phi_base-&gt;req();
4254     // see if the phi is unfinished
4255     for (uint i = 1; i &lt; phi_len; i++) {
4256       if (phi_base-&gt;in(i) == NULL) {
4257         // incomplete phi; do not look at it yet!
4258         phi_reg = NULL;
4259         phi_len = (uint)-1;
4260         break;
4261       }
4262     }
4263   }
4264 
4265   // Note:  We do not call verify_sparse on entry, because inputs
4266   // can normalize to the base_memory via subsume_node or similar
4267   // mechanisms.  This method repairs that damage.
4268 
4269   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4270 
4271   // Look at each slice.
4272   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4273     Node* old_in = in(i);
4274     // calculate the old memory value
4275     Node* old_mem = old_in;
4276     if (old_mem == empty_mem)  old_mem = old_base;
4277     assert(old_mem == memory_at(i), "");
4278 
4279     // maybe update (reslice) the old memory value
4280 
4281     // simplify stacked MergeMems
4282     Node* new_mem = old_mem;
4283     MergeMemNode* old_mmem;
4284     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4285       old_mmem = old_mem-&gt;as_MergeMem();
4286     else
4287       old_mmem = NULL;
4288     if (old_mmem == this) {
4289       // This can happen if loops break up and safepoints disappear.
4290       // A merge of BotPtr (default) with a RawPtr memory derived from a
4291       // safepoint can be rewritten to a merge of the same BotPtr with
4292       // the BotPtr phi coming into the loop.  If that phi disappears
4293       // also, we can end up with a self-loop of the mergemem.
4294       // In general, if loops degenerate and memory effects disappear,
4295       // a mergemem can be left looking at itself.  This simply means
4296       // that the mergemem's default should be used, since there is
4297       // no longer any apparent effect on this slice.
4298       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4299       //       from start.  Update the input to TOP.
4300       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4301     }
4302     else if (old_mmem != NULL) {
4303       new_mem = old_mmem-&gt;memory_at(i);
4304     }
4305     // else preceding memory was not a MergeMem
4306 
4307     // replace equivalent phis (unfortunately, they do not GVN together)
4308     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4309         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4310       if (new_mem-&gt;is_Phi()) {
4311         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4312         for (uint i = 1; i &lt; phi_len; i++) {
4313           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4314             phi_mem = NULL;
4315             break;
4316           }
4317         }
4318         if (phi_mem != NULL) {
4319           // equivalent phi nodes; revert to the def
4320           new_mem = new_base;
4321         }
4322       }
4323     }
4324 
4325     // maybe store down a new value
4326     Node* new_in = new_mem;
4327     if (new_in == new_base)  new_in = empty_mem;
4328 
4329     if (new_in != old_in) {
4330       // Warning:  Do not combine this "if" with the previous "if"
4331       // A memory slice might have be be rewritten even if it is semantically
4332       // unchanged, if the base_memory value has changed.
4333       set_req(i, new_in);
4334       progress = this;          // Report progress
4335     }
4336   }
4337 
4338   if (new_base != old_base) {
4339     set_req(Compile::AliasIdxBot, new_base);
4340     // Don't use set_base_memory(new_base), because we need to update du.
4341     assert(base_memory() == new_base, "");
4342     progress = this;
4343   }
4344 
4345   if( base_memory() == this ) {
4346     // a self cycle indicates this memory path is dead
4347     set_req(Compile::AliasIdxBot, empty_mem);
4348   }
4349 
4350   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4351   // Recursion must occur after the self cycle check above
4352   if( base_memory()-&gt;is_MergeMem() ) {
4353     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4354     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4355     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4356         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4357       // propagate rollup of dead cycle to self
4358       set_req(Compile::AliasIdxBot, empty_mem);
4359     }
4360   }
4361 
4362   if( base_memory() == empty_mem ) {
4363     progress = this;
4364     // Cut inputs during Parse phase only.
4365     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4366     if( !can_reshape ) {
4367       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4368         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4369       }
4370     }
4371   }
4372 
4373   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4374     // Check if PhiNode::Ideal's "Split phis through memory merges"
4375     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4376     uint merge_width = req();
4377     if (merge_width &gt; Compile::AliasIdxRaw) {
4378       PhiNode* phi = base_memory()-&gt;as_Phi();
4379       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4380         if (phi-&gt;in(i) == this) {
4381           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4382           break;
4383         }
4384       }
4385     }
4386   }
4387 
4388   assert(progress || verify_sparse(), "please, no dups of base");
4389   return progress;
4390 }
4391 
4392 //-------------------------set_base_memory-------------------------------------
4393 void MergeMemNode::set_base_memory(Node *new_base) {
4394   Node* empty_mem = empty_memory();
4395   set_req(Compile::AliasIdxBot, new_base);
4396   assert(memory_at(req()) == new_base, "must set default memory");
4397   // Clear out other occurrences of new_base:
4398   if (new_base != empty_mem) {
4399     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4400       if (in(i) == new_base)  set_req(i, empty_mem);
4401     }
4402   }
4403 }
4404 
4405 //------------------------------out_RegMask------------------------------------
4406 const RegMask &amp;MergeMemNode::out_RegMask() const {
4407   return RegMask::Empty;
4408 }
4409 
4410 //------------------------------dump_spec--------------------------------------
4411 #ifndef PRODUCT
4412 void MergeMemNode::dump_spec(outputStream *st) const {
4413   st-&gt;print(" {");
4414   Node* base_mem = base_memory();
4415   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4416     Node* mem = memory_at(i);
4417     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4418     st-&gt;print( " N%d:", mem-&gt;_idx );
4419     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4420   }
4421   st-&gt;print(" }");
4422 }
4423 #endif // !PRODUCT
4424 
4425 
4426 #ifdef ASSERT
4427 static bool might_be_same(Node* a, Node* b) {
4428   if (a == b)  return true;
4429   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4430   // phis shift around during optimization
4431   return true;  // pretty stupid...
4432 }
4433 
4434 // verify a narrow slice (either incoming or outgoing)
4435 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4436   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4437   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4438   if (Node::in_dump())      return;  // muzzle asserts when printing
4439   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4440   assert(n != NULL, "");
4441   // Elide intervening MergeMem's
4442   while (n-&gt;is_MergeMem()) {
4443     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4444   }
4445   Compile* C = Compile::current();
4446   const TypePtr* n_adr_type = n-&gt;adr_type();
4447   if (n == m-&gt;empty_memory()) {
4448     // Implicit copy of base_memory()
4449   } else if (n_adr_type != TypePtr::BOTTOM) {
4450     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4451     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4452   } else {
4453     // A few places like make_runtime_call "know" that VM calls are narrow,
4454     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4455     bool expected_wide_mem = false;
4456     if (n == m-&gt;base_memory()) {
4457       expected_wide_mem = true;
4458     } else if (alias_idx == Compile::AliasIdxRaw ||
4459                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4460       expected_wide_mem = true;
4461     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4462       // memory can "leak through" calls on channels that
4463       // are write-once.  Allow this also.
4464       expected_wide_mem = true;
4465     }
4466     assert(expected_wide_mem, "expected narrow slice replacement");
4467   }
4468 }
4469 #else // !ASSERT
4470 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4471 #endif
4472 
4473 
4474 //-----------------------------memory_at---------------------------------------
4475 Node* MergeMemNode::memory_at(uint alias_idx) const {
4476   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4477          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4478          "must avoid base_memory and AliasIdxTop");
4479 
4480   // Otherwise, it is a narrow slice.
4481   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4482   Compile *C = Compile::current();
4483   if (is_empty_memory(n)) {
4484     // the array is sparse; empty slots are the "top" node
4485     n = base_memory();
4486     assert(Node::in_dump()
4487            || n == NULL || n-&gt;bottom_type() == Type::TOP
4488            || n-&gt;adr_type() == NULL // address is TOP
4489            || n-&gt;adr_type() == TypePtr::BOTTOM
4490            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4491            || Compile::current()-&gt;AliasLevel() == 0,
4492            "must be a wide memory");
4493     // AliasLevel == 0 if we are organizing the memory states manually.
4494     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4495   } else {
4496     // make sure the stored slice is sane
4497     #ifdef ASSERT
4498     if (is_error_reported() || Node::in_dump()) {
4499     } else if (might_be_same(n, base_memory())) {
4500       // Give it a pass:  It is a mostly harmless repetition of the base.
4501       // This can arise normally from node subsumption during optimization.
4502     } else {
4503       verify_memory_slice(this, alias_idx, n);
4504     }
4505     #endif
4506   }
4507   return n;
4508 }
4509 
4510 //---------------------------set_memory_at-------------------------------------
4511 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4512   verify_memory_slice(this, alias_idx, n);
4513   Node* empty_mem = empty_memory();
4514   if (n == base_memory())  n = empty_mem;  // collapse default
4515   uint need_req = alias_idx+1;
4516   if (req() &lt; need_req) {
4517     if (n == empty_mem)  return;  // already the default, so do not grow me
4518     // grow the sparse array
4519     do {
4520       add_req(empty_mem);
4521     } while (req() &lt; need_req);
4522   }
4523   set_req( alias_idx, n );
4524 }
4525 
4526 
4527 
4528 //--------------------------iteration_setup------------------------------------
4529 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4530   if (other != NULL) {
4531     grow_to_match(other);
4532     // invariant:  the finite support of mm2 is within mm-&gt;req()
4533     #ifdef ASSERT
4534     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4535       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4536     }
4537     #endif
4538   }
4539   // Replace spurious copies of base_memory by top.
4540   Node* base_mem = base_memory();
4541   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4542     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4543       if (in(i) == base_mem)
4544         set_req(i, empty_memory());
4545     }
4546   }
4547 }
4548 
4549 //---------------------------grow_to_match-------------------------------------
4550 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4551   Node* empty_mem = empty_memory();
4552   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4553   // look for the finite support of the other memory
4554   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4555     if (other-&gt;in(i) != empty_mem) {
4556       uint new_len = i+1;
4557       while (req() &lt; new_len)  add_req(empty_mem);
4558       break;
4559     }
4560   }
4561 }
4562 
4563 //---------------------------verify_sparse-------------------------------------
4564 #ifndef PRODUCT
4565 bool MergeMemNode::verify_sparse() const {
4566   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4567   Node* base_mem = base_memory();
4568   // The following can happen in degenerate cases, since empty==top.
4569   if (is_empty_memory(base_mem))  return true;
4570   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4571     assert(in(i) != NULL, "sane slice");
4572     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4573   }
4574   return true;
4575 }
4576 
4577 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4578   Node* n;
4579   n = mm-&gt;in(idx);
4580   if (mem == n)  return true;  // might be empty_memory()
4581   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4582   if (mem == n)  return true;
4583   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4584     if (mem == n)  return true;
4585     if (n == NULL)  break;
4586   }
4587   return false;
4588 }
4589 #endif // !PRODUCT
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
