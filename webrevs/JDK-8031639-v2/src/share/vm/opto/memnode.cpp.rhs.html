<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/cfgnode.hpp"
  32 #include "opto/compile.hpp"
  33 #include "opto/connode.hpp"
  34 #include "opto/loopnode.hpp"
  35 #include "opto/machnode.hpp"
  36 #include "opto/matcher.hpp"
  37 #include "opto/memnode.hpp"
  38 #include "opto/mulnode.hpp"
  39 #include "opto/phaseX.hpp"
  40 #include "opto/regmask.hpp"
  41 
  42 // Portions of code courtesy of Clifford Click
  43 
  44 // Optimization - Graph Style
  45 
  46 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  47 
  48 //=============================================================================
  49 uint MemNode::size_of() const { return sizeof(*this); }
  50 
  51 const TypePtr *MemNode::adr_type() const {
  52   Node* adr = in(Address);
  53   const TypePtr* cross_check = NULL;
  54   DEBUG_ONLY(cross_check = _adr_type);
  55   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  56 }
  57 
  58 #ifndef PRODUCT
  59 void MemNode::dump_spec(outputStream *st) const {
  60   if (in(Address) == NULL)  return; // node is dead
  61 #ifndef ASSERT
  62   // fake the missing field
  63   const TypePtr* _adr_type = NULL;
  64   if (in(Address) != NULL)
  65     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  66 #endif
  67   dump_adr_type(this, _adr_type, st);
  68 
  69   Compile* C = Compile::current();
  70   if( C-&gt;alias_type(_adr_type)-&gt;is_volatile() )
  71     st-&gt;print(" Volatile!");
  72 }
  73 
  74 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  75   st-&gt;print(" @");
  76   if (adr_type == NULL) {
  77     st-&gt;print("NULL");
  78   } else {
  79     adr_type-&gt;dump_on(st);
  80     Compile* C = Compile::current();
  81     Compile::AliasType* atp = NULL;
  82     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  83     if (atp == NULL)
  84       st-&gt;print(", idx=?\?;");
  85     else if (atp-&gt;index() == Compile::AliasIdxBot)
  86       st-&gt;print(", idx=Bot;");
  87     else if (atp-&gt;index() == Compile::AliasIdxTop)
  88       st-&gt;print(", idx=Top;");
  89     else if (atp-&gt;index() == Compile::AliasIdxRaw)
  90       st-&gt;print(", idx=Raw;");
  91     else {
  92       ciField* field = atp-&gt;field();
  93       if (field) {
  94         st-&gt;print(", name=");
  95         field-&gt;print_name_on(st);
  96       }
  97       st-&gt;print(", idx=%d;", atp-&gt;index());
  98     }
  99   }
 100 }
 101 
 102 extern void print_alias_types();
 103 
 104 #endif
 105 
 106 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 107   assert((t_oop != NULL), "sanity");
 108   bool is_instance = t_oop-&gt;is_known_instance_field();
 109   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 110                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 111                              (phase-&gt;is_IterGVN() != NULL);
 112   if (!(is_instance || is_boxed_value_load))
 113     return mchain;  // don't try to optimize non-instance types
 114   uint instance_id = t_oop-&gt;instance_id();
 115   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 116   Node *prev = NULL;
 117   Node *result = mchain;
 118   while (prev != result) {
 119     prev = result;
 120     if (result == start_mem)
 121       break;  // hit one of our sentinels
 122     // skip over a call which does not affect this memory slice
 123     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 124       Node *proj_in = result-&gt;in(0);
 125       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 126         break;  // hit one of our sentinels
 127       } else if (proj_in-&gt;is_Call()) {
 128         CallNode *call = proj_in-&gt;as_Call();
 129         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 130           result = call-&gt;in(TypeFunc::Memory);
 131         }
 132       } else if (proj_in-&gt;is_Initialize()) {
 133         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 134         // Stop if this is the initialization for the object instance which
 135         // which contains this memory slice, otherwise skip over it.
 136         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 137           break;
 138         }
 139         if (is_instance) {
 140           result = proj_in-&gt;in(TypeFunc::Memory);
 141         } else if (is_boxed_value_load) {
 142           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 143           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 144           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 145             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 146           }
 147         }
 148       } else if (proj_in-&gt;is_MemBar()) {
 149         result = proj_in-&gt;in(TypeFunc::Memory);
 150       } else {
 151         assert(false, "unexpected projection");
 152       }
 153     } else if (result-&gt;is_ClearArray()) {
 154       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 155         // Can not bypass initialization of the instance
 156         // we are looking for.
 157         break;
 158       }
 159       // Otherwise skip it (the call updated 'result' value).
 160     } else if (result-&gt;is_MergeMem()) {
 161       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 162     }
 163   }
 164   return result;
 165 }
 166 
 167 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 168   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 169   if (t_oop == NULL)
 170     return mchain;  // don't try to optimize non-oop types
 171   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 172   bool is_instance = t_oop-&gt;is_known_instance_field();
 173   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 174   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 175     PhiNode *mphi = result-&gt;as_Phi();
 176     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 177     const TypePtr *t = mphi-&gt;adr_type();
 178     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 179         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 180         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 181          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 182          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 183       // clone the Phi with our address type
 184       result = mphi-&gt;split_out_instance(t_adr, igvn);
 185     } else {
 186       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 187     }
 188   }
 189   return result;
 190 }
 191 
 192 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 193   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 194   Node *mem = mmem;
 195 #ifdef ASSERT
 196   {
 197     // Check that current type is consistent with the alias index used during graph construction
 198     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 199     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 200                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 201     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 202     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 203                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 204         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 205         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 206           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 207           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 208       // don't assert if it is dead code.
 209       consistent = true;
 210     }
 211     if( !consistent ) {
 212       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 213       if( adr_check == NULL ) {
 214         st-&gt;print("NULL");
 215       } else {
 216         adr_check-&gt;dump();
 217       }
 218       st-&gt;cr();
 219       print_alias_types();
 220       assert(consistent, "adr_check must match alias idx");
 221     }
 222   }
 223 #endif
 224   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 225   // means an array I have not precisely typed yet.  Do not do any
 226   // alias stuff with it any time soon.
 227   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 228   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 229       !(toop &amp;&amp;
 230         toop-&gt;klass() != NULL &amp;&amp;
 231         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 232         toop-&gt;offset() == Type::OffsetBot) ) {
 233     // compress paths and change unreachable cycles to TOP
 234     // If not, we can update the input infinitely along a MergeMem cycle
 235     // Equivalent code in PhiNode::Ideal
 236     Node* m  = phase-&gt;transform(mmem);
 237     // If transformed to a MergeMem, get the desired slice
 238     // Otherwise the returned node represents memory for every slice
 239     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 240     // Update input if it is progress over what we have now
 241   }
 242   return mem;
 243 }
 244 
 245 //--------------------------Ideal_common---------------------------------------
 246 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 247 // Unhook non-raw memories from complete (macro-expanded) initializations.
 248 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 249   // If our control input is a dead region, kill all below the region
 250   Node *ctl = in(MemNode::Control);
 251   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 252     return this;
 253   ctl = in(MemNode::Control);
 254   // Don't bother trying to transform a dead node
 255   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 256 
 257   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 258   // Wait if control on the worklist.
 259   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 260     Node* bol = NULL;
 261     Node* cmp = NULL;
 262     if (ctl-&gt;in(0)-&gt;is_If()) {
 263       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 264       bol = ctl-&gt;in(0)-&gt;in(1);
 265       if (bol-&gt;is_Bool())
 266         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 267     }
 268     if (igvn-&gt;_worklist.member(ctl) ||
 269         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 270         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 271       // This control path may be dead.
 272       // Delay this memory node transformation until the control is processed.
 273       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 274       return NodeSentinel; // caller will return NULL
 275     }
 276   }
 277   // Ignore if memory is dead, or self-loop
 278   Node *mem = in(MemNode::Memory);
 279   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 280   assert(mem != this, "dead loop in MemNode::Ideal");
 281 
 282   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 283     // This memory slice may be dead.
 284     // Delay this mem node transformation until the memory is processed.
 285     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 286     return NodeSentinel; // caller will return NULL
 287   }
 288 
 289   Node *address = in(MemNode::Address);
 290   const Type *t_adr = phase-&gt;type(address);
 291   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 292 
 293   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 294       (igvn-&gt;_worklist.member(address) ||
 295        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 296     // The address's base and type may change when the address is processed.
 297     // Delay this mem node transformation until the address is processed.
 298     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 299     return NodeSentinel; // caller will return NULL
 300   }
 301 
 302   // Do NOT remove or optimize the next lines: ensure a new alias index
 303   // is allocated for an oop pointer type before Escape Analysis.
 304   // Note: C++ will not remove it since the call has side effect.
 305   if (t_adr-&gt;isa_oopptr()) {
 306     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 307   }
 308 
 309 #ifdef ASSERT
 310   Node* base = NULL;
 311   if (address-&gt;is_AddP())
 312     base = address-&gt;in(AddPNode::Base);
 313   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 314       !t_adr-&gt;isa_rawptr()) {
 315     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 316     Compile* C = phase-&gt;C;
 317     tty-&gt;cr();
 318     tty-&gt;print_cr("===== NULL+offs not RAW address =====");
 319     if (C-&gt;is_dead_node(this-&gt;_idx))    tty-&gt;print_cr("'this' is dead");
 320     if ((ctl != NULL) &amp;&amp; C-&gt;is_dead_node(ctl-&gt;_idx)) tty-&gt;print_cr("'ctl' is dead");
 321     if (C-&gt;is_dead_node(mem-&gt;_idx))     tty-&gt;print_cr("'mem' is dead");
 322     if (C-&gt;is_dead_node(address-&gt;_idx)) tty-&gt;print_cr("'address' is dead");
 323     if (C-&gt;is_dead_node(base-&gt;_idx))    tty-&gt;print_cr("'base' is dead");
 324     tty-&gt;cr();
 325     base-&gt;dump(1);
 326     tty-&gt;cr();
 327     this-&gt;dump(2);
 328     tty-&gt;print("this-&gt;adr_type():     "); adr_type()-&gt;dump(); tty-&gt;cr();
 329     tty-&gt;print("phase-&gt;type(address): "); t_adr-&gt;dump(); tty-&gt;cr();
 330     tty-&gt;print("phase-&gt;type(base):    "); phase-&gt;type(address)-&gt;dump(); tty-&gt;cr();
 331     tty-&gt;cr();
 332   }
 333   assert(base == NULL || t_adr-&gt;isa_rawptr() ||
 334         !phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR), "NULL+offs not RAW address?");
 335 #endif
 336 
 337   // Avoid independent memory operations
 338   Node* old_mem = mem;
 339 
 340   // The code which unhooks non-raw memories from complete (macro-expanded)
 341   // initializations was removed. After macro-expansion all stores catched
 342   // by Initialize node became raw stores and there is no information
 343   // which memory slices they modify. So it is unsafe to move any memory
 344   // operation above these stores. Also in most cases hooked non-raw memories
 345   // were already unhooked by using information from detect_ptr_independence()
 346   // and find_previous_store().
 347 
 348   if (mem-&gt;is_MergeMem()) {
 349     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 350     const TypePtr *tp = t_adr-&gt;is_ptr();
 351 
 352     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 353   }
 354 
 355   if (mem != old_mem) {
 356     set_req(MemNode::Memory, mem);
 357     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 358         igvn-&gt;_worklist.push(old_mem);
 359     }
 360     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 361     return this;
 362   }
 363 
 364   // let the subclass continue analyzing...
 365   return NULL;
 366 }
 367 
 368 // Helper function for proving some simple control dominations.
 369 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 370 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 371 // is not a constant (dominated by the method's StartNode).
 372 // Used by MemNode::find_previous_store to prove that the
 373 // control input of a memory operation predates (dominates)
 374 // an allocation it wants to look past.
 375 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 376   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 377     return false; // Conservative answer for dead code
 378 
 379   // Check 'dom'. Skip Proj and CatchProj nodes.
 380   dom = dom-&gt;find_exact_control(dom);
 381   if (dom == NULL || dom-&gt;is_top())
 382     return false; // Conservative answer for dead code
 383 
 384   if (dom == sub) {
 385     // For the case when, for example, 'sub' is Initialize and the original
 386     // 'dom' is Proj node of the 'sub'.
 387     return false;
 388   }
 389 
 390   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 391     return true;
 392 
 393   // 'dom' dominates 'sub' if its control edge and control edges
 394   // of all its inputs dominate or equal to sub's control edge.
 395 
 396   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 397   // Or Region for the check in LoadNode::Ideal();
 398   // 'sub' should have sub-&gt;in(0) != NULL.
 399   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 400          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 401 
 402   // Get control edge of 'sub'.
 403   Node* orig_sub = sub;
 404   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 405   if (sub == NULL || sub-&gt;is_top())
 406     return false; // Conservative answer for dead code
 407 
 408   assert(sub-&gt;is_CFG(), "expecting control");
 409 
 410   if (sub == dom)
 411     return true;
 412 
 413   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 414     return false;
 415 
 416   {
 417     // Check all control edges of 'dom'.
 418 
 419     ResourceMark rm;
 420     Arena* arena = Thread::current()-&gt;resource_area();
 421     Node_List nlist(arena);
 422     Unique_Node_List dom_list(arena);
 423 
 424     dom_list.push(dom);
 425     bool only_dominating_controls = false;
 426 
 427     for (uint next = 0; next &lt; dom_list.size(); next++) {
 428       Node* n = dom_list.at(next);
 429       if (n == orig_sub)
 430         return false; // One of dom's inputs dominated by sub.
 431       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 432         // Check only own control edge for pinned non-control nodes.
 433         n = n-&gt;find_exact_control(n-&gt;in(0));
 434         if (n == NULL || n-&gt;is_top())
 435           return false; // Conservative answer for dead code
 436         assert(n-&gt;is_CFG(), "expecting control");
 437         dom_list.push(n);
 438       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 439         only_dominating_controls = true;
 440       } else if (n-&gt;is_CFG()) {
 441         if (n-&gt;dominates(sub, nlist))
 442           only_dominating_controls = true;
 443         else
 444           return false;
 445       } else {
 446         // First, own control edge.
 447         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 448         if (m != NULL) {
 449           if (m-&gt;is_top())
 450             return false; // Conservative answer for dead code
 451           dom_list.push(m);
 452         }
 453         // Now, the rest of edges.
 454         uint cnt = n-&gt;req();
 455         for (uint i = 1; i &lt; cnt; i++) {
 456           m = n-&gt;find_exact_control(n-&gt;in(i));
 457           if (m == NULL || m-&gt;is_top())
 458             continue;
 459           dom_list.push(m);
 460         }
 461       }
 462     }
 463     return only_dominating_controls;
 464   }
 465 }
 466 
 467 //---------------------detect_ptr_independence---------------------------------
 468 // Used by MemNode::find_previous_store to prove that two base
 469 // pointers are never equal.
 470 // The pointers are accompanied by their associated allocations,
 471 // if any, which have been previously discovered by the caller.
 472 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 473                                       Node* p2, AllocateNode* a2,
 474                                       PhaseTransform* phase) {
 475   // Attempt to prove that these two pointers cannot be aliased.
 476   // They may both manifestly be allocations, and they should differ.
 477   // Or, if they are not both allocations, they can be distinct constants.
 478   // Otherwise, one is an allocation and the other a pre-existing value.
 479   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 480     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 481   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 482     return (a1 != a2);
 483   } else if (a1 != NULL) {                  // one allocation a1
 484     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 485     return all_controls_dominate(p2, a1);
 486   } else { //(a2 != NULL)                   // one allocation a2
 487     return all_controls_dominate(p1, a2);
 488   }
 489   return false;
 490 }
 491 
 492 
 493 // The logic for reordering loads and stores uses four steps:
 494 // (a) Walk carefully past stores and initializations which we
 495 //     can prove are independent of this load.
 496 // (b) Observe that the next memory state makes an exact match
 497 //     with self (load or store), and locate the relevant store.
 498 // (c) Ensure that, if we were to wire self directly to the store,
 499 //     the optimizer would fold it up somehow.
 500 // (d) Do the rewiring, and return, depending on some other part of
 501 //     the optimizer to fold up the load.
 502 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 503 // specific to loads and stores, so they are handled by the callers.
 504 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 505 //
 506 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 507   Node*         ctrl   = in(MemNode::Control);
 508   Node*         adr    = in(MemNode::Address);
 509   intptr_t      offset = 0;
 510   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 511   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 512 
 513   if (offset == Type::OffsetBot)
 514     return NULL;            // cannot unalias unless there are precise offsets
 515 
 516   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 517 
 518   intptr_t size_in_bytes = memory_size();
 519 
 520   Node* mem = in(MemNode::Memory);   // start searching here...
 521 
 522   int cnt = 50;             // Cycle limiter
 523   for (;;) {                // While we can dance past unrelated stores...
 524     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 525 
 526     if (mem-&gt;is_Store()) {
 527       Node* st_adr = mem-&gt;in(MemNode::Address);
 528       intptr_t st_offset = 0;
 529       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 530       if (st_base == NULL)
 531         break;              // inscrutable pointer
 532       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 533         const int MAX_STORE = BytesPerLong;
 534         if (st_offset &gt;= offset + size_in_bytes ||
 535             st_offset &lt;= offset - MAX_STORE ||
 536             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 537           // Success:  The offsets are provably independent.
 538           // (You may ask, why not just test st_offset != offset and be done?
 539           // The answer is that stores of different sizes can co-exist
 540           // in the same sequence of RawMem effects.  We sometimes initialize
 541           // a whole 'tile' of array elements with a single jint or jlong.)
 542           mem = mem-&gt;in(MemNode::Memory);
 543           continue;           // (a) advance through independent store memory
 544         }
 545       }
 546       if (st_base != base &amp;&amp;
 547           detect_ptr_independence(base, alloc,
 548                                   st_base,
 549                                   AllocateNode::Ideal_allocation(st_base, phase),
 550                                   phase)) {
 551         // Success:  The bases are provably independent.
 552         mem = mem-&gt;in(MemNode::Memory);
 553         continue;           // (a) advance through independent store memory
 554       }
 555 
 556       // (b) At this point, if the bases or offsets do not agree, we lose,
 557       // since we have not managed to prove 'this' and 'mem' independent.
 558       if (st_base == base &amp;&amp; st_offset == offset) {
 559         return mem;         // let caller handle steps (c), (d)
 560       }
 561 
 562     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 563       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 564       AllocateNode*  st_alloc = st_init-&gt;allocation();
 565       if (st_alloc == NULL)
 566         break;              // something degenerated
 567       bool known_identical = false;
 568       bool known_independent = false;
 569       if (alloc == st_alloc)
 570         known_identical = true;
 571       else if (alloc != NULL)
 572         known_independent = true;
 573       else if (all_controls_dominate(this, st_alloc))
 574         known_independent = true;
 575 
 576       if (known_independent) {
 577         // The bases are provably independent: Either they are
 578         // manifestly distinct allocations, or else the control
 579         // of this load dominates the store's allocation.
 580         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 581         if (alias_idx == Compile::AliasIdxRaw) {
 582           mem = st_alloc-&gt;in(TypeFunc::Memory);
 583         } else {
 584           mem = st_init-&gt;memory(alias_idx);
 585         }
 586         continue;           // (a) advance through independent store memory
 587       }
 588 
 589       // (b) at this point, if we are not looking at a store initializing
 590       // the same allocation we are loading from, we lose.
 591       if (known_identical) {
 592         // From caller, can_see_stored_value will consult find_captured_store.
 593         return mem;         // let caller handle steps (c), (d)
 594       }
 595 
 596     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 597       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 598       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 599         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 600         if (!call-&gt;may_modify(addr_t, phase)) {
 601           mem = call-&gt;in(TypeFunc::Memory);
 602           continue;         // (a) advance through independent call memory
 603         }
 604       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 605         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 606         continue;           // (a) advance through independent MemBar memory
 607       } else if (mem-&gt;is_ClearArray()) {
 608         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 609           // (the call updated 'mem' value)
 610           continue;         // (a) advance through independent allocation memory
 611         } else {
 612           // Can not bypass initialization of the instance
 613           // we are looking for.
 614           return mem;
 615         }
 616       } else if (mem-&gt;is_MergeMem()) {
 617         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 618         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 619         continue;           // (a) advance through independent MergeMem memory
 620       }
 621     }
 622 
 623     // Unless there is an explicit 'continue', we must bail out here,
 624     // because 'mem' is an inscrutable memory state (e.g., a call).
 625     break;
 626   }
 627 
 628   return NULL;              // bail out
 629 }
 630 
 631 //----------------------calculate_adr_type-------------------------------------
 632 // Helper function.  Notices when the given type of address hits top or bottom.
 633 // Also, asserts a cross-check of the type against the expected address type.
 634 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 635   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 636   #ifdef PRODUCT
 637   cross_check = NULL;
 638   #else
 639   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 640   #endif
 641   const TypePtr* tp = t-&gt;isa_ptr();
 642   if (tp == NULL) {
 643     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 644     return TypePtr::BOTTOM;           // touches lots of memory
 645   } else {
 646     #ifdef ASSERT
 647     // %%%% [phh] We don't check the alias index if cross_check is
 648     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 649     if (cross_check != NULL &amp;&amp;
 650         cross_check != TypePtr::BOTTOM &amp;&amp;
 651         cross_check != TypeRawPtr::BOTTOM) {
 652       // Recheck the alias index, to see if it has changed (due to a bug).
 653       Compile* C = Compile::current();
 654       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 655              "must stay in the original alias category");
 656       // The type of the address must be contained in the adr_type,
 657       // disregarding "null"-ness.
 658       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 659       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 660       assert(cross_check-&gt;meet(tp_notnull) == cross_check,
 661              "real address must not escape from expected memory type");
 662     }
 663     #endif
 664     return tp;
 665   }
 666 }
 667 
 668 //------------------------adr_phi_is_loop_invariant----------------------------
 669 // A helper function for Ideal_DU_postCCP to check if a Phi in a counted
 670 // loop is loop invariant. Make a quick traversal of Phi and associated
 671 // CastPP nodes, looking to see if they are a closed group within the loop.
 672 bool MemNode::adr_phi_is_loop_invariant(Node* adr_phi, Node* cast) {
 673   // The idea is that the phi-nest must boil down to only CastPP nodes
 674   // with the same data. This implies that any path into the loop already
 675   // includes such a CastPP, and so the original cast, whatever its input,
 676   // must be covered by an equivalent cast, with an earlier control input.
 677   ResourceMark rm;
 678 
 679   // The loop entry input of the phi should be the unique dominating
 680   // node for every Phi/CastPP in the loop.
 681   Unique_Node_List closure;
 682   closure.push(adr_phi-&gt;in(LoopNode::EntryControl));
 683 
 684   // Add the phi node and the cast to the worklist.
 685   Unique_Node_List worklist;
 686   worklist.push(adr_phi);
 687   if( cast != NULL ){
 688     if( !cast-&gt;is_ConstraintCast() ) return false;
 689     worklist.push(cast);
 690   }
 691 
 692   // Begin recursive walk of phi nodes.
 693   while( worklist.size() ){
 694     // Take a node off the worklist
 695     Node *n = worklist.pop();
 696     if( !closure.member(n) ){
 697       // Add it to the closure.
 698       closure.push(n);
 699       // Make a sanity check to ensure we don't waste too much time here.
 700       if( closure.size() &gt; 20) return false;
 701       // This node is OK if:
 702       //  - it is a cast of an identical value
 703       //  - or it is a phi node (then we add its inputs to the worklist)
 704       // Otherwise, the node is not OK, and we presume the cast is not invariant
 705       if( n-&gt;is_ConstraintCast() ){
 706         worklist.push(n-&gt;in(1));
 707       } else if( n-&gt;is_Phi() ) {
 708         for( uint i = 1; i &lt; n-&gt;req(); i++ ) {
 709           worklist.push(n-&gt;in(i));
 710         }
 711       } else {
 712         return false;
 713       }
 714     }
 715   }
 716 
 717   // Quit when the worklist is empty, and we've found no offending nodes.
 718   return true;
 719 }
 720 
 721 //------------------------------Ideal_DU_postCCP-------------------------------
 722 // Find any cast-away of null-ness and keep its control.  Null cast-aways are
 723 // going away in this pass and we need to make this memory op depend on the
 724 // gating null check.
 725 Node *MemNode::Ideal_DU_postCCP( PhaseCCP *ccp ) {
 726   return Ideal_common_DU_postCCP(ccp, this, in(MemNode::Address));
 727 }
 728 
 729 // I tried to leave the CastPP's in.  This makes the graph more accurate in
 730 // some sense; we get to keep around the knowledge that an oop is not-null
 731 // after some test.  Alas, the CastPP's interfere with GVN (some values are
 732 // the regular oop, some are the CastPP of the oop, all merge at Phi's which
 733 // cannot collapse, etc).  This cost us 10% on SpecJVM, even when I removed
 734 // some of the more trivial cases in the optimizer.  Removing more useless
 735 // Phi's started allowing Loads to illegally float above null checks.  I gave
 736 // up on this approach.  CNC 10/20/2000
 737 // This static method may be called not from MemNode (EncodePNode calls it).
 738 // Only the control edge of the node 'n' might be updated.
 739 Node *MemNode::Ideal_common_DU_postCCP( PhaseCCP *ccp, Node* n, Node* adr ) {
 740   Node *skipped_cast = NULL;
 741   // Need a null check?  Regular static accesses do not because they are
 742   // from constant addresses.  Array ops are gated by the range check (which
 743   // always includes a NULL check).  Just check field ops.
 744   if( n-&gt;in(MemNode::Control) == NULL ) {
 745     // Scan upwards for the highest location we can place this memory op.
 746     while( true ) {
 747       switch( adr-&gt;Opcode() ) {
 748 
 749       case Op_AddP:             // No change to NULL-ness, so peek thru AddP's
 750         adr = adr-&gt;in(AddPNode::Base);
 751         continue;
 752 
 753       case Op_DecodeN:         // No change to NULL-ness, so peek thru
 754       case Op_DecodeNKlass:
 755         adr = adr-&gt;in(1);
 756         continue;
 757 
 758       case Op_EncodeP:
 759       case Op_EncodePKlass:
 760         // EncodeP node's control edge could be set by this method
 761         // when EncodeP node depends on CastPP node.
 762         //
 763         // Use its control edge for memory op because EncodeP may go away
 764         // later when it is folded with following or preceding DecodeN node.
 765         if (adr-&gt;in(0) == NULL) {
 766           // Keep looking for cast nodes.
 767           adr = adr-&gt;in(1);
 768           continue;
 769         }
 770         ccp-&gt;hash_delete(n);
 771         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 772         ccp-&gt;hash_insert(n);
 773         return n;
 774 
 775       case Op_CastPP:
 776         // If the CastPP is useless, just peek on through it.
 777         if( ccp-&gt;type(adr) == ccp-&gt;type(adr-&gt;in(1)) ) {
 778           // Remember the cast that we've peeked though. If we peek
 779           // through more than one, then we end up remembering the highest
 780           // one, that is, if in a loop, the one closest to the top.
 781           skipped_cast = adr;
 782           adr = adr-&gt;in(1);
 783           continue;
 784         }
 785         // CastPP is going away in this pass!  We need this memory op to be
 786         // control-dependent on the test that is guarding the CastPP.
 787         ccp-&gt;hash_delete(n);
 788         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 789         ccp-&gt;hash_insert(n);
 790         return n;
 791 
 792       case Op_Phi:
 793         // Attempt to float above a Phi to some dominating point.
 794         if (adr-&gt;in(0) != NULL &amp;&amp; adr-&gt;in(0)-&gt;is_CountedLoop()) {
 795           // If we've already peeked through a Cast (which could have set the
 796           // control), we can't float above a Phi, because the skipped Cast
 797           // may not be loop invariant.
 798           if (adr_phi_is_loop_invariant(adr, skipped_cast)) {
 799             adr = adr-&gt;in(1);
 800             continue;
 801           }
 802         }
 803 
 804         // Intentional fallthrough!
 805 
 806         // No obvious dominating point.  The mem op is pinned below the Phi
 807         // by the Phi itself.  If the Phi goes away (no true value is merged)
 808         // then the mem op can float, but not indefinitely.  It must be pinned
 809         // behind the controls leading to the Phi.
 810       case Op_CheckCastPP:
 811         // These usually stick around to change address type, however a
 812         // useless one can be elided and we still need to pick up a control edge
 813         if (adr-&gt;in(0) == NULL) {
 814           // This CheckCastPP node has NO control and is likely useless. But we
 815           // need check further up the ancestor chain for a control input to keep
 816           // the node in place. 4959717.
 817           skipped_cast = adr;
 818           adr = adr-&gt;in(1);
 819           continue;
 820         }
 821         ccp-&gt;hash_delete(n);
 822         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 823         ccp-&gt;hash_insert(n);
 824         return n;
 825 
 826         // List of "safe" opcodes; those that implicitly block the memory
 827         // op below any null check.
 828       case Op_CastX2P:          // no null checks on native pointers
 829       case Op_Parm:             // 'this' pointer is not null
 830       case Op_LoadP:            // Loading from within a klass
 831       case Op_LoadN:            // Loading from within a klass
 832       case Op_LoadKlass:        // Loading from within a klass
 833       case Op_LoadNKlass:       // Loading from within a klass
 834       case Op_ConP:             // Loading from a klass
 835       case Op_ConN:             // Loading from a klass
 836       case Op_ConNKlass:        // Loading from a klass
 837       case Op_CreateEx:         // Sucking up the guts of an exception oop
 838       case Op_Con:              // Reading from TLS
 839       case Op_CMoveP:           // CMoveP is pinned
 840       case Op_CMoveN:           // CMoveN is pinned
 841         break;                  // No progress
 842 
 843       case Op_Proj:             // Direct call to an allocation routine
 844       case Op_SCMemProj:        // Memory state from store conditional ops
 845 #ifdef ASSERT
 846         {
 847           assert(adr-&gt;as_Proj()-&gt;_con == TypeFunc::Parms, "must be return value");
 848           const Node* call = adr-&gt;in(0);
 849           if (call-&gt;is_CallJava()) {
 850             const CallJavaNode* call_java = call-&gt;as_CallJava();
 851             const TypeTuple *r = call_java-&gt;tf()-&gt;range();
 852             assert(r-&gt;cnt() &gt; TypeFunc::Parms, "must return value");
 853             const Type* ret_type = r-&gt;field_at(TypeFunc::Parms);
 854             assert(ret_type &amp;&amp; ret_type-&gt;isa_ptr(), "must return pointer");
 855             // We further presume that this is one of
 856             // new_instance_Java, new_array_Java, or
 857             // the like, but do not assert for this.
 858           } else if (call-&gt;is_Allocate()) {
 859             // similar case to new_instance_Java, etc.
 860           } else if (!call-&gt;is_CallLeaf()) {
 861             // Projections from fetch_oop (OSR) are allowed as well.
 862             ShouldNotReachHere();
 863           }
 864         }
 865 #endif
 866         break;
 867       default:
 868         ShouldNotReachHere();
 869       }
 870       break;
 871     }
 872   }
 873 
 874   return  NULL;               // No progress
 875 }
 876 
 877 
 878 //=============================================================================
 879 uint LoadNode::size_of() const { return sizeof(*this); }
 880 uint LoadNode::cmp( const Node &amp;n ) const
 881 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 882 const Type *LoadNode::bottom_type() const { return _type; }
 883 uint LoadNode::ideal_reg() const {
 884   return _type-&gt;ideal_reg();
 885 }
 886 
 887 #ifndef PRODUCT
 888 void LoadNode::dump_spec(outputStream *st) const {
 889   MemNode::dump_spec(st);
 890   if( !Verbose &amp;&amp; !WizardMode ) {
 891     // standard dump does this in Verbose and WizardMode
 892     st-&gt;print(" #"); _type-&gt;dump_on(st);
 893   }
 894 }
 895 #endif
 896 
 897 #ifdef ASSERT
 898 //----------------------------is_immutable_value-------------------------------
 899 // Helper function to allow a raw load without control edge for some cases
 900 bool LoadNode::is_immutable_value(Node* adr) {
 901   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 902           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 903           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 904            in_bytes(JavaThread::osthread_offset())));
 905 }
 906 #endif
 907 
 908 //----------------------------LoadNode::make-----------------------------------
 909 // Polymorphic factory method:
 910 Node *LoadNode::make( PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt ) {
 911   Compile* C = gvn.C;
 912 
 913   // sanity check the alias category against the created node type
 914   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 915            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 916          "use LoadKlassNode instead");
 917   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 918            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 919          "use LoadRangeNode instead");
 920   // Check control edge of raw loads
 921   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 922           // oop will be recorded in oop map if load crosses safepoint
 923           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 924           "raw memory operations should have control edge");
 925   switch (bt) {
 926   case T_BOOLEAN: return new (C) LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 927   case T_BYTE:    return new (C) LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 928   case T_INT:     return new (C) LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 929   case T_CHAR:    return new (C) LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 930   case T_SHORT:   return new (C) LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int()    );
 931   case T_LONG:    return new (C) LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long()   );
 932   case T_FLOAT:   return new (C) LoadFNode (ctl, mem, adr, adr_type, rt              );
 933   case T_DOUBLE:  return new (C) LoadDNode (ctl, mem, adr, adr_type, rt              );
 934   case T_ADDRESS: return new (C) LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr()    );
 935   case T_OBJECT:
 936 #ifdef _LP64
 937     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 938       Node* load  = gvn.transform(new (C) LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop()));
 939       return new (C) DecodeNNode(load, load-&gt;bottom_type()-&gt;make_ptr());
 940     } else
 941 #endif
 942     {
 943       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 944       return new (C) LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr());
 945     }
 946   }
 947   ShouldNotReachHere();
 948   return (LoadNode*)NULL;
 949 }
 950 
 951 LoadLNode* LoadLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt) {
 952   bool require_atomic = true;
 953   return new (C) LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), require_atomic);
 954 }
 955 
 956 
 957 
 958 
 959 //------------------------------hash-------------------------------------------
 960 uint LoadNode::hash() const {
 961   // unroll addition of interesting fields
 962   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 963 }
 964 
 965 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 966   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 967     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 968     bool is_stable_ary = FoldStableValues &amp;&amp;
 969                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 970                          tp-&gt;isa_aryptr()-&gt;is_stable();
 971 
 972     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 973   }
 974 
 975   return false;
 976 }
 977 
 978 //---------------------------can_see_stored_value------------------------------
 979 // This routine exists to make sure this set of tests is done the same
 980 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 981 // will change the graph shape in a way which makes memory alive twice at the
 982 // same time (uses the Oracle model of aliasing), then some
 983 // LoadXNode::Identity will fold things back to the equivalence-class model
 984 // of aliasing.
 985 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 986   Node* ld_adr = in(MemNode::Address);
 987   intptr_t ld_off = 0;
 988   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 989   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 990   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 991   // This is more general than load from boxing objects.
 992   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 993     uint alias_idx = atp-&gt;index();
 994     bool final = !atp-&gt;is_rewritable();
 995     Node* result = NULL;
 996     Node* current = st;
 997     // Skip through chains of MemBarNodes checking the MergeMems for
 998     // new states for the slice of this load.  Stop once any other
 999     // kind of node is encountered.  Loads from final memory can skip
1000     // through any kind of MemBar but normal loads shouldn't skip
1001     // through MemBarAcquire since the could allow them to move out of
1002     // a synchronized region.
1003     while (current-&gt;is_Proj()) {
1004       int opc = current-&gt;in(0)-&gt;Opcode();
1005       if ((final &amp;&amp; (opc == Op_MemBarAcquire || opc == Op_MemBarAcquireLock)) ||
1006           opc == Op_MemBarRelease || opc == Op_MemBarCPUOrder ||
1007           opc == Op_MemBarReleaseLock) {
1008         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
1009         if (mem-&gt;is_MergeMem()) {
1010           MergeMemNode* merge = mem-&gt;as_MergeMem();
1011           Node* new_st = merge-&gt;memory_at(alias_idx);
1012           if (new_st == merge-&gt;base_memory()) {
1013             // Keep searching
1014             current = new_st;
1015             continue;
1016           }
1017           // Save the new memory state for the slice and fall through
1018           // to exit.
1019           result = new_st;
1020         }
1021       }
1022       break;
1023     }
1024     if (result != NULL) {
1025       st = result;
1026     }
1027   }
1028 
1029   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1030   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1031   for (int trip = 0; trip &lt;= 1; trip++) {
1032 
1033     if (st-&gt;is_Store()) {
1034       Node* st_adr = st-&gt;in(MemNode::Address);
1035       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1036         // Try harder before giving up...  Match raw and non-raw pointers.
1037         intptr_t st_off = 0;
1038         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
1039         if (alloc == NULL)       return NULL;
1040         if (alloc != ld_alloc)   return NULL;
1041         if (ld_off != st_off)    return NULL;
1042         // At this point we have proven something like this setup:
1043         //  A = Allocate(...)
1044         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
1045         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
1046         // (Actually, we haven't yet proven the Q's are the same.)
1047         // In other words, we are loading from a casted version of
1048         // the same pointer-and-offset that we stored to.
1049         // Thus, we are able to replace L by V.
1050       }
1051       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1052       if (store_Opcode() != st-&gt;Opcode())
1053         return NULL;
1054       return st-&gt;in(MemNode::ValueIn);
1055     }
1056 
1057     // A load from a freshly-created object always returns zero.
1058     // (This can happen after LoadNode::Ideal resets the load's memory input
1059     // to find_captured_store, which returned InitializeNode::zero_memory.)
1060     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1061         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1062         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1063       // return a zero value for the load's basic type
1064       // (This is one of the few places where a generic PhaseTransform
1065       // can create new nodes.  Think of it as lazily manifesting
1066       // virtually pre-existing constants.)
1067       return phase-&gt;zerocon(memory_type());
1068     }
1069 
1070     // A load from an initialization barrier can match a captured store.
1071     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1072       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1073       AllocateNode* alloc = init-&gt;allocation();
1074       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1075         // examine a captured store value
1076         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1077         if (st != NULL)
1078           continue;             // take one more trip around
1079       }
1080     }
1081 
1082     // Load boxed value from result of valueOf() call is input parameter.
1083     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1084         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1085       intptr_t ignore = 0;
1086       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1087       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1088           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1089           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1090           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1091         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1092       }
1093     }
1094 
1095     break;
1096   }
1097 
1098   return NULL;
1099 }
1100 
1101 //----------------------is_instance_field_load_with_local_phi------------------
1102 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1103   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1104       in(Address)-&gt;is_AddP() ) {
1105     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1106     // Only instances and boxed values.
1107     if( t_oop != NULL &amp;&amp;
1108         (t_oop-&gt;is_ptr_to_boxed_value() ||
1109          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1110         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1111         t_oop-&gt;offset() != Type::OffsetTop) {
1112       return true;
1113     }
1114   }
1115   return false;
1116 }
1117 
1118 //------------------------------Identity---------------------------------------
1119 // Loads are identity if previous store is to same address
1120 Node *LoadNode::Identity( PhaseTransform *phase ) {
1121   // If the previous store-maker is the right kind of Store, and the store is
1122   // to the same address, then we are equal to the value stored.
1123   Node* mem = in(Memory);
1124   Node* value = can_see_stored_value(mem, phase);
1125   if( value ) {
1126     // byte, short &amp; char stores truncate naturally.
1127     // A load has to load the truncated value which requires
1128     // some sort of masking operation and that requires an
1129     // Ideal call instead of an Identity call.
1130     if (memory_size() &lt; BytesPerInt) {
1131       // If the input to the store does not fit with the load's result type,
1132       // it must be truncated via an Ideal call.
1133       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1134         return this;
1135     }
1136     // (This works even when value is a Con, but LoadNode::Value
1137     // usually runs first, producing the singleton type of the Con.)
1138     return value;
1139   }
1140 
1141   // Search for an existing data phi which was generated before for the same
1142   // instance's field to avoid infinite generation of phis in a loop.
1143   Node *region = mem-&gt;in(0);
1144   if (is_instance_field_load_with_local_phi(region)) {
1145     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1146     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1147     int this_offset = addr_t-&gt;offset();
1148     int this_iid    = addr_t-&gt;instance_id();
1149     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1150          addr_t-&gt;is_ptr_to_boxed_value()) {
1151       // Use _idx of address base (could be Phi node) for boxed values.
1152       intptr_t   ignore = 0;
1153       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1154       this_iid = base-&gt;_idx;
1155     }
1156     const Type* this_type = bottom_type();
1157     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1158       Node* phi = region-&gt;fast_out(i);
1159       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1160           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1161         return phi;
1162       }
1163     }
1164   }
1165 
1166   return this;
1167 }
1168 
1169 // We're loading from an object which has autobox behaviour.
1170 // If this object is result of a valueOf call we'll have a phi
1171 // merging a newly allocated object and a load from the cache.
1172 // We want to replace this load with the original incoming
1173 // argument to the valueOf call.
1174 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1175   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1176   intptr_t ignore = 0;
1177   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1178   if ((base == NULL) || base-&gt;is_Phi()) {
1179     // Push the loads from the phi that comes from valueOf up
1180     // through it to allow elimination of the loads and the recovery
1181     // of the original value. It is done in split_through_phi().
1182     return NULL;
1183   } else if (base-&gt;is_Load() ||
1184              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1185     // Eliminate the load of boxed value for integer types from the cache
1186     // array by deriving the value from the index into the array.
1187     // Capture the offset of the load and then reverse the computation.
1188 
1189     // Get LoadN node which loads a boxing object from 'cache' array.
1190     if (base-&gt;is_DecodeN()) {
1191       base = base-&gt;in(1);
1192     }
1193     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1194       return NULL; // Complex address
1195     }
1196     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1197     Node* cache_base = address-&gt;in(AddPNode::Base);
1198     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1199       // Get ConP node which is static 'cache' field.
1200       cache_base = cache_base-&gt;in(1);
1201     }
1202     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1203       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1204       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1205         Node* elements[4];
1206         int shift = exact_log2(type2aelembytes(T_OBJECT));
1207         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1208         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1209             ((count == 1) ||
1210              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1211                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1212           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1213           // Fetch the box object cache[0] at the base of the array and get its value
1214           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1215           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1216           assert(ik-&gt;is_box_klass(), "sanity");
1217           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1218           if (ik-&gt;nof_nonstatic_fields() == 1) {
1219             // This should be true nonstatic_field_at requires calling
1220             // nof_nonstatic_fields so check it anyway
1221             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1222             BasicType bt = c.basic_type();
1223             // Only integer types have boxing cache.
1224             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1225                    bt == T_BYTE    || bt == T_SHORT ||
1226                    bt == T_INT     || bt == T_LONG, err_msg_res("wrong type = %s", type2name(bt)));
1227             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1228             if (cache_low != (int)cache_low) {
1229               return NULL; // should not happen since cache is array indexed by value
1230             }
1231             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1232             if (offset != (int)offset) {
1233               return NULL; // should not happen since cache is array indexed by value
1234             }
1235            // Add up all the offsets making of the address of the load
1236             Node* result = elements[0];
1237             for (int i = 1; i &lt; count; i++) {
1238               result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, elements[i]));
1239             }
1240             // Remove the constant offset from the address and then
1241             result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1242             // remove the scaling of the offset to recover the original index.
1243             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1244               // Peel the shift off directly but wrap it in a dummy node
1245               // since Ideal can't return existing nodes
1246               result = new (phase-&gt;C) RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1247             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1248                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1249                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1250               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1251               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1252               // (there is range check) so we do this optimizatrion by hand here.
1253               Node* add_con = new (phase-&gt;C) RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1254               result = new (phase-&gt;C) AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1255             } else {
1256               result = new (phase-&gt;C) RShiftXNode(result, phase-&gt;intcon(shift));
1257             }
1258 #ifdef _LP64
1259             if (bt != T_LONG) {
1260               result = new (phase-&gt;C) ConvL2INode(phase-&gt;transform(result));
1261             }
1262 #else
1263             if (bt == T_LONG) {
1264               result = new (phase-&gt;C) ConvI2LNode(phase-&gt;transform(result));
1265             }
1266 #endif
1267             return result;
1268           }
1269         }
1270       }
1271     }
1272   }
1273   return NULL;
1274 }
1275 
1276 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1277   Node* region = phi-&gt;in(0);
1278   if (region == NULL) {
1279     return false; // Wait stable graph
1280   }
1281   uint cnt = phi-&gt;req();
1282   for (uint i = 1; i &lt; cnt; i++) {
1283     Node* rc = region-&gt;in(i);
1284     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1285       return false; // Wait stable graph
1286     Node* in = phi-&gt;in(i);
1287     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1288       return false; // Wait stable graph
1289   }
1290   return true;
1291 }
1292 //------------------------------split_through_phi------------------------------
1293 // Split instance or boxed field load through Phi.
1294 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1295   Node* mem     = in(Memory);
1296   Node* address = in(Address);
1297   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1298 
1299   assert((t_oop != NULL) &amp;&amp;
1300          (t_oop-&gt;is_known_instance_field() ||
1301           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1302 
1303   Compile* C = phase-&gt;C;
1304   intptr_t ignore = 0;
1305   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1306   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1307   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1308                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1309                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1310 
1311   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1312         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1313     return NULL; // memory is not Phi
1314   }
1315 
1316   if (mem-&gt;is_Phi()) {
1317     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1318       return NULL; // Wait stable graph
1319     }
1320     uint cnt = mem-&gt;req();
1321     // Check for loop invariant memory.
1322     if (cnt == 3) {
1323       for (uint i = 1; i &lt; cnt; i++) {
1324         Node* in = mem-&gt;in(i);
1325         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1326         if (m == mem) {
1327           set_req(Memory, mem-&gt;in(cnt - i));
1328           return this; // made change
1329         }
1330       }
1331     }
1332   }
1333   if (base_is_phi) {
1334     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1335       return NULL; // Wait stable graph
1336     }
1337     uint cnt = base-&gt;req();
1338     // Check for loop invariant memory.
1339     if (cnt == 3) {
1340       for (uint i = 1; i &lt; cnt; i++) {
1341         if (base-&gt;in(i) == base) {
1342           return NULL; // Wait stable graph
1343         }
1344       }
1345     }
1346   }
1347 
1348   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1349 
1350   // Split through Phi (see original code in loopopts.cpp).
1351   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1352 
1353   // Do nothing here if Identity will find a value
1354   // (to avoid infinite chain of value phis generation).
1355   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1356     return NULL;
1357 
1358   // Select Region to split through.
1359   Node* region;
1360   if (!base_is_phi) {
1361     assert(mem-&gt;is_Phi(), "sanity");
1362     region = mem-&gt;in(0);
1363     // Skip if the region dominates some control edge of the address.
1364     if (!MemNode::all_controls_dominate(address, region))
1365       return NULL;
1366   } else if (!mem-&gt;is_Phi()) {
1367     assert(base_is_phi, "sanity");
1368     region = base-&gt;in(0);
1369     // Skip if the region dominates some control edge of the memory.
1370     if (!MemNode::all_controls_dominate(mem, region))
1371       return NULL;
1372   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1373     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1374     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1375       region = base-&gt;in(0);
1376     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1377       region = mem-&gt;in(0);
1378     } else {
1379       return NULL; // complex graph
1380     }
1381   } else {
1382     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1383     region = mem-&gt;in(0);
1384   }
1385 
1386   const Type* this_type = this-&gt;bottom_type();
1387   int this_index  = C-&gt;get_alias_index(t_oop);
1388   int this_offset = t_oop-&gt;offset();
1389   int this_iid    = t_oop-&gt;instance_id();
1390   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1391     // Use _idx of address base for boxed values.
1392     this_iid = base-&gt;_idx;
1393   }
1394   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1395   Node* phi = new (C) PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1396   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1397     Node* x;
1398     Node* the_clone = NULL;
1399     if (region-&gt;in(i) == C-&gt;top()) {
1400       x = C-&gt;top();      // Dead path?  Use a dead data op
1401     } else {
1402       x = this-&gt;clone();        // Else clone up the data op
1403       the_clone = x;            // Remember for possible deletion.
1404       // Alter data node to use pre-phi inputs
1405       if (this-&gt;in(0) == region) {
1406         x-&gt;set_req(0, region-&gt;in(i));
1407       } else {
1408         x-&gt;set_req(0, NULL);
1409       }
1410       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1411         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1412       }
1413       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1414         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1415       }
1416       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1417         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1418         Node* adr_x = phase-&gt;transform(new (C) AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1419         x-&gt;set_req(Address, adr_x);
1420       }
1421     }
1422     // Check for a 'win' on some paths
1423     const Type *t = x-&gt;Value(igvn);
1424 
1425     bool singleton = t-&gt;singleton();
1426 
1427     // See comments in PhaseIdealLoop::split_thru_phi().
1428     if (singleton &amp;&amp; t == Type::TOP) {
1429       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1430     }
1431 
1432     if (singleton) {
1433       x = igvn-&gt;makecon(t);
1434     } else {
1435       // We now call Identity to try to simplify the cloned node.
1436       // Note that some Identity methods call phase-&gt;type(this).
1437       // Make sure that the type array is big enough for
1438       // our new node, even though we may throw the node away.
1439       // (This tweaking with igvn only works because x is a new node.)
1440       igvn-&gt;set_type(x, t);
1441       // If x is a TypeNode, capture any more-precise type permanently into Node
1442       // otherwise it will be not updated during igvn-&gt;transform since
1443       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1444       x-&gt;raise_bottom_type(t);
1445       Node *y = x-&gt;Identity(igvn);
1446       if (y != x) {
1447         x = y;
1448       } else {
1449         y = igvn-&gt;hash_find_insert(x);
1450         if (y) {
1451           x = y;
1452         } else {
1453           // Else x is a new node we are keeping
1454           // We do not need register_new_node_with_optimizer
1455           // because set_type has already been called.
1456           igvn-&gt;_worklist.push(x);
1457         }
1458       }
1459     }
1460     if (x != the_clone &amp;&amp; the_clone != NULL) {
1461       igvn-&gt;remove_dead_node(the_clone);
1462     }
1463     phi-&gt;set_req(i, x);
1464   }
1465   // Record Phi
1466   igvn-&gt;register_new_node_with_optimizer(phi);
1467   return phi;
1468 }
1469 
1470 //------------------------------Ideal------------------------------------------
1471 // If the load is from Field memory and the pointer is non-null, we can
1472 // zero out the control input.
1473 // If the offset is constant and the base is an object allocation,
1474 // try to hook me up to the exact initializing store.
1475 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1476   Node* p = MemNode::Ideal_common(phase, can_reshape);
1477   if (p)  return (p == NodeSentinel) ? NULL : p;
1478 
1479   Node* ctrl    = in(MemNode::Control);
1480   Node* address = in(MemNode::Address);
1481 
1482   // Skip up past a SafePoint control.  Cannot do this for Stores because
1483   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1484   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1485       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1486     ctrl = ctrl-&gt;in(0);
1487     set_req(MemNode::Control,ctrl);
1488   }
1489 
1490   intptr_t ignore = 0;
1491   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1492   if (base != NULL
1493       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1494     // Check for useless control edge in some common special cases
1495     if (in(MemNode::Control) != NULL
1496         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1497         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1498       // A method-invariant, non-null address (constant or 'this' argument).
1499       set_req(MemNode::Control, NULL);
1500     }
1501   }
1502 
1503   Node* mem = in(MemNode::Memory);
1504   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1505 
1506   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1507     // try to optimize our memory input
1508     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1509     if (opt_mem != mem) {
1510       set_req(MemNode::Memory, opt_mem);
1511       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1512       return this;
1513     }
1514     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1515     if ((t_oop != NULL) &amp;&amp;
1516         (t_oop-&gt;is_known_instance_field() ||
1517          t_oop-&gt;is_ptr_to_boxed_value())) {
1518       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1519       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1520         // Delay this transformation until memory Phi is processed.
1521         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1522         return NULL;
1523       }
1524       // Split instance field load through Phi.
1525       Node* result = split_through_phi(phase);
1526       if (result != NULL) return result;
1527 
1528       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1529         Node* result = eliminate_autobox(phase);
1530         if (result != NULL) return result;
1531       }
1532     }
1533   }
1534 
1535   // Check for prior store with a different base or offset; make Load
1536   // independent.  Skip through any number of them.  Bail out if the stores
1537   // are in an endless dead cycle and report no progress.  This is a key
1538   // transform for Reflection.  However, if after skipping through the Stores
1539   // we can't then fold up against a prior store do NOT do the transform as
1540   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1541   // array memory alive twice: once for the hoisted Load and again after the
1542   // bypassed Store.  This situation only works if EVERYBODY who does
1543   // anti-dependence work knows how to bypass.  I.e. we need all
1544   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1545   // the alias index stuff.  So instead, peek through Stores and IFF we can
1546   // fold up, do so.
1547   Node* prev_mem = find_previous_store(phase);
1548   // Steps (a), (b):  Walk past independent stores to find an exact match.
1549   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1550     // (c) See if we can fold up on the spot, but don't fold up here.
1551     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1552     // just return a prior value, which is done by Identity calls.
1553     if (can_see_stored_value(prev_mem, phase)) {
1554       // Make ready for step (d):
1555       set_req(MemNode::Memory, prev_mem);
1556       return this;
1557     }
1558   }
1559 
1560   return NULL;                  // No further progress
1561 }
1562 
1563 // Helper to recognize certain Klass fields which are invariant across
1564 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1565 const Type*
1566 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1567                                  ciKlass* klass) const {
1568   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1569     // The field is Klass::_modifier_flags.  Return its (constant) value.
1570     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1571     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1572     return TypeInt::make(klass-&gt;modifier_flags());
1573   }
1574   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1575     // The field is Klass::_access_flags.  Return its (constant) value.
1576     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1577     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1578     return TypeInt::make(klass-&gt;access_flags());
1579   }
1580   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1581     // The field is Klass::_layout_helper.  Return its constant value if known.
1582     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1583     return TypeInt::make(klass-&gt;layout_helper());
1584   }
1585 
1586   // No match.
1587   return NULL;
1588 }
1589 
1590 // Try to constant-fold a stable array element.
1591 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1592   assert(ary-&gt;is_stable(), "array should be stable");
1593 
1594   if (ary-&gt;const_oop() != NULL) {
1595     // Decode the results of GraphKit::array_element_address.
1596     ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1597     ciConstant con = aobj-&gt;element_value_by_offset(off);
1598 
1599     if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1600       const Type* con_type = Type::make_from_constant(con);
1601       if (con_type != NULL) {
1602         if (con_type-&gt;isa_aryptr()) {
1603           // Join with the array element type, in case it is also stable.
1604           int dim = ary-&gt;stable_dimension();
1605           con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1606         }
1607         if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1608           con_type = con_type-&gt;make_narrowoop();
1609         }
1610 #ifndef PRODUCT
1611         if (TraceIterativeGVN) {
1612           tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1613           con_type-&gt;dump(); tty-&gt;cr();
1614         }
1615 #endif //PRODUCT
1616         return con_type;
1617       }
1618     }
1619   }
1620 
1621   return NULL;
1622 }
1623 
1624 //------------------------------Value-----------------------------------------
1625 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1626   // Either input is TOP ==&gt; the result is TOP
1627   Node* mem = in(MemNode::Memory);
1628   const Type *t1 = phase-&gt;type(mem);
1629   if (t1 == Type::TOP)  return Type::TOP;
1630   Node* adr = in(MemNode::Address);
1631   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1632   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1633   int off = tp-&gt;offset();
1634   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1635   Compile* C = phase-&gt;C;
1636 
1637   // Try to guess loaded type from pointer type
1638   if (tp-&gt;isa_aryptr()) {
1639     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1640     const Type *t = ary-&gt;elem();
1641 
1642     // Determine whether the reference is beyond the header or not, by comparing
1643     // the offset against the offset of the start of the array's data.
1644     // Different array types begin at slightly different offsets (12 vs. 16).
1645     // We choose T_BYTE as an example base type that is least restrictive
1646     // as to alignment, which will therefore produce the smallest
1647     // possible base offset.
1648     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1649     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1650 
1651     // Try to constant-fold a stable array element.
1652     if (FoldStableValues &amp;&amp; ary-&gt;is_stable()) {
1653       // Make sure the reference is not into the header
1654       if (off_beyond_header &amp;&amp; off != Type::OffsetBot) {
1655         assert(adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Offset)-&gt;is_Con(), "offset is a constant");
1656         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1657         if (con_type != NULL) {
1658           return con_type;
1659         }
1660       }
1661     }
1662 
1663     // Don't do this for integer types. There is only potential profit if
1664     // the element type t is lower than _type; that is, for int types, if _type is
1665     // more restrictive than t.  This only happens here if one is short and the other
1666     // char (both 16 bits), and in those cases we've made an intentional decision
1667     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1668     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1669     //
1670     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1671     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1672     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1673     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1674     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1675     // In fact, that could have been the original type of p1, and p1 could have
1676     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1677     // expression (LShiftL quux 3) independently optimized to the constant 8.
1678     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1679         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1680         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1681       // t might actually be lower than _type, if _type is a unique
1682       // concrete subclass of abstract class t.
1683       if (off_beyond_header) {  // is the offset beyond the header?
1684         const Type* jt = t-&gt;join(_type);
1685         // In any case, do not allow the join, per se, to empty out the type.
1686         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1687           // This can happen if a interface-typed array narrows to a class type.
1688           jt = _type;
1689         }
1690 #ifdef ASSERT
1691         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1692           // The pointers in the autobox arrays are always non-null
1693           Node* base = adr-&gt;in(AddPNode::Base);
1694           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1695             // Get LoadN node which loads IntegerCache.cache field
1696             base = base-&gt;in(1);
1697           }
1698           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1699             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1700             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1701               // It could be narrow oop
1702               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1703             }
1704           }
1705         }
1706 #endif
1707         return jt;
1708       }
1709     }
1710   } else if (tp-&gt;base() == Type::InstPtr) {
1711     ciEnv* env = C-&gt;env();
1712     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1713     ciKlass* klass = tinst-&gt;klass();
1714     assert( off != Type::OffsetBot ||
1715             // arrays can be cast to Objects
1716             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1717             // unsafe field access may not have a constant offset
1718             C-&gt;has_unsafe_access(),
1719             "Field accesses must be precise" );
1720     // For oop loads, we expect the _type to be precise
1721     if (klass == env-&gt;String_klass() &amp;&amp;
1722         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1723       // For constant Strings treat the final fields as compile time constants.
1724       Node* base = adr-&gt;in(AddPNode::Base);
1725       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1726       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1727         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1728         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1729           ciObject* string = t-&gt;const_oop();
1730           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1731           if (constant.basic_type() == T_INT) {
1732             return TypeInt::make(constant.as_int());
1733           } else if (constant.basic_type() == T_ARRAY) {
1734             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1735               return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1736             } else {
1737               return TypeOopPtr::make_from_constant(constant.as_object(), true);
1738             }
1739           }
1740         }
1741       }
1742     }
1743     // Optimizations for constant objects
1744     ciObject* const_oop = tinst-&gt;const_oop();
1745     if (const_oop != NULL) {
1746       // For constant Boxed value treat the target field as a compile time constant.
1747       if (tinst-&gt;is_ptr_to_boxed_value()) {
1748         return tinst-&gt;get_const_boxed_value();
1749       } else
1750       // For constant CallSites treat the target field as a compile time constant.
1751       if (const_oop-&gt;is_call_site()) {
1752         ciCallSite* call_site = const_oop-&gt;as_call_site();
1753         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1754         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1755           ciMethodHandle* target = call_site-&gt;get_target();
1756           if (target != NULL) {  // just in case
1757             ciConstant constant(T_OBJECT, target);
1758             const Type* t;
1759             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1760               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1761             } else {
1762               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1763             }
1764             // Add a dependence for invalidation of the optimization.
1765             if (!call_site-&gt;is_constant_call_site()) {
<a name="1" id="anc1"></a><span class="new">1766               VM_ENTRY_MARK;</span>
1767               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1768             }
1769             return t;
1770           }
1771         }
1772       }
1773     }
1774   } else if (tp-&gt;base() == Type::KlassPtr) {
1775     assert( off != Type::OffsetBot ||
1776             // arrays can be cast to Objects
1777             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1778             // also allow array-loading from the primary supertype
1779             // array during subtype checks
1780             Opcode() == Op_LoadKlass,
1781             "Field accesses must be precise" );
1782     // For klass/static loads, we expect the _type to be precise
1783   }
1784 
1785   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1786   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1787     ciKlass* klass = tkls-&gt;klass();
1788     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1789       // We are loading a field from a Klass metaobject whose identity
1790       // is known at compile time (the type is "exact" or "precise").
1791       // Check for fields we know are maintained as constants by the VM.
1792       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1793         // The field is Klass::_super_check_offset.  Return its (constant) value.
1794         // (Folds up type checking code.)
1795         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1796         return TypeInt::make(klass-&gt;super_check_offset());
1797       }
1798       // Compute index into primary_supers array
1799       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1800       // Check for overflowing; use unsigned compare to handle the negative case.
1801       if( depth &lt; ciKlass::primary_super_limit() ) {
1802         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1803         // (Folds up type checking code.)
1804         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1805         ciKlass *ss = klass-&gt;super_of_depth(depth);
1806         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1807       }
1808       const Type* aift = load_array_final_field(tkls, klass);
1809       if (aift != NULL)  return aift;
1810       if (tkls-&gt;offset() == in_bytes(ArrayKlass::component_mirror_offset())
1811           &amp;&amp; klass-&gt;is_array_klass()) {
1812         // The field is ArrayKlass::_component_mirror.  Return its (constant) value.
1813         // (Folds up aClassConstant.getComponentType, common in Arrays.copyOf.)
1814         assert(Opcode() == Op_LoadP, "must load an oop from _component_mirror");
1815         return TypeInstPtr::make(klass-&gt;as_array_klass()-&gt;component_mirror());
1816       }
1817       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1818         // The field is Klass::_java_mirror.  Return its (constant) value.
1819         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1820         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1821         return TypeInstPtr::make(klass-&gt;java_mirror());
1822       }
1823     }
1824 
1825     // We can still check if we are loading from the primary_supers array at a
1826     // shallow enough depth.  Even though the klass is not exact, entries less
1827     // than or equal to its super depth are correct.
1828     if (klass-&gt;is_loaded() ) {
1829       ciType *inner = klass;
1830       while( inner-&gt;is_obj_array_klass() )
1831         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1832       if( inner-&gt;is_instance_klass() &amp;&amp;
1833           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1834         // Compute index into primary_supers array
1835         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1836         // Check for overflowing; use unsigned compare to handle the negative case.
1837         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1838             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1839           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1840           // (Folds up type checking code.)
1841           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1842           ciKlass *ss = klass-&gt;super_of_depth(depth);
1843           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1844         }
1845       }
1846     }
1847 
1848     // If the type is enough to determine that the thing is not an array,
1849     // we can give the layout_helper a positive interval type.
1850     // This will help short-circuit some reflective code.
1851     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1852         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1853         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1854         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1855         ) {
1856       // Note:  When interfaces are reliable, we can narrow the interface
1857       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1858       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1859       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1860       // The key property of this type is that it folds up tests
1861       // for array-ness, since it proves that the layout_helper is positive.
1862       // Thus, a generic value like the basic object layout helper works fine.
1863       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1864     }
1865   }
1866 
1867   // If we are loading from a freshly-allocated object, produce a zero,
1868   // if the load is provably beyond the header of the object.
1869   // (Also allow a variable load from a fresh array to produce zero.)
1870   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1871   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1872   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1873   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1874     Node* value = can_see_stored_value(mem,phase);
1875     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1876       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1877       return value-&gt;bottom_type();
1878     }
1879   }
1880 
1881   if (is_instance) {
1882     // If we have an instance type and our memory input is the
1883     // programs's initial memory state, there is no matching store,
1884     // so just return a zero of the appropriate type
1885     Node *mem = in(MemNode::Memory);
1886     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1887       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1888       return Type::get_zero_type(_type-&gt;basic_type());
1889     }
1890   }
1891   return _type;
1892 }
1893 
1894 //------------------------------match_edge-------------------------------------
1895 // Do we Match on this edge index or not?  Match only the address.
1896 uint LoadNode::match_edge(uint idx) const {
1897   return idx == MemNode::Address;
1898 }
1899 
1900 //--------------------------LoadBNode::Ideal--------------------------------------
1901 //
1902 //  If the previous store is to the same address as this load,
1903 //  and the value stored was larger than a byte, replace this load
1904 //  with the value stored truncated to a byte.  If no truncation is
1905 //  needed, the replacement is done in LoadNode::Identity().
1906 //
1907 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1908   Node* mem = in(MemNode::Memory);
1909   Node* value = can_see_stored_value(mem,phase);
1910   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1911     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(24)) );
1912     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(24));
1913   }
1914   // Identity call will handle the case where truncation is not needed.
1915   return LoadNode::Ideal(phase, can_reshape);
1916 }
1917 
1918 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1919   Node* mem = in(MemNode::Memory);
1920   Node* value = can_see_stored_value(mem,phase);
1921   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1922       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1923     // If the input to the store does not fit with the load's result type,
1924     // it must be truncated. We can't delay until Ideal call since
1925     // a singleton Value is needed for split_thru_phi optimization.
1926     int con = value-&gt;get_int();
1927     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1928   }
1929   return LoadNode::Value(phase);
1930 }
1931 
1932 //--------------------------LoadUBNode::Ideal-------------------------------------
1933 //
1934 //  If the previous store is to the same address as this load,
1935 //  and the value stored was larger than a byte, replace this load
1936 //  with the value stored truncated to a byte.  If no truncation is
1937 //  needed, the replacement is done in LoadNode::Identity().
1938 //
1939 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1940   Node* mem = in(MemNode::Memory);
1941   Node* value = can_see_stored_value(mem, phase);
1942   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1943     return new (phase-&gt;C) AndINode(value, phase-&gt;intcon(0xFF));
1944   // Identity call will handle the case where truncation is not needed.
1945   return LoadNode::Ideal(phase, can_reshape);
1946 }
1947 
1948 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1949   Node* mem = in(MemNode::Memory);
1950   Node* value = can_see_stored_value(mem,phase);
1951   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1952       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1953     // If the input to the store does not fit with the load's result type,
1954     // it must be truncated. We can't delay until Ideal call since
1955     // a singleton Value is needed for split_thru_phi optimization.
1956     int con = value-&gt;get_int();
1957     return TypeInt::make(con &amp; 0xFF);
1958   }
1959   return LoadNode::Value(phase);
1960 }
1961 
1962 //--------------------------LoadUSNode::Ideal-------------------------------------
1963 //
1964 //  If the previous store is to the same address as this load,
1965 //  and the value stored was larger than a char, replace this load
1966 //  with the value stored truncated to a char.  If no truncation is
1967 //  needed, the replacement is done in LoadNode::Identity().
1968 //
1969 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1970   Node* mem = in(MemNode::Memory);
1971   Node* value = can_see_stored_value(mem,phase);
1972   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1973     return new (phase-&gt;C) AndINode(value,phase-&gt;intcon(0xFFFF));
1974   // Identity call will handle the case where truncation is not needed.
1975   return LoadNode::Ideal(phase, can_reshape);
1976 }
1977 
1978 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1979   Node* mem = in(MemNode::Memory);
1980   Node* value = can_see_stored_value(mem,phase);
1981   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1982       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1983     // If the input to the store does not fit with the load's result type,
1984     // it must be truncated. We can't delay until Ideal call since
1985     // a singleton Value is needed for split_thru_phi optimization.
1986     int con = value-&gt;get_int();
1987     return TypeInt::make(con &amp; 0xFFFF);
1988   }
1989   return LoadNode::Value(phase);
1990 }
1991 
1992 //--------------------------LoadSNode::Ideal--------------------------------------
1993 //
1994 //  If the previous store is to the same address as this load,
1995 //  and the value stored was larger than a short, replace this load
1996 //  with the value stored truncated to a short.  If no truncation is
1997 //  needed, the replacement is done in LoadNode::Identity().
1998 //
1999 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2000   Node* mem = in(MemNode::Memory);
2001   Node* value = can_see_stored_value(mem,phase);
2002   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
2003     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(16)) );
2004     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(16));
2005   }
2006   // Identity call will handle the case where truncation is not needed.
2007   return LoadNode::Ideal(phase, can_reshape);
2008 }
2009 
2010 const Type* LoadSNode::Value(PhaseTransform *phase) const {
2011   Node* mem = in(MemNode::Memory);
2012   Node* value = can_see_stored_value(mem,phase);
2013   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2014       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2015     // If the input to the store does not fit with the load's result type,
2016     // it must be truncated. We can't delay until Ideal call since
2017     // a singleton Value is needed for split_thru_phi optimization.
2018     int con = value-&gt;get_int();
2019     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2020   }
2021   return LoadNode::Value(phase);
2022 }
2023 
2024 //=============================================================================
2025 //----------------------------LoadKlassNode::make------------------------------
2026 // Polymorphic factory method:
2027 Node *LoadKlassNode::make( PhaseGVN&amp; gvn, Node *mem, Node *adr, const TypePtr* at, const TypeKlassPtr *tk ) {
2028   Compile* C = gvn.C;
2029   Node *ctl = NULL;
2030   // sanity check the alias category against the created node type
2031   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2032   assert(adr_type != NULL, "expecting TypeKlassPtr");
2033 #ifdef _LP64
2034   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2035     assert(UseCompressedClassPointers, "no compressed klasses");
2036     Node* load_klass = gvn.transform(new (C) LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass()));
2037     return new (C) DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2038   }
2039 #endif
2040   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2041   return new (C) LoadKlassNode(ctl, mem, adr, at, tk);
2042 }
2043 
2044 //------------------------------Value------------------------------------------
2045 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2046   return klass_value_common(phase);
2047 }
2048 
2049 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2050   // Either input is TOP ==&gt; the result is TOP
2051   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2052   if (t1 == Type::TOP)  return Type::TOP;
2053   Node *adr = in(MemNode::Address);
2054   const Type *t2 = phase-&gt;type( adr );
2055   if (t2 == Type::TOP)  return Type::TOP;
2056   const TypePtr *tp = t2-&gt;is_ptr();
2057   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2058       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2059 
2060   // Return a more precise klass, if possible
2061   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2062   if (tinst != NULL) {
2063     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2064     int offset = tinst-&gt;offset();
2065     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2066         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2067             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2068       // We are loading a special hidden field from a Class mirror object,
2069       // the field which points to the VM's Klass metaobject.
2070       ciType* t = tinst-&gt;java_mirror_type();
2071       // java_mirror_type returns non-null for compile-time Class constants.
2072       if (t != NULL) {
2073         // constant oop =&gt; constant klass
2074         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2075           if (t-&gt;is_void()) {
2076             // We cannot create a void array.  Since void is a primitive type return null
2077             // klass.  Users of this result need to do a null check on the returned klass.
2078             return TypePtr::NULL_PTR;
2079           }
2080           return TypeKlassPtr::make(ciArrayKlass::make(t));
2081         }
2082         if (!t-&gt;is_klass()) {
2083           // a primitive Class (e.g., int.class) has NULL for a klass field
2084           return TypePtr::NULL_PTR;
2085         }
2086         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2087         return TypeKlassPtr::make(t-&gt;as_klass());
2088       }
2089       // non-constant mirror, so we can't tell what's going on
2090     }
2091     if( !ik-&gt;is_loaded() )
2092       return _type;             // Bail out if not loaded
2093     if (offset == oopDesc::klass_offset_in_bytes()) {
2094       if (tinst-&gt;klass_is_exact()) {
2095         return TypeKlassPtr::make(ik);
2096       }
2097       // See if we can become precise: no subklasses and no interface
2098       // (Note:  We need to support verified interfaces.)
2099       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2100         //assert(!UseExactTypes, "this code should be useless with exact types");
2101         // Add a dependence; if any subclass added we need to recompile
2102         if (!ik-&gt;is_final()) {
2103           // %%% should use stronger assert_unique_concrete_subtype instead
2104           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2105         }
2106         // Return precise klass
2107         return TypeKlassPtr::make(ik);
2108       }
2109 
2110       // Return root of possible klass
2111       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2112     }
2113   }
2114 
2115   // Check for loading klass from an array
2116   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2117   if( tary != NULL ) {
2118     ciKlass *tary_klass = tary-&gt;klass();
2119     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2120         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2121       if (tary-&gt;klass_is_exact()) {
2122         return TypeKlassPtr::make(tary_klass);
2123       }
2124       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2125       // If the klass is an object array, we defer the question to the
2126       // array component klass.
2127       if( ak-&gt;is_obj_array_klass() ) {
2128         assert( ak-&gt;is_loaded(), "" );
2129         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2130         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2131           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2132           // See if we can become precise: no subklasses and no interface
2133           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2134             //assert(!UseExactTypes, "this code should be useless with exact types");
2135             // Add a dependence; if any subclass added we need to recompile
2136             if (!ik-&gt;is_final()) {
2137               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2138             }
2139             // Return precise array klass
2140             return TypeKlassPtr::make(ak);
2141           }
2142         }
2143         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2144       } else {                  // Found a type-array?
2145         //assert(!UseExactTypes, "this code should be useless with exact types");
2146         assert( ak-&gt;is_type_array_klass(), "" );
2147         return TypeKlassPtr::make(ak); // These are always precise
2148       }
2149     }
2150   }
2151 
2152   // Check for loading klass from an array klass
2153   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2154   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2155     ciKlass* klass = tkls-&gt;klass();
2156     if( !klass-&gt;is_loaded() )
2157       return _type;             // Bail out if not loaded
2158     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2159         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2160       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2161       // // Always returning precise element type is incorrect,
2162       // // e.g., element type could be object and array may contain strings
2163       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2164 
2165       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2166       // according to the element type's subclassing.
2167       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2168     }
2169     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2170         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2171       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2172       // The field is Klass::_super.  Return its (constant) value.
2173       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2174       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2175     }
2176   }
2177 
2178   // Bailout case
2179   return LoadNode::Value(phase);
2180 }
2181 
2182 //------------------------------Identity---------------------------------------
2183 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2184 // Also feed through the klass in Allocate(...klass...)._klass.
2185 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2186   return klass_identity_common(phase);
2187 }
2188 
2189 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2190   Node* x = LoadNode::Identity(phase);
2191   if (x != this)  return x;
2192 
2193   // Take apart the address into an oop and and offset.
2194   // Return 'this' if we cannot.
2195   Node*    adr    = in(MemNode::Address);
2196   intptr_t offset = 0;
2197   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2198   if (base == NULL)     return this;
2199   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2200   if (toop == NULL)     return this;
2201 
2202   // We can fetch the klass directly through an AllocateNode.
2203   // This works even if the klass is not constant (clone or newArray).
2204   if (offset == oopDesc::klass_offset_in_bytes()) {
2205     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2206     if (allocated_klass != NULL) {
2207       return allocated_klass;
2208     }
2209   }
2210 
2211   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2212   // Simplify ak.component_mirror.array_klass to plain ak, ak an ArrayKlass.
2213   // See inline_native_Class_query for occurrences of these patterns.
2214   // Java Example:  x.getClass().isAssignableFrom(y)
2215   // Java Example:  Array.newInstance(x.getClass().getComponentType(), n)
2216   //
2217   // This improves reflective code, often making the Class
2218   // mirror go completely dead.  (Current exception:  Class
2219   // mirrors may appear in debug info, but we could clean them out by
2220   // introducing a new debug info operator for Klass*.java_mirror).
2221   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2222       &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2223           offset == java_lang_Class::array_klass_offset_in_bytes())) {
2224     // We are loading a special hidden field from a Class mirror,
2225     // the field which points to its Klass or ArrayKlass metaobject.
2226     if (base-&gt;is_Load()) {
2227       Node* adr2 = base-&gt;in(MemNode::Address);
2228       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2229       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2230           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2231               tkls-&gt;klass()-&gt;is_array_klass())
2232           &amp;&amp; adr2-&gt;is_AddP()
2233           ) {
2234         int mirror_field = in_bytes(Klass::java_mirror_offset());
2235         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2236           mirror_field = in_bytes(ArrayKlass::component_mirror_offset());
2237         }
2238         if (tkls-&gt;offset() == mirror_field) {
2239           return adr2-&gt;in(AddPNode::Base);
2240         }
2241       }
2242     }
2243   }
2244 
2245   return this;
2246 }
2247 
2248 
2249 //------------------------------Value------------------------------------------
2250 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2251   const Type *t = klass_value_common(phase);
2252   if (t == Type::TOP)
2253     return t;
2254 
2255   return t-&gt;make_narrowklass();
2256 }
2257 
2258 //------------------------------Identity---------------------------------------
2259 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2260 // Also feed through the klass in Allocate(...klass...)._klass.
2261 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2262   Node *x = klass_identity_common(phase);
2263 
2264   const Type *t = phase-&gt;type( x );
2265   if( t == Type::TOP ) return x;
2266   if( t-&gt;isa_narrowklass()) return x;
2267   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2268 
2269   return phase-&gt;transform(new (phase-&gt;C) EncodePKlassNode(x, t-&gt;make_narrowklass()));
2270 }
2271 
2272 //------------------------------Value-----------------------------------------
2273 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2274   // Either input is TOP ==&gt; the result is TOP
2275   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2276   if( t1 == Type::TOP ) return Type::TOP;
2277   Node *adr = in(MemNode::Address);
2278   const Type *t2 = phase-&gt;type( adr );
2279   if( t2 == Type::TOP ) return Type::TOP;
2280   const TypePtr *tp = t2-&gt;is_ptr();
2281   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2282   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2283   if( !tap ) return _type;
2284   return tap-&gt;size();
2285 }
2286 
2287 //-------------------------------Ideal---------------------------------------
2288 // Feed through the length in AllocateArray(...length...)._length.
2289 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2290   Node* p = MemNode::Ideal_common(phase, can_reshape);
2291   if (p)  return (p == NodeSentinel) ? NULL : p;
2292 
2293   // Take apart the address into an oop and and offset.
2294   // Return 'this' if we cannot.
2295   Node*    adr    = in(MemNode::Address);
2296   intptr_t offset = 0;
2297   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2298   if (base == NULL)     return NULL;
2299   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2300   if (tary == NULL)     return NULL;
2301 
2302   // We can fetch the length directly through an AllocateArrayNode.
2303   // This works even if the length is not constant (clone or newArray).
2304   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2305     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2306     if (alloc != NULL) {
2307       Node* allocated_length = alloc-&gt;Ideal_length();
2308       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2309       if (allocated_length != len) {
2310         // New CastII improves on this.
2311         return len;
2312       }
2313     }
2314   }
2315 
2316   return NULL;
2317 }
2318 
2319 //------------------------------Identity---------------------------------------
2320 // Feed through the length in AllocateArray(...length...)._length.
2321 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2322   Node* x = LoadINode::Identity(phase);
2323   if (x != this)  return x;
2324 
2325   // Take apart the address into an oop and and offset.
2326   // Return 'this' if we cannot.
2327   Node*    adr    = in(MemNode::Address);
2328   intptr_t offset = 0;
2329   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2330   if (base == NULL)     return this;
2331   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2332   if (tary == NULL)     return this;
2333 
2334   // We can fetch the length directly through an AllocateArrayNode.
2335   // This works even if the length is not constant (clone or newArray).
2336   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2337     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2338     if (alloc != NULL) {
2339       Node* allocated_length = alloc-&gt;Ideal_length();
2340       // Do not allow make_ideal_length to allocate a CastII node.
2341       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2342       if (allocated_length == len) {
2343         // Return allocated_length only if it would not be improved by a CastII.
2344         return allocated_length;
2345       }
2346     }
2347   }
2348 
2349   return this;
2350 
2351 }
2352 
2353 //=============================================================================
2354 //---------------------------StoreNode::make-----------------------------------
2355 // Polymorphic factory method:
2356 StoreNode* StoreNode::make( PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt ) {
2357   Compile* C = gvn.C;
2358   assert( C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2359           ctl != NULL, "raw memory operations should have control edge");
2360 
2361   switch (bt) {
2362   case T_BOOLEAN:
2363   case T_BYTE:    return new (C) StoreBNode(ctl, mem, adr, adr_type, val);
2364   case T_INT:     return new (C) StoreINode(ctl, mem, adr, adr_type, val);
2365   case T_CHAR:
2366   case T_SHORT:   return new (C) StoreCNode(ctl, mem, adr, adr_type, val);
2367   case T_LONG:    return new (C) StoreLNode(ctl, mem, adr, adr_type, val);
2368   case T_FLOAT:   return new (C) StoreFNode(ctl, mem, adr, adr_type, val);
2369   case T_DOUBLE:  return new (C) StoreDNode(ctl, mem, adr, adr_type, val);
2370   case T_METADATA:
2371   case T_ADDRESS:
2372   case T_OBJECT:
2373 #ifdef _LP64
2374     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2375       val = gvn.transform(new (C) EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2376       return new (C) StoreNNode(ctl, mem, adr, adr_type, val);
2377     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2378                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2379                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2380       val = gvn.transform(new (C) EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2381       return new (C) StoreNKlassNode(ctl, mem, adr, adr_type, val);
2382     }
2383 #endif
2384     {
2385       return new (C) StorePNode(ctl, mem, adr, adr_type, val);
2386     }
2387   }
2388   ShouldNotReachHere();
2389   return (StoreNode*)NULL;
2390 }
2391 
2392 StoreLNode* StoreLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val) {
2393   bool require_atomic = true;
2394   return new (C) StoreLNode(ctl, mem, adr, adr_type, val, require_atomic);
2395 }
2396 
2397 
2398 //--------------------------bottom_type----------------------------------------
2399 const Type *StoreNode::bottom_type() const {
2400   return Type::MEMORY;
2401 }
2402 
2403 //------------------------------hash-------------------------------------------
2404 uint StoreNode::hash() const {
2405   // unroll addition of interesting fields
2406   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2407 
2408   // Since they are not commoned, do not hash them:
2409   return NO_HASH;
2410 }
2411 
2412 //------------------------------Ideal------------------------------------------
2413 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2414 // When a store immediately follows a relevant allocation/initialization,
2415 // try to capture it into the initialization, or hoist it above.
2416 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2417   Node* p = MemNode::Ideal_common(phase, can_reshape);
2418   if (p)  return (p == NodeSentinel) ? NULL : p;
2419 
2420   Node* mem     = in(MemNode::Memory);
2421   Node* address = in(MemNode::Address);
2422 
2423   // Back-to-back stores to same address?  Fold em up.  Generally
2424   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2425   // since they must follow each StoreP operation.  Redundant StoreCMs
2426   // are eliminated just before matching in final_graph_reshape.
2427   if (mem-&gt;is_Store() &amp;&amp; mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2428       mem-&gt;Opcode() != Op_StoreCM) {
2429     // Looking at a dead closed cycle of memory?
2430     assert(mem != mem-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2431 
2432     assert(Opcode() == mem-&gt;Opcode() ||
2433            phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw,
2434            "no mismatched stores, except on raw memory");
2435 
2436     if (mem-&gt;outcnt() == 1 &amp;&amp;           // check for intervening uses
2437         mem-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2438       // If anybody other than 'this' uses 'mem', we cannot fold 'mem' away.
2439       // For example, 'mem' might be the final state at a conditional return.
2440       // Or, 'mem' might be used by some node which is live at the same time
2441       // 'this' is live, which might be unschedulable.  So, require exactly
2442       // ONE user, the 'this' store, until such time as we clone 'mem' for
2443       // each of 'mem's uses (thus making the exactly-1-user-rule hold true).
2444       if (can_reshape) {  // (%%% is this an anachronism?)
2445         set_req_X(MemNode::Memory, mem-&gt;in(MemNode::Memory),
2446                   phase-&gt;is_IterGVN());
2447       } else {
2448         // It's OK to do this in the parser, since DU info is always accurate,
2449         // and the parser always refers to nodes via SafePointNode maps.
2450         set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2451       }
2452       return this;
2453     }
2454   }
2455 
2456   // Capture an unaliased, unconditional, simple store into an initializer.
2457   // Or, if it is independent of the allocation, hoist it above the allocation.
2458   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2459       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2460     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2461     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2462     if (offset &gt; 0) {
2463       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2464       // If the InitializeNode captured me, it made a raw copy of me,
2465       // and I need to disappear.
2466       if (moved != NULL) {
2467         // %%% hack to ensure that Ideal returns a new node:
2468         mem = MergeMemNode::make(phase-&gt;C, mem);
2469         return mem;             // fold me away
2470       }
2471     }
2472   }
2473 
2474   return NULL;                  // No further progress
2475 }
2476 
2477 //------------------------------Value-----------------------------------------
2478 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2479   // Either input is TOP ==&gt; the result is TOP
2480   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2481   if( t1 == Type::TOP ) return Type::TOP;
2482   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2483   if( t2 == Type::TOP ) return Type::TOP;
2484   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2485   if( t3 == Type::TOP ) return Type::TOP;
2486   return Type::MEMORY;
2487 }
2488 
2489 //------------------------------Identity---------------------------------------
2490 // Remove redundant stores:
2491 //   Store(m, p, Load(m, p)) changes to m.
2492 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2493 Node *StoreNode::Identity( PhaseTransform *phase ) {
2494   Node* mem = in(MemNode::Memory);
2495   Node* adr = in(MemNode::Address);
2496   Node* val = in(MemNode::ValueIn);
2497 
2498   // Load then Store?  Then the Store is useless
2499   if (val-&gt;is_Load() &amp;&amp;
2500       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2501       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2502       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2503     return mem;
2504   }
2505 
2506   // Two stores in a row of the same value?
2507   if (mem-&gt;is_Store() &amp;&amp;
2508       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2509       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2510       mem-&gt;Opcode() == Opcode()) {
2511     return mem;
2512   }
2513 
2514   // Store of zero anywhere into a freshly-allocated object?
2515   // Then the store is useless.
2516   // (It must already have been captured by the InitializeNode.)
2517   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2518     // a newly allocated object is already all-zeroes everywhere
2519     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2520       return mem;
2521     }
2522 
2523     // the store may also apply to zero-bits in an earlier object
2524     Node* prev_mem = find_previous_store(phase);
2525     // Steps (a), (b):  Walk past independent stores to find an exact match.
2526     if (prev_mem != NULL) {
2527       Node* prev_val = can_see_stored_value(prev_mem, phase);
2528       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2529         // prev_val and val might differ by a cast; it would be good
2530         // to keep the more informative of the two.
2531         return mem;
2532       }
2533     }
2534   }
2535 
2536   return this;
2537 }
2538 
2539 //------------------------------match_edge-------------------------------------
2540 // Do we Match on this edge index or not?  Match only memory &amp; value
2541 uint StoreNode::match_edge(uint idx) const {
2542   return idx == MemNode::Address || idx == MemNode::ValueIn;
2543 }
2544 
2545 //------------------------------cmp--------------------------------------------
2546 // Do not common stores up together.  They generally have to be split
2547 // back up anyways, so do not bother.
2548 uint StoreNode::cmp( const Node &amp;n ) const {
2549   return (&amp;n == this);          // Always fail except on self
2550 }
2551 
2552 //------------------------------Ideal_masked_input-----------------------------
2553 // Check for a useless mask before a partial-word store
2554 // (StoreB ... (AndI valIn conIa) )
2555 // If (conIa &amp; mask == mask) this simplifies to
2556 // (StoreB ... (valIn) )
2557 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2558   Node *val = in(MemNode::ValueIn);
2559   if( val-&gt;Opcode() == Op_AndI ) {
2560     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2561     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2562       set_req(MemNode::ValueIn, val-&gt;in(1));
2563       return this;
2564     }
2565   }
2566   return NULL;
2567 }
2568 
2569 
2570 //------------------------------Ideal_sign_extended_input----------------------
2571 // Check for useless sign-extension before a partial-word store
2572 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2573 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2574 // (StoreB ... (valIn) )
2575 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2576   Node *val = in(MemNode::ValueIn);
2577   if( val-&gt;Opcode() == Op_RShiftI ) {
2578     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2579     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2580       Node *shl = val-&gt;in(1);
2581       if( shl-&gt;Opcode() == Op_LShiftI ) {
2582         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2583         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2584           set_req(MemNode::ValueIn, shl-&gt;in(1));
2585           return this;
2586         }
2587       }
2588     }
2589   }
2590   return NULL;
2591 }
2592 
2593 //------------------------------value_never_loaded-----------------------------------
2594 // Determine whether there are any possible loads of the value stored.
2595 // For simplicity, we actually check if there are any loads from the
2596 // address stored to, not just for loads of the value stored by this node.
2597 //
2598 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2599   Node *adr = in(Address);
2600   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2601   if (adr_oop == NULL)
2602     return false;
2603   if (!adr_oop-&gt;is_known_instance_field())
2604     return false; // if not a distinct instance, there may be aliases of the address
2605   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2606     Node *use = adr-&gt;fast_out(i);
2607     int opc = use-&gt;Opcode();
2608     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2609       return false;
2610     }
2611   }
2612   return true;
2613 }
2614 
2615 //=============================================================================
2616 //------------------------------Ideal------------------------------------------
2617 // If the store is from an AND mask that leaves the low bits untouched, then
2618 // we can skip the AND operation.  If the store is from a sign-extension
2619 // (a left shift, then right shift) we can skip both.
2620 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2621   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2622   if( progress != NULL ) return progress;
2623 
2624   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2625   if( progress != NULL ) return progress;
2626 
2627   // Finally check the default case
2628   return StoreNode::Ideal(phase, can_reshape);
2629 }
2630 
2631 //=============================================================================
2632 //------------------------------Ideal------------------------------------------
2633 // If the store is from an AND mask that leaves the low bits untouched, then
2634 // we can skip the AND operation
2635 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2636   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2637   if( progress != NULL ) return progress;
2638 
2639   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2640   if( progress != NULL ) return progress;
2641 
2642   // Finally check the default case
2643   return StoreNode::Ideal(phase, can_reshape);
2644 }
2645 
2646 //=============================================================================
2647 //------------------------------Identity---------------------------------------
2648 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2649   // No need to card mark when storing a null ptr
2650   Node* my_store = in(MemNode::OopStore);
2651   if (my_store-&gt;is_Store()) {
2652     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2653     if( t1 == TypePtr::NULL_PTR ) {
2654       return in(MemNode::Memory);
2655     }
2656   }
2657   return this;
2658 }
2659 
2660 //=============================================================================
2661 //------------------------------Ideal---------------------------------------
2662 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2663   Node* progress = StoreNode::Ideal(phase, can_reshape);
2664   if (progress != NULL) return progress;
2665 
2666   Node* my_store = in(MemNode::OopStore);
2667   if (my_store-&gt;is_MergeMem()) {
2668     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2669     set_req(MemNode::OopStore, mem);
2670     return this;
2671   }
2672 
2673   return NULL;
2674 }
2675 
2676 //------------------------------Value-----------------------------------------
2677 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2678   // Either input is TOP ==&gt; the result is TOP
2679   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2680   if( t == Type::TOP ) return Type::TOP;
2681   t = phase-&gt;type( in(MemNode::Address) );
2682   if( t == Type::TOP ) return Type::TOP;
2683   t = phase-&gt;type( in(MemNode::ValueIn) );
2684   if( t == Type::TOP ) return Type::TOP;
2685   // If extra input is TOP ==&gt; the result is TOP
2686   t = phase-&gt;type( in(MemNode::OopStore) );
2687   if( t == Type::TOP ) return Type::TOP;
2688 
2689   return StoreNode::Value( phase );
2690 }
2691 
2692 
2693 //=============================================================================
2694 //----------------------------------SCMemProjNode------------------------------
2695 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2696 {
2697   return bottom_type();
2698 }
2699 
2700 //=============================================================================
2701 //----------------------------------LoadStoreNode------------------------------
2702 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2703   : Node(required),
2704     _type(rt),
2705     _adr_type(at)
2706 {
2707   init_req(MemNode::Control, c  );
2708   init_req(MemNode::Memory , mem);
2709   init_req(MemNode::Address, adr);
2710   init_req(MemNode::ValueIn, val);
2711   init_class_id(Class_LoadStore);
2712 }
2713 
2714 uint LoadStoreNode::ideal_reg() const {
2715   return _type-&gt;ideal_reg();
2716 }
2717 
2718 bool LoadStoreNode::result_not_used() const {
2719   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2720     Node *x = fast_out(i);
2721     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2722     return false;
2723   }
2724   return true;
2725 }
2726 
2727 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2728 
2729 //=============================================================================
2730 //----------------------------------LoadStoreConditionalNode--------------------
2731 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2732   init_req(ExpectedIn, ex );
2733 }
2734 
2735 //=============================================================================
2736 //-------------------------------adr_type--------------------------------------
2737 // Do we Match on this edge index or not?  Do not match memory
2738 const TypePtr* ClearArrayNode::adr_type() const {
2739   Node *adr = in(3);
2740   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2741 }
2742 
2743 //------------------------------match_edge-------------------------------------
2744 // Do we Match on this edge index or not?  Do not match memory
2745 uint ClearArrayNode::match_edge(uint idx) const {
2746   return idx &gt; 1;
2747 }
2748 
2749 //------------------------------Identity---------------------------------------
2750 // Clearing a zero length array does nothing
2751 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2752   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2753 }
2754 
2755 //------------------------------Idealize---------------------------------------
2756 // Clearing a short array is faster with stores
2757 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2758   const int unit = BytesPerLong;
2759   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2760   if (!t)  return NULL;
2761   if (!t-&gt;is_con())  return NULL;
2762   intptr_t raw_count = t-&gt;get_con();
2763   intptr_t size = raw_count;
2764   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2765   // Clearing nothing uses the Identity call.
2766   // Negative clears are possible on dead ClearArrays
2767   // (see jck test stmt114.stmt11402.val).
2768   if (size &lt;= 0 || size % unit != 0)  return NULL;
2769   intptr_t count = size / unit;
2770   // Length too long; use fast hardware clear
2771   if (size &gt; Matcher::init_array_short_size)  return NULL;
2772   Node *mem = in(1);
2773   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2774   Node *adr = in(3);
2775   const Type* at = phase-&gt;type(adr);
2776   if( at==Type::TOP ) return NULL;
2777   const TypePtr* atp = at-&gt;isa_ptr();
2778   // adjust atp to be the correct array element address type
2779   if (atp == NULL)  atp = TypePtr::BOTTOM;
2780   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2781   // Get base for derived pointer purposes
2782   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2783   Node *base = adr-&gt;in(1);
2784 
2785   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2786   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2787   mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero);
2788   count--;
2789   while( count-- ) {
2790     mem = phase-&gt;transform(mem);
2791     adr = phase-&gt;transform(new (phase-&gt;C) AddPNode(base,adr,off));
2792     mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero);
2793   }
2794   return mem;
2795 }
2796 
2797 //----------------------------step_through----------------------------------
2798 // Return allocation input memory edge if it is different instance
2799 // or itself if it is the one we are looking for.
2800 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2801   Node* n = *np;
2802   assert(n-&gt;is_ClearArray(), "sanity");
2803   intptr_t offset;
2804   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2805   // This method is called only before Allocate nodes are expanded during
2806   // macro nodes expansion. Before that ClearArray nodes are only generated
2807   // in LibraryCallKit::generate_arraycopy() which follows allocations.
2808   assert(alloc != NULL, "should have allocation");
2809   if (alloc-&gt;_idx == instance_id) {
2810     // Can not bypass initialization of the instance we are looking for.
2811     return false;
2812   }
2813   // Otherwise skip it.
2814   InitializeNode* init = alloc-&gt;initialization();
2815   if (init != NULL)
2816     *np = init-&gt;in(TypeFunc::Memory);
2817   else
2818     *np = alloc-&gt;in(TypeFunc::Memory);
2819   return true;
2820 }
2821 
2822 //----------------------------clear_memory-------------------------------------
2823 // Generate code to initialize object storage to zero.
2824 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2825                                    intptr_t start_offset,
2826                                    Node* end_offset,
2827                                    PhaseGVN* phase) {
2828   Compile* C = phase-&gt;C;
2829   intptr_t offset = start_offset;
2830 
2831   int unit = BytesPerLong;
2832   if ((offset % unit) != 0) {
2833     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2834     adr = phase-&gt;transform(adr);
2835     const TypePtr* atp = TypeRawPtr::BOTTOM;
2836     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT);
2837     mem = phase-&gt;transform(mem);
2838     offset += BytesPerInt;
2839   }
2840   assert((offset % unit) == 0, "");
2841 
2842   // Initialize the remaining stuff, if any, with a ClearArray.
2843   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2844 }
2845 
2846 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2847                                    Node* start_offset,
2848                                    Node* end_offset,
2849                                    PhaseGVN* phase) {
2850   if (start_offset == end_offset) {
2851     // nothing to do
2852     return mem;
2853   }
2854 
2855   Compile* C = phase-&gt;C;
2856   int unit = BytesPerLong;
2857   Node* zbase = start_offset;
2858   Node* zend  = end_offset;
2859 
2860   // Scale to the unit required by the CPU:
2861   if (!Matcher::init_array_count_is_in_bytes) {
2862     Node* shift = phase-&gt;intcon(exact_log2(unit));
2863     zbase = phase-&gt;transform( new(C) URShiftXNode(zbase, shift) );
2864     zend  = phase-&gt;transform( new(C) URShiftXNode(zend,  shift) );
2865   }
2866 
2867   // Bulk clear double-words
2868   Node* zsize = phase-&gt;transform( new(C) SubXNode(zend, zbase) );
2869   Node* adr = phase-&gt;transform( new(C) AddPNode(dest, dest, start_offset) );
2870   mem = new (C) ClearArrayNode(ctl, mem, zsize, adr);
2871   return phase-&gt;transform(mem);
2872 }
2873 
2874 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2875                                    intptr_t start_offset,
2876                                    intptr_t end_offset,
2877                                    PhaseGVN* phase) {
2878   if (start_offset == end_offset) {
2879     // nothing to do
2880     return mem;
2881   }
2882 
2883   Compile* C = phase-&gt;C;
2884   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2885   intptr_t done_offset = end_offset;
2886   if ((done_offset % BytesPerLong) != 0) {
2887     done_offset -= BytesPerInt;
2888   }
2889   if (done_offset &gt; start_offset) {
2890     mem = clear_memory(ctl, mem, dest,
2891                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2892   }
2893   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2894     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2895     adr = phase-&gt;transform(adr);
2896     const TypePtr* atp = TypeRawPtr::BOTTOM;
2897     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT);
2898     mem = phase-&gt;transform(mem);
2899     done_offset += BytesPerInt;
2900   }
2901   assert(done_offset == end_offset, "");
2902   return mem;
2903 }
2904 
2905 //=============================================================================
2906 // Do not match memory edge.
2907 uint StrIntrinsicNode::match_edge(uint idx) const {
2908   return idx == 2 || idx == 3;
2909 }
2910 
2911 //------------------------------Ideal------------------------------------------
2912 // Return a node which is more "ideal" than the current node.  Strip out
2913 // control copies
2914 Node *StrIntrinsicNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2915   if (remove_dead_region(phase, can_reshape)) return this;
2916   // Don't bother trying to transform a dead node
2917   if (in(0) &amp;&amp; in(0)-&gt;is_top())  return NULL;
2918 
2919   if (can_reshape) {
2920     Node* mem = phase-&gt;transform(in(MemNode::Memory));
2921     // If transformed to a MergeMem, get the desired slice
2922     uint alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
2923     mem = mem-&gt;is_MergeMem() ? mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : mem;
2924     if (mem != in(MemNode::Memory)) {
2925       set_req(MemNode::Memory, mem);
2926       return this;
2927     }
2928   }
2929   return NULL;
2930 }
2931 
2932 //------------------------------Value------------------------------------------
2933 const Type *StrIntrinsicNode::Value( PhaseTransform *phase ) const {
2934   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2935   return bottom_type();
2936 }
2937 
2938 //=============================================================================
2939 //------------------------------match_edge-------------------------------------
2940 // Do not match memory edge
2941 uint EncodeISOArrayNode::match_edge(uint idx) const {
2942   return idx == 2 || idx == 3; // EncodeISOArray src (Binary dst len)
2943 }
2944 
2945 //------------------------------Ideal------------------------------------------
2946 // Return a node which is more "ideal" than the current node.  Strip out
2947 // control copies
2948 Node *EncodeISOArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2949   return remove_dead_region(phase, can_reshape) ? this : NULL;
2950 }
2951 
2952 //------------------------------Value------------------------------------------
2953 const Type *EncodeISOArrayNode::Value(PhaseTransform *phase) const {
2954   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2955   return bottom_type();
2956 }
2957 
2958 //=============================================================================
2959 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2960   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2961     _adr_type(C-&gt;get_adr_type(alias_idx))
2962 {
2963   init_class_id(Class_MemBar);
2964   Node* top = C-&gt;top();
2965   init_req(TypeFunc::I_O,top);
2966   init_req(TypeFunc::FramePtr,top);
2967   init_req(TypeFunc::ReturnAdr,top);
2968   if (precedent != NULL)
2969     init_req(TypeFunc::Parms, precedent);
2970 }
2971 
2972 //------------------------------cmp--------------------------------------------
2973 uint MemBarNode::hash() const { return NO_HASH; }
2974 uint MemBarNode::cmp( const Node &amp;n ) const {
2975   return (&amp;n == this);          // Always fail except on self
2976 }
2977 
2978 //------------------------------make-------------------------------------------
2979 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2980   switch (opcode) {
2981   case Op_MemBarAcquire:   return new(C) MemBarAcquireNode(C,  atp, pn);
2982   case Op_MemBarRelease:   return new(C) MemBarReleaseNode(C,  atp, pn);
2983   case Op_MemBarAcquireLock: return new(C) MemBarAcquireLockNode(C,  atp, pn);
2984   case Op_MemBarReleaseLock: return new(C) MemBarReleaseLockNode(C,  atp, pn);
2985   case Op_MemBarVolatile:  return new(C) MemBarVolatileNode(C, atp, pn);
2986   case Op_MemBarCPUOrder:  return new(C) MemBarCPUOrderNode(C, atp, pn);
2987   case Op_Initialize:      return new(C) InitializeNode(C,     atp, pn);
2988   case Op_MemBarStoreStore: return new(C) MemBarStoreStoreNode(C,  atp, pn);
2989   default:                 ShouldNotReachHere(); return NULL;
2990   }
2991 }
2992 
2993 //------------------------------Ideal------------------------------------------
2994 // Return a node which is more "ideal" than the current node.  Strip out
2995 // control copies
2996 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2997   if (remove_dead_region(phase, can_reshape)) return this;
2998   // Don't bother trying to transform a dead node
2999   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
3000     return NULL;
3001   }
3002 
3003   // Eliminate volatile MemBars for scalar replaced objects.
3004   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3005     bool eliminate = false;
3006     int opc = Opcode();
3007     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3008       // Volatile field loads and stores.
3009       Node* my_mem = in(MemBarNode::Precedent);
3010       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3011       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3012         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3013         // replace this Precedent (decodeN) with the Load instead.
3014         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3015           Node* load_node = my_mem-&gt;in(1);
3016           set_req(MemBarNode::Precedent, load_node);
3017           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3018           my_mem = load_node;
3019         } else {
3020           assert(my_mem-&gt;unique_out() == this, "sanity");
3021           del_req(Precedent);
3022           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
3023           my_mem = NULL;
3024         }
3025       }
3026       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
3027         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
3028         // Check for scalar replaced object reference.
3029         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
3030             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
3031             t_oop-&gt;offset() != Type::OffsetTop) {
3032           eliminate = true;
3033         }
3034       }
3035     } else if (opc == Op_MemBarRelease) {
3036       // Final field stores.
3037       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
3038       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
3039           alloc-&gt;as_Allocate()-&gt;_is_non_escaping) {
3040         // The allocated object does not escape.
3041         eliminate = true;
3042       }
3043     }
3044     if (eliminate) {
3045       // Replace MemBar projections by its inputs.
3046       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3047       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3048       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3049       // Must return either the original node (now dead) or a new node
3050       // (Do not return a top here, since that would break the uniqueness of top.)
3051       return new (phase-&gt;C) ConINode(TypeInt::ZERO);
3052     }
3053   }
3054   return NULL;
3055 }
3056 
3057 //------------------------------Value------------------------------------------
3058 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3059   if( !in(0) ) return Type::TOP;
3060   if( phase-&gt;type(in(0)) == Type::TOP )
3061     return Type::TOP;
3062   return TypeTuple::MEMBAR;
3063 }
3064 
3065 //------------------------------match------------------------------------------
3066 // Construct projections for memory.
3067 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3068   switch (proj-&gt;_con) {
3069   case TypeFunc::Control:
3070   case TypeFunc::Memory:
3071     return new (m-&gt;C) MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3072   }
3073   ShouldNotReachHere();
3074   return NULL;
3075 }
3076 
3077 //===========================InitializeNode====================================
3078 // SUMMARY:
3079 // This node acts as a memory barrier on raw memory, after some raw stores.
3080 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3081 // The Initialize can 'capture' suitably constrained stores as raw inits.
3082 // It can coalesce related raw stores into larger units (called 'tiles').
3083 // It can avoid zeroing new storage for memory units which have raw inits.
3084 // At macro-expansion, it is marked 'complete', and does not optimize further.
3085 //
3086 // EXAMPLE:
3087 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3088 //   ctl = incoming control; mem* = incoming memory
3089 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3090 // First allocate uninitialized memory and fill in the header:
3091 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3092 //   ctl := alloc.Control; mem* := alloc.Memory*
3093 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3094 // Then initialize to zero the non-header parts of the raw memory block:
3095 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3096 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3097 // After the initialize node executes, the object is ready for service:
3098 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3099 // Suppose its body is immediately initialized as {1,2}:
3100 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3101 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3102 //   mem.SLICE(#short[*]) := store2
3103 //
3104 // DETAILS:
3105 // An InitializeNode collects and isolates object initialization after
3106 // an AllocateNode and before the next possible safepoint.  As a
3107 // memory barrier (MemBarNode), it keeps critical stores from drifting
3108 // down past any safepoint or any publication of the allocation.
3109 // Before this barrier, a newly-allocated object may have uninitialized bits.
3110 // After this barrier, it may be treated as a real oop, and GC is allowed.
3111 //
3112 // The semantics of the InitializeNode include an implicit zeroing of
3113 // the new object from object header to the end of the object.
3114 // (The object header and end are determined by the AllocateNode.)
3115 //
3116 // Certain stores may be added as direct inputs to the InitializeNode.
3117 // These stores must update raw memory, and they must be to addresses
3118 // derived from the raw address produced by AllocateNode, and with
3119 // a constant offset.  They must be ordered by increasing offset.
3120 // The first one is at in(RawStores), the last at in(req()-1).
3121 // Unlike most memory operations, they are not linked in a chain,
3122 // but are displayed in parallel as users of the rawmem output of
3123 // the allocation.
3124 //
3125 // (See comments in InitializeNode::capture_store, which continue
3126 // the example given above.)
3127 //
3128 // When the associated Allocate is macro-expanded, the InitializeNode
3129 // may be rewritten to optimize collected stores.  A ClearArrayNode
3130 // may also be created at that point to represent any required zeroing.
3131 // The InitializeNode is then marked 'complete', prohibiting further
3132 // capturing of nearby memory operations.
3133 //
3134 // During macro-expansion, all captured initializations which store
3135 // constant values of 32 bits or smaller are coalesced (if advantageous)
3136 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3137 // initialized in fewer memory operations.  Memory words which are
3138 // covered by neither tiles nor non-constant stores are pre-zeroed
3139 // by explicit stores of zero.  (The code shape happens to do all
3140 // zeroing first, then all other stores, with both sequences occurring
3141 // in order of ascending offsets.)
3142 //
3143 // Alternatively, code may be inserted between an AllocateNode and its
3144 // InitializeNode, to perform arbitrary initialization of the new object.
3145 // E.g., the object copying intrinsics insert complex data transfers here.
3146 // The initialization must then be marked as 'complete' disable the
3147 // built-in zeroing semantics and the collection of initializing stores.
3148 //
3149 // While an InitializeNode is incomplete, reads from the memory state
3150 // produced by it are optimizable if they match the control edge and
3151 // new oop address associated with the allocation/initialization.
3152 // They return a stored value (if the offset matches) or else zero.
3153 // A write to the memory state, if it matches control and address,
3154 // and if it is to a constant offset, may be 'captured' by the
3155 // InitializeNode.  It is cloned as a raw memory operation and rewired
3156 // inside the initialization, to the raw oop produced by the allocation.
3157 // Operations on addresses which are provably distinct (e.g., to
3158 // other AllocateNodes) are allowed to bypass the initialization.
3159 //
3160 // The effect of all this is to consolidate object initialization
3161 // (both arrays and non-arrays, both piecewise and bulk) into a
3162 // single location, where it can be optimized as a unit.
3163 //
3164 // Only stores with an offset less than TrackedInitializationLimit words
3165 // will be considered for capture by an InitializeNode.  This puts a
3166 // reasonable limit on the complexity of optimized initializations.
3167 
3168 //---------------------------InitializeNode------------------------------------
3169 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3170   : _is_complete(Incomplete), _does_not_escape(false),
3171     MemBarNode(C, adr_type, rawoop)
3172 {
3173   init_class_id(Class_Initialize);
3174 
3175   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3176   assert(in(RawAddress) == rawoop, "proper init");
3177   // Note:  allocation() can be NULL, for secondary initialization barriers
3178 }
3179 
3180 // Since this node is not matched, it will be processed by the
3181 // register allocator.  Declare that there are no constraints
3182 // on the allocation of the RawAddress edge.
3183 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3184   // This edge should be set to top, by the set_complete.  But be conservative.
3185   if (idx == InitializeNode::RawAddress)
3186     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3187   return RegMask::Empty;
3188 }
3189 
3190 Node* InitializeNode::memory(uint alias_idx) {
3191   Node* mem = in(Memory);
3192   if (mem-&gt;is_MergeMem()) {
3193     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3194   } else {
3195     // incoming raw memory is not split
3196     return mem;
3197   }
3198 }
3199 
3200 bool InitializeNode::is_non_zero() {
3201   if (is_complete())  return false;
3202   remove_extra_zeroes();
3203   return (req() &gt; RawStores);
3204 }
3205 
3206 void InitializeNode::set_complete(PhaseGVN* phase) {
3207   assert(!is_complete(), "caller responsibility");
3208   _is_complete = Complete;
3209 
3210   // After this node is complete, it contains a bunch of
3211   // raw-memory initializations.  There is no need for
3212   // it to have anything to do with non-raw memory effects.
3213   // Therefore, tell all non-raw users to re-optimize themselves,
3214   // after skipping the memory effects of this initialization.
3215   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3216   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3217 }
3218 
3219 // convenience function
3220 // return false if the init contains any stores already
3221 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3222   InitializeNode* init = initialization();
3223   if (init == NULL || init-&gt;is_complete())  return false;
3224   init-&gt;remove_extra_zeroes();
3225   // for now, if this allocation has already collected any inits, bail:
3226   if (init-&gt;is_non_zero())  return false;
3227   init-&gt;set_complete(phase);
3228   return true;
3229 }
3230 
3231 void InitializeNode::remove_extra_zeroes() {
3232   if (req() == RawStores)  return;
3233   Node* zmem = zero_memory();
3234   uint fill = RawStores;
3235   for (uint i = fill; i &lt; req(); i++) {
3236     Node* n = in(i);
3237     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3238     if (fill &lt; i)  set_req(fill, n);          // compact
3239     ++fill;
3240   }
3241   // delete any empty spaces created:
3242   while (fill &lt; req()) {
3243     del_req(fill);
3244   }
3245 }
3246 
3247 // Helper for remembering which stores go with which offsets.
3248 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3249   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3250   intptr_t offset = -1;
3251   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3252                                                phase, offset);
3253   if (base == NULL)     return -1;  // something is dead,
3254   if (offset &lt; 0)       return -1;  //        dead, dead
3255   return offset;
3256 }
3257 
3258 // Helper for proving that an initialization expression is
3259 // "simple enough" to be folded into an object initialization.
3260 // Attempts to prove that a store's initial value 'n' can be captured
3261 // within the initialization without creating a vicious cycle, such as:
3262 //     { Foo p = new Foo(); p.next = p; }
3263 // True for constants and parameters and small combinations thereof.
3264 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3265   if (n == NULL)      return true;   // (can this really happen?)
3266   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3267   if (n == this)      return false;  // found a cycle
3268   if (n-&gt;is_Con())    return true;
3269   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3270   if (n-&gt;is_Root())   return true;   // even better
3271 
3272   Node* ctl = n-&gt;in(0);
3273   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3274     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3275     if (ctl == this)  return false;
3276 
3277     // If we already know that the enclosing memory op is pinned right after
3278     // the init, then any control flow that the store has picked up
3279     // must have preceded the init, or else be equal to the init.
3280     // Even after loop optimizations (which might change control edges)
3281     // a store is never pinned *before* the availability of its inputs.
3282     if (!MemNode::all_controls_dominate(n, this))
3283       return false;                  // failed to prove a good control
3284   }
3285 
3286   // Check data edges for possible dependencies on 'this'.
3287   if ((count += 1) &gt; 20)  return false;  // complexity limit
3288   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3289     Node* m = n-&gt;in(i);
3290     if (m == NULL || m == n || m-&gt;is_top())  continue;
3291     uint first_i = n-&gt;find_edge(m);
3292     if (i != first_i)  continue;  // process duplicate edge just once
3293     if (!detect_init_independence(m, count)) {
3294       return false;
3295     }
3296   }
3297 
3298   return true;
3299 }
3300 
3301 // Here are all the checks a Store must pass before it can be moved into
3302 // an initialization.  Returns zero if a check fails.
3303 // On success, returns the (constant) offset to which the store applies,
3304 // within the initialized memory.
3305 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3306   const int FAIL = 0;
3307   if (st-&gt;req() != MemNode::ValueIn + 1)
3308     return FAIL;                // an inscrutable StoreNode (card mark?)
3309   Node* ctl = st-&gt;in(MemNode::Control);
3310   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3311     return FAIL;                // must be unconditional after the initialization
3312   Node* mem = st-&gt;in(MemNode::Memory);
3313   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3314     return FAIL;                // must not be preceded by other stores
3315   Node* adr = st-&gt;in(MemNode::Address);
3316   intptr_t offset;
3317   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3318   if (alloc == NULL)
3319     return FAIL;                // inscrutable address
3320   if (alloc != allocation())
3321     return FAIL;                // wrong allocation!  (store needs to float up)
3322   Node* val = st-&gt;in(MemNode::ValueIn);
3323   int complexity_count = 0;
3324   if (!detect_init_independence(val, complexity_count))
3325     return FAIL;                // stored value must be 'simple enough'
3326 
3327   // The Store can be captured only if nothing after the allocation
3328   // and before the Store is using the memory location that the store
3329   // overwrites.
3330   bool failed = false;
3331   // If is_complete_with_arraycopy() is true the shape of the graph is
3332   // well defined and is safe so no need for extra checks.
3333   if (!is_complete_with_arraycopy()) {
3334     // We are going to look at each use of the memory state following
3335     // the allocation to make sure nothing reads the memory that the
3336     // Store writes.
3337     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3338     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3339     ResourceMark rm;
3340     Unique_Node_List mems;
3341     mems.push(mem);
3342     Node* unique_merge = NULL;
3343     for (uint next = 0; next &lt; mems.size(); ++next) {
3344       Node *m  = mems.at(next);
3345       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3346         Node *n = m-&gt;fast_out(j);
3347         if (n-&gt;outcnt() == 0) {
3348           continue;
3349         }
3350         if (n == st) {
3351           continue;
3352         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3353           // If the control of this use is different from the control
3354           // of the Store which is right after the InitializeNode then
3355           // this node cannot be between the InitializeNode and the
3356           // Store.
3357           continue;
3358         } else if (n-&gt;is_MergeMem()) {
3359           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3360             // We can hit a MergeMemNode (that will likely go away
3361             // later) that is a direct use of the memory state
3362             // following the InitializeNode on the same slice as the
3363             // store node that we'd like to capture. We need to check
3364             // the uses of the MergeMemNode.
3365             mems.push(n);
3366           }
3367         } else if (n-&gt;is_Mem()) {
3368           Node* other_adr = n-&gt;in(MemNode::Address);
3369           if (other_adr == adr) {
3370             failed = true;
3371             break;
3372           } else {
3373             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3374             if (other_t_adr != NULL) {
3375               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3376               if (other_alias_idx == alias_idx) {
3377                 // A load from the same memory slice as the store right
3378                 // after the InitializeNode. We check the control of the
3379                 // object/array that is loaded from. If it's the same as
3380                 // the store control then we cannot capture the store.
3381                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3382                 Node* base = other_adr;
3383                 assert(base-&gt;is_AddP(), err_msg_res("should be addp but is %s", base-&gt;Name()));
3384                 base = base-&gt;in(AddPNode::Base);
3385                 if (base != NULL) {
3386                   base = base-&gt;uncast();
3387                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3388                     failed = true;
3389                     break;
3390                   }
3391                 }
3392               }
3393             }
3394           }
3395         } else {
3396           failed = true;
3397           break;
3398         }
3399       }
3400     }
3401   }
3402   if (failed) {
3403     if (!can_reshape) {
3404       // We decided we couldn't capture the store during parsing. We
3405       // should try again during the next IGVN once the graph is
3406       // cleaner.
3407       phase-&gt;C-&gt;record_for_igvn(st);
3408     }
3409     return FAIL;
3410   }
3411 
3412   return offset;                // success
3413 }
3414 
3415 // Find the captured store in(i) which corresponds to the range
3416 // [start..start+size) in the initialized object.
3417 // If there is one, return its index i.  If there isn't, return the
3418 // negative of the index where it should be inserted.
3419 // Return 0 if the queried range overlaps an initialization boundary
3420 // or if dead code is encountered.
3421 // If size_in_bytes is zero, do not bother with overlap checks.
3422 int InitializeNode::captured_store_insertion_point(intptr_t start,
3423                                                    int size_in_bytes,
3424                                                    PhaseTransform* phase) {
3425   const int FAIL = 0, MAX_STORE = BytesPerLong;
3426 
3427   if (is_complete())
3428     return FAIL;                // arraycopy got here first; punt
3429 
3430   assert(allocation() != NULL, "must be present");
3431 
3432   // no negatives, no header fields:
3433   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3434 
3435   // after a certain size, we bail out on tracking all the stores:
3436   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3437   if (start &gt;= ti_limit)  return FAIL;
3438 
3439   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3440     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3441 
3442     Node*    st     = in(i);
3443     intptr_t st_off = get_store_offset(st, phase);
3444     if (st_off &lt; 0) {
3445       if (st != zero_memory()) {
3446         return FAIL;            // bail out if there is dead garbage
3447       }
3448     } else if (st_off &gt; start) {
3449       // ...we are done, since stores are ordered
3450       if (st_off &lt; start + size_in_bytes) {
3451         return FAIL;            // the next store overlaps
3452       }
3453       return -(int)i;           // not found; here is where to put it
3454     } else if (st_off &lt; start) {
3455       if (size_in_bytes != 0 &amp;&amp;
3456           start &lt; st_off + MAX_STORE &amp;&amp;
3457           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3458         return FAIL;            // the previous store overlaps
3459       }
3460     } else {
3461       if (size_in_bytes != 0 &amp;&amp;
3462           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3463         return FAIL;            // mismatched store size
3464       }
3465       return i;
3466     }
3467 
3468     ++i;
3469   }
3470 }
3471 
3472 // Look for a captured store which initializes at the offset 'start'
3473 // with the given size.  If there is no such store, and no other
3474 // initialization interferes, then return zero_memory (the memory
3475 // projection of the AllocateNode).
3476 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3477                                           PhaseTransform* phase) {
3478   assert(stores_are_sane(phase), "");
3479   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3480   if (i == 0) {
3481     return NULL;                // something is dead
3482   } else if (i &lt; 0) {
3483     return zero_memory();       // just primordial zero bits here
3484   } else {
3485     Node* st = in(i);           // here is the store at this position
3486     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3487     return st;
3488   }
3489 }
3490 
3491 // Create, as a raw pointer, an address within my new object at 'offset'.
3492 Node* InitializeNode::make_raw_address(intptr_t offset,
3493                                        PhaseTransform* phase) {
3494   Node* addr = in(RawAddress);
3495   if (offset != 0) {
3496     Compile* C = phase-&gt;C;
3497     addr = phase-&gt;transform( new (C) AddPNode(C-&gt;top(), addr,
3498                                                  phase-&gt;MakeConX(offset)) );
3499   }
3500   return addr;
3501 }
3502 
3503 // Clone the given store, converting it into a raw store
3504 // initializing a field or element of my new object.
3505 // Caller is responsible for retiring the original store,
3506 // with subsume_node or the like.
3507 //
3508 // From the example above InitializeNode::InitializeNode,
3509 // here are the old stores to be captured:
3510 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3511 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3512 //
3513 // Here is the changed code; note the extra edges on init:
3514 //   alloc = (Allocate ...)
3515 //   rawoop = alloc.RawAddress
3516 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3517 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3518 //   init = (Initialize alloc.Control alloc.Memory rawoop
3519 //                      rawstore1 rawstore2)
3520 //
3521 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3522                                     PhaseTransform* phase, bool can_reshape) {
3523   assert(stores_are_sane(phase), "");
3524 
3525   if (start &lt; 0)  return NULL;
3526   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3527 
3528   Compile* C = phase-&gt;C;
3529   int size_in_bytes = st-&gt;memory_size();
3530   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3531   if (i == 0)  return NULL;     // bail out
3532   Node* prev_mem = NULL;        // raw memory for the captured store
3533   if (i &gt; 0) {
3534     prev_mem = in(i);           // there is a pre-existing store under this one
3535     set_req(i, C-&gt;top());       // temporarily disconnect it
3536     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3537   } else {
3538     i = -i;                     // no pre-existing store
3539     prev_mem = zero_memory();   // a slice of the newly allocated object
3540     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3541       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3542     else
3543       ins_req(i, C-&gt;top());     // build a new edge
3544   }
3545   Node* new_st = st-&gt;clone();
3546   new_st-&gt;set_req(MemNode::Control, in(Control));
3547   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3548   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3549   new_st = phase-&gt;transform(new_st);
3550 
3551   // At this point, new_st might have swallowed a pre-existing store
3552   // at the same offset, or perhaps new_st might have disappeared,
3553   // if it redundantly stored the same value (or zero to fresh memory).
3554 
3555   // In any case, wire it in:
3556   set_req(i, new_st);
3557 
3558   // The caller may now kill the old guy.
3559   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3560   assert(check_st == new_st || check_st == NULL, "must be findable");
3561   assert(!is_complete(), "");
3562   return new_st;
3563 }
3564 
3565 static bool store_constant(jlong* tiles, int num_tiles,
3566                            intptr_t st_off, int st_size,
3567                            jlong con) {
3568   if ((st_off &amp; (st_size-1)) != 0)
3569     return false;               // strange store offset (assume size==2**N)
3570   address addr = (address)tiles + st_off;
3571   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3572   switch (st_size) {
3573   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3574   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3575   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3576   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3577   default: return false;        // strange store size (detect size!=2**N here)
3578   }
3579   return true;                  // return success to caller
3580 }
3581 
3582 // Coalesce subword constants into int constants and possibly
3583 // into long constants.  The goal, if the CPU permits,
3584 // is to initialize the object with a small number of 64-bit tiles.
3585 // Also, convert floating-point constants to bit patterns.
3586 // Non-constants are not relevant to this pass.
3587 //
3588 // In terms of the running example on InitializeNode::InitializeNode
3589 // and InitializeNode::capture_store, here is the transformation
3590 // of rawstore1 and rawstore2 into rawstore12:
3591 //   alloc = (Allocate ...)
3592 //   rawoop = alloc.RawAddress
3593 //   tile12 = 0x00010002
3594 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3595 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3596 //
3597 void
3598 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3599                                         Node* size_in_bytes,
3600                                         PhaseGVN* phase) {
3601   Compile* C = phase-&gt;C;
3602 
3603   assert(stores_are_sane(phase), "");
3604   // Note:  After this pass, they are not completely sane,
3605   // since there may be some overlaps.
3606 
3607   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3608 
3609   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3610   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3611   size_limit = MIN2(size_limit, ti_limit);
3612   size_limit = align_size_up(size_limit, BytesPerLong);
3613   int num_tiles = size_limit / BytesPerLong;
3614 
3615   // allocate space for the tile map:
3616   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3617   jlong  tiles_buf[small_len];
3618   Node*  nodes_buf[small_len];
3619   jlong  inits_buf[small_len];
3620   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3621                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3622   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3623                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3624   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3625                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3626   // tiles: exact bitwise model of all primitive constants
3627   // nodes: last constant-storing node subsumed into the tiles model
3628   // inits: which bytes (in each tile) are touched by any initializations
3629 
3630   //// Pass A: Fill in the tile model with any relevant stores.
3631 
3632   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3633   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3634   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3635   Node* zmem = zero_memory(); // initially zero memory state
3636   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3637     Node* st = in(i);
3638     intptr_t st_off = get_store_offset(st, phase);
3639 
3640     // Figure out the store's offset and constant value:
3641     if (st_off &lt; header_size)             continue; //skip (ignore header)
3642     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3643     int st_size = st-&gt;as_Store()-&gt;memory_size();
3644     if (st_off + st_size &gt; size_limit)    break;
3645 
3646     // Record which bytes are touched, whether by constant or not.
3647     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3648       continue;                 // skip (strange store size)
3649 
3650     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3651     if (!val-&gt;singleton())                continue; //skip (non-con store)
3652     BasicType type = val-&gt;basic_type();
3653 
3654     jlong con = 0;
3655     switch (type) {
3656     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3657     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3658     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3659     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3660     default:                              continue; //skip (odd store type)
3661     }
3662 
3663     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3664         st-&gt;Opcode() == Op_StoreL) {
3665       continue;                 // This StoreL is already optimal.
3666     }
3667 
3668     // Store down the constant.
3669     store_constant(tiles, num_tiles, st_off, st_size, con);
3670 
3671     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3672 
3673     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3674         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3675       jlong lcon = tiles[j];
3676       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3677           st-&gt;Opcode() == Op_StoreI) {
3678         // This StoreI is already optimal by itself.
3679         jint* intcon = (jint*) &amp;tiles[j];
3680         intcon[1] = 0;  // undo the store_constant()
3681 
3682         // If the previous store is also optimal by itself, back up and
3683         // undo the action of the previous loop iteration... if we can.
3684         // But if we can't, just let the previous half take care of itself.
3685         st = nodes[j];
3686         st_off -= BytesPerInt;
3687         con = intcon[0];
3688         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3689           assert(st_off &gt;= header_size, "still ignoring header");
3690           assert(get_store_offset(st, phase) == st_off, "must be");
3691           assert(in(i-1) == zmem, "must be");
3692           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3693           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3694           // Undo the effects of the previous loop trip, which swallowed st:
3695           intcon[0] = 0;        // undo store_constant()
3696           set_req(i-1, st);     // undo set_req(i, zmem)
3697           nodes[j] = NULL;      // undo nodes[j] = st
3698           --old_subword;        // undo ++old_subword
3699         }
3700         continue;               // This StoreI is already optimal.
3701       }
3702     }
3703 
3704     // This store is not needed.
3705     set_req(i, zmem);
3706     nodes[j] = st;              // record for the moment
3707     if (st_size &lt; BytesPerLong) // something has changed
3708           ++old_subword;        // includes int/float, but who's counting...
3709     else  ++old_long;
3710   }
3711 
3712   if ((old_subword + old_long) == 0)
3713     return;                     // nothing more to do
3714 
3715   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3716   // Be sure to insert them before overlapping non-constant stores.
3717   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3718   for (int j = 0; j &lt; num_tiles; j++) {
3719     jlong con  = tiles[j];
3720     jlong init = inits[j];
3721     if (con == 0)  continue;
3722     jint con0,  con1;           // split the constant, address-wise
3723     jint init0, init1;          // split the init map, address-wise
3724     { union { jlong con; jint intcon[2]; } u;
3725       u.con = con;
3726       con0  = u.intcon[0];
3727       con1  = u.intcon[1];
3728       u.con = init;
3729       init0 = u.intcon[0];
3730       init1 = u.intcon[1];
3731     }
3732 
3733     Node* old = nodes[j];
3734     assert(old != NULL, "need the prior store");
3735     intptr_t offset = (j * BytesPerLong);
3736 
3737     bool split = !Matcher::isSimpleConstant64(con);
3738 
3739     if (offset &lt; header_size) {
3740       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3741       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3742       split = true;             // only the second word counts
3743       // Example:  int a[] = { 42 ... }
3744     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3745       split = true;             // first word is covered by full inits
3746       // Example:  int a[] = { ... foo(), 42 ... }
3747     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3748       split = true;             // second word is covered by full inits
3749       // Example:  int a[] = { ... 42, foo() ... }
3750     }
3751 
3752     // Here's a case where init0 is neither 0 nor -1:
3753     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3754     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3755     // In this case the tile is not split; it is (jlong)42.
3756     // The big tile is stored down, and then the foo() value is inserted.
3757     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3758 
3759     Node* ctl = old-&gt;in(MemNode::Control);
3760     Node* adr = make_raw_address(offset, phase);
3761     const TypePtr* atp = TypeRawPtr::BOTTOM;
3762 
3763     // One or two coalesced stores to plop down.
3764     Node*    st[2];
3765     intptr_t off[2];
3766     int  nst = 0;
3767     if (!split) {
3768       ++new_long;
3769       off[nst] = offset;
3770       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3771                                   phase-&gt;longcon(con), T_LONG);
3772     } else {
3773       // Omit either if it is a zero.
3774       if (con0 != 0) {
3775         ++new_int;
3776         off[nst]  = offset;
3777         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3778                                     phase-&gt;intcon(con0), T_INT);
3779       }
3780       if (con1 != 0) {
3781         ++new_int;
3782         offset += BytesPerInt;
3783         adr = make_raw_address(offset, phase);
3784         off[nst]  = offset;
3785         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3786                                     phase-&gt;intcon(con1), T_INT);
3787       }
3788     }
3789 
3790     // Insert second store first, then the first before the second.
3791     // Insert each one just before any overlapping non-constant stores.
3792     while (nst &gt; 0) {
3793       Node* st1 = st[--nst];
3794       C-&gt;copy_node_notes_to(st1, old);
3795       st1 = phase-&gt;transform(st1);
3796       offset = off[nst];
3797       assert(offset &gt;= header_size, "do not smash header");
3798       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3799       guarantee(ins_idx != 0, "must re-insert constant store");
3800       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3801       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3802         set_req(--ins_idx, st1);
3803       else
3804         ins_req(ins_idx, st1);
3805     }
3806   }
3807 
3808   if (PrintCompilation &amp;&amp; WizardMode)
3809     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3810                   old_subword, old_long, new_int, new_long);
3811   if (C-&gt;log() != NULL)
3812     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3813                    old_subword, old_long, new_int, new_long);
3814 
3815   // Clean up any remaining occurrences of zmem:
3816   remove_extra_zeroes();
3817 }
3818 
3819 // Explore forward from in(start) to find the first fully initialized
3820 // word, and return its offset.  Skip groups of subword stores which
3821 // together initialize full words.  If in(start) is itself part of a
3822 // fully initialized word, return the offset of in(start).  If there
3823 // are no following full-word stores, or if something is fishy, return
3824 // a negative value.
3825 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3826   int       int_map = 0;
3827   intptr_t  int_map_off = 0;
3828   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3829 
3830   for (uint i = start, limit = req(); i &lt; limit; i++) {
3831     Node* st = in(i);
3832 
3833     intptr_t st_off = get_store_offset(st, phase);
3834     if (st_off &lt; 0)  break;  // return conservative answer
3835 
3836     int st_size = st-&gt;as_Store()-&gt;memory_size();
3837     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3838       return st_off;            // we found a complete word init
3839     }
3840 
3841     // update the map:
3842 
3843     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3844     if (this_int_off != int_map_off) {
3845       // reset the map:
3846       int_map = 0;
3847       int_map_off = this_int_off;
3848     }
3849 
3850     int subword_off = st_off - this_int_off;
3851     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3852     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3853       return this_int_off;      // we found a complete word init
3854     }
3855 
3856     // Did this store hit or cross the word boundary?
3857     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3858     if (next_int_off == this_int_off + BytesPerInt) {
3859       // We passed the current int, without fully initializing it.
3860       int_map_off = next_int_off;
3861       int_map &gt;&gt;= BytesPerInt;
3862     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3863       // We passed the current and next int.
3864       return this_int_off + BytesPerInt;
3865     }
3866   }
3867 
3868   return -1;
3869 }
3870 
3871 
3872 // Called when the associated AllocateNode is expanded into CFG.
3873 // At this point, we may perform additional optimizations.
3874 // Linearize the stores by ascending offset, to make memory
3875 // activity as coherent as possible.
3876 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3877                                       intptr_t header_size,
3878                                       Node* size_in_bytes,
3879                                       PhaseGVN* phase) {
3880   assert(!is_complete(), "not already complete");
3881   assert(stores_are_sane(phase), "");
3882   assert(allocation() != NULL, "must be present");
3883 
3884   remove_extra_zeroes();
3885 
3886   if (ReduceFieldZeroing || ReduceBulkZeroing)
3887     // reduce instruction count for common initialization patterns
3888     coalesce_subword_stores(header_size, size_in_bytes, phase);
3889 
3890   Node* zmem = zero_memory();   // initially zero memory state
3891   Node* inits = zmem;           // accumulating a linearized chain of inits
3892   #ifdef ASSERT
3893   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3894   intptr_t last_init_off = first_offset;  // previous init offset
3895   intptr_t last_init_end = first_offset;  // previous init offset+size
3896   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3897   #endif
3898   intptr_t zeroes_done = header_size;
3899 
3900   bool do_zeroing = true;       // we might give up if inits are very sparse
3901   int  big_init_gaps = 0;       // how many large gaps have we seen?
3902 
3903   if (ZeroTLAB)  do_zeroing = false;
3904   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3905 
3906   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3907     Node* st = in(i);
3908     intptr_t st_off = get_store_offset(st, phase);
3909     if (st_off &lt; 0)
3910       break;                    // unknown junk in the inits
3911     if (st-&gt;in(MemNode::Memory) != zmem)
3912       break;                    // complicated store chains somehow in list
3913 
3914     int st_size = st-&gt;as_Store()-&gt;memory_size();
3915     intptr_t next_init_off = st_off + st_size;
3916 
3917     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3918       // See if this store needs a zero before it or under it.
3919       intptr_t zeroes_needed = st_off;
3920 
3921       if (st_size &lt; BytesPerInt) {
3922         // Look for subword stores which only partially initialize words.
3923         // If we find some, we must lay down some word-level zeroes first,
3924         // underneath the subword stores.
3925         //
3926         // Examples:
3927         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3928         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3929         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3930         //
3931         // Note:  coalesce_subword_stores may have already done this,
3932         // if it was prompted by constant non-zero subword initializers.
3933         // But this case can still arise with non-constant stores.
3934 
3935         intptr_t next_full_store = find_next_fullword_store(i, phase);
3936 
3937         // In the examples above:
3938         //   in(i)          p   q   r   s     x   y     z
3939         //   st_off        12  13  14  15    12  13    14
3940         //   st_size        1   1   1   1     1   1     1
3941         //   next_full_s.  12  16  16  16    16  16    16
3942         //   z's_done      12  16  16  16    12  16    12
3943         //   z's_needed    12  16  16  16    16  16    16
3944         //   zsize          0   0   0   0     4   0     4
3945         if (next_full_store &lt; 0) {
3946           // Conservative tack:  Zero to end of current word.
3947           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3948         } else {
3949           // Zero to beginning of next fully initialized word.
3950           // Or, don't zero at all, if we are already in that word.
3951           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3952           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3953           zeroes_needed = next_full_store;
3954         }
3955       }
3956 
3957       if (zeroes_needed &gt; zeroes_done) {
3958         intptr_t zsize = zeroes_needed - zeroes_done;
3959         // Do some incremental zeroing on rawmem, in parallel with inits.
3960         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3961         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3962                                               zeroes_done, zeroes_needed,
3963                                               phase);
3964         zeroes_done = zeroes_needed;
3965         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3966           do_zeroing = false;   // leave the hole, next time
3967       }
3968     }
3969 
3970     // Collect the store and move on:
3971     st-&gt;set_req(MemNode::Memory, inits);
3972     inits = st;                 // put it on the linearized chain
3973     set_req(i, zmem);           // unhook from previous position
3974 
3975     if (zeroes_done == st_off)
3976       zeroes_done = next_init_off;
3977 
3978     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3979 
3980     #ifdef ASSERT
3981     // Various order invariants.  Weaker than stores_are_sane because
3982     // a large constant tile can be filled in by smaller non-constant stores.
3983     assert(st_off &gt;= last_init_off, "inits do not reverse");
3984     last_init_off = st_off;
3985     const Type* val = NULL;
3986     if (st_size &gt;= BytesPerInt &amp;&amp;
3987         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3988         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3989       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3990       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3991       last_tile_end = MAX2(last_tile_end, next_init_off);
3992     } else {
3993       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
3994       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
3995       assert(st_off      &gt;= last_init_end, "inits do not overlap");
3996       last_init_end = next_init_off;  // it's a non-tile
3997     }
3998     #endif //ASSERT
3999   }
4000 
4001   remove_extra_zeroes();        // clear out all the zmems left over
4002   add_req(inits);
4003 
4004   if (!ZeroTLAB) {
4005     // If anything remains to be zeroed, zero it all now.
4006     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
4007     // if it is the last unused 4 bytes of an instance, forget about it
4008     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4009     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4010       assert(allocation() != NULL, "");
4011       if (allocation()-&gt;Opcode() == Op_Allocate) {
4012         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
4013         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4014         if (zeroes_done == k-&gt;layout_helper())
4015           zeroes_done = size_limit;
4016       }
4017     }
4018     if (zeroes_done &lt; size_limit) {
4019       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4020                                             zeroes_done, size_in_bytes, phase);
4021     }
4022   }
4023 
4024   set_complete(phase);
4025   return rawmem;
4026 }
4027 
4028 
4029 #ifdef ASSERT
4030 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4031   if (is_complete())
4032     return true;                // stores could be anything at this point
4033   assert(allocation() != NULL, "must be present");
4034   intptr_t last_off = allocation()-&gt;minimum_header_size();
4035   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4036     Node* st = in(i);
4037     intptr_t st_off = get_store_offset(st, phase);
4038     if (st_off &lt; 0)  continue;  // ignore dead garbage
4039     if (last_off &gt; st_off) {
4040       tty-&gt;print_cr("*** bad store offset at %d: %d &gt; %d", i, last_off, st_off);
4041       this-&gt;dump(2);
4042       assert(false, "ascending store offsets");
4043       return false;
4044     }
4045     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
4046   }
4047   return true;
4048 }
4049 #endif //ASSERT
4050 
4051 
4052 
4053 
4054 //============================MergeMemNode=====================================
4055 //
4056 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4057 // contributing store or call operations.  Each contributor provides the memory
4058 // state for a particular "alias type" (see Compile::alias_type).  For example,
4059 // if a MergeMem has an input X for alias category #6, then any memory reference
4060 // to alias category #6 may use X as its memory state input, as an exact equivalent
4061 // to using the MergeMem as a whole.
4062 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4063 //
4064 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4065 //
4066 // In one special case (and more cases in the future), alias categories overlap.
4067 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4068 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4069 // it is exactly equivalent to that state W:
4070 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4071 //
4072 // Usually, the merge has more than one input.  In that case, where inputs
4073 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4074 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4075 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4076 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4077 //
4078 // A merge can take a "wide" memory state as one of its narrow inputs.
4079 // This simply means that the merge observes out only the relevant parts of
4080 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4081 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4082 //
4083 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4084 // and that memory slices "leak through":
4085 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4086 //
4087 // But, in such a cascade, repeated memory slices can "block the leak":
4088 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4089 //
4090 // In the last example, Y is not part of the combined memory state of the
4091 // outermost MergeMem.  The system must, of course, prevent unschedulable
4092 // memory states from arising, so you can be sure that the state Y is somehow
4093 // a precursor to state Y'.
4094 //
4095 //
4096 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4097 // of each MergeMemNode array are exactly the numerical alias indexes, including
4098 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4099 // Compile::alias_type (and kin) produce and manage these indexes.
4100 //
4101 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4102 // (Note that this provides quick access to the top node inside MergeMem methods,
4103 // without the need to reach out via TLS to Compile::current.)
4104 //
4105 // As a consequence of what was just described, a MergeMem that represents a full
4106 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4107 // containing all alias categories.
4108 //
4109 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4110 //
4111 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4112 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4113 // there is no particular input for that alias type.  Note that the length of
4114 // a MergeMem is variable, and may be extended at any time to accommodate new
4115 // memory states at larger alias indexes.  When merges grow, they are of course
4116 // filled with "top" in the unused in() positions.
4117 //
4118 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4119 // (Top was chosen because it works smoothly with passes like GCM.)
4120 //
4121 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4122 // the type of random VM bits like TLS references.)  Since it is always the
4123 // first non-Bot memory slice, some low-level loops use it to initialize an
4124 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4125 //
4126 //
4127 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4128 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4129 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4130 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4131 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4132 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4133 //
4134 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4135 // really that different from the other memory inputs.  An abbreviation called
4136 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4137 //
4138 //
4139 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4140 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4141 // that "emerges though" the base memory will be marked as excluding the alias types
4142 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4143 //
4144 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4145 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4146 //
4147 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4148 // (It is currently unimplemented.)  As you can see, the resulting merge is
4149 // actually a disjoint union of memory states, rather than an overlay.
4150 //
4151 
4152 //------------------------------MergeMemNode-----------------------------------
4153 Node* MergeMemNode::make_empty_memory() {
4154   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4155   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4156   return empty_memory;
4157 }
4158 
4159 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4160   init_class_id(Class_MergeMem);
4161   // all inputs are nullified in Node::Node(int)
4162   // set_input(0, NULL);  // no control input
4163 
4164   // Initialize the edges uniformly to top, for starters.
4165   Node* empty_mem = make_empty_memory();
4166   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4167     init_req(i,empty_mem);
4168   }
4169   assert(empty_memory() == empty_mem, "");
4170 
4171   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4172     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4173     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4174     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4175       mms.set_memory(mms.memory2());
4176     }
4177     assert(base_memory() == mdef-&gt;base_memory(), "");
4178   } else {
4179     set_base_memory(new_base);
4180   }
4181 }
4182 
4183 // Make a new, untransformed MergeMem with the same base as 'mem'.
4184 // If mem is itself a MergeMem, populate the result with the same edges.
4185 MergeMemNode* MergeMemNode::make(Compile* C, Node* mem) {
4186   return new(C) MergeMemNode(mem);
4187 }
4188 
4189 //------------------------------cmp--------------------------------------------
4190 uint MergeMemNode::hash() const { return NO_HASH; }
4191 uint MergeMemNode::cmp( const Node &amp;n ) const {
4192   return (&amp;n == this);          // Always fail except on self
4193 }
4194 
4195 //------------------------------Identity---------------------------------------
4196 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4197   // Identity if this merge point does not record any interesting memory
4198   // disambiguations.
4199   Node* base_mem = base_memory();
4200   Node* empty_mem = empty_memory();
4201   if (base_mem != empty_mem) {  // Memory path is not dead?
4202     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4203       Node* mem = in(i);
4204       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4205         return this;            // Many memory splits; no change
4206       }
4207     }
4208   }
4209   return base_mem;              // No memory splits; ID on the one true input
4210 }
4211 
4212 //------------------------------Ideal------------------------------------------
4213 // This method is invoked recursively on chains of MergeMem nodes
4214 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4215   // Remove chain'd MergeMems
4216   //
4217   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4218   // relative to the "in(Bot)".  Since we are patching both at the same time,
4219   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4220   // but rewrite each "in(i)" relative to the new "in(Bot)".
4221   Node *progress = NULL;
4222 
4223 
4224   Node* old_base = base_memory();
4225   Node* empty_mem = empty_memory();
4226   if (old_base == empty_mem)
4227     return NULL; // Dead memory path.
4228 
4229   MergeMemNode* old_mbase;
4230   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4231     old_mbase = old_base-&gt;as_MergeMem();
4232   else
4233     old_mbase = NULL;
4234   Node* new_base = old_base;
4235 
4236   // simplify stacked MergeMems in base memory
4237   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4238 
4239   // the base memory might contribute new slices beyond my req()
4240   if (old_mbase)  grow_to_match(old_mbase);
4241 
4242   // Look carefully at the base node if it is a phi.
4243   PhiNode* phi_base;
4244   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4245     phi_base = new_base-&gt;as_Phi();
4246   else
4247     phi_base = NULL;
4248 
4249   Node*    phi_reg = NULL;
4250   uint     phi_len = (uint)-1;
4251   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4252     // do not examine phi if degraded to a copy
4253     phi_reg = phi_base-&gt;region();
4254     phi_len = phi_base-&gt;req();
4255     // see if the phi is unfinished
4256     for (uint i = 1; i &lt; phi_len; i++) {
4257       if (phi_base-&gt;in(i) == NULL) {
4258         // incomplete phi; do not look at it yet!
4259         phi_reg = NULL;
4260         phi_len = (uint)-1;
4261         break;
4262       }
4263     }
4264   }
4265 
4266   // Note:  We do not call verify_sparse on entry, because inputs
4267   // can normalize to the base_memory via subsume_node or similar
4268   // mechanisms.  This method repairs that damage.
4269 
4270   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4271 
4272   // Look at each slice.
4273   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4274     Node* old_in = in(i);
4275     // calculate the old memory value
4276     Node* old_mem = old_in;
4277     if (old_mem == empty_mem)  old_mem = old_base;
4278     assert(old_mem == memory_at(i), "");
4279 
4280     // maybe update (reslice) the old memory value
4281 
4282     // simplify stacked MergeMems
4283     Node* new_mem = old_mem;
4284     MergeMemNode* old_mmem;
4285     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4286       old_mmem = old_mem-&gt;as_MergeMem();
4287     else
4288       old_mmem = NULL;
4289     if (old_mmem == this) {
4290       // This can happen if loops break up and safepoints disappear.
4291       // A merge of BotPtr (default) with a RawPtr memory derived from a
4292       // safepoint can be rewritten to a merge of the same BotPtr with
4293       // the BotPtr phi coming into the loop.  If that phi disappears
4294       // also, we can end up with a self-loop of the mergemem.
4295       // In general, if loops degenerate and memory effects disappear,
4296       // a mergemem can be left looking at itself.  This simply means
4297       // that the mergemem's default should be used, since there is
4298       // no longer any apparent effect on this slice.
4299       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4300       //       from start.  Update the input to TOP.
4301       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4302     }
4303     else if (old_mmem != NULL) {
4304       new_mem = old_mmem-&gt;memory_at(i);
4305     }
4306     // else preceding memory was not a MergeMem
4307 
4308     // replace equivalent phis (unfortunately, they do not GVN together)
4309     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4310         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4311       if (new_mem-&gt;is_Phi()) {
4312         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4313         for (uint i = 1; i &lt; phi_len; i++) {
4314           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4315             phi_mem = NULL;
4316             break;
4317           }
4318         }
4319         if (phi_mem != NULL) {
4320           // equivalent phi nodes; revert to the def
4321           new_mem = new_base;
4322         }
4323       }
4324     }
4325 
4326     // maybe store down a new value
4327     Node* new_in = new_mem;
4328     if (new_in == new_base)  new_in = empty_mem;
4329 
4330     if (new_in != old_in) {
4331       // Warning:  Do not combine this "if" with the previous "if"
4332       // A memory slice might have be be rewritten even if it is semantically
4333       // unchanged, if the base_memory value has changed.
4334       set_req(i, new_in);
4335       progress = this;          // Report progress
4336     }
4337   }
4338 
4339   if (new_base != old_base) {
4340     set_req(Compile::AliasIdxBot, new_base);
4341     // Don't use set_base_memory(new_base), because we need to update du.
4342     assert(base_memory() == new_base, "");
4343     progress = this;
4344   }
4345 
4346   if( base_memory() == this ) {
4347     // a self cycle indicates this memory path is dead
4348     set_req(Compile::AliasIdxBot, empty_mem);
4349   }
4350 
4351   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4352   // Recursion must occur after the self cycle check above
4353   if( base_memory()-&gt;is_MergeMem() ) {
4354     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4355     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4356     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4357         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4358       // propagate rollup of dead cycle to self
4359       set_req(Compile::AliasIdxBot, empty_mem);
4360     }
4361   }
4362 
4363   if( base_memory() == empty_mem ) {
4364     progress = this;
4365     // Cut inputs during Parse phase only.
4366     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4367     if( !can_reshape ) {
4368       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4369         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4370       }
4371     }
4372   }
4373 
4374   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4375     // Check if PhiNode::Ideal's "Split phis through memory merges"
4376     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4377     uint merge_width = req();
4378     if (merge_width &gt; Compile::AliasIdxRaw) {
4379       PhiNode* phi = base_memory()-&gt;as_Phi();
4380       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4381         if (phi-&gt;in(i) == this) {
4382           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4383           break;
4384         }
4385       }
4386     }
4387   }
4388 
4389   assert(progress || verify_sparse(), "please, no dups of base");
4390   return progress;
4391 }
4392 
4393 //-------------------------set_base_memory-------------------------------------
4394 void MergeMemNode::set_base_memory(Node *new_base) {
4395   Node* empty_mem = empty_memory();
4396   set_req(Compile::AliasIdxBot, new_base);
4397   assert(memory_at(req()) == new_base, "must set default memory");
4398   // Clear out other occurrences of new_base:
4399   if (new_base != empty_mem) {
4400     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4401       if (in(i) == new_base)  set_req(i, empty_mem);
4402     }
4403   }
4404 }
4405 
4406 //------------------------------out_RegMask------------------------------------
4407 const RegMask &amp;MergeMemNode::out_RegMask() const {
4408   return RegMask::Empty;
4409 }
4410 
4411 //------------------------------dump_spec--------------------------------------
4412 #ifndef PRODUCT
4413 void MergeMemNode::dump_spec(outputStream *st) const {
4414   st-&gt;print(" {");
4415   Node* base_mem = base_memory();
4416   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4417     Node* mem = memory_at(i);
4418     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4419     st-&gt;print( " N%d:", mem-&gt;_idx );
4420     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4421   }
4422   st-&gt;print(" }");
4423 }
4424 #endif // !PRODUCT
4425 
4426 
4427 #ifdef ASSERT
4428 static bool might_be_same(Node* a, Node* b) {
4429   if (a == b)  return true;
4430   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4431   // phis shift around during optimization
4432   return true;  // pretty stupid...
4433 }
4434 
4435 // verify a narrow slice (either incoming or outgoing)
4436 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4437   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4438   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4439   if (Node::in_dump())      return;  // muzzle asserts when printing
4440   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4441   assert(n != NULL, "");
4442   // Elide intervening MergeMem's
4443   while (n-&gt;is_MergeMem()) {
4444     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4445   }
4446   Compile* C = Compile::current();
4447   const TypePtr* n_adr_type = n-&gt;adr_type();
4448   if (n == m-&gt;empty_memory()) {
4449     // Implicit copy of base_memory()
4450   } else if (n_adr_type != TypePtr::BOTTOM) {
4451     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4452     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4453   } else {
4454     // A few places like make_runtime_call "know" that VM calls are narrow,
4455     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4456     bool expected_wide_mem = false;
4457     if (n == m-&gt;base_memory()) {
4458       expected_wide_mem = true;
4459     } else if (alias_idx == Compile::AliasIdxRaw ||
4460                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4461       expected_wide_mem = true;
4462     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4463       // memory can "leak through" calls on channels that
4464       // are write-once.  Allow this also.
4465       expected_wide_mem = true;
4466     }
4467     assert(expected_wide_mem, "expected narrow slice replacement");
4468   }
4469 }
4470 #else // !ASSERT
4471 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4472 #endif
4473 
4474 
4475 //-----------------------------memory_at---------------------------------------
4476 Node* MergeMemNode::memory_at(uint alias_idx) const {
4477   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4478          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4479          "must avoid base_memory and AliasIdxTop");
4480 
4481   // Otherwise, it is a narrow slice.
4482   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4483   Compile *C = Compile::current();
4484   if (is_empty_memory(n)) {
4485     // the array is sparse; empty slots are the "top" node
4486     n = base_memory();
4487     assert(Node::in_dump()
4488            || n == NULL || n-&gt;bottom_type() == Type::TOP
4489            || n-&gt;adr_type() == NULL // address is TOP
4490            || n-&gt;adr_type() == TypePtr::BOTTOM
4491            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4492            || Compile::current()-&gt;AliasLevel() == 0,
4493            "must be a wide memory");
4494     // AliasLevel == 0 if we are organizing the memory states manually.
4495     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4496   } else {
4497     // make sure the stored slice is sane
4498     #ifdef ASSERT
4499     if (is_error_reported() || Node::in_dump()) {
4500     } else if (might_be_same(n, base_memory())) {
4501       // Give it a pass:  It is a mostly harmless repetition of the base.
4502       // This can arise normally from node subsumption during optimization.
4503     } else {
4504       verify_memory_slice(this, alias_idx, n);
4505     }
4506     #endif
4507   }
4508   return n;
4509 }
4510 
4511 //---------------------------set_memory_at-------------------------------------
4512 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4513   verify_memory_slice(this, alias_idx, n);
4514   Node* empty_mem = empty_memory();
4515   if (n == base_memory())  n = empty_mem;  // collapse default
4516   uint need_req = alias_idx+1;
4517   if (req() &lt; need_req) {
4518     if (n == empty_mem)  return;  // already the default, so do not grow me
4519     // grow the sparse array
4520     do {
4521       add_req(empty_mem);
4522     } while (req() &lt; need_req);
4523   }
4524   set_req( alias_idx, n );
4525 }
4526 
4527 
4528 
4529 //--------------------------iteration_setup------------------------------------
4530 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4531   if (other != NULL) {
4532     grow_to_match(other);
4533     // invariant:  the finite support of mm2 is within mm-&gt;req()
4534     #ifdef ASSERT
4535     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4536       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4537     }
4538     #endif
4539   }
4540   // Replace spurious copies of base_memory by top.
4541   Node* base_mem = base_memory();
4542   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4543     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4544       if (in(i) == base_mem)
4545         set_req(i, empty_memory());
4546     }
4547   }
4548 }
4549 
4550 //---------------------------grow_to_match-------------------------------------
4551 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4552   Node* empty_mem = empty_memory();
4553   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4554   // look for the finite support of the other memory
4555   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4556     if (other-&gt;in(i) != empty_mem) {
4557       uint new_len = i+1;
4558       while (req() &lt; new_len)  add_req(empty_mem);
4559       break;
4560     }
4561   }
4562 }
4563 
4564 //---------------------------verify_sparse-------------------------------------
4565 #ifndef PRODUCT
4566 bool MergeMemNode::verify_sparse() const {
4567   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4568   Node* base_mem = base_memory();
4569   // The following can happen in degenerate cases, since empty==top.
4570   if (is_empty_memory(base_mem))  return true;
4571   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4572     assert(in(i) != NULL, "sane slice");
4573     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4574   }
4575   return true;
4576 }
4577 
4578 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4579   Node* n;
4580   n = mm-&gt;in(idx);
4581   if (mem == n)  return true;  // might be empty_memory()
4582   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4583   if (mem == n)  return true;
4584   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4585     if (mem == n)  return true;
4586     if (n == NULL)  break;
4587   }
4588   return false;
4589 }
4590 #endif // !PRODUCT
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
