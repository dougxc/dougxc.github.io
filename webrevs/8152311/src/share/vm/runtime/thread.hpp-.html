<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/runtime/thread.hpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_RUNTIME_THREAD_HPP
  26 #define SHARE_VM_RUNTIME_THREAD_HPP
  27 
  28 #include "gc/shared/threadLocalAllocBuffer.hpp"
  29 #include "memory/allocation.hpp"
  30 #include "oops/oop.hpp"
  31 #include "prims/jni.h"
  32 #include "prims/jvmtiExport.hpp"
  33 #include "runtime/frame.hpp"
  34 #include "runtime/javaFrameAnchor.hpp"
  35 #include "runtime/jniHandles.hpp"
  36 #include "runtime/mutexLocker.hpp"
  37 #include "runtime/os.hpp"
  38 #include "runtime/osThread.hpp"
  39 #include "runtime/park.hpp"
  40 #include "runtime/safepoint.hpp"
  41 #include "runtime/stubRoutines.hpp"
  42 #include "runtime/threadLocalStorage.hpp"
  43 #include "runtime/thread_ext.hpp"
  44 #include "runtime/unhandledOops.hpp"
  45 #include "trace/traceBackend.hpp"
  46 #include "trace/traceMacros.hpp"
  47 #include "utilities/exceptions.hpp"
  48 #include "utilities/macros.hpp"
  49 #if INCLUDE_ALL_GCS
  50 #include "gc/g1/dirtyCardQueue.hpp"
  51 #include "gc/g1/satbMarkQueue.hpp"
  52 #endif // INCLUDE_ALL_GCS
  53 #ifdef TARGET_ARCH_zero
  54 # include "stack_zero.hpp"
  55 #endif
  56 
  57 class ThreadSafepointState;
  58 class ThreadProfiler;
  59 
  60 class JvmtiThreadState;
  61 class JvmtiGetLoadedClassesClosure;
  62 class ThreadStatistics;
  63 class ConcurrentLocksDump;
  64 class ParkEvent;
  65 class Parker;
  66 
  67 class ciEnv;
  68 class CompileThread;
  69 class CompileLog;
  70 class CompileTask;
  71 class CompileQueue;
  72 class CompilerCounters;
  73 class vframeArray;
  74 
  75 class DeoptResourceMark;
  76 class jvmtiDeferredLocalVariableSet;
  77 
  78 class GCTaskQueue;
  79 class ThreadClosure;
  80 class IdealGraphPrinter;
  81 
  82 class Metadata;
  83 template &lt;class T, MEMFLAGS F&gt; class ChunkedList;
  84 typedef ChunkedList&lt;Metadata*, mtInternal&gt; MetadataOnStackBuffer;
  85 
  86 DEBUG_ONLY(class ResourceMark;)
  87 
  88 class WorkerThread;
  89 
  90 // Class hierarchy
  91 // - Thread
  92 //   - NamedThread
  93 //     - VMThread
  94 //     - ConcurrentGCThread
  95 //     - WorkerThread
  96 //       - GangWorker
  97 //       - GCTaskThread
  98 //   - JavaThread
  99 //   - WatcherThread
 100 
 101 class Thread: public ThreadShadow {
 102   friend class VMStructs;
 103   friend class JVMCIVMStructs;
 104  private:
 105 
 106 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 107   // Current thread is maintained as a thread-local variable
 108   static THREAD_LOCAL_DECL Thread* _thr_current;
 109 #endif
 110 
 111   // Exception handling
 112   // (Note: _pending_exception and friends are in ThreadShadow)
 113   //oop       _pending_exception;                // pending exception for current thread
 114   // const char* _exception_file;                   // file information for exception (debugging only)
 115   // int         _exception_line;                   // line information for exception (debugging only)
 116  protected:
 117   // Support for forcing alignment of thread objects for biased locking
 118   void*       _real_malloc_address;
 119  public:
 120   void* operator new(size_t size) throw() { return allocate(size, true); }
 121   void* operator new(size_t size, const std::nothrow_t&amp; nothrow_constant) throw() {
 122     return allocate(size, false); }
 123   void  operator delete(void* p);
 124 
 125  protected:
 126   static void* allocate(size_t size, bool throw_excpt, MEMFLAGS flags = mtThread);
 127  private:
 128 
 129   // ***************************************************************
 130   // Suspend and resume support
 131   // ***************************************************************
 132   //
 133   // VM suspend/resume no longer exists - it was once used for various
 134   // things including safepoints but was deprecated and finally removed
 135   // in Java 7. Because VM suspension was considered "internal" Java-level
 136   // suspension was considered "external", and this legacy naming scheme
 137   // remains.
 138   //
 139   // External suspend/resume requests come from JVM_SuspendThread,
 140   // JVM_ResumeThread, JVMTI SuspendThread, and finally JVMTI
 141   // ResumeThread. External
 142   // suspend requests cause _external_suspend to be set and external
 143   // resume requests cause _external_suspend to be cleared.
 144   // External suspend requests do not nest on top of other external
 145   // suspend requests. The higher level APIs reject suspend requests
 146   // for already suspended threads.
 147   //
 148   // The external_suspend
 149   // flag is checked by has_special_runtime_exit_condition() and java thread
 150   // will self-suspend when handle_special_runtime_exit_condition() is
 151   // called. Most uses of the _thread_blocked state in JavaThreads are
 152   // considered the same as being externally suspended; if the blocking
 153   // condition lifts, the JavaThread will self-suspend. Other places
 154   // where VM checks for external_suspend include:
 155   //   + mutex granting (do not enter monitors when thread is suspended)
 156   //   + state transitions from _thread_in_native
 157   //
 158   // In general, java_suspend() does not wait for an external suspend
 159   // request to complete. When it returns, the only guarantee is that
 160   // the _external_suspend field is true.
 161   //
 162   // wait_for_ext_suspend_completion() is used to wait for an external
 163   // suspend request to complete. External suspend requests are usually
 164   // followed by some other interface call that requires the thread to
 165   // be quiescent, e.g., GetCallTrace(). By moving the "wait time" into
 166   // the interface that requires quiescence, we give the JavaThread a
 167   // chance to self-suspend before we need it to be quiescent. This
 168   // improves overall suspend/query performance.
 169   //
 170   // _suspend_flags controls the behavior of java_ suspend/resume.
 171   // It must be set under the protection of SR_lock. Read from the flag is
 172   // OK without SR_lock as long as the value is only used as a hint.
 173   // (e.g., check _external_suspend first without lock and then recheck
 174   // inside SR_lock and finish the suspension)
 175   //
 176   // _suspend_flags is also overloaded for other "special conditions" so
 177   // that a single check indicates whether any special action is needed
 178   // eg. for async exceptions.
 179   // -------------------------------------------------------------------
 180   // Notes:
 181   // 1. The suspend/resume logic no longer uses ThreadState in OSThread
 182   // but we still update its value to keep other part of the system (mainly
 183   // JVMTI) happy. ThreadState is legacy code (see notes in
 184   // osThread.hpp).
 185   //
 186   // 2. It would be more natural if set_external_suspend() is private and
 187   // part of java_suspend(), but that probably would affect the suspend/query
 188   // performance. Need more investigation on this.
 189 
 190   // suspend/resume lock: used for self-suspend
 191   Monitor* _SR_lock;
 192 
 193  protected:
 194   enum SuspendFlags {
 195     // NOTE: avoid using the sign-bit as cc generates different test code
 196     //       when the sign-bit is used, and sometimes incorrectly - see CR 6398077
 197 
 198     _external_suspend       = 0x20000000U, // thread is asked to self suspend
 199     _ext_suspended          = 0x40000000U, // thread has self-suspended
 200     _deopt_suspend          = 0x10000000U, // thread needs to self suspend for deopt
 201 
 202     _has_async_exception    = 0x00000001U, // there is a pending async exception
 203     _critical_native_unlock = 0x00000002U  // Must call back to unlock JNI critical lock
 204   };
 205 
 206   // various suspension related flags - atomically updated
 207   // overloaded for async exception checking in check_special_condition_for_native_trans.
 208   volatile uint32_t _suspend_flags;
 209 
 210  private:
 211   int _num_nested_signal;
 212 
 213   DEBUG_ONLY(bool _suspendible_thread;)
 214 
 215  public:
 216   void enter_signal_handler() { _num_nested_signal++; }
 217   void leave_signal_handler() { _num_nested_signal--; }
 218   bool is_inside_signal_handler() const { return _num_nested_signal &gt; 0; }
 219 
 220 #ifdef ASSERT
 221   void set_suspendible_thread() {
 222     _suspendible_thread = true;
 223   }
 224 
 225   void clear_suspendible_thread() {
 226     _suspendible_thread = false;
 227   }
 228 
 229   bool is_suspendible_thread() { return _suspendible_thread; }
 230 #endif
 231 
 232  private:
 233   // Active_handles points to a block of handles
 234   JNIHandleBlock* _active_handles;
 235 
 236   // One-element thread local free list
 237   JNIHandleBlock* _free_handle_block;
 238 
 239   // Point to the last handle mark
 240   HandleMark* _last_handle_mark;
 241 
 242   // The parity of the last strong_roots iteration in which this thread was
 243   // claimed as a task.
 244   jint _oops_do_parity;
 245 
 246  public:
 247   void set_last_handle_mark(HandleMark* mark)   { _last_handle_mark = mark; }
 248   HandleMark* last_handle_mark() const          { return _last_handle_mark; }
 249  private:
 250 
 251   // debug support for checking if code does allow safepoints or not
 252   // GC points in the VM can happen because of allocation, invoking a VM operation, or blocking on
 253   // mutex, or blocking on an object synchronizer (Java locking).
 254   // If !allow_safepoint(), then an assertion failure will happen in any of the above cases
 255   // If !allow_allocation(), then an assertion failure will happen during allocation
 256   // (Hence, !allow_safepoint() =&gt; !allow_allocation()).
 257   //
 258   // The two classes NoSafepointVerifier and No_Allocation_Verifier are used to set these counters.
 259   //
 260   NOT_PRODUCT(int _allow_safepoint_count;)      // If 0, thread allow a safepoint to happen
 261   debug_only(int _allow_allocation_count;)     // If 0, the thread is allowed to allocate oops.
 262 
 263   // Used by SkipGCALot class.
 264   NOT_PRODUCT(bool _skip_gcalot;)               // Should we elide gc-a-lot?
 265 
 266   friend class NoAllocVerifier;
 267   friend class NoSafepointVerifier;
 268   friend class PauseNoSafepointVerifier;
 269   friend class GCLocker;
 270 
 271   ThreadLocalAllocBuffer _tlab;                 // Thread-local eden
 272   jlong _allocated_bytes;                       // Cumulative number of bytes allocated on
 273                                                 // the Java heap
 274 
 275   mutable TRACE_DATA _trace_data;               // Thread-local data for tracing
 276 
 277   ThreadExt _ext;
 278 
 279   int   _vm_operation_started_count;            // VM_Operation support
 280   int   _vm_operation_completed_count;          // VM_Operation support
 281 
 282   ObjectMonitor* _current_pending_monitor;      // ObjectMonitor this thread
 283                                                 // is waiting to lock
 284   bool _current_pending_monitor_is_from_java;   // locking is from Java code
 285 
 286   // ObjectMonitor on which this thread called Object.wait()
 287   ObjectMonitor* _current_waiting_monitor;
 288 
 289   // Private thread-local objectmonitor list - a simple cache organized as a SLL.
 290  public:
 291   ObjectMonitor* omFreeList;
 292   int omFreeCount;                              // length of omFreeList
 293   int omFreeProvision;                          // reload chunk size
 294   ObjectMonitor* omInUseList;                   // SLL to track monitors in circulation
 295   int omInUseCount;                             // length of omInUseList
 296 
 297 #ifdef ASSERT
 298  private:
 299   bool _visited_for_critical_count;
 300 
 301  public:
 302   void set_visited_for_critical_count(bool z) { _visited_for_critical_count = z; }
 303   bool was_visited_for_critical_count() const   { return _visited_for_critical_count; }
 304 #endif
 305 
 306  public:
 307   enum {
 308     is_definitely_current_thread = true
 309   };
 310 
 311   // Constructor
 312   Thread();
 313   virtual ~Thread();
 314 
 315   // Manage Thread::current()
 316   void initialize_thread_current();
 317   private:
 318   void clear_thread_current(); // needed for detaching JNI threads
 319 
 320   public:
 321   // thread entry point
 322   virtual void run();
 323 
 324   // Testers
 325   virtual bool is_VM_thread()       const            { return false; }
 326   virtual bool is_Java_thread()     const            { return false; }
 327   virtual bool is_Compiler_thread() const            { return false; }
 328   virtual bool is_Code_cache_sweeper_thread() const  { return false; }
 329   virtual bool is_hidden_from_external_view() const  { return false; }
 330   virtual bool is_jvmti_agent_thread() const         { return false; }
 331   // True iff the thread can perform GC operations at a safepoint.
 332   // Generally will be true only of VM thread and parallel GC WorkGang
 333   // threads.
 334   virtual bool is_GC_task_thread() const             { return false; }
 335   virtual bool is_Watcher_thread() const             { return false; }
 336   virtual bool is_ConcurrentGC_thread() const        { return false; }
 337   virtual bool is_Named_thread() const               { return false; }
 338   virtual bool is_Worker_thread() const              { return false; }
 339 
 340   // Can this thread make Java upcalls
 341   virtual bool can_call_java() const                 { return false; }
 342 
 343   // Casts
 344   virtual WorkerThread* as_Worker_thread() const     { return NULL; }
 345 
 346   virtual char* name() const { return (char*)"Unknown thread"; }
 347 
 348   // Returns the current thread (ASSERTS if NULL)
 349   static inline Thread* current();
 350   // Returns the current thread, or NULL if not attached
 351   static inline Thread* current_or_null();
 352   // Returns the current thread, or NULL if not attached, and is
 353   // safe for use from signal-handlers
 354   static inline Thread* current_or_null_safe();
 355 
 356   // Common thread operations
 357   static void set_priority(Thread* thread, ThreadPriority priority);
 358   static ThreadPriority get_priority(const Thread* const thread);
 359   static void start(Thread* thread);
 360   static void interrupt(Thread* thr);
 361   static bool is_interrupted(Thread* thr, bool clear_interrupted);
 362 
 363   void set_native_thread_name(const char *name) {
 364     assert(Thread::current() == this, "set_native_thread_name can only be called on the current thread");
 365     os::set_native_thread_name(name);
 366   }
 367 
 368   ObjectMonitor** omInUseList_addr()             { return (ObjectMonitor **)&amp;omInUseList; }
 369   Monitor* SR_lock() const                       { return _SR_lock; }
 370 
 371   bool has_async_exception() const { return (_suspend_flags &amp; _has_async_exception) != 0; }
 372 
 373   inline void set_suspend_flag(SuspendFlags f);
 374   inline void clear_suspend_flag(SuspendFlags f);
 375 
 376   inline void set_has_async_exception();
 377   inline void clear_has_async_exception();
 378 
 379   bool do_critical_native_unlock() const { return (_suspend_flags &amp; _critical_native_unlock) != 0; }
 380 
 381   inline void set_critical_native_unlock();
 382   inline void clear_critical_native_unlock();
 383 
 384   // Support for Unhandled Oop detection
 385   // Add the field for both, fastdebug and debug, builds to keep
 386   // Thread's fields layout the same.
 387   // Note: CHECK_UNHANDLED_OOPS is defined only for fastdebug build.
 388 #ifdef CHECK_UNHANDLED_OOPS
 389  private:
 390   UnhandledOops* _unhandled_oops;
 391 #elif defined(ASSERT)
 392  private:
 393   void* _unhandled_oops;
 394 #endif
 395 #ifdef CHECK_UNHANDLED_OOPS
 396  public:
 397   UnhandledOops* unhandled_oops() { return _unhandled_oops; }
 398   // Mark oop safe for gc.  It may be stack allocated but won't move.
 399   void allow_unhandled_oop(oop *op) {
 400     if (CheckUnhandledOops) unhandled_oops()-&gt;allow_unhandled_oop(op);
 401   }
 402   // Clear oops at safepoint so crashes point to unhandled oop violator
 403   void clear_unhandled_oops() {
 404     if (CheckUnhandledOops) unhandled_oops()-&gt;clear_unhandled_oops();
 405   }
 406 #endif // CHECK_UNHANDLED_OOPS
 407 
 408  public:
 409 #ifndef PRODUCT
 410   bool skip_gcalot()           { return _skip_gcalot; }
 411   void set_skip_gcalot(bool v) { _skip_gcalot = v;    }
 412 #endif
 413 
 414   // Installs a pending exception to be inserted later
 415   static void send_async_exception(oop thread_oop, oop java_throwable);
 416 
 417   // Resource area
 418   ResourceArea* resource_area() const            { return _resource_area; }
 419   void set_resource_area(ResourceArea* area)     { _resource_area = area; }
 420 
 421   OSThread* osthread() const                     { return _osthread;   }
 422   void set_osthread(OSThread* thread)            { _osthread = thread; }
 423 
 424   // JNI handle support
 425   JNIHandleBlock* active_handles() const         { return _active_handles; }
 426   void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }
 427   JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }
 428   void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }
 429 
 430   // Internal handle support
 431   HandleArea* handle_area() const                { return _handle_area; }
 432   void set_handle_area(HandleArea* area)         { _handle_area = area; }
 433 
 434   GrowableArray&lt;Metadata*&gt;* metadata_handles() const          { return _metadata_handles; }
 435   void set_metadata_handles(GrowableArray&lt;Metadata*&gt;* handles){ _metadata_handles = handles; }
 436 
 437   // Thread-Local Allocation Buffer (TLAB) support
 438   ThreadLocalAllocBuffer&amp; tlab()                 { return _tlab; }
 439   void initialize_tlab() {
 440     if (UseTLAB) {
 441       tlab().initialize();
 442     }
 443   }
 444 
 445   jlong allocated_bytes()               { return _allocated_bytes; }
 446   void set_allocated_bytes(jlong value) { _allocated_bytes = value; }
 447   void incr_allocated_bytes(jlong size) { _allocated_bytes += size; }
 448   inline jlong cooked_allocated_bytes();
 449 
 450   TRACE_DEFINE_THREAD_TRACE_DATA_OFFSET;
 451   TRACE_DATA* trace_data() const        { return &amp;_trace_data; }
 452 
 453   const ThreadExt&amp; ext() const          { return _ext; }
 454   ThreadExt&amp; ext()                      { return _ext; }
 455 
 456   // VM operation support
 457   int vm_operation_ticket()                      { return ++_vm_operation_started_count; }
 458   int vm_operation_completed_count()             { return _vm_operation_completed_count; }
 459   void increment_vm_operation_completed_count()  { _vm_operation_completed_count++; }
 460 
 461   // For tracking the heavyweight monitor the thread is pending on.
 462   ObjectMonitor* current_pending_monitor() {
 463     return _current_pending_monitor;
 464   }
 465   void set_current_pending_monitor(ObjectMonitor* monitor) {
 466     _current_pending_monitor = monitor;
 467   }
 468   void set_current_pending_monitor_is_from_java(bool from_java) {
 469     _current_pending_monitor_is_from_java = from_java;
 470   }
 471   bool current_pending_monitor_is_from_java() {
 472     return _current_pending_monitor_is_from_java;
 473   }
 474 
 475   // For tracking the ObjectMonitor on which this thread called Object.wait()
 476   ObjectMonitor* current_waiting_monitor() {
 477     return _current_waiting_monitor;
 478   }
 479   void set_current_waiting_monitor(ObjectMonitor* monitor) {
 480     _current_waiting_monitor = monitor;
 481   }
 482 
 483   // GC support
 484   // Apply "f-&gt;do_oop" to all root oops in "this".
 485   //   Used by JavaThread::oops_do.
 486   // Apply "cf-&gt;do_code_blob" (if !NULL) to all code blobs active in frames
 487   virtual void oops_do(OopClosure* f, CodeBlobClosure* cf);
 488 
 489   // Handles the parallel case for the method below.
 490  private:
 491   bool claim_oops_do_par_case(int collection_parity);
 492  public:
 493   // Requires that "collection_parity" is that of the current roots
 494   // iteration.  If "is_par" is false, sets the parity of "this" to
 495   // "collection_parity", and returns "true".  If "is_par" is true,
 496   // uses an atomic instruction to set the current threads parity to
 497   // "collection_parity", if it is not already.  Returns "true" iff the
 498   // calling thread does the update, this indicates that the calling thread
 499   // has claimed the thread's stack as a root groop in the current
 500   // collection.
 501   bool claim_oops_do(bool is_par, int collection_parity) {
 502     if (!is_par) {
 503       _oops_do_parity = collection_parity;
 504       return true;
 505     } else {
 506       return claim_oops_do_par_case(collection_parity);
 507     }
 508   }
 509 
 510   // jvmtiRedefineClasses support
 511   void metadata_handles_do(void f(Metadata*));
 512 
 513   // Used by fast lock support
 514   virtual bool is_lock_owned(address adr) const;
 515 
 516   // Check if address is in the stack of the thread (not just for locks).
 517   // Warning: the method can only be used on the running thread
 518   bool is_in_stack(address adr) const;
 519   // Check if address is in the usable part of the stack (excludes protected
 520   // guard pages)
 521   bool is_in_usable_stack(address adr) const;
 522 
 523   // Sets this thread as starting thread. Returns failure if thread
 524   // creation fails due to lack of memory, too many threads etc.
 525   bool set_as_starting_thread();
 526 
 527 protected:
 528   // OS data associated with the thread
 529   OSThread* _osthread;  // Platform-specific thread information
 530 
 531   // Thread local resource area for temporary allocation within the VM
 532   ResourceArea* _resource_area;
 533 
 534   DEBUG_ONLY(ResourceMark* _current_resource_mark;)
 535 
 536   // Thread local handle area for allocation of handles within the VM
 537   HandleArea* _handle_area;
 538   GrowableArray&lt;Metadata*&gt;* _metadata_handles;
 539 
 540   // Support for stack overflow handling, get_thread, etc.
 541   address          _stack_base;
 542   size_t           _stack_size;
 543   uintptr_t        _self_raw_id;      // used by get_thread (mutable)
 544   int              _lgrp_id;
 545 
 546  public:
 547   // Stack overflow support
 548   address stack_base() const           { assert(_stack_base != NULL,"Sanity check"); return _stack_base; }
 549   void    set_stack_base(address base) { _stack_base = base; }
 550   size_t  stack_size() const           { return _stack_size; }
 551   void    set_stack_size(size_t size)  { _stack_size = size; }
 552   address stack_end()  const           { return stack_base() - stack_size(); }
 553   void    record_stack_base_and_size();
 554 
 555   bool    on_local_stack(address adr) const {
 556     // QQQ this has knowledge of direction, ought to be a stack method
 557     return (_stack_base &gt;= adr &amp;&amp; adr &gt;= stack_end());
 558   }
 559 
 560   uintptr_t self_raw_id()                    { return _self_raw_id; }
 561   void      set_self_raw_id(uintptr_t value) { _self_raw_id = value; }
 562 
 563   int     lgrp_id() const        { return _lgrp_id; }
 564   void    set_lgrp_id(int value) { _lgrp_id = value; }
 565 
 566   // Printing
 567   virtual void print_on(outputStream* st) const;
 568   void print() const { print_on(tty); }
 569   virtual void print_on_error(outputStream* st, char* buf, int buflen) const;
 570 
 571   // Debug-only code
 572 #ifdef ASSERT
 573  private:
 574   // Deadlock detection support for Mutex locks. List of locks own by thread.
 575   Monitor* _owned_locks;
 576   // Mutex::set_owner_implementation is the only place where _owned_locks is modified,
 577   // thus the friendship
 578   friend class Mutex;
 579   friend class Monitor;
 580 
 581  public:
 582   void print_owned_locks_on(outputStream* st) const;
 583   void print_owned_locks() const                 { print_owned_locks_on(tty);    }
 584   Monitor* owned_locks() const                   { return _owned_locks;          }
 585   bool owns_locks() const                        { return owned_locks() != NULL; }
 586   bool owns_locks_but_compiled_lock() const;
 587   int oops_do_parity() const                     { return _oops_do_parity; }
 588 
 589   // Deadlock detection
 590   bool allow_allocation()                        { return _allow_allocation_count == 0; }
 591   ResourceMark* current_resource_mark()          { return _current_resource_mark; }
 592   void set_current_resource_mark(ResourceMark* rm) { _current_resource_mark = rm; }
 593 #endif
 594 
 595   void check_for_valid_safepoint_state(bool potential_vm_operation) PRODUCT_RETURN;
 596 
 597  private:
 598   volatile int _jvmti_env_iteration_count;
 599 
 600  public:
 601   void entering_jvmti_env_iteration()            { ++_jvmti_env_iteration_count; }
 602   void leaving_jvmti_env_iteration()             { --_jvmti_env_iteration_count; }
 603   bool is_inside_jvmti_env_iteration()           { return _jvmti_env_iteration_count &gt; 0; }
 604 
 605   // Code generation
 606   static ByteSize exception_file_offset()        { return byte_offset_of(Thread, _exception_file); }
 607   static ByteSize exception_line_offset()        { return byte_offset_of(Thread, _exception_line); }
 608   static ByteSize active_handles_offset()        { return byte_offset_of(Thread, _active_handles); }
 609 
 610   static ByteSize stack_base_offset()            { return byte_offset_of(Thread, _stack_base); }
 611   static ByteSize stack_size_offset()            { return byte_offset_of(Thread, _stack_size); }
 612 
 613 #define TLAB_FIELD_OFFSET(name) \
 614   static ByteSize tlab_##name##_offset()         { return byte_offset_of(Thread, _tlab) + ThreadLocalAllocBuffer::name##_offset(); }
 615 
 616   TLAB_FIELD_OFFSET(start)
 617   TLAB_FIELD_OFFSET(end)
 618   TLAB_FIELD_OFFSET(top)
 619   TLAB_FIELD_OFFSET(pf_top)
 620   TLAB_FIELD_OFFSET(size)                   // desired_size
 621   TLAB_FIELD_OFFSET(refill_waste_limit)
 622   TLAB_FIELD_OFFSET(number_of_refills)
 623   TLAB_FIELD_OFFSET(fast_refill_waste)
 624   TLAB_FIELD_OFFSET(slow_allocations)
 625 
 626 #undef TLAB_FIELD_OFFSET
 627 
 628   static ByteSize allocated_bytes_offset()       { return byte_offset_of(Thread, _allocated_bytes); }
 629 
 630  public:
 631   volatile intptr_t _Stalled;
 632   volatile int _TypeTag;
 633   ParkEvent * _ParkEvent;                     // for synchronized()
 634   ParkEvent * _SleepEvent;                    // for Thread.sleep
 635   ParkEvent * _MutexEvent;                    // for native internal Mutex/Monitor
 636   ParkEvent * _MuxEvent;                      // for low-level muxAcquire-muxRelease
 637   int NativeSyncRecursion;                    // diagnostic
 638 
 639   volatile int _OnTrap;                       // Resume-at IP delta
 640   jint _hashStateW;                           // Marsaglia Shift-XOR thread-local RNG
 641   jint _hashStateX;                           // thread-specific hashCode generator state
 642   jint _hashStateY;
 643   jint _hashStateZ;
 644   void * _schedctl;
 645 
 646 
 647   volatile jint rng[4];                      // RNG for spin loop
 648 
 649   // Low-level leaf-lock primitives used to implement synchronization
 650   // and native monitor-mutex infrastructure.
 651   // Not for general synchronization use.
 652   static void SpinAcquire(volatile int * Lock, const char * Name);
 653   static void SpinRelease(volatile int * Lock);
 654   static void muxAcquire(volatile intptr_t * Lock, const char * Name);
 655   static void muxAcquireW(volatile intptr_t * Lock, ParkEvent * ev);
 656   static void muxRelease(volatile intptr_t * Lock);
 657 };
 658 
 659 // Inline implementation of Thread::current()
 660 inline Thread* Thread::current() {
 661   Thread* current = current_or_null();
 662   assert(current != NULL, "Thread::current() called on detached thread");
 663   return current;
 664 }
 665 
 666 inline Thread* Thread::current_or_null() {
 667 #ifndef USE_LIBRARY_BASED_TLS_ONLY
 668   return _thr_current;
 669 #else
 670   return ThreadLocalStorage::thread();
 671 #endif
 672 }
 673 
 674 inline Thread* Thread::current_or_null_safe() {
 675   return ThreadLocalStorage::thread();
 676 }
 677 
 678 // Name support for threads.  non-JavaThread subclasses with multiple
 679 // uniquely named instances should derive from this.
 680 class NamedThread: public Thread {
 681   friend class VMStructs;
 682   enum {
 683     max_name_len = 64
 684   };
 685  private:
 686   char* _name;
 687   // log JavaThread being processed by oops_do
 688   JavaThread* _processed_thread;
 689   uint _gc_id; // The current GC id when a thread takes part in GC
 690 
 691  public:
 692   NamedThread();
 693   ~NamedThread();
 694   // May only be called once per thread.
 695   void set_name(const char* format, ...)  ATTRIBUTE_PRINTF(2, 3);
 696   void initialize_named_thread();
 697   virtual bool is_Named_thread() const { return true; }
 698   virtual char* name() const { return _name == NULL ? (char*)"Unknown Thread" : _name; }
 699   JavaThread *processed_thread() { return _processed_thread; }
 700   void set_processed_thread(JavaThread *thread) { _processed_thread = thread; }
 701   virtual void print_on(outputStream* st) const;
 702 
 703   void set_gc_id(uint gc_id) { _gc_id = gc_id; }
 704   uint gc_id() { return _gc_id; }
 705 };
 706 
 707 // Worker threads are named and have an id of an assigned work.
 708 class WorkerThread: public NamedThread {
 709  private:
 710   uint _id;
 711  public:
 712   WorkerThread() : _id(0)               { }
 713   virtual bool is_Worker_thread() const { return true; }
 714 
 715   virtual WorkerThread* as_Worker_thread() const {
 716     assert(is_Worker_thread(), "Dubious cast to WorkerThread*?");
 717     return (WorkerThread*) this;
 718   }
 719 
 720   void set_id(uint work_id)             { _id = work_id; }
 721   uint id() const                       { return _id; }
 722 };
 723 
 724 // A single WatcherThread is used for simulating timer interrupts.
 725 class WatcherThread: public Thread {
 726   friend class VMStructs;
 727  public:
 728   virtual void run();
 729 
 730  private:
 731   static WatcherThread* _watcher_thread;
 732 
 733   static bool _startable;
 734   // volatile due to at least one lock-free read
 735   volatile static bool _should_terminate;
 736 
 737   os::WatcherThreadCrashProtection* _crash_protection;
 738  public:
 739   enum SomeConstants {
 740     delay_interval = 10                          // interrupt delay in milliseconds
 741   };
 742 
 743   // Constructor
 744   WatcherThread();
 745 
 746   // Tester
 747   bool is_Watcher_thread() const                 { return true; }
 748 
 749   // Printing
 750   char* name() const { return (char*)"VM Periodic Task Thread"; }
 751   void print_on(outputStream* st) const;
 752   void unpark();
 753 
 754   // Returns the single instance of WatcherThread
 755   static WatcherThread* watcher_thread()         { return _watcher_thread; }
 756 
 757   // Create and start the single instance of WatcherThread, or stop it on shutdown
 758   static void start();
 759   static void stop();
 760   // Only allow start once the VM is sufficiently initialized
 761   // Otherwise the first task to enroll will trigger the start
 762   static void make_startable();
 763 
 764   void set_crash_protection(os::WatcherThreadCrashProtection* crash_protection) {
 765     assert(Thread::current()-&gt;is_Watcher_thread(), "Can only be set by WatcherThread");
 766     _crash_protection = crash_protection;
 767   }
 768 
 769   bool has_crash_protection() const { return _crash_protection != NULL; }
 770   os::WatcherThreadCrashProtection* crash_protection() const { return _crash_protection; }
 771 
 772  private:
 773   int sleep() const;
 774 };
 775 
 776 
 777 class CompilerThread;
 778 
 779 typedef void (*ThreadFunction)(JavaThread*, TRAPS);
 780 
 781 class JavaThread: public Thread {
 782   friend class VMStructs;
 783   friend class JVMCIVMStructs;
 784   friend class WhiteBox;
 785  private:
 786   JavaThread*    _next;                          // The next thread in the Threads list
 787   oop            _threadObj;                     // The Java level thread object
 788 
 789 #ifdef ASSERT
 790  private:
 791   int _java_call_counter;
 792 
 793  public:
 794   int  java_call_counter()                       { return _java_call_counter; }
 795   void inc_java_call_counter()                   { _java_call_counter++; }
 796   void dec_java_call_counter() {
 797     assert(_java_call_counter &gt; 0, "Invalid nesting of JavaCallWrapper");
 798     _java_call_counter--;
 799   }
 800  private:  // restore original namespace restriction
 801 #endif  // ifdef ASSERT
 802 
 803 #ifndef PRODUCT
 804  public:
 805   enum {
 806     jump_ring_buffer_size = 16
 807   };
 808  private:  // restore original namespace restriction
 809 #endif
 810 
 811   JavaFrameAnchor _anchor;                       // Encapsulation of current java frame and it state
 812 
 813   ThreadFunction _entry_point;
 814 
 815   JNIEnv        _jni_environment;
 816 
 817   // Deopt support
 818   DeoptResourceMark*  _deopt_mark;               // Holds special ResourceMark for deoptimization
 819 
 820   intptr_t*      _must_deopt_id;                 // id of frame that needs to be deopted once we
 821                                                  // transition out of native
 822   CompiledMethod*       _deopt_nmethod;         // CompiledMethod that is currently being deoptimized
 823   vframeArray*  _vframe_array_head;              // Holds the heap of the active vframeArrays
 824   vframeArray*  _vframe_array_last;              // Holds last vFrameArray we popped
 825   // Because deoptimization is lazy we must save jvmti requests to set locals
 826   // in compiled frames until we deoptimize and we have an interpreter frame.
 827   // This holds the pointer to array (yeah like there might be more than one) of
 828   // description of compiled vframes that have locals that need to be updated.
 829   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* _deferred_locals_updates;
 830 
 831   // Handshake value for fixing 6243940. We need a place for the i2c
 832   // adapter to store the callee Method*. This value is NEVER live
 833   // across a gc point so it does NOT have to be gc'd
 834   // The handshake is open ended since we can't be certain that it will
 835   // be NULLed. This is because we rarely ever see the race and end up
 836   // in handle_wrong_method which is the backend of the handshake. See
 837   // code in i2c adapters and handle_wrong_method.
 838 
 839   Method*       _callee_target;
 840 
 841   // Used to pass back results to the interpreter or generated code running Java code.
 842   oop           _vm_result;    // oop result is GC-preserved
 843   Metadata*     _vm_result_2;  // non-oop result
 844 
 845   // See ReduceInitialCardMarks: this holds the precise space interval of
 846   // the most recent slow path allocation for which compiled code has
 847   // elided card-marks for performance along the fast-path.
 848   MemRegion     _deferred_card_mark;
 849 
 850   MonitorChunk* _monitor_chunks;                 // Contains the off stack monitors
 851                                                  // allocated during deoptimization
 852                                                  // and by JNI_MonitorEnter/Exit
 853 
 854   // Async. requests support
 855   enum AsyncRequests {
 856     _no_async_condition = 0,
 857     _async_exception,
 858     _async_unsafe_access_error
 859   };
 860   AsyncRequests _special_runtime_exit_condition; // Enum indicating pending async. request
 861   oop           _pending_async_exception;
 862 
 863   // Safepoint support
 864  public:                                         // Expose _thread_state for SafeFetchInt()
 865   volatile JavaThreadState _thread_state;
 866  private:
 867   ThreadSafepointState *_safepoint_state;        // Holds information about a thread during a safepoint
 868   address               _saved_exception_pc;     // Saved pc of instruction where last implicit exception happened
 869 
 870   // JavaThread termination support
 871   enum TerminatedTypes {
 872     _not_terminated = 0xDEAD - 2,
 873     _thread_exiting,                             // JavaThread::exit() has been called for this thread
 874     _thread_terminated,                          // JavaThread is removed from thread list
 875     _vm_exited                                   // JavaThread is still executing native code, but VM is terminated
 876                                                  // only VM_Exit can set _vm_exited
 877   };
 878 
 879   // In general a JavaThread's _terminated field transitions as follows:
 880   //
 881   //   _not_terminated =&gt; _thread_exiting =&gt; _thread_terminated
 882   //
 883   // _vm_exited is a special value to cover the case of a JavaThread
 884   // executing native code after the VM itself is terminated.
 885   volatile TerminatedTypes _terminated;
 886   // suspend/resume support
 887   volatile bool         _suspend_equivalent;     // Suspend equivalent condition
 888   jint                  _in_deopt_handler;       // count of deoptimization
 889                                                  // handlers thread is in
 890   volatile bool         _doing_unsafe_access;    // Thread may fault due to unsafe access
 891   bool                  _do_not_unlock_if_synchronized;  // Do not unlock the receiver of a synchronized method (since it was
 892                                                          // never locked) when throwing an exception. Used by interpreter only.
 893 
 894   // JNI attach states:
 895   enum JNIAttachStates {
 896     _not_attaching_via_jni = 1,  // thread is not attaching via JNI
 897     _attaching_via_jni,          // thread is attaching via JNI
 898     _attached_via_jni            // thread has attached via JNI
 899   };
 900 
 901   // A regular JavaThread's _jni_attach_state is _not_attaching_via_jni.
 902   // A native thread that is attaching via JNI starts with a value
 903   // of _attaching_via_jni and transitions to _attached_via_jni.
 904   volatile JNIAttachStates _jni_attach_state;
 905 
 906  public:
 907   // State of the stack guard pages for this thread.
 908   enum StackGuardState {
 909     stack_guard_unused,         // not needed
 910     stack_guard_reserved_disabled,
 911     stack_guard_yellow_reserved_disabled,// disabled (temporarily) after stack overflow
 912     stack_guard_enabled         // enabled
 913   };
 914 
 915  private:
 916 
 917 #if INCLUDE_JVMCI
 918   // The _pending_* fields below are used to communicate extra information
 919   // from an uncommon trap in JVMCI compiled code to the uncommon trap handler.
 920 
 921   // Communicates the DeoptReason and DeoptAction of the uncommon trap
 922   int       _pending_deoptimization;
 923 
 924   // Specifies whether the uncommon trap is to bci 0 of a synchronized method
 925   // before the monitor has been acquired.
 926   bool      _pending_monitorenter;
 927 
 928   // Specifies if the DeoptReason for the last uncommon trap was Reason_transfer_to_interpreter
 929   bool      _pending_transfer_to_interpreter;
 930 
 931   // An object that JVMCI compiled code can use to further describe and
 932   // uniquely identify the  speculative optimization guarded by the uncommon trap
 933   oop       _pending_failed_speculation;
 934 
 935   // These fields are mutually exclusive in terms of live ranges.
 936   union {
 937     // Communicates the pc at which the most recent implicit exception occurred
 938     // from the signal handler to a deoptimization stub.
 939     address   _implicit_exception_pc;
 940 
 941     // Communicates an alternative call target to an i2c stub from a JavaCall .
 942     address   _alternate_call_target;
 943   } _jvmci;
 944 
 945   // Support for high precision, thread sensitive counters in JVMCI compiled code.
 946   jlong*    _jvmci_counters;
 947 
 948  public:
 949   static jlong* _jvmci_old_thread_counters;
 950   static void collect_counters(typeArrayOop array);
 951  private:
 952 #endif // INCLUDE_JVMCI
 953 
 954   StackGuardState  _stack_guard_state;
 955 
 956   // Precompute the limit of the stack as used in stack overflow checks.
 957   // We load it from here to simplify the stack overflow check in assembly.
 958   address          _stack_overflow_limit;
 959   address          _reserved_stack_activation;
 960 
 961   // Compiler exception handling (NOTE: The _exception_oop is *NOT* the same as _pending_exception. It is
 962   // used to temp. parsing values into and out of the runtime system during exception handling for compiled
 963   // code)
 964   volatile oop     _exception_oop;               // Exception thrown in compiled code
 965   volatile address _exception_pc;                // PC where exception happened
 966   volatile address _exception_handler_pc;        // PC for handler of exception
 967   volatile int     _is_method_handle_return;     // true (== 1) if the current exception PC is a MethodHandle call site.
 968 
 969  private:
 970   // support for JNI critical regions
 971   jint    _jni_active_critical;                  // count of entries into JNI critical region
 972 
 973   // Checked JNI: function name requires exception check
 974   char* _pending_jni_exception_check_fn;
 975 
 976   // For deadlock detection.
 977   int _depth_first_number;
 978 
 979   // JVMTI PopFrame support
 980   // This is set to popframe_pending to signal that top Java frame should be popped immediately
 981   int _popframe_condition;
 982 
 983   // If reallocation of scalar replaced objects fails, we throw OOM
 984   // and during exception propagation, pop the top
 985   // _frames_to_pop_failed_realloc frames, the ones that reference
 986   // failed reallocations.
 987   int _frames_to_pop_failed_realloc;
 988 
 989 #ifndef PRODUCT
 990   int _jmp_ring_index;
 991   struct {
 992     // We use intptr_t instead of address so debugger doesn't try and display strings
 993     intptr_t _target;
 994     intptr_t _instruction;
 995     const char*  _file;
 996     int _line;
 997   }   _jmp_ring[jump_ring_buffer_size];
 998 #endif // PRODUCT
 999 
1000 #if INCLUDE_ALL_GCS
1001   // Support for G1 barriers
1002 
1003   SATBMarkQueue _satb_mark_queue;        // Thread-local log for SATB barrier.
1004   // Set of all such queues.
1005   static SATBMarkQueueSet _satb_mark_queue_set;
1006 
1007   DirtyCardQueue _dirty_card_queue;      // Thread-local log for dirty cards.
1008   // Set of all such queues.
1009   static DirtyCardQueueSet _dirty_card_queue_set;
1010 
1011   void flush_barrier_queues();
1012 #endif // INCLUDE_ALL_GCS
1013 
1014   friend class VMThread;
1015   friend class ThreadWaitTransition;
1016   friend class VM_Exit;
1017 
1018   void initialize();                             // Initialized the instance variables
1019 
1020  public:
1021   // Constructor
1022   JavaThread(bool is_attaching_via_jni = false); // for main thread and JNI attached threads
1023   JavaThread(ThreadFunction entry_point, size_t stack_size = 0);
1024   ~JavaThread();
1025 
1026 #ifdef ASSERT
1027   // verify this JavaThread hasn't be published in the Threads::list yet
1028   void verify_not_published();
1029 #endif
1030 
1031   //JNI functiontable getter/setter for JVMTI jni function table interception API.
1032   void set_jni_functions(struct JNINativeInterface_* functionTable) {
1033     _jni_environment.functions = functionTable;
1034   }
1035   struct JNINativeInterface_* get_jni_functions() {
1036     return (struct JNINativeInterface_ *)_jni_environment.functions;
1037   }
1038 
1039   // This function is called at thread creation to allow
1040   // platform specific thread variables to be initialized.
1041   void cache_global_variables();
1042 
1043   // Executes Shutdown.shutdown()
1044   void invoke_shutdown_hooks();
1045 
1046   // Cleanup on thread exit
1047   enum ExitType {
1048     normal_exit,
1049     jni_detach
1050   };
1051   void exit(bool destroy_vm, ExitType exit_type = normal_exit);
1052 
1053   void cleanup_failed_attach_current_thread();
1054 
1055   // Testers
1056   virtual bool is_Java_thread() const            { return true;  }
1057   virtual bool can_call_java() const             { return true; }
1058 
1059   // Thread chain operations
1060   JavaThread* next() const                       { return _next; }
1061   void set_next(JavaThread* p)                   { _next = p; }
1062 
1063   // Thread oop. threadObj() can be NULL for initial JavaThread
1064   // (or for threads attached via JNI)
1065   oop threadObj() const                          { return _threadObj; }
1066   void set_threadObj(oop p)                      { _threadObj = p; }
1067 
1068   ThreadPriority java_priority() const;          // Read from threadObj()
1069 
1070   // Prepare thread and add to priority queue.  If a priority is
1071   // not specified, use the priority of the thread object. Threads_lock
1072   // must be held while this function is called.
1073   void prepare(jobject jni_thread, ThreadPriority prio=NoPriority);
1074   void prepare_ext();
1075 
1076   void set_saved_exception_pc(address pc)        { _saved_exception_pc = pc; }
1077   address saved_exception_pc()                   { return _saved_exception_pc; }
1078 
1079 
1080   ThreadFunction entry_point() const             { return _entry_point; }
1081 
1082   // Allocates a new Java level thread object for this thread. thread_name may be NULL.
1083   void allocate_threadObj(Handle thread_group, const char* thread_name, bool daemon, TRAPS);
1084 
1085   // Last frame anchor routines
1086 
1087   JavaFrameAnchor* frame_anchor(void)            { return &amp;_anchor; }
1088 
1089   // last_Java_sp
1090   bool has_last_Java_frame() const               { return _anchor.has_last_Java_frame(); }
1091   intptr_t* last_Java_sp() const                 { return _anchor.last_Java_sp(); }
1092 
1093   // last_Java_pc
1094 
1095   address last_Java_pc(void)                     { return _anchor.last_Java_pc(); }
1096 
1097   // Safepoint support
1098 #if !(defined(PPC64) || defined(AARCH64))
1099   JavaThreadState thread_state() const           { return _thread_state; }
1100   void set_thread_state(JavaThreadState s)       { _thread_state = s;    }
1101 #else
1102   // Use membars when accessing volatile _thread_state. See
1103   // Threads::create_vm() for size checks.
1104   inline JavaThreadState thread_state() const;
1105   inline void set_thread_state(JavaThreadState s);
1106 #endif
1107   ThreadSafepointState *safepoint_state() const  { return _safepoint_state; }
1108   void set_safepoint_state(ThreadSafepointState *state) { _safepoint_state = state; }
1109   bool is_at_poll_safepoint()                    { return _safepoint_state-&gt;is_at_poll_safepoint(); }
1110 
1111   // thread has called JavaThread::exit() or is terminated
1112   bool is_exiting()                              { return _terminated == _thread_exiting || is_terminated(); }
1113   // thread is terminated (no longer on the threads list); we compare
1114   // against the two non-terminated values so that a freed JavaThread
1115   // will also be considered terminated.
1116   bool is_terminated()                           { return _terminated != _not_terminated &amp;&amp; _terminated != _thread_exiting; }
1117   void set_terminated(TerminatedTypes t)         { _terminated = t; }
1118   // special for Threads::remove() which is static:
1119   void set_terminated_value()                    { _terminated = _thread_terminated; }
1120   void block_if_vm_exited();
1121 
1122   bool doing_unsafe_access()                     { return _doing_unsafe_access; }
1123   void set_doing_unsafe_access(bool val)         { _doing_unsafe_access = val; }
1124 
1125   bool do_not_unlock_if_synchronized()             { return _do_not_unlock_if_synchronized; }
1126   void set_do_not_unlock_if_synchronized(bool val) { _do_not_unlock_if_synchronized = val; }
1127 
1128   // Suspend/resume support for JavaThread
1129  private:
1130   inline void set_ext_suspended();
1131   inline void clear_ext_suspended();
1132 
1133  public:
1134   void java_suspend();
1135   void java_resume();
1136   int  java_suspend_self();
1137 
1138   void check_and_wait_while_suspended() {
1139     assert(JavaThread::current() == this, "sanity check");
1140 
1141     bool do_self_suspend;
1142     do {
1143       // were we externally suspended while we were waiting?
1144       do_self_suspend = handle_special_suspend_equivalent_condition();
1145       if (do_self_suspend) {
1146         // don't surprise the thread that suspended us by returning
1147         java_suspend_self();
1148         set_suspend_equivalent();
1149       }
1150     } while (do_self_suspend);
1151   }
1152   static void check_safepoint_and_suspend_for_native_trans(JavaThread *thread);
1153   // Check for async exception in addition to safepoint and suspend request.
1154   static void check_special_condition_for_native_trans(JavaThread *thread);
1155 
1156   // Same as check_special_condition_for_native_trans but finishes the
1157   // transition into thread_in_Java mode so that it can potentially
1158   // block.
1159   static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);
1160 
1161   bool is_ext_suspend_completed(bool called_by_wait, int delay, uint32_t *bits);
1162   bool is_ext_suspend_completed_with_lock(uint32_t *bits) {
1163     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1164     // Warning: is_ext_suspend_completed() may temporarily drop the
1165     // SR_lock to allow the thread to reach a stable thread state if
1166     // it is currently in a transient thread state.
1167     return is_ext_suspend_completed(false /* !called_by_wait */,
1168                                     SuspendRetryDelay, bits);
1169   }
1170 
1171   // We cannot allow wait_for_ext_suspend_completion() to run forever or
1172   // we could hang. SuspendRetryCount and SuspendRetryDelay are normally
1173   // passed as the count and delay parameters. Experiments with specific
1174   // calls to wait_for_ext_suspend_completion() can be done by passing
1175   // other values in the code. Experiments with all calls can be done
1176   // via the appropriate -XX options.
1177   bool wait_for_ext_suspend_completion(int count, int delay, uint32_t *bits);
1178 
1179   inline void set_external_suspend();
1180   inline void clear_external_suspend();
1181 
1182   inline void set_deopt_suspend();
1183   inline void clear_deopt_suspend();
1184   bool is_deopt_suspend()         { return (_suspend_flags &amp; _deopt_suspend) != 0; }
1185 
1186   bool is_external_suspend() const {
1187     return (_suspend_flags &amp; _external_suspend) != 0;
1188   }
1189   // Whenever a thread transitions from native to vm/java it must suspend
1190   // if external|deopt suspend is present.
1191   bool is_suspend_after_native() const {
1192     return (_suspend_flags &amp; (_external_suspend | _deopt_suspend)) != 0;
1193   }
1194 
1195   // external suspend request is completed
1196   bool is_ext_suspended() const {
1197     return (_suspend_flags &amp; _ext_suspended) != 0;
1198   }
1199 
1200   bool is_external_suspend_with_lock() const {
1201     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1202     return is_external_suspend();
1203   }
1204 
1205   // Special method to handle a pending external suspend request
1206   // when a suspend equivalent condition lifts.
1207   bool handle_special_suspend_equivalent_condition() {
1208     assert(is_suspend_equivalent(),
1209            "should only be called in a suspend equivalence condition");
1210     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1211     bool ret = is_external_suspend();
1212     if (!ret) {
1213       // not about to self-suspend so clear suspend equivalence
1214       clear_suspend_equivalent();
1215     }
1216     // implied else:
1217     // We have a pending external suspend request so we leave the
1218     // suspend_equivalent flag set until java_suspend_self() sets
1219     // the ext_suspended flag and clears the suspend_equivalent
1220     // flag. This insures that wait_for_ext_suspend_completion()
1221     // will return consistent values.
1222     return ret;
1223   }
1224 
1225   // utility methods to see if we are doing some kind of suspension
1226   bool is_being_ext_suspended() const            {
1227     MutexLockerEx ml(SR_lock(), Mutex::_no_safepoint_check_flag);
1228     return is_ext_suspended() || is_external_suspend();
1229   }
1230 
1231   bool is_suspend_equivalent() const             { return _suspend_equivalent; }
1232 
1233   void set_suspend_equivalent()                  { _suspend_equivalent = true; }
1234   void clear_suspend_equivalent()                { _suspend_equivalent = false; }
1235 
1236   // Thread.stop support
1237   void send_thread_stop(oop throwable);
1238   AsyncRequests clear_special_runtime_exit_condition() {
1239     AsyncRequests x = _special_runtime_exit_condition;
1240     _special_runtime_exit_condition = _no_async_condition;
1241     return x;
1242   }
1243 
1244   // Are any async conditions present?
1245   bool has_async_condition() { return (_special_runtime_exit_condition != _no_async_condition); }
1246 
1247   void check_and_handle_async_exceptions(bool check_unsafe_error = true);
1248 
1249   // these next two are also used for self-suspension and async exception support
1250   void handle_special_runtime_exit_condition(bool check_asyncs = true);
1251 
1252   // Return true if JavaThread has an asynchronous condition or
1253   // if external suspension is requested.
1254   bool has_special_runtime_exit_condition() {
1255     // We call is_external_suspend() last since external suspend should
1256     // be less common. Because we don't use is_external_suspend_with_lock
1257     // it is possible that we won't see an asynchronous external suspend
1258     // request that has just gotten started, i.e., SR_lock grabbed but
1259     // _external_suspend field change either not made yet or not visible
1260     // yet. However, this is okay because the request is asynchronous and
1261     // we will see the new flag value the next time through. It's also
1262     // possible that the external suspend request is dropped after
1263     // we have checked is_external_suspend(), we will recheck its value
1264     // under SR_lock in java_suspend_self().
1265     return (_special_runtime_exit_condition != _no_async_condition) ||
1266             is_external_suspend() || is_deopt_suspend();
1267   }
1268 
1269   void set_pending_unsafe_access_error()          { _special_runtime_exit_condition = _async_unsafe_access_error; }
1270 
1271   inline void set_pending_async_exception(oop e);
1272 
1273   // Fast-locking support
1274   bool is_lock_owned(address adr) const;
1275 
1276   // Accessors for vframe array top
1277   // The linked list of vframe arrays are sorted on sp. This means when we
1278   // unpack the head must contain the vframe array to unpack.
1279   void set_vframe_array_head(vframeArray* value) { _vframe_array_head = value; }
1280   vframeArray* vframe_array_head() const         { return _vframe_array_head;  }
1281 
1282   // Side structure for deferring update of java frame locals until deopt occurs
1283   GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* deferred_locals() const { return _deferred_locals_updates; }
1284   void set_deferred_locals(GrowableArray&lt;jvmtiDeferredLocalVariableSet *&gt;* vf) { _deferred_locals_updates = vf; }
1285 
1286   // These only really exist to make debugging deopt problems simpler
1287 
1288   void set_vframe_array_last(vframeArray* value) { _vframe_array_last = value; }
1289   vframeArray* vframe_array_last() const         { return _vframe_array_last;  }
1290 
1291   // The special resourceMark used during deoptimization
1292 
1293   void set_deopt_mark(DeoptResourceMark* value)  { _deopt_mark = value; }
1294   DeoptResourceMark* deopt_mark(void)            { return _deopt_mark; }
1295 
1296   intptr_t* must_deopt_id()                      { return _must_deopt_id; }
1297   void     set_must_deopt_id(intptr_t* id)       { _must_deopt_id = id; }
1298   void     clear_must_deopt_id()                 { _must_deopt_id = NULL; }
1299 
1300   void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }
1301   CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }
1302 
1303   Method*    callee_target() const               { return _callee_target; }
1304   void set_callee_target  (Method* x)          { _callee_target   = x; }
1305 
1306   // Oop results of vm runtime calls
1307   oop  vm_result() const                         { return _vm_result; }
1308   void set_vm_result  (oop x)                    { _vm_result   = x; }
1309 
1310   Metadata*    vm_result_2() const               { return _vm_result_2; }
1311   void set_vm_result_2  (Metadata* x)          { _vm_result_2   = x; }
1312 
1313   MemRegion deferred_card_mark() const           { return _deferred_card_mark; }
1314   void set_deferred_card_mark(MemRegion mr)      { _deferred_card_mark = mr;   }
1315 
1316 #if INCLUDE_JVMCI
1317   int  pending_deoptimization() const             { return _pending_deoptimization; }
1318   oop  pending_failed_speculation() const         { return _pending_failed_speculation; }
1319   bool has_pending_monitorenter() const           { return _pending_monitorenter; }
1320   void set_pending_monitorenter(bool b)           { _pending_monitorenter = b; }
1321   void set_pending_deoptimization(int reason)     { _pending_deoptimization = reason; }
1322   void set_pending_failed_speculation(oop failed_speculation) { _pending_failed_speculation = failed_speculation; }
1323   void set_pending_transfer_to_interpreter(bool b) { _pending_transfer_to_interpreter = b; }
1324   void set_jvmci_alternate_call_target(address a) { assert(_jvmci._alternate_call_target == NULL, "must be"); _jvmci._alternate_call_target = a; }
1325   void set_jvmci_implicit_exception_pc(address a) { assert(_jvmci._implicit_exception_pc == NULL, "must be"); _jvmci._implicit_exception_pc = a; }
1326 #endif // INCLUDE_JVMCI
1327 
1328   // Exception handling for compiled methods
1329   oop      exception_oop() const                 { return _exception_oop; }
1330   address  exception_pc() const                  { return _exception_pc; }
1331   address  exception_handler_pc() const          { return _exception_handler_pc; }
1332   bool     is_method_handle_return() const       { return _is_method_handle_return == 1; }
1333 
1334   void set_exception_oop(oop o)                  { (void)const_cast&lt;oop&amp;&gt;(_exception_oop = o); }
1335   void set_exception_pc(address a)               { _exception_pc = a; }
1336   void set_exception_handler_pc(address a)       { _exception_handler_pc = a; }
1337   void set_is_method_handle_return(bool value)   { _is_method_handle_return = value ? 1 : 0; }
1338 
1339   void clear_exception_oop_and_pc() {
1340     set_exception_oop(NULL);
1341     set_exception_pc(NULL);
1342   }
1343 
1344   // Stack overflow support
1345   //
1346   //  (small addresses)
1347   //
1348   //  --  &lt;-- stack_end()                   ---
1349   //  |                                      |
1350   //  |  red pages                           |
1351   //  |                                      |
1352   //  --  &lt;-- stack_red_zone_base()          |
1353   //  |                                      |
1354   //  |                                     guard
1355   //  |  yellow pages                       zone
1356   //  |                                      |
1357   //  |                                      |
1358   //  --  &lt;-- stack_yellow_zone_base()       |
1359   //  |                                      |
1360   //  |                                      |
1361   //  |  reserved pages                      |
1362   //  |                                      |
1363   //  --  &lt;-- stack_reserved_zone_base()    ---      ---
1364   //                                                 /|\  shadow
1365   //                                                  |   zone
1366   //                                                 \|/  size
1367   //  some untouched memory                          ---         &lt;--  stack_overflow_limit()
1368   //
1369   //
1370   //  --
1371   //  |
1372   //  |  shadow zone
1373   //  |
1374   //  --
1375   //  x    frame n
1376   //  --
1377   //  x    frame n-1
1378   //  x
1379   //  --
1380   //  ...
1381   //
1382   //  --
1383   //  x    frame 0
1384   //  --  &lt;-- stack_base()
1385   //
1386   //  (large addresses)
1387   //
1388 
1389  private:
1390   // These values are derived from flags StackRedPages, StackYellowPages,
1391   // StackReservedPages and StackShadowPages. The zone size is determined
1392   // ergonomically if page_size &gt; 4K.
1393   static size_t _stack_red_zone_size;
1394   static size_t _stack_yellow_zone_size;
1395   static size_t _stack_reserved_zone_size;
1396   static size_t _stack_shadow_zone_size;
1397  public:
1398   inline size_t stack_available(address cur_sp);
1399 
1400   static size_t stack_red_zone_size() {
1401     assert(_stack_red_zone_size &gt; 0, "Don't call this before the field is initialized.");
1402     return _stack_red_zone_size;
1403   }
1404   static void set_stack_red_zone_size(size_t s) {
1405     assert(is_size_aligned(s, os::vm_page_size()),
1406            "We can not protect if the red zone size is not page aligned.");
1407     assert(_stack_red_zone_size == 0, "This should be called only once.");
1408     _stack_red_zone_size = s;
1409   }
1410   address stack_red_zone_base() {
1411     return (address)(stack_end() + stack_red_zone_size());
1412   }
1413   bool in_stack_red_zone(address a) {
1414     return a &lt;= stack_red_zone_base() &amp;&amp; a &gt;= stack_end();
1415   }
1416 
1417   static size_t stack_yellow_zone_size() {
1418     assert(_stack_yellow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1419     return _stack_yellow_zone_size;
1420   }
1421   static void set_stack_yellow_zone_size(size_t s) {
1422     assert(is_size_aligned(s, os::vm_page_size()),
1423            "We can not protect if the yellow zone size is not page aligned.");
1424     assert(_stack_yellow_zone_size == 0, "This should be called only once.");
1425     _stack_yellow_zone_size = s;
1426   }
1427 
1428   static size_t stack_reserved_zone_size() {
1429     // _stack_reserved_zone_size may be 0. This indicates the feature is off.
1430     return _stack_reserved_zone_size;
1431   }
1432   static void set_stack_reserved_zone_size(size_t s) {
1433     assert(is_size_aligned(s, os::vm_page_size()),
1434            "We can not protect if the reserved zone size is not page aligned.");
1435     assert(_stack_reserved_zone_size == 0, "This should be called only once.");
1436     _stack_reserved_zone_size = s;
1437   }
1438   address stack_reserved_zone_base() {
1439     return (address)(stack_end() +
1440                      (stack_red_zone_size() + stack_yellow_zone_size() + stack_reserved_zone_size()));
1441   }
1442   bool in_stack_reserved_zone(address a) {
1443     return (a &lt;= stack_reserved_zone_base()) &amp;&amp;
1444            (a &gt;= (address)((intptr_t)stack_reserved_zone_base() - stack_reserved_zone_size()));
1445   }
1446 
1447   static size_t stack_yellow_reserved_zone_size() {
1448     return _stack_yellow_zone_size + _stack_reserved_zone_size;
1449   }
1450   bool in_stack_yellow_reserved_zone(address a) {
1451     return (a &lt;= stack_reserved_zone_base()) &amp;&amp; (a &gt;= stack_red_zone_base());
1452   }
1453 
1454   // Size of red + yellow + reserved zones.
1455   static size_t stack_guard_zone_size() {
1456     return stack_red_zone_size() + stack_yellow_reserved_zone_size();
1457   }
1458 
1459   static size_t stack_shadow_zone_size() {
1460     assert(_stack_shadow_zone_size &gt; 0, "Don't call this before the field is initialized.");
1461     return _stack_shadow_zone_size;
1462   }
1463   static void set_stack_shadow_zone_size(size_t s) {
1464     // The shadow area is not allocated or protected, so
1465     // it needs not be page aligned.
1466     // But the stack bang currently assumes that it is a
1467     // multiple of page size. This guarantees that the bang
1468     // loop touches all pages in the shadow zone.
1469     // This can be guaranteed differently, as well.  E.g., if
1470     // the page size is a multiple of 4K, banging in 4K steps
1471     // suffices to touch all pages. (Some pages are banged
1472     // several times, though.)
1473     assert(is_size_aligned(s, os::vm_page_size()),
1474            "Stack bang assumes multiple of page size.");
1475     assert(_stack_shadow_zone_size == 0, "This should be called only once.");
1476     _stack_shadow_zone_size = s;
1477   }
1478 
1479   void create_stack_guard_pages();
1480   void remove_stack_guard_pages();
1481 
1482   void enable_stack_reserved_zone();
1483   void disable_stack_reserved_zone();
1484   void enable_stack_yellow_reserved_zone();
1485   void disable_stack_yellow_reserved_zone();
1486   void enable_stack_red_zone();
1487   void disable_stack_red_zone();
1488 
1489   inline bool stack_guard_zone_unused();
1490   inline bool stack_yellow_reserved_zone_disabled();
1491   inline bool stack_reserved_zone_disabled();
1492   inline bool stack_guards_enabled();
1493 
1494   address reserved_stack_activation() const { return _reserved_stack_activation; }
1495   void set_reserved_stack_activation(address addr) {
1496     assert(_reserved_stack_activation == stack_base()
1497             || _reserved_stack_activation == NULL
1498             || addr == stack_base(), "Must not be set twice");
1499     _reserved_stack_activation = addr;
1500   }
1501 
1502   // Attempt to reguard the stack after a stack overflow may have occurred.
1503   // Returns true if (a) guard pages are not needed on this thread, (b) the
1504   // pages are already guarded, or (c) the pages were successfully reguarded.
1505   // Returns false if there is not enough stack space to reguard the pages, in
1506   // which case the caller should unwind a frame and try again.  The argument
1507   // should be the caller's (approximate) sp.
1508   bool reguard_stack(address cur_sp);
1509   // Similar to above but see if current stackpoint is out of the guard area
1510   // and reguard if possible.
1511   bool reguard_stack(void);
1512 
1513   address stack_overflow_limit() { return _stack_overflow_limit; }
1514   void set_stack_overflow_limit() {
1515     _stack_overflow_limit = stack_end() +
1516                             (JavaThread::stack_guard_zone_size() +
1517                              JavaThread::stack_shadow_zone_size());
1518   }
1519 
1520   // Misc. accessors/mutators
1521   void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }
1522   void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }
1523   bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }
1524 
1525 #ifndef PRODUCT
1526   void record_jump(address target, address instr, const char* file, int line);
1527 #endif // PRODUCT
1528 
1529   // For assembly stub generation
1530   static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }
1531 #ifndef PRODUCT
1532   static ByteSize jmp_ring_index_offset()        { return byte_offset_of(JavaThread, _jmp_ring_index); }
1533   static ByteSize jmp_ring_offset()              { return byte_offset_of(JavaThread, _jmp_ring); }
1534 #endif // PRODUCT
1535   static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }
1536   static ByteSize last_Java_sp_offset() {
1537     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();
1538   }
1539   static ByteSize last_Java_pc_offset() {
1540     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();
1541   }
1542   static ByteSize frame_anchor_offset() {
1543     return byte_offset_of(JavaThread, _anchor);
1544   }
1545   static ByteSize callee_target_offset()         { return byte_offset_of(JavaThread, _callee_target); }
1546   static ByteSize vm_result_offset()             { return byte_offset_of(JavaThread, _vm_result); }
1547   static ByteSize vm_result_2_offset()           { return byte_offset_of(JavaThread, _vm_result_2); }
1548   static ByteSize thread_state_offset()          { return byte_offset_of(JavaThread, _thread_state); }
1549   static ByteSize saved_exception_pc_offset()    { return byte_offset_of(JavaThread, _saved_exception_pc); }
1550   static ByteSize osthread_offset()              { return byte_offset_of(JavaThread, _osthread); }
1551 #if INCLUDE_JVMCI
1552   static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }
1553   static ByteSize pending_monitorenter_offset()  { return byte_offset_of(JavaThread, _pending_monitorenter); }
1554   static ByteSize pending_failed_speculation_offset() { return byte_offset_of(JavaThread, _pending_failed_speculation); }
1555   static ByteSize jvmci_alternate_call_target_offset() { return byte_offset_of(JavaThread, _jvmci._alternate_call_target); }
1556   static ByteSize jvmci_implicit_exception_pc_offset() { return byte_offset_of(JavaThread, _jvmci._implicit_exception_pc); }
1557   static ByteSize jvmci_counters_offset()        { return byte_offset_of(JavaThread, _jvmci_counters); }
1558 #endif // INCLUDE_JVMCI
1559   static ByteSize exception_oop_offset()         { return byte_offset_of(JavaThread, _exception_oop); }
1560   static ByteSize exception_pc_offset()          { return byte_offset_of(JavaThread, _exception_pc); }
1561   static ByteSize exception_handler_pc_offset()  { return byte_offset_of(JavaThread, _exception_handler_pc); }
1562   static ByteSize stack_overflow_limit_offset()  { return byte_offset_of(JavaThread, _stack_overflow_limit); }
1563   static ByteSize is_method_handle_return_offset() { return byte_offset_of(JavaThread, _is_method_handle_return); }
1564   static ByteSize stack_guard_state_offset()     { return byte_offset_of(JavaThread, _stack_guard_state); }
1565   static ByteSize reserved_stack_activation_offset() { return byte_offset_of(JavaThread, _reserved_stack_activation); }
1566   static ByteSize suspend_flags_offset()         { return byte_offset_of(JavaThread, _suspend_flags); }
1567 
1568   static ByteSize do_not_unlock_if_synchronized_offset() { return byte_offset_of(JavaThread, _do_not_unlock_if_synchronized); }
1569   static ByteSize should_post_on_exceptions_flag_offset() {
1570     return byte_offset_of(JavaThread, _should_post_on_exceptions_flag);
1571   }
1572 
1573 #if INCLUDE_ALL_GCS
1574   static ByteSize satb_mark_queue_offset()       { return byte_offset_of(JavaThread, _satb_mark_queue); }
1575   static ByteSize dirty_card_queue_offset()      { return byte_offset_of(JavaThread, _dirty_card_queue); }
1576 #endif // INCLUDE_ALL_GCS
1577 
1578   // Returns the jni environment for this thread
1579   JNIEnv* jni_environment()                      { return &amp;_jni_environment; }
1580 
1581   static JavaThread* thread_from_jni_environment(JNIEnv* env) {
1582     JavaThread *thread_from_jni_env = (JavaThread*)((intptr_t)env - in_bytes(jni_environment_offset()));
1583     // Only return NULL if thread is off the thread list; starting to
1584     // exit should not return NULL.
1585     if (thread_from_jni_env-&gt;is_terminated()) {
1586       thread_from_jni_env-&gt;block_if_vm_exited();
1587       return NULL;
1588     } else {
1589       return thread_from_jni_env;
1590     }
1591   }
1592 
1593   // JNI critical regions. These can nest.
1594   bool in_critical()    { return _jni_active_critical &gt; 0; }
1595   bool in_last_critical()  { return _jni_active_critical == 1; }
1596   void enter_critical() {
1597     assert(Thread::current() == this ||
1598            (Thread::current()-&gt;is_VM_thread() &amp;&amp;
1599            SafepointSynchronize::is_synchronizing()),
1600            "this must be current thread or synchronizing");
1601     _jni_active_critical++;
1602   }
1603   void exit_critical() {
1604     assert(Thread::current() == this, "this must be current thread");
1605     _jni_active_critical--;
1606     assert(_jni_active_critical &gt;= 0, "JNI critical nesting problem?");
1607   }
1608 
1609   // Checked JNI, is the programmer required to check for exceptions, specify which function name
1610   bool is_pending_jni_exception_check() const { return _pending_jni_exception_check_fn != NULL; }
1611   void clear_pending_jni_exception_check() { _pending_jni_exception_check_fn = NULL; }
1612   const char* get_pending_jni_exception_check() const { return _pending_jni_exception_check_fn; }
1613   void set_pending_jni_exception_check(const char* fn_name) { _pending_jni_exception_check_fn = (char*) fn_name; }
1614 
1615   // For deadlock detection
1616   int depth_first_number() { return _depth_first_number; }
1617   void set_depth_first_number(int dfn) { _depth_first_number = dfn; }
1618 
1619  private:
1620   void set_monitor_chunks(MonitorChunk* monitor_chunks) { _monitor_chunks = monitor_chunks; }
1621 
1622  public:
1623   MonitorChunk* monitor_chunks() const           { return _monitor_chunks; }
1624   void add_monitor_chunk(MonitorChunk* chunk);
1625   void remove_monitor_chunk(MonitorChunk* chunk);
1626   bool in_deopt_handler() const                  { return _in_deopt_handler &gt; 0; }
1627   void inc_in_deopt_handler()                    { _in_deopt_handler++; }
1628   void dec_in_deopt_handler() {
1629     assert(_in_deopt_handler &gt; 0, "mismatched deopt nesting");
1630     if (_in_deopt_handler &gt; 0) { // robustness
1631       _in_deopt_handler--;
1632     }
1633   }
1634 
1635  private:
1636   void set_entry_point(ThreadFunction entry_point) { _entry_point = entry_point; }
1637 
1638  public:
1639 
1640   // Frame iteration; calls the function f for all frames on the stack
1641   void frames_do(void f(frame*, const RegisterMap*));
1642 
1643   // Memory operations
1644   void oops_do(OopClosure* f, CodeBlobClosure* cf);
1645 
1646   // Sweeper operations
1647   virtual void nmethods_do(CodeBlobClosure* cf);
1648 
1649   // RedefineClasses Support
1650   void metadata_do(void f(Metadata*));
1651 
1652   // Misc. operations
1653   char* name() const { return (char*)get_thread_name(); }
1654   void print_on(outputStream* st) const;
1655   void print_value();
1656   void print_thread_state_on(outputStream*) const      PRODUCT_RETURN;
1657   void print_thread_state() const                      PRODUCT_RETURN;
1658   void print_on_error(outputStream* st, char* buf, int buflen) const;
1659   void print_name_on_error(outputStream* st, char* buf, int buflen) const;
1660   void verify();
1661   const char* get_thread_name() const;
1662  private:
1663   // factor out low-level mechanics for use in both normal and error cases
1664   const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;
1665  public:
1666   const char* get_threadgroup_name() const;
1667   const char* get_parent_name() const;
1668 
1669   // Accessing frames
1670   frame last_frame() {
1671     _anchor.make_walkable(this);
1672     return pd_last_frame();
1673   }
1674   javaVFrame* last_java_vframe(RegisterMap* reg_map);
1675 
1676   // Returns method at 'depth' java or native frames down the stack
1677   // Used for security checks
1678   Klass* security_get_caller_class(int depth);
1679 
1680   // Print stack trace in external format
1681   void print_stack_on(outputStream* st);
1682   void print_stack() { print_stack_on(tty); }
1683 
1684   // Print stack traces in various internal formats
1685   void trace_stack()                             PRODUCT_RETURN;
1686   void trace_stack_from(vframe* start_vf)        PRODUCT_RETURN;
1687   void trace_frames()                            PRODUCT_RETURN;
1688   void trace_oops()                              PRODUCT_RETURN;
1689 
1690   // Print an annotated view of the stack frames
1691   void print_frame_layout(int depth = 0, bool validate_only = false) NOT_DEBUG_RETURN;
1692   void validate_frame_layout() {
1693     print_frame_layout(0, true);
1694   }
1695 
1696   // Returns the number of stack frames on the stack
1697   int depth() const;
1698 
1699   // Function for testing deoptimization
1700   void deoptimize();
1701   void make_zombies();
1702 
1703   void deoptimized_wrt_marked_nmethods();
1704 
1705   // Profiling operation (see fprofile.cpp)
1706  public:
1707   bool profile_last_Java_frame(frame* fr);
1708 
1709  private:
1710   ThreadProfiler* _thread_profiler;
1711  private:
1712   friend class FlatProfiler;                    // uses both [gs]et_thread_profiler.
1713   friend class FlatProfilerTask;                // uses get_thread_profiler.
1714   friend class ThreadProfilerMark;              // uses get_thread_profiler.
1715   ThreadProfiler* get_thread_profiler()         { return _thread_profiler; }
1716   ThreadProfiler* set_thread_profiler(ThreadProfiler* tp) {
1717     ThreadProfiler* result = _thread_profiler;
1718     _thread_profiler = tp;
1719     return result;
1720   }
1721 
1722  public:
1723   // Returns the running thread as a JavaThread
1724   static inline JavaThread* current();
1725 
1726   // Returns the active Java thread.  Do not use this if you know you are calling
1727   // from a JavaThread, as it's slower than JavaThread::current.  If called from
1728   // the VMThread, it also returns the JavaThread that instigated the VMThread's
1729   // operation.  You may not want that either.
1730   static JavaThread* active();
1731 
1732   inline CompilerThread* as_CompilerThread();
1733 
1734  public:
1735   virtual void run();
1736   void thread_main_inner();
1737 
1738  private:
1739   // PRIVILEGED STACK
1740   PrivilegedElement*  _privileged_stack_top;
1741   GrowableArray&lt;oop&gt;* _array_for_gc;
1742  public:
1743 
1744   // Returns the privileged_stack information.
1745   PrivilegedElement* privileged_stack_top() const       { return _privileged_stack_top; }
1746   void set_privileged_stack_top(PrivilegedElement *e)   { _privileged_stack_top = e; }
1747   void register_array_for_gc(GrowableArray&lt;oop&gt;* array) { _array_for_gc = array; }
1748 
1749  public:
1750   // Thread local information maintained by JVMTI.
1751   void set_jvmti_thread_state(JvmtiThreadState *value)                           { _jvmti_thread_state = value; }
1752   // A JvmtiThreadState is lazily allocated. This jvmti_thread_state()
1753   // getter is used to get this JavaThread's JvmtiThreadState if it has
1754   // one which means NULL can be returned. JvmtiThreadState::state_for()
1755   // is used to get the specified JavaThread's JvmtiThreadState if it has
1756   // one or it allocates a new JvmtiThreadState for the JavaThread and
1757   // returns it. JvmtiThreadState::state_for() will return NULL only if
1758   // the specified JavaThread is exiting.
1759   JvmtiThreadState *jvmti_thread_state() const                                   { return _jvmti_thread_state; }
1760   static ByteSize jvmti_thread_state_offset()                                    { return byte_offset_of(JavaThread, _jvmti_thread_state); }
1761   void set_jvmti_get_loaded_classes_closure(JvmtiGetLoadedClassesClosure* value) { _jvmti_get_loaded_classes_closure = value; }
1762   JvmtiGetLoadedClassesClosure* get_jvmti_get_loaded_classes_closure() const     { return _jvmti_get_loaded_classes_closure; }
1763 
1764   // JVMTI PopFrame support
1765   // Setting and clearing popframe_condition
1766   // All of these enumerated values are bits. popframe_pending
1767   // indicates that a PopFrame() has been requested and not yet been
1768   // completed. popframe_processing indicates that that PopFrame() is in
1769   // the process of being completed. popframe_force_deopt_reexecution_bit
1770   // indicates that special handling is required when returning to a
1771   // deoptimized caller.
1772   enum PopCondition {
1773     popframe_inactive                      = 0x00,
1774     popframe_pending_bit                   = 0x01,
1775     popframe_processing_bit                = 0x02,
1776     popframe_force_deopt_reexecution_bit   = 0x04
1777   };
1778   PopCondition popframe_condition()                   { return (PopCondition) _popframe_condition; }
1779   void set_popframe_condition(PopCondition c)         { _popframe_condition = c; }
1780   void set_popframe_condition_bit(PopCondition c)     { _popframe_condition |= c; }
1781   void clear_popframe_condition()                     { _popframe_condition = popframe_inactive; }
1782   static ByteSize popframe_condition_offset()         { return byte_offset_of(JavaThread, _popframe_condition); }
1783   bool has_pending_popframe()                         { return (popframe_condition() &amp; popframe_pending_bit) != 0; }
1784   bool popframe_forcing_deopt_reexecution()           { return (popframe_condition() &amp; popframe_force_deopt_reexecution_bit) != 0; }
1785   void clear_popframe_forcing_deopt_reexecution()     { _popframe_condition &amp;= ~popframe_force_deopt_reexecution_bit; }
1786 #ifdef CC_INTERP
1787   bool pop_frame_pending(void)                        { return ((_popframe_condition &amp; popframe_pending_bit) != 0); }
1788   void clr_pop_frame_pending(void)                    { _popframe_condition = popframe_inactive; }
1789   bool pop_frame_in_process(void)                     { return ((_popframe_condition &amp; popframe_processing_bit) != 0); }
1790   void set_pop_frame_in_process(void)                 { _popframe_condition |= popframe_processing_bit; }
1791   void clr_pop_frame_in_process(void)                 { _popframe_condition &amp;= ~popframe_processing_bit; }
1792 #endif
1793 
1794   int frames_to_pop_failed_realloc() const            { return _frames_to_pop_failed_realloc; }
1795   void set_frames_to_pop_failed_realloc(int nb)       { _frames_to_pop_failed_realloc = nb; }
1796   void dec_frames_to_pop_failed_realloc()             { _frames_to_pop_failed_realloc--; }
1797 
1798  private:
1799   // Saved incoming arguments to popped frame.
1800   // Used only when popped interpreted frame returns to deoptimized frame.
1801   void*    _popframe_preserved_args;
1802   int      _popframe_preserved_args_size;
1803 
1804  public:
1805   void  popframe_preserve_args(ByteSize size_in_bytes, void* start);
1806   void* popframe_preserved_args();
1807   ByteSize popframe_preserved_args_size();
1808   WordSize popframe_preserved_args_size_in_words();
1809   void  popframe_free_preserved_args();
1810 
1811 
1812  private:
1813   JvmtiThreadState *_jvmti_thread_state;
1814   JvmtiGetLoadedClassesClosure* _jvmti_get_loaded_classes_closure;
1815 
1816   // Used by the interpreter in fullspeed mode for frame pop, method
1817   // entry, method exit and single stepping support. This field is
1818   // only set to non-zero by the VM_EnterInterpOnlyMode VM operation.
1819   // It can be set to zero asynchronously (i.e., without a VM operation
1820   // or a lock) so we have to be very careful.
1821   int               _interp_only_mode;
1822 
1823  public:
1824   // used by the interpreter for fullspeed debugging support (see above)
1825   static ByteSize interp_only_mode_offset() { return byte_offset_of(JavaThread, _interp_only_mode); }
1826   bool is_interp_only_mode()                { return (_interp_only_mode != 0); }
1827   int get_interp_only_mode()                { return _interp_only_mode; }
1828   void increment_interp_only_mode()         { ++_interp_only_mode; }
1829   void decrement_interp_only_mode()         { --_interp_only_mode; }
1830 
1831   // support for cached flag that indicates whether exceptions need to be posted for this thread
1832   // if this is false, we can avoid deoptimizing when events are thrown
1833   // this gets set to reflect whether jvmtiExport::post_exception_throw would actually do anything
1834  private:
1835   int    _should_post_on_exceptions_flag;
1836 
1837  public:
1838   int   should_post_on_exceptions_flag()  { return _should_post_on_exceptions_flag; }
1839   void  set_should_post_on_exceptions_flag(int val)  { _should_post_on_exceptions_flag = val; }
1840 
1841  private:
1842   ThreadStatistics *_thread_stat;
1843 
1844  public:
1845   ThreadStatistics* get_thread_stat() const    { return _thread_stat; }
1846 
1847   // Return a blocker object for which this thread is blocked parking.
1848   oop current_park_blocker();
1849 
1850  private:
1851   static size_t _stack_size_at_create;
1852 
1853  public:
1854   static inline size_t stack_size_at_create(void) {
1855     return _stack_size_at_create;
1856   }
1857   static inline void set_stack_size_at_create(size_t value) {
1858     _stack_size_at_create = value;
1859   }
1860 
1861 #if INCLUDE_ALL_GCS
1862   // SATB marking queue support
1863   SATBMarkQueue&amp; satb_mark_queue() { return _satb_mark_queue; }
1864   static SATBMarkQueueSet&amp; satb_mark_queue_set() {
1865     return _satb_mark_queue_set;
1866   }
1867 
1868   // Dirty card queue support
1869   DirtyCardQueue&amp; dirty_card_queue() { return _dirty_card_queue; }
1870   static DirtyCardQueueSet&amp; dirty_card_queue_set() {
1871     return _dirty_card_queue_set;
1872   }
1873 #endif // INCLUDE_ALL_GCS
1874 
1875   // This method initializes the SATB and dirty card queues before a
1876   // JavaThread is added to the Java thread list. Right now, we don't
1877   // have to do anything to the dirty card queue (it should have been
1878   // activated when the thread was created), but we have to activate
1879   // the SATB queue if the thread is created while a marking cycle is
1880   // in progress. The activation / de-activation of the SATB queues at
1881   // the beginning / end of a marking cycle is done during safepoints
1882   // so we have to make sure this method is called outside one to be
1883   // able to safely read the active field of the SATB queue set. Right
1884   // now, it is called just before the thread is added to the Java
1885   // thread list in the Threads::add() method. That method is holding
1886   // the Threads_lock which ensures we are outside a safepoint. We
1887   // cannot do the obvious and set the active field of the SATB queue
1888   // when the thread is created given that, in some cases, safepoints
1889   // might happen between the JavaThread constructor being called and the
1890   // thread being added to the Java thread list (an example of this is
1891   // when the structure for the DestroyJavaVM thread is created).
1892 #if INCLUDE_ALL_GCS
1893   void initialize_queues();
1894 #else  // INCLUDE_ALL_GCS
1895   void initialize_queues() { }
1896 #endif // INCLUDE_ALL_GCS
1897 
1898   // Machine dependent stuff
1899 #ifdef TARGET_OS_ARCH_linux_x86
1900 # include "thread_linux_x86.hpp"
1901 #endif
1902 #ifdef TARGET_OS_ARCH_linux_sparc
1903 # include "thread_linux_sparc.hpp"
1904 #endif
1905 #ifdef TARGET_OS_ARCH_linux_zero
1906 # include "thread_linux_zero.hpp"
1907 #endif
1908 #ifdef TARGET_OS_ARCH_solaris_x86
1909 # include "thread_solaris_x86.hpp"
1910 #endif
1911 #ifdef TARGET_OS_ARCH_solaris_sparc
1912 # include "thread_solaris_sparc.hpp"
1913 #endif
1914 #ifdef TARGET_OS_ARCH_windows_x86
1915 # include "thread_windows_x86.hpp"
1916 #endif
1917 #ifdef TARGET_OS_ARCH_linux_arm
1918 # include "thread_linux_arm.hpp"
1919 #endif
1920 #ifdef TARGET_OS_ARCH_linux_ppc
1921 # include "thread_linux_ppc.hpp"
1922 #endif
1923 #ifdef TARGET_OS_ARCH_linux_aarch64
1924 # include "thread_linux_aarch64.hpp"
1925 #endif
1926 #ifdef TARGET_OS_ARCH_aix_ppc
1927 # include "thread_aix_ppc.hpp"
1928 #endif
1929 #ifdef TARGET_OS_ARCH_bsd_x86
1930 # include "thread_bsd_x86.hpp"
1931 #endif
1932 #ifdef TARGET_OS_ARCH_bsd_zero
1933 # include "thread_bsd_zero.hpp"
1934 #endif
1935 
1936 
1937  public:
1938   void set_blocked_on_compilation(bool value) {
1939     _blocked_on_compilation = value;
1940   }
1941 
1942   bool blocked_on_compilation() {
1943     return _blocked_on_compilation;
1944   }
1945  protected:
1946   bool         _blocked_on_compilation;
1947 
1948 
1949   // JSR166 per-thread parker
1950  private:
1951   Parker*    _parker;
1952  public:
1953   Parker*     parker() { return _parker; }
1954 
1955   // Biased locking support
1956  private:
1957   GrowableArray&lt;MonitorInfo*&gt;* _cached_monitor_info;
1958  public:
1959   GrowableArray&lt;MonitorInfo*&gt;* cached_monitor_info() { return _cached_monitor_info; }
1960   void set_cached_monitor_info(GrowableArray&lt;MonitorInfo*&gt;* info) { _cached_monitor_info = info; }
1961 
1962   // clearing/querying jni attach status
1963   bool is_attaching_via_jni() const { return _jni_attach_state == _attaching_via_jni; }
1964   bool has_attached_via_jni() const { return is_attaching_via_jni() || _jni_attach_state == _attached_via_jni; }
1965   inline void set_done_attaching_via_jni();
1966 };
1967 
1968 // Inline implementation of JavaThread::current
1969 inline JavaThread* JavaThread::current() {
1970   Thread* thread = Thread::current();
1971   assert(thread-&gt;is_Java_thread(), "just checking");
1972   return (JavaThread*)thread;
1973 }
1974 
1975 inline CompilerThread* JavaThread::as_CompilerThread() {
1976   assert(is_Compiler_thread(), "just checking");
1977   return (CompilerThread*)this;
1978 }
1979 
1980 // Dedicated thread to sweep the code cache
1981 class CodeCacheSweeperThread : public JavaThread {
1982   CompiledMethod*       _scanned_compiled_method; // nmethod being scanned by the sweeper
1983  public:
1984   CodeCacheSweeperThread();
1985   // Track the nmethod currently being scanned by the sweeper
1986   void set_scanned_compiled_method(CompiledMethod* cm) {
1987     assert(_scanned_compiled_method == NULL || cm == NULL, "should reset to NULL before writing a new value");
1988     _scanned_compiled_method = cm;
1989   }
1990 
1991   // Hide sweeper thread from external view.
1992   bool is_hidden_from_external_view() const { return true; }
1993 
1994   bool is_Code_cache_sweeper_thread() const { return true; }
1995 
1996   // Prevent GC from unloading _scanned_compiled_method
1997   void oops_do(OopClosure* f, CodeBlobClosure* cf);
1998   void nmethods_do(CodeBlobClosure* cf);
1999 };
2000 
2001 // A thread used for Compilation.
2002 class CompilerThread : public JavaThread {
2003   friend class VMStructs;
2004  private:
2005   CompilerCounters* _counters;
2006 
2007   ciEnv*            _env;
2008   CompileLog*       _log;
2009   CompileTask*      _task;
2010   CompileQueue*     _queue;
2011   BufferBlob*       _buffer_blob;
2012 
2013   AbstractCompiler* _compiler;
2014 
2015  public:
2016 
2017   static CompilerThread* current();
2018 
2019   CompilerThread(CompileQueue* queue, CompilerCounters* counters);
2020 
2021   bool is_Compiler_thread() const                { return true; }
2022 
2023   virtual bool can_call_java() const;
2024 
2025   // Hide native compiler threads from external view.
2026   bool is_hidden_from_external_view() const      { return !can_call_java(); }
2027 
2028   void set_compiler(AbstractCompiler* c)         { _compiler = c; }
2029   AbstractCompiler* compiler() const             { return _compiler; }
2030 
2031   CompileQueue* queue()        const             { return _queue; }
2032   CompilerCounters* counters() const             { return _counters; }
2033 
2034   // Get/set the thread's compilation environment.
2035   ciEnv*        env()                            { return _env; }
2036   void          set_env(ciEnv* env)              { _env = env; }
2037 
2038   BufferBlob*   get_buffer_blob() const          { return _buffer_blob; }
2039   void          set_buffer_blob(BufferBlob* b)   { _buffer_blob = b; }
2040 
2041   // Get/set the thread's logging information
2042   CompileLog*   log()                            { return _log; }
2043   void          init_log(CompileLog* log) {
2044     // Set once, for good.
2045     assert(_log == NULL, "set only once");
2046     _log = log;
2047   }
2048 
2049 #ifndef PRODUCT
2050  private:
2051   IdealGraphPrinter *_ideal_graph_printer;
2052  public:
2053   IdealGraphPrinter *ideal_graph_printer()           { return _ideal_graph_printer; }
2054   void set_ideal_graph_printer(IdealGraphPrinter *n) { _ideal_graph_printer = n; }
2055 #endif
2056 
2057   // Get/set the thread's current task
2058   CompileTask* task()                      { return _task; }
2059   void         set_task(CompileTask* task) { _task = task; }
2060 };
2061 
2062 inline CompilerThread* CompilerThread::current() {
2063   return JavaThread::current()-&gt;as_CompilerThread();
2064 }
2065 
2066 // The active thread queue. It also keeps track of the current used
2067 // thread priorities.
2068 class Threads: AllStatic {
2069   friend class VMStructs;
2070  private:
2071   static JavaThread* _thread_list;
2072   static int         _number_of_threads;
2073   static int         _number_of_non_daemon_threads;
2074   static int         _return_code;
2075   static int         _thread_claim_parity;
2076 #ifdef ASSERT
2077   static bool        _vm_complete;
2078 #endif
2079 
2080   static void initialize_java_lang_classes(JavaThread* main_thread, TRAPS);
2081   static void initialize_jsr292_core_classes(TRAPS);
2082  public:
2083   // Thread management
2084   // force_daemon is a concession to JNI, where we may need to add a
2085   // thread to the thread list before allocating its thread object
2086   static void add(JavaThread* p, bool force_daemon = false);
2087   static void remove(JavaThread* p);
2088   static bool includes(JavaThread* p);
2089   static JavaThread* first()                     { return _thread_list; }
2090   static void threads_do(ThreadClosure* tc);
2091 
2092   // Initializes the vm and creates the vm thread
2093   static jint create_vm(JavaVMInitArgs* args, bool* canTryAgain);
2094   static void convert_vm_init_libraries_to_agents();
2095   static void create_vm_init_libraries();
2096   static void create_vm_init_agents();
2097   static void shutdown_vm_agents();
2098   static bool destroy_vm();
2099   // Supported VM versions via JNI
2100   // Includes JNI_VERSION_1_1
2101   static jboolean is_supported_jni_version_including_1_1(jint version);
2102   // Does not include JNI_VERSION_1_1
2103   static jboolean is_supported_jni_version(jint version);
2104 
2105   // The "thread claim parity" provides a way for threads to be claimed
2106   // by parallel worker tasks.
2107   //
2108   // Each thread contains a a "parity" field. A task will claim the
2109   // thread only if its parity field is the same as the global parity,
2110   // which is updated by calling change_thread_claim_parity().
2111   //
2112   // For this to work change_thread_claim_parity() needs to be called
2113   // exactly once in sequential code before starting parallel tasks
2114   // that should claim threads.
2115   //
2116   // New threads get their parity set to 0 and change_thread_claim_parity()
2117   // never set the global parity to 0.
2118   static int thread_claim_parity() { return _thread_claim_parity; }
2119   static void change_thread_claim_parity();
2120   static void assert_all_threads_claimed() NOT_DEBUG_RETURN;
2121 
2122   // Apply "f-&gt;do_oop" to all root oops in all threads.
2123   // This version may only be called by sequential code.
2124   static void oops_do(OopClosure* f, CodeBlobClosure* cf);
2125   // This version may be called by sequential or parallel code.
2126   static void possibly_parallel_oops_do(bool is_par, OopClosure* f, CodeBlobClosure* cf);
2127   // This creates a list of GCTasks, one per thread.
2128   static void create_thread_roots_tasks(GCTaskQueue* q);
2129   // This creates a list of GCTasks, one per thread, for marking objects.
2130   static void create_thread_roots_marking_tasks(GCTaskQueue* q);
2131 
2132   // Apply "f-&gt;do_oop" to roots in all threads that
2133   // are part of compiled frames
2134   static void compiled_frame_oops_do(OopClosure* f, CodeBlobClosure* cf);
2135 
2136   static void convert_hcode_pointers();
2137   static void restore_hcode_pointers();
2138 
2139   // Sweeper
2140   static void nmethods_do(CodeBlobClosure* cf);
2141 
2142   // RedefineClasses support
2143   static void metadata_do(void f(Metadata*));
2144   static void metadata_handles_do(void f(Metadata*));
2145 
2146 #ifdef ASSERT
2147   static bool is_vm_complete() { return _vm_complete; }
2148 #endif
2149 
2150   // Verification
2151   static void verify();
2152   static void print_on(outputStream* st, bool print_stacks, bool internal_format, bool print_concurrent_locks);
2153   static void print(bool print_stacks, bool internal_format) {
2154     // this function is only used by debug.cpp
2155     print_on(tty, print_stacks, internal_format, false /* no concurrent lock printed */);
2156   }
2157   static void print_on_error(outputStream* st, Thread* current, char* buf, int buflen);
2158   static void print_on_error(Thread* this_thread, outputStream* st, Thread* current, char* buf,
2159                              int buflen, bool* found_current);
2160   static void print_threads_compiling(outputStream* st, char* buf, int buflen);
2161 
2162   // Get Java threads that are waiting to enter a monitor. If doLock
2163   // is true, then Threads_lock is grabbed as needed. Otherwise, the
2164   // VM needs to be at a safepoint.
2165   static GrowableArray&lt;JavaThread*&gt;* get_pending_threads(int count,
2166                                                          address monitor, bool doLock);
2167 
2168   // Get owning Java thread from the monitor's owner field. If doLock
2169   // is true, then Threads_lock is grabbed as needed. Otherwise, the
2170   // VM needs to be at a safepoint.
2171   static JavaThread *owning_thread_from_monitor_owner(address owner,
2172                                                       bool doLock);
2173 
2174   // Number of threads on the active threads list
2175   static int number_of_threads()                 { return _number_of_threads; }
2176   // Number of non-daemon threads on the active threads list
2177   static int number_of_non_daemon_threads()      { return _number_of_non_daemon_threads; }
2178 
2179   // Deoptimizes all frames tied to marked nmethods
2180   static void deoptimized_wrt_marked_nmethods();
2181 
2182   static JavaThread* find_java_thread_from_java_tid(jlong java_tid);
2183 
2184 };
2185 
2186 
2187 // Thread iterator
2188 class ThreadClosure: public StackObj {
2189  public:
2190   virtual void do_thread(Thread* thread) = 0;
2191 };
2192 
2193 class SignalHandlerMark: public StackObj {
2194  private:
2195   Thread* _thread;
2196  public:
2197   SignalHandlerMark(Thread* t) {
2198     _thread = t;
2199     if (_thread) _thread-&gt;enter_signal_handler();
2200   }
2201   ~SignalHandlerMark() {
2202     if (_thread) _thread-&gt;leave_signal_handler();
2203     _thread = NULL;
2204   }
2205 };
2206 
2207 
2208 #endif // SHARE_VM_RUNTIME_THREAD_HPP
</pre></body></html>
