<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "jvm.h"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "code/codeCache.hpp"
  29 #include "code/debugInfoRec.hpp"
  30 #include "code/nmethod.hpp"
  31 #include "code/pcDesc.hpp"
  32 #include "code/scopeDesc.hpp"
  33 #include "interpreter/bytecode.hpp"
  34 #include "interpreter/interpreter.hpp"
  35 #include "interpreter/oopMapCache.hpp"
  36 #include "memory/allocation.inline.hpp"
  37 #include "memory/oopFactory.hpp"
  38 #include "memory/resourceArea.hpp"
  39 #include "oops/method.hpp"
  40 #include "oops/objArrayOop.inline.hpp"
  41 #include "oops/oop.inline.hpp"
  42 #include "oops/fieldStreams.hpp"
  43 #include "oops/typeArrayOop.inline.hpp"
  44 #include "oops/verifyOopClosure.hpp"
  45 #include "prims/jvmtiThreadState.hpp"
  46 #include "runtime/biasedLocking.hpp"
  47 #include "runtime/compilationPolicy.hpp"
  48 #include "runtime/deoptimization.hpp"
  49 #include "runtime/frame.inline.hpp"
  50 #include "runtime/interfaceSupport.inline.hpp"
  51 #include "runtime/safepointVerifiers.hpp"
  52 #include "runtime/sharedRuntime.hpp"
  53 #include "runtime/signature.hpp"
  54 #include "runtime/stubRoutines.hpp"
  55 #include "runtime/thread.hpp"
  56 #include "runtime/threadSMR.hpp"
  57 #include "runtime/vframe.hpp"
  58 #include "runtime/vframeArray.hpp"
  59 #include "runtime/vframe_hp.hpp"
  60 #include "utilities/events.hpp"
  61 #include "utilities/preserveException.hpp"
  62 #include "utilities/xmlstream.hpp"
  63 
  64 #if INCLUDE_JVMCI
  65 #include "jvmci/jvmciRuntime.hpp"
  66 #include "jvmci/jvmciJavaClasses.hpp"
  67 #endif
  68 
  69 
  70 bool DeoptimizationMarker::_is_active = false;
  71 
  72 Deoptimization::UnrollBlock::UnrollBlock(int  size_of_deoptimized_frame,
  73                                          int  caller_adjustment,
  74                                          int  caller_actual_parameters,
  75                                          int  number_of_frames,
  76                                          intptr_t* frame_sizes,
  77                                          address* frame_pcs,
  78                                          BasicType return_type,
  79                                          int exec_mode) {
  80   _size_of_deoptimized_frame = size_of_deoptimized_frame;
  81   _caller_adjustment         = caller_adjustment;
  82   _caller_actual_parameters  = caller_actual_parameters;
  83   _number_of_frames          = number_of_frames;
  84   _frame_sizes               = frame_sizes;
  85   _frame_pcs                 = frame_pcs;
  86   _register_block            = NEW_C_HEAP_ARRAY(intptr_t, RegisterMap::reg_count * 2, mtCompiler);
  87   _return_type               = return_type;
  88   _initial_info              = 0;
  89   // PD (x86 only)
  90   _counter_temp              = 0;
  91   _unpack_kind               = exec_mode;
  92   _sender_sp_temp            = 0;
  93 
  94   _total_frame_sizes         = size_of_frames();
  95   assert(exec_mode &gt;= 0 &amp;&amp; exec_mode &lt; Unpack_LIMIT, "Unexpected exec_mode");
  96 }
  97 
  98 
  99 Deoptimization::UnrollBlock::~UnrollBlock() {
 100   FREE_C_HEAP_ARRAY(intptr_t, _frame_sizes);
 101   FREE_C_HEAP_ARRAY(intptr_t, _frame_pcs);
 102   FREE_C_HEAP_ARRAY(intptr_t, _register_block);
 103 }
 104 
 105 
 106 intptr_t* Deoptimization::UnrollBlock::value_addr_at(int register_number) const {
 107   assert(register_number &lt; RegisterMap::reg_count, "checking register number");
 108   return &amp;_register_block[register_number * 2];
 109 }
 110 
 111 
 112 
 113 int Deoptimization::UnrollBlock::size_of_frames() const {
 114   // Acount first for the adjustment of the initial frame
 115   int result = _caller_adjustment;
 116   for (int index = 0; index &lt; number_of_frames(); index++) {
 117     result += frame_sizes()[index];
 118   }
 119   return result;
 120 }
 121 
 122 
 123 void Deoptimization::UnrollBlock::print() {
 124   ttyLocker ttyl;
 125   tty-&gt;print_cr("UnrollBlock");
 126   tty-&gt;print_cr("  size_of_deoptimized_frame = %d", _size_of_deoptimized_frame);
 127   tty-&gt;print(   "  frame_sizes: ");
 128   for (int index = 0; index &lt; number_of_frames(); index++) {
 129     tty-&gt;print(INTX_FORMAT " ", frame_sizes()[index]);
 130   }
 131   tty-&gt;cr();
 132 }
 133 
 134 
 135 // In order to make fetch_unroll_info work properly with escape
 136 // analysis, The method was changed from JRT_LEAF to JRT_BLOCK_ENTRY and
 137 // ResetNoHandleMark and HandleMark were removed from it. The actual reallocation
 138 // of previously eliminated objects occurs in realloc_objects, which is
 139 // called from the method fetch_unroll_info_helper below.
 140 JRT_BLOCK_ENTRY(Deoptimization::UnrollBlock*, Deoptimization::fetch_unroll_info(JavaThread* thread, int exec_mode))
 141   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 142   // but makes the entry a little slower. There is however a little dance we have to
 143   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 144 
 145   // fetch_unroll_info() is called at the beginning of the deoptimization
 146   // handler. Note this fact before we start generating temporary frames
 147   // that can confuse an asynchronous stack walker. This counter is
 148   // decremented at the end of unpack_frames().
 149   if (TraceDeoptimization) {
 150     tty-&gt;print_cr("Deoptimizing thread " INTPTR_FORMAT, p2i(thread));
 151   }
 152   thread-&gt;inc_in_deopt_handler();
 153 
 154   return fetch_unroll_info_helper(thread, exec_mode);
 155 JRT_END
 156 
 157 
 158 // This is factored, since it is both called from a JRT_LEAF (deoptimization) and a JRT_ENTRY (uncommon_trap)
 159 Deoptimization::UnrollBlock* Deoptimization::fetch_unroll_info_helper(JavaThread* thread, int exec_mode) {
 160 
 161   // Note: there is a safepoint safety issue here. No matter whether we enter
 162   // via vanilla deopt or uncommon trap we MUST NOT stop at a safepoint once
 163   // the vframeArray is created.
 164   //
 165 
 166   // Allocate our special deoptimization ResourceMark
 167   DeoptResourceMark* dmark = new DeoptResourceMark(thread);
 168   assert(thread-&gt;deopt_mark() == NULL, "Pending deopt!");
 169   thread-&gt;set_deopt_mark(dmark);
 170 
 171   frame stub_frame = thread-&gt;last_frame(); // Makes stack walkable as side effect
 172   RegisterMap map(thread, true);
 173   RegisterMap dummy_map(thread, false);
 174   // Now get the deoptee with a valid map
 175   frame deoptee = stub_frame.sender(&amp;map);
 176   // Set the deoptee nmethod
 177   assert(thread-&gt;deopt_compiled_method() == NULL, "Pending deopt!");
 178   CompiledMethod* cm = deoptee.cb()-&gt;as_compiled_method_or_null();
 179   thread-&gt;set_deopt_compiled_method(cm);
 180 
 181   if (VerifyStack) {
 182     thread-&gt;validate_frame_layout();
 183   }
 184 
 185   // Create a growable array of VFrames where each VFrame represents an inlined
 186   // Java frame.  This storage is allocated with the usual system arena.
 187   assert(deoptee.is_compiled_frame(), "Wrong frame type");
 188   GrowableArray&lt;compiledVFrame*&gt;* chunk = new GrowableArray&lt;compiledVFrame*&gt;(10);
 189   vframe* vf = vframe::new_vframe(&amp;deoptee, &amp;map, thread);
 190   while (!vf-&gt;is_top()) {
 191     assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 192     chunk-&gt;push(compiledVFrame::cast(vf));
 193     vf = vf-&gt;sender();
 194   }
 195   assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 196   chunk-&gt;push(compiledVFrame::cast(vf));
 197 
 198   bool realloc_failures = false;
 199 
 200 #if COMPILER2_OR_JVMCI
 201   // Reallocate the non-escaping objects and restore their fields. Then
 202   // relock objects if synchronization on them was eliminated.
 203 #if !INCLUDE_JVMCI
 204   if (DoEscapeAnalysis || EliminateNestedLocks) {
 205     if (EliminateAllocations) {
 206 #endif // INCLUDE_JVMCI
 207       assert (chunk-&gt;at(0)-&gt;scope() != NULL,"expect only compiled java frames");
 208       GrowableArray&lt;ScopeValue*&gt;* objects = chunk-&gt;at(0)-&gt;scope()-&gt;objects();
 209 
 210       // The flag return_oop() indicates call sites which return oop
 211       // in compiled code. Such sites include java method calls,
 212       // runtime calls (for example, used to allocate new objects/arrays
 213       // on slow code path) and any other calls generated in compiled code.
 214       // It is not guaranteed that we can get such information here only
 215       // by analyzing bytecode in deoptimized frames. This is why this flag
 216       // is set during method compilation (see Compile::Process_OopMap_Node()).
 217       // If the previous frame was popped or if we are dispatching an exception,
 218       // we don't have an oop result.
 219       bool save_oop_result = chunk-&gt;at(0)-&gt;scope()-&gt;return_oop() &amp;&amp; !thread-&gt;popframe_forcing_deopt_reexecution() &amp;&amp; (exec_mode == Unpack_deopt);
 220       Handle return_value;
 221       if (save_oop_result) {
 222         // Reallocation may trigger GC. If deoptimization happened on return from
 223         // call which returns oop we need to save it since it is not in oopmap.
 224         oop result = deoptee.saved_oop_result(&amp;map);
 225         assert(oopDesc::is_oop_or_null(result), "must be oop");
 226         return_value = Handle(thread, result);
 227         assert(Universe::heap()-&gt;is_in_or_null(result), "must be heap pointer");
 228         if (TraceDeoptimization) {
 229           ttyLocker ttyl;
 230           tty-&gt;print_cr("SAVED OOP RESULT " INTPTR_FORMAT " in thread " INTPTR_FORMAT, p2i(result), p2i(thread));
 231         }
 232       }
 233       if (objects != NULL) {
 234         JRT_BLOCK
<a name="1" id="anc1"></a><span class="changed"> 235           realloc_failures = realloc_objects(thread, &amp;deoptee, objects, THREAD);</span>
 236         JRT_END
 237         bool skip_internal = (cm != NULL) &amp;&amp; !cm-&gt;is_compiled_by_jvmci();
 238         reassign_fields(&amp;deoptee, &amp;map, objects, realloc_failures, skip_internal);
 239 #ifndef PRODUCT
 240         if (TraceDeoptimization) {
 241           ttyLocker ttyl;
 242           tty-&gt;print_cr("REALLOC OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 243           print_objects(objects, realloc_failures);
 244         }
 245 #endif
 246       }
 247       if (save_oop_result) {
 248         // Restore result.
 249         deoptee.set_saved_oop_result(&amp;map, return_value());
 250       }
 251 #if !INCLUDE_JVMCI
 252     }
 253     if (EliminateLocks) {
 254 #endif // INCLUDE_JVMCI
 255 #ifndef PRODUCT
 256       bool first = true;
 257 #endif
 258       for (int i = 0; i &lt; chunk-&gt;length(); i++) {
 259         compiledVFrame* cvf = chunk-&gt;at(i);
 260         assert (cvf-&gt;scope() != NULL,"expect only compiled java frames");
 261         GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
 262         if (monitors-&gt;is_nonempty()) {
 263           relock_objects(monitors, thread, realloc_failures);
 264 #ifndef PRODUCT
 265           if (PrintDeoptimizationDetails) {
 266             ttyLocker ttyl;
 267             for (int j = 0; j &lt; monitors-&gt;length(); j++) {
 268               MonitorInfo* mi = monitors-&gt;at(j);
 269               if (mi-&gt;eliminated()) {
 270                 if (first) {
 271                   first = false;
 272                   tty-&gt;print_cr("RELOCK OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 273                 }
 274                 if (mi-&gt;owner_is_scalar_replaced()) {
 275                   Klass* k = java_lang_Class::as_Klass(mi-&gt;owner_klass());
 276                   tty-&gt;print_cr("     failed reallocation for klass %s", k-&gt;external_name());
 277                 } else {
 278                   tty-&gt;print_cr("     object &lt;" INTPTR_FORMAT "&gt; locked", p2i(mi-&gt;owner()));
 279                 }
 280               }
 281             }
 282           }
 283 #endif // !PRODUCT
 284         }
 285       }
 286 #if !INCLUDE_JVMCI
 287     }
 288   }
 289 #endif // INCLUDE_JVMCI
 290 #endif // COMPILER2_OR_JVMCI
 291 
 292   ScopeDesc* trap_scope = chunk-&gt;at(0)-&gt;scope();
 293   Handle exceptionObject;
 294   if (trap_scope-&gt;rethrow_exception()) {
 295     if (PrintDeoptimizationDetails) {
 296       tty-&gt;print_cr("Exception to be rethrown in the interpreter for method %s::%s at bci %d", trap_scope-&gt;method()-&gt;method_holder()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;method()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;bci());
 297     }
 298     GrowableArray&lt;ScopeValue*&gt;* expressions = trap_scope-&gt;expressions();
 299     guarantee(expressions != NULL &amp;&amp; expressions-&gt;length() &gt; 0, "must have exception to throw");
 300     ScopeValue* topOfStack = expressions-&gt;top();
 301     exceptionObject = StackValue::create_stack_value(&amp;deoptee, &amp;map, topOfStack)-&gt;get_obj();
 302     guarantee(exceptionObject() != NULL, "exception oop can not be null");
 303   }
 304 
 305   // Ensure that no safepoint is taken after pointers have been stored
 306   // in fields of rematerialized objects.  If a safepoint occurs from here on
 307   // out the java state residing in the vframeArray will be missed.
 308   NoSafepointVerifier no_safepoint;
 309 
 310   vframeArray* array = create_vframeArray(thread, deoptee, &amp;map, chunk, realloc_failures);
 311 #if COMPILER2_OR_JVMCI
 312   if (realloc_failures) {
 313     pop_frames_failed_reallocs(thread, array);
 314   }
 315 #endif
 316 
 317   assert(thread-&gt;vframe_array_head() == NULL, "Pending deopt!");
 318   thread-&gt;set_vframe_array_head(array);
 319 
 320   // Now that the vframeArray has been created if we have any deferred local writes
 321   // added by jvmti then we can free up that structure as the data is now in the
 322   // vframeArray
 323 
 324   if (thread-&gt;deferred_locals() != NULL) {
 325     GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* list = thread-&gt;deferred_locals();
 326     int i = 0;
 327     do {
 328       // Because of inlining we could have multiple vframes for a single frame
 329       // and several of the vframes could have deferred writes. Find them all.
 330       if (list-&gt;at(i)-&gt;id() == array-&gt;original().id()) {
 331         jvmtiDeferredLocalVariableSet* dlv = list-&gt;at(i);
 332         list-&gt;remove_at(i);
 333         // individual jvmtiDeferredLocalVariableSet are CHeapObj's
 334         delete dlv;
 335       } else {
 336         i++;
 337       }
 338     } while ( i &lt; list-&gt;length() );
 339     if (list-&gt;length() == 0) {
 340       thread-&gt;set_deferred_locals(NULL);
 341       // free the list and elements back to C heap.
 342       delete list;
 343     }
 344 
 345   }
 346 
 347   // Compute the caller frame based on the sender sp of stub_frame and stored frame sizes info.
 348   CodeBlob* cb = stub_frame.cb();
 349   // Verify we have the right vframeArray
 350   assert(cb-&gt;frame_size() &gt;= 0, "Unexpected frame size");
 351   intptr_t* unpack_sp = stub_frame.sp() + cb-&gt;frame_size();
 352 
 353   // If the deopt call site is a MethodHandle invoke call site we have
 354   // to adjust the unpack_sp.
 355   nmethod* deoptee_nm = deoptee.cb()-&gt;as_nmethod_or_null();
 356   if (deoptee_nm != NULL &amp;&amp; deoptee_nm-&gt;is_method_handle_return(deoptee.pc()))
 357     unpack_sp = deoptee.unextended_sp();
 358 
 359 #ifdef ASSERT
 360   assert(cb-&gt;is_deoptimization_stub() ||
 361          cb-&gt;is_uncommon_trap_stub() ||
 362          strcmp("Stub&lt;DeoptimizationStub.deoptimizationHandler&gt;", cb-&gt;name()) == 0 ||
 363          strcmp("Stub&lt;UncommonTrapStub.uncommonTrapHandler&gt;", cb-&gt;name()) == 0,
 364          "unexpected code blob: %s", cb-&gt;name());
 365 #endif
 366 
 367   // This is a guarantee instead of an assert because if vframe doesn't match
 368   // we will unpack the wrong deoptimized frame and wind up in strange places
 369   // where it will be very difficult to figure out what went wrong. Better
 370   // to die an early death here than some very obscure death later when the
 371   // trail is cold.
 372   // Note: on ia64 this guarantee can be fooled by frames with no memory stack
 373   // in that it will fail to detect a problem when there is one. This needs
 374   // more work in tiger timeframe.
 375   guarantee(array-&gt;unextended_sp() == unpack_sp, "vframe_array_head must contain the vframeArray to unpack");
 376 
 377   int number_of_frames = array-&gt;frames();
 378 
 379   // Compute the vframes' sizes.  Note that frame_sizes[] entries are ordered from outermost to innermost
 380   // virtual activation, which is the reverse of the elements in the vframes array.
 381   intptr_t* frame_sizes = NEW_C_HEAP_ARRAY(intptr_t, number_of_frames, mtCompiler);
 382   // +1 because we always have an interpreter return address for the final slot.
 383   address* frame_pcs = NEW_C_HEAP_ARRAY(address, number_of_frames + 1, mtCompiler);
 384   int popframe_extra_args = 0;
 385   // Create an interpreter return address for the stub to use as its return
 386   // address so the skeletal frames are perfectly walkable
 387   frame_pcs[number_of_frames] = Interpreter::deopt_entry(vtos, 0);
 388 
 389   // PopFrame requires that the preserved incoming arguments from the recently-popped topmost
 390   // activation be put back on the expression stack of the caller for reexecution
 391   if (JvmtiExport::can_pop_frame() &amp;&amp; thread-&gt;popframe_forcing_deopt_reexecution()) {
 392     popframe_extra_args = in_words(thread-&gt;popframe_preserved_args_size_in_words());
 393   }
 394 
 395   // Find the current pc for sender of the deoptee. Since the sender may have been deoptimized
 396   // itself since the deoptee vframeArray was created we must get a fresh value of the pc rather
 397   // than simply use array-&gt;sender.pc(). This requires us to walk the current set of frames
 398   //
 399   frame deopt_sender = stub_frame.sender(&amp;dummy_map); // First is the deoptee frame
 400   deopt_sender = deopt_sender.sender(&amp;dummy_map);     // Now deoptee caller
 401 
 402   // It's possible that the number of parameters at the call site is
 403   // different than number of arguments in the callee when method
 404   // handles are used.  If the caller is interpreted get the real
 405   // value so that the proper amount of space can be added to it's
 406   // frame.
 407   bool caller_was_method_handle = false;
 408   if (deopt_sender.is_interpreted_frame()) {
 409     methodHandle method = deopt_sender.interpreter_frame_method();
 410     Bytecode_invoke cur = Bytecode_invoke_check(method, deopt_sender.interpreter_frame_bci());
 411     if (cur.is_invokedynamic() || cur.is_invokehandle()) {
 412       // Method handle invokes may involve fairly arbitrary chains of
 413       // calls so it's impossible to know how much actual space the
 414       // caller has for locals.
 415       caller_was_method_handle = true;
 416     }
 417   }
 418 
 419   //
 420   // frame_sizes/frame_pcs[0] oldest frame (int or c2i)
 421   // frame_sizes/frame_pcs[1] next oldest frame (int)
 422   // frame_sizes/frame_pcs[n] youngest frame (int)
 423   //
 424   // Now a pc in frame_pcs is actually the return address to the frame's caller (a frame
 425   // owns the space for the return address to it's caller).  Confusing ain't it.
 426   //
 427   // The vframe array can address vframes with indices running from
 428   // 0.._frames-1. Index  0 is the youngest frame and _frame - 1 is the oldest (root) frame.
 429   // When we create the skeletal frames we need the oldest frame to be in the zero slot
 430   // in the frame_sizes/frame_pcs so the assembly code can do a trivial walk.
 431   // so things look a little strange in this loop.
 432   //
 433   int callee_parameters = 0;
 434   int callee_locals = 0;
 435   for (int index = 0; index &lt; array-&gt;frames(); index++ ) {
 436     // frame[number_of_frames - 1 ] = on_stack_size(youngest)
 437     // frame[number_of_frames - 2 ] = on_stack_size(sender(youngest))
 438     // frame[number_of_frames - 3 ] = on_stack_size(sender(sender(youngest)))
 439     frame_sizes[number_of_frames - 1 - index] = BytesPerWord * array-&gt;element(index)-&gt;on_stack_size(callee_parameters,
 440                                                                                                     callee_locals,
 441                                                                                                     index == 0,
 442                                                                                                     popframe_extra_args);
 443     // This pc doesn't have to be perfect just good enough to identify the frame
 444     // as interpreted so the skeleton frame will be walkable
 445     // The correct pc will be set when the skeleton frame is completely filled out
 446     // The final pc we store in the loop is wrong and will be overwritten below
 447     frame_pcs[number_of_frames - 1 - index ] = Interpreter::deopt_entry(vtos, 0) - frame::pc_return_offset;
 448 
 449     callee_parameters = array-&gt;element(index)-&gt;method()-&gt;size_of_parameters();
 450     callee_locals = array-&gt;element(index)-&gt;method()-&gt;max_locals();
 451     popframe_extra_args = 0;
 452   }
 453 
 454   // Compute whether the root vframe returns a float or double value.
 455   BasicType return_type;
 456   {
 457     methodHandle method(thread, array-&gt;element(0)-&gt;method());
 458     Bytecode_invoke invoke = Bytecode_invoke_check(method, array-&gt;element(0)-&gt;bci());
 459     return_type = invoke.is_valid() ? invoke.result_type() : T_ILLEGAL;
 460   }
 461 
 462   // Compute information for handling adapters and adjusting the frame size of the caller.
 463   int caller_adjustment = 0;
 464 
 465   // Compute the amount the oldest interpreter frame will have to adjust
 466   // its caller's stack by. If the caller is a compiled frame then
 467   // we pretend that the callee has no parameters so that the
 468   // extension counts for the full amount of locals and not just
 469   // locals-parms. This is because without a c2i adapter the parm
 470   // area as created by the compiled frame will not be usable by
 471   // the interpreter. (Depending on the calling convention there
 472   // may not even be enough space).
 473 
 474   // QQQ I'd rather see this pushed down into last_frame_adjust
 475   // and have it take the sender (aka caller).
 476 
 477   if (deopt_sender.is_compiled_frame() || caller_was_method_handle) {
 478     caller_adjustment = last_frame_adjust(0, callee_locals);
 479   } else if (callee_locals &gt; callee_parameters) {
 480     // The caller frame may need extending to accommodate
 481     // non-parameter locals of the first unpacked interpreted frame.
 482     // Compute that adjustment.
 483     caller_adjustment = last_frame_adjust(callee_parameters, callee_locals);
 484   }
 485 
 486   // If the sender is deoptimized the we must retrieve the address of the handler
 487   // since the frame will "magically" show the original pc before the deopt
 488   // and we'd undo the deopt.
 489 
 490   frame_pcs[0] = deopt_sender.raw_pc();
 491 
 492   assert(CodeCache::find_blob_unsafe(frame_pcs[0]) != NULL, "bad pc");
 493 
 494 #if INCLUDE_JVMCI
 495   if (exceptionObject() != NULL) {
 496     thread-&gt;set_exception_oop(exceptionObject());
 497     exec_mode = Unpack_exception;
 498   }
 499 #endif
 500 
 501   if (thread-&gt;frames_to_pop_failed_realloc() &gt; 0 &amp;&amp; exec_mode != Unpack_uncommon_trap) {
 502     assert(thread-&gt;has_pending_exception(), "should have thrown OOME");
 503     thread-&gt;set_exception_oop(thread-&gt;pending_exception());
 504     thread-&gt;clear_pending_exception();
 505     exec_mode = Unpack_exception;
 506   }
 507 
 508 #if INCLUDE_JVMCI
 509   if (thread-&gt;frames_to_pop_failed_realloc() &gt; 0) {
 510     thread-&gt;set_pending_monitorenter(false);
 511   }
 512 #endif
 513 
 514   UnrollBlock* info = new UnrollBlock(array-&gt;frame_size() * BytesPerWord,
 515                                       caller_adjustment * BytesPerWord,
 516                                       caller_was_method_handle ? 0 : callee_parameters,
 517                                       number_of_frames,
 518                                       frame_sizes,
 519                                       frame_pcs,
 520                                       return_type,
 521                                       exec_mode);
 522   // On some platforms, we need a way to pass some platform dependent
 523   // information to the unpacking code so the skeletal frames come out
 524   // correct (initial fp value, unextended sp, ...)
 525   info-&gt;set_initial_info((intptr_t) array-&gt;sender().initial_deoptimization_info());
 526 
 527   if (array-&gt;frames() &gt; 1) {
 528     if (VerifyStack &amp;&amp; TraceDeoptimization) {
 529       ttyLocker ttyl;
 530       tty-&gt;print_cr("Deoptimizing method containing inlining");
 531     }
 532   }
 533 
 534   array-&gt;set_unroll_block(info);
 535   return info;
 536 }
 537 
 538 // Called to cleanup deoptimization data structures in normal case
 539 // after unpacking to stack and when stack overflow error occurs
 540 void Deoptimization::cleanup_deopt_info(JavaThread *thread,
 541                                         vframeArray *array) {
 542 
 543   // Get array if coming from exception
 544   if (array == NULL) {
 545     array = thread-&gt;vframe_array_head();
 546   }
 547   thread-&gt;set_vframe_array_head(NULL);
 548 
 549   // Free the previous UnrollBlock
 550   vframeArray* old_array = thread-&gt;vframe_array_last();
 551   thread-&gt;set_vframe_array_last(array);
 552 
 553   if (old_array != NULL) {
 554     UnrollBlock* old_info = old_array-&gt;unroll_block();
 555     old_array-&gt;set_unroll_block(NULL);
 556     delete old_info;
 557     delete old_array;
 558   }
 559 
 560   // Deallocate any resource creating in this routine and any ResourceObjs allocated
 561   // inside the vframeArray (StackValueCollections)
 562 
 563   delete thread-&gt;deopt_mark();
 564   thread-&gt;set_deopt_mark(NULL);
 565   thread-&gt;set_deopt_compiled_method(NULL);
 566 
 567 
 568   if (JvmtiExport::can_pop_frame()) {
 569 #ifndef CC_INTERP
 570     // Regardless of whether we entered this routine with the pending
 571     // popframe condition bit set, we should always clear it now
 572     thread-&gt;clear_popframe_condition();
 573 #else
 574     // C++ interpreter will clear has_pending_popframe when it enters
 575     // with method_resume. For deopt_resume2 we clear it now.
 576     if (thread-&gt;popframe_forcing_deopt_reexecution())
 577         thread-&gt;clear_popframe_condition();
 578 #endif /* CC_INTERP */
 579   }
 580 
 581   // unpack_frames() is called at the end of the deoptimization handler
 582   // and (in C2) at the end of the uncommon trap handler. Note this fact
 583   // so that an asynchronous stack walker can work again. This counter is
 584   // incremented at the beginning of fetch_unroll_info() and (in C2) at
 585   // the beginning of uncommon_trap().
 586   thread-&gt;dec_in_deopt_handler();
 587 }
 588 
 589 // Moved from cpu directories because none of the cpus has callee save values.
 590 // If a cpu implements callee save values, move this to deoptimization_&lt;cpu&gt;.cpp.
 591 void Deoptimization::unwind_callee_save_values(frame* f, vframeArray* vframe_array) {
 592 
 593   // This code is sort of the equivalent of C2IAdapter::setup_stack_frame back in
 594   // the days we had adapter frames. When we deoptimize a situation where a
 595   // compiled caller calls a compiled caller will have registers it expects
 596   // to survive the call to the callee. If we deoptimize the callee the only
 597   // way we can restore these registers is to have the oldest interpreter
 598   // frame that we create restore these values. That is what this routine
 599   // will accomplish.
 600 
 601   // At the moment we have modified c2 to not have any callee save registers
 602   // so this problem does not exist and this routine is just a place holder.
 603 
 604   assert(f-&gt;is_interpreted_frame(), "must be interpreted");
 605 }
 606 
 607 // Return BasicType of value being returned
 608 JRT_LEAF(BasicType, Deoptimization::unpack_frames(JavaThread* thread, int exec_mode))
 609 
 610   // We are already active in the special DeoptResourceMark any ResourceObj's we
 611   // allocate will be freed at the end of the routine.
 612 
 613   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 614   // but makes the entry a little slower. There is however a little dance we have to
 615   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 616   ResetNoHandleMark rnhm; // No-op in release/product versions
 617   HandleMark hm;
 618 
 619   frame stub_frame = thread-&gt;last_frame();
 620 
 621   // Since the frame to unpack is the top frame of this thread, the vframe_array_head
 622   // must point to the vframeArray for the unpack frame.
 623   vframeArray* array = thread-&gt;vframe_array_head();
 624 
 625 #ifndef PRODUCT
 626   if (TraceDeoptimization) {
 627     ttyLocker ttyl;
 628     tty-&gt;print_cr("DEOPT UNPACKING thread " INTPTR_FORMAT " vframeArray " INTPTR_FORMAT " mode %d",
 629                   p2i(thread), p2i(array), exec_mode);
 630   }
 631 #endif
 632   Events::log(thread, "DEOPT UNPACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT " mode %d",
 633               p2i(stub_frame.pc()), p2i(stub_frame.sp()), exec_mode);
 634 
 635   UnrollBlock* info = array-&gt;unroll_block();
 636 
 637   // Unpack the interpreter frames and any adapter frame (c2 only) we might create.
 638   array-&gt;unpack_to_stack(stub_frame, exec_mode, info-&gt;caller_actual_parameters());
 639 
 640   BasicType bt = info-&gt;return_type();
 641 
 642   // If we have an exception pending, claim that the return type is an oop
 643   // so the deopt_blob does not overwrite the exception_oop.
 644 
 645   if (exec_mode == Unpack_exception)
 646     bt = T_OBJECT;
 647 
 648   // Cleanup thread deopt data
 649   cleanup_deopt_info(thread, array);
 650 
 651 #ifndef PRODUCT
 652   if (VerifyStack) {
 653     ResourceMark res_mark;
 654     // Clear pending exception to not break verification code (restored afterwards)
 655     PRESERVE_EXCEPTION_MARK;
 656 
 657     thread-&gt;validate_frame_layout();
 658 
 659     // Verify that the just-unpacked frames match the interpreter's
 660     // notions of expression stack and locals
 661     vframeArray* cur_array = thread-&gt;vframe_array_last();
 662     RegisterMap rm(thread, false);
 663     rm.set_include_argument_oops(false);
 664     bool is_top_frame = true;
 665     int callee_size_of_parameters = 0;
 666     int callee_max_locals = 0;
 667     for (int i = 0; i &lt; cur_array-&gt;frames(); i++) {
 668       vframeArrayElement* el = cur_array-&gt;element(i);
 669       frame* iframe = el-&gt;iframe();
 670       guarantee(iframe-&gt;is_interpreted_frame(), "Wrong frame type");
 671 
 672       // Get the oop map for this bci
 673       InterpreterOopMap mask;
 674       int cur_invoke_parameter_size = 0;
 675       bool try_next_mask = false;
 676       int next_mask_expression_stack_size = -1;
 677       int top_frame_expression_stack_adjustment = 0;
 678       methodHandle mh(thread, iframe-&gt;interpreter_frame_method());
 679       OopMapCache::compute_one_oop_map(mh, iframe-&gt;interpreter_frame_bci(), &amp;mask);
 680       BytecodeStream str(mh);
 681       str.set_start(iframe-&gt;interpreter_frame_bci());
 682       int max_bci = mh-&gt;code_size();
 683       // Get to the next bytecode if possible
 684       assert(str.bci() &lt; max_bci, "bci in interpreter frame out of bounds");
 685       // Check to see if we can grab the number of outgoing arguments
 686       // at an uncommon trap for an invoke (where the compiler
 687       // generates debug info before the invoke has executed)
 688       Bytecodes::Code cur_code = str.next();
 689       if (Bytecodes::is_invoke(cur_code)) {
 690         Bytecode_invoke invoke(mh, iframe-&gt;interpreter_frame_bci());
 691         cur_invoke_parameter_size = invoke.size_of_parameters();
 692         if (i != 0 &amp;&amp; !invoke.is_invokedynamic() &amp;&amp; MethodHandles::has_member_arg(invoke.klass(), invoke.name())) {
 693           callee_size_of_parameters++;
 694         }
 695       }
 696       if (str.bci() &lt; max_bci) {
 697         Bytecodes::Code next_code = str.next();
 698         if (next_code &gt;= 0) {
 699           // The interpreter oop map generator reports results before
 700           // the current bytecode has executed except in the case of
 701           // calls. It seems to be hard to tell whether the compiler
 702           // has emitted debug information matching the "state before"
 703           // a given bytecode or the state after, so we try both
 704           if (!Bytecodes::is_invoke(cur_code) &amp;&amp; cur_code != Bytecodes::_athrow) {
 705             // Get expression stack size for the next bytecode
 706             InterpreterOopMap next_mask;
 707             OopMapCache::compute_one_oop_map(mh, str.bci(), &amp;next_mask);
 708             next_mask_expression_stack_size = next_mask.expression_stack_size();
 709             if (Bytecodes::is_invoke(next_code)) {
 710               Bytecode_invoke invoke(mh, str.bci());
 711               next_mask_expression_stack_size += invoke.size_of_parameters();
 712             }
 713             // Need to subtract off the size of the result type of
 714             // the bytecode because this is not described in the
 715             // debug info but returned to the interpreter in the TOS
 716             // caching register
 717             BasicType bytecode_result_type = Bytecodes::result_type(cur_code);
 718             if (bytecode_result_type != T_ILLEGAL) {
 719               top_frame_expression_stack_adjustment = type2size[bytecode_result_type];
 720             }
 721             assert(top_frame_expression_stack_adjustment &gt;= 0, "stack adjustment must be positive");
 722             try_next_mask = true;
 723           }
 724         }
 725       }
 726 
 727       // Verify stack depth and oops in frame
 728       // This assertion may be dependent on the platform we're running on and may need modification (tested on x86 and sparc)
 729       if (!(
 730             /* SPARC */
 731             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_size_of_parameters) ||
 732             /* x86 */
 733             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_max_locals) ||
 734             (try_next_mask &amp;&amp;
 735              (iframe-&gt;interpreter_frame_expression_stack_size() == (next_mask_expression_stack_size -
 736                                                                     top_frame_expression_stack_adjustment))) ||
 737             (is_top_frame &amp;&amp; (exec_mode == Unpack_exception) &amp;&amp; iframe-&gt;interpreter_frame_expression_stack_size() == 0) ||
 738             (is_top_frame &amp;&amp; (exec_mode == Unpack_uncommon_trap || exec_mode == Unpack_reexecute || el-&gt;should_reexecute()) &amp;&amp;
 739              (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + cur_invoke_parameter_size))
 740             )) {
 741         {
 742           ttyLocker ttyl;
 743 
 744           // Print out some information that will help us debug the problem
 745           tty-&gt;print_cr("Wrong number of expression stack elements during deoptimization");
 746           tty-&gt;print_cr("  Error occurred while verifying frame %d (0..%d, 0 is topmost)", i, cur_array-&gt;frames() - 1);
 747           tty-&gt;print_cr("  Fabricated interpreter frame had %d expression stack elements",
 748                         iframe-&gt;interpreter_frame_expression_stack_size());
 749           tty-&gt;print_cr("  Interpreter oop map had %d expression stack elements", mask.expression_stack_size());
 750           tty-&gt;print_cr("  try_next_mask = %d", try_next_mask);
 751           tty-&gt;print_cr("  next_mask_expression_stack_size = %d", next_mask_expression_stack_size);
 752           tty-&gt;print_cr("  callee_size_of_parameters = %d", callee_size_of_parameters);
 753           tty-&gt;print_cr("  callee_max_locals = %d", callee_max_locals);
 754           tty-&gt;print_cr("  top_frame_expression_stack_adjustment = %d", top_frame_expression_stack_adjustment);
 755           tty-&gt;print_cr("  exec_mode = %d", exec_mode);
 756           tty-&gt;print_cr("  cur_invoke_parameter_size = %d", cur_invoke_parameter_size);
 757           tty-&gt;print_cr("  Thread = " INTPTR_FORMAT ", thread ID = %d", p2i(thread), thread-&gt;osthread()-&gt;thread_id());
 758           tty-&gt;print_cr("  Interpreted frames:");
 759           for (int k = 0; k &lt; cur_array-&gt;frames(); k++) {
 760             vframeArrayElement* el = cur_array-&gt;element(k);
 761             tty-&gt;print_cr("    %s (bci %d)", el-&gt;method()-&gt;name_and_sig_as_C_string(), el-&gt;bci());
 762           }
 763           cur_array-&gt;print_on_2(tty);
 764         } // release tty lock before calling guarantee
 765         guarantee(false, "wrong number of expression stack elements during deopt");
 766       }
 767       VerifyOopClosure verify;
 768       iframe-&gt;oops_interpreted_do(&amp;verify, &amp;rm, false);
 769       callee_size_of_parameters = mh-&gt;size_of_parameters();
 770       callee_max_locals = mh-&gt;max_locals();
 771       is_top_frame = false;
 772     }
 773   }
 774 #endif /* !PRODUCT */
 775 
 776 
 777   return bt;
 778 JRT_END
 779 
 780 
 781 int Deoptimization::deoptimize_dependents() {
 782   Threads::deoptimized_wrt_marked_nmethods();
 783   return 0;
 784 }
 785 
 786 Deoptimization::DeoptAction Deoptimization::_unloaded_action
 787   = Deoptimization::Action_reinterpret;
 788 
 789 #if COMPILER2_OR_JVMCI
<a name="2" id="anc2"></a><span class="changed"> 790 bool Deoptimization::realloc_objects(JavaThread* thread, frame* fr, GrowableArray&lt;ScopeValue*&gt;* objects, TRAPS) {</span>
 791   Handle pending_exception(THREAD, thread-&gt;pending_exception());
 792   const char* exception_file = thread-&gt;exception_file();
 793   int exception_line = thread-&gt;exception_line();
 794   thread-&gt;clear_pending_exception();
 795 
 796   bool failures = false;
 797 
 798   for (int i = 0; i &lt; objects-&gt;length(); i++) {
 799     assert(objects-&gt;at(i)-&gt;is_object(), "invalid debug information");
 800     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
 801 
 802     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
<a name="3" id="anc3"></a><span class="changed"> 803     oop obj = NULL;</span>




 804 
<a name="4" id="anc4"></a>
 805     if (k-&gt;is_instance_klass()) {
 806       InstanceKlass* ik = InstanceKlass::cast(k);
 807       obj = ik-&gt;allocate_instance(THREAD);
 808     } else if (k-&gt;is_typeArray_klass()) {
 809       TypeArrayKlass* ak = TypeArrayKlass::cast(k);
 810       assert(sv-&gt;field_size() % type2size[ak-&gt;element_type()] == 0, "non-integral array length");
 811       int len = sv-&gt;field_size() / type2size[ak-&gt;element_type()];
 812       obj = ak-&gt;allocate(len, THREAD);
 813     } else if (k-&gt;is_objArray_klass()) {
 814       ObjArrayKlass* ak = ObjArrayKlass::cast(k);
 815       obj = ak-&gt;allocate(sv-&gt;field_size(), THREAD);
 816     }
<a name="5" id="anc5"></a>
 817 
 818     if (obj == NULL) {
 819       failures = true;
 820     }
 821 
 822     assert(sv-&gt;value().is_null(), "redundant reallocation");
 823     assert(obj != NULL || HAS_PENDING_EXCEPTION, "allocation should succeed or we should get an exception");
 824     CLEAR_PENDING_EXCEPTION;
 825     sv-&gt;set_value(obj);
 826   }
 827 
 828   if (failures) {
 829     THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), failures);
 830   } else if (pending_exception.not_null()) {
 831     thread-&gt;set_pending_exception(pending_exception(), exception_file, exception_line);
 832   }
 833 
 834   return failures;
 835 }
 836 
 837 // restore elements of an eliminated type array
 838 void Deoptimization::reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type) {
 839   int index = 0;
 840   intptr_t val;
 841 
 842   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 843     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
<a name="6" id="anc6"></a>




 844     switch(type) {
 845     case T_LONG: case T_DOUBLE: {
 846       assert(value-&gt;type() == T_INT, "Agreement.");
 847       StackValue* low =
 848         StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 849 #ifdef _LP64
 850       jlong res = (jlong)low-&gt;get_int();
 851 #else
 852 #ifdef SPARC
 853       // For SPARC we have to swap high and low words.
 854       jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 855 #else
 856       jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 857 #endif //SPARC
 858 #endif
 859       obj-&gt;long_at_put(index, res);
 860       break;
 861     }
 862 
 863     // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 864     case T_INT: case T_FLOAT: { // 4 bytes.
 865       assert(value-&gt;type() == T_INT, "Agreement.");
 866       bool big_value = false;
 867       if (i + 1 &lt; sv-&gt;field_size() &amp;&amp; type == T_INT) {
 868         if (sv-&gt;field_at(i)-&gt;is_location()) {
 869           Location::Type type = ((LocationValue*) sv-&gt;field_at(i))-&gt;location().type();
 870           if (type == Location::dbl || type == Location::lng) {
 871             big_value = true;
 872           }
 873         } else if (sv-&gt;field_at(i)-&gt;is_constant_int()) {
 874           ScopeValue* next_scope_field = sv-&gt;field_at(i + 1);
 875           if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 876             big_value = true;
 877           }
 878         }
 879       }
 880 
 881       if (big_value) {
 882         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 883   #ifdef _LP64
 884         jlong res = (jlong)low-&gt;get_int();
 885   #else
 886   #ifdef SPARC
 887         // For SPARC we have to swap high and low words.
 888         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 889   #else
 890         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 891   #endif //SPARC
 892   #endif
 893         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;res));
 894         obj-&gt;int_at_put(++index, (jint)*(((jint*)&amp;res) + 1));
 895       } else {
 896         val = value-&gt;get_int();
 897         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;val));
 898       }
 899       break;
 900     }
 901 
 902     case T_SHORT:
 903       assert(value-&gt;type() == T_INT, "Agreement.");
 904       val = value-&gt;get_int();
 905       obj-&gt;short_at_put(index, (jshort)*((jint*)&amp;val));
 906       break;
 907 
 908     case T_CHAR:
 909       assert(value-&gt;type() == T_INT, "Agreement.");
 910       val = value-&gt;get_int();
 911       obj-&gt;char_at_put(index, (jchar)*((jint*)&amp;val));
 912       break;
 913 
 914     case T_BYTE:
 915       assert(value-&gt;type() == T_INT, "Agreement.");
 916       val = value-&gt;get_int();
 917       obj-&gt;byte_at_put(index, (jbyte)*((jint*)&amp;val));
 918       break;
 919 
 920     case T_BOOLEAN:
 921       assert(value-&gt;type() == T_INT, "Agreement.");
 922       val = value-&gt;get_int();
 923       obj-&gt;bool_at_put(index, (jboolean)*((jint*)&amp;val));
 924       break;
 925 
 926       default:
 927         ShouldNotReachHere();
 928     }
 929     index++;
 930   }
 931 }
 932 
 933 
 934 // restore fields of an eliminated object array
 935 void Deoptimization::reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj) {
 936   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 937     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
<a name="7" id="anc7"></a>



 938     assert(value-&gt;type() == T_OBJECT, "object element expected");
 939     obj-&gt;obj_at_put(i, value-&gt;get_obj()());
 940   }
 941 }
 942 
 943 class ReassignedField {
 944 public:
 945   int _offset;
 946   BasicType _type;
 947 public:
 948   ReassignedField() {
 949     _offset = 0;
 950     _type = T_ILLEGAL;
 951   }
 952 };
 953 
 954 int compare(ReassignedField* left, ReassignedField* right) {
 955   return left-&gt;_offset - right-&gt;_offset;
 956 }
 957 
 958 // Restore fields of an eliminated instance object using the same field order
 959 // returned by HotSpotResolvedObjectTypeImpl.getInstanceFields(true)
 960 static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {
 961   if (klass-&gt;superklass() != NULL) {
 962     svIndex = reassign_fields_by_klass(klass-&gt;superklass(), fr, reg_map, sv, svIndex, obj, skip_internal);
 963   }
 964 
 965   GrowableArray&lt;ReassignedField&gt;* fields = new GrowableArray&lt;ReassignedField&gt;();
 966   for (AllFieldStream fs(klass); !fs.done(); fs.next()) {
 967     if (!fs.access_flags().is_static() &amp;&amp; (!skip_internal || !fs.access_flags().is_internal())) {
 968       ReassignedField field;
 969       field._offset = fs.offset();
 970       field._type = FieldType::basic_type(fs.signature());
 971       fields-&gt;append(field);
 972     }
 973   }
 974   fields-&gt;sort(compare);
 975   for (int i = 0; i &lt; fields-&gt;length(); i++) {
 976     intptr_t val;
 977     ScopeValue* scope_field = sv-&gt;field_at(svIndex);
 978     StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);
 979     int offset = fields-&gt;at(i)._offset;
 980     BasicType type = fields-&gt;at(i)._type;
<a name="8" id="anc8"></a>




 981     switch (type) {
 982       case T_OBJECT: case T_ARRAY:
 983         assert(value-&gt;type() == T_OBJECT, "Agreement.");
 984         obj-&gt;obj_field_put(offset, value-&gt;get_obj()());
 985         break;
 986 
 987       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 988       case T_INT: case T_FLOAT: { // 4 bytes.
 989         assert(value-&gt;type() == T_INT, "Agreement.");
 990         bool big_value = false;
 991         if (i+1 &lt; fields-&gt;length() &amp;&amp; fields-&gt;at(i+1)._type == T_INT) {
 992           if (scope_field-&gt;is_location()) {
 993             Location::Type type = ((LocationValue*) scope_field)-&gt;location().type();
 994             if (type == Location::dbl || type == Location::lng) {
 995               big_value = true;
 996             }
 997           }
 998           if (scope_field-&gt;is_constant_int()) {
 999             ScopeValue* next_scope_field = sv-&gt;field_at(svIndex + 1);
1000             if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
1001               big_value = true;
1002             }
1003           }
1004         }
1005 
1006         if (big_value) {
1007           i++;
1008           assert(i &lt; fields-&gt;length(), "second T_INT field needed");
1009           assert(fields-&gt;at(i)._type == T_INT, "T_INT field needed");
1010         } else {
1011           val = value-&gt;get_int();
1012           obj-&gt;int_field_put(offset, (jint)*((jint*)&amp;val));
1013           break;
1014         }
1015       }
1016         /* no break */
1017 
1018       case T_LONG: case T_DOUBLE: {
1019         assert(value-&gt;type() == T_INT, "Agreement.");
1020         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++svIndex));
1021 #ifdef _LP64
1022         jlong res = (jlong)low-&gt;get_int();
1023 #else
1024 #ifdef SPARC
1025         // For SPARC we have to swap high and low words.
1026         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
1027 #else
1028         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
1029 #endif //SPARC
1030 #endif
1031         obj-&gt;long_field_put(offset, res);
1032         break;
1033       }
1034 
1035       case T_SHORT:
1036         assert(value-&gt;type() == T_INT, "Agreement.");
1037         val = value-&gt;get_int();
1038         obj-&gt;short_field_put(offset, (jshort)*((jint*)&amp;val));
1039         break;
1040 
1041       case T_CHAR:
1042         assert(value-&gt;type() == T_INT, "Agreement.");
1043         val = value-&gt;get_int();
1044         obj-&gt;char_field_put(offset, (jchar)*((jint*)&amp;val));
1045         break;
1046 
1047       case T_BYTE:
1048         assert(value-&gt;type() == T_INT, "Agreement.");
1049         val = value-&gt;get_int();
1050         obj-&gt;byte_field_put(offset, (jbyte)*((jint*)&amp;val));
1051         break;
1052 
1053       case T_BOOLEAN:
1054         assert(value-&gt;type() == T_INT, "Agreement.");
1055         val = value-&gt;get_int();
1056         obj-&gt;bool_field_put(offset, (jboolean)*((jint*)&amp;val));
1057         break;
1058 
1059       default:
1060         ShouldNotReachHere();
1061     }
1062     svIndex++;
1063   }
1064   return svIndex;
1065 }
1066 
1067 // restore fields of all eliminated objects and arrays
1068 void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures, bool skip_internal) {
1069   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1070     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1071     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
1072     Handle obj = sv-&gt;value();
1073     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1074     if (PrintDeoptimizationDetails) {
1075       tty-&gt;print_cr("reassign fields for object of type %s!", k-&gt;name()-&gt;as_C_string());
1076     }
1077     if (obj.is_null()) {
1078       continue;
1079     }
1080 
1081     if (k-&gt;is_instance_klass()) {
1082       InstanceKlass* ik = InstanceKlass::cast(k);
1083       reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);
1084     } else if (k-&gt;is_typeArray_klass()) {
1085       TypeArrayKlass* ak = TypeArrayKlass::cast(k);
1086       reassign_type_array_elements(fr, reg_map, sv, (typeArrayOop) obj(), ak-&gt;element_type());
1087     } else if (k-&gt;is_objArray_klass()) {
1088       reassign_object_array_elements(fr, reg_map, sv, (objArrayOop) obj());
1089     }
1090   }
1091 }
1092 
1093 
1094 // relock objects for which synchronization was eliminated
1095 void Deoptimization::relock_objects(GrowableArray&lt;MonitorInfo*&gt;* monitors, JavaThread* thread, bool realloc_failures) {
1096   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1097     MonitorInfo* mon_info = monitors-&gt;at(i);
1098     if (mon_info-&gt;eliminated()) {
1099       assert(!mon_info-&gt;owner_is_scalar_replaced() || realloc_failures, "reallocation was missed");
1100       if (!mon_info-&gt;owner_is_scalar_replaced()) {
1101         Handle obj(thread, mon_info-&gt;owner());
1102         markOop mark = obj-&gt;mark();
1103         if (UseBiasedLocking &amp;&amp; mark-&gt;has_bias_pattern()) {
1104           // New allocated objects may have the mark set to anonymously biased.
1105           // Also the deoptimized method may called methods with synchronization
1106           // where the thread-local object is bias locked to the current thread.
1107           assert(mark-&gt;is_biased_anonymously() ||
1108                  mark-&gt;biased_locker() == thread, "should be locked to current thread");
1109           // Reset mark word to unbiased prototype.
1110           markOop unbiased_prototype = markOopDesc::prototype()-&gt;set_age(mark-&gt;age());
1111           obj-&gt;set_mark(unbiased_prototype);
1112         }
1113         BasicLock* lock = mon_info-&gt;lock();
1114         ObjectSynchronizer::slow_enter(obj, lock, thread);
1115         assert(mon_info-&gt;owner()-&gt;is_locked(), "object must be locked now");
1116       }
1117     }
1118   }
1119 }
1120 
1121 
1122 #ifndef PRODUCT
1123 // print information about reallocated objects
1124 void Deoptimization::print_objects(GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures) {
1125   fieldDescriptor fd;
1126 
1127   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1128     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1129     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
1130     Handle obj = sv-&gt;value();
1131 
1132     tty-&gt;print("     object &lt;" INTPTR_FORMAT "&gt; of type ", p2i(sv-&gt;value()()));
1133     k-&gt;print_value();
1134     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1135     if (obj.is_null()) {
1136       tty-&gt;print(" allocation failed");
1137     } else {
1138       tty-&gt;print(" allocated (%d bytes)", obj-&gt;size() * HeapWordSize);
1139     }
1140     tty-&gt;cr();
1141 
1142     if (Verbose &amp;&amp; !obj.is_null()) {
1143       k-&gt;oop_print_on(obj(), tty);
1144     }
1145   }
1146 }
1147 #endif
1148 #endif // COMPILER2_OR_JVMCI
1149 
1150 vframeArray* Deoptimization::create_vframeArray(JavaThread* thread, frame fr, RegisterMap *reg_map, GrowableArray&lt;compiledVFrame*&gt;* chunk, bool realloc_failures) {
1151   Events::log(thread, "DEOPT PACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT, p2i(fr.pc()), p2i(fr.sp()));
1152 
1153 #ifndef PRODUCT
1154   if (PrintDeoptimizationDetails) {
1155     ttyLocker ttyl;
1156     tty-&gt;print("DEOPT PACKING thread " INTPTR_FORMAT " ", p2i(thread));
1157     fr.print_on(tty);
1158     tty-&gt;print_cr("     Virtual frames (innermost first):");
1159     for (int index = 0; index &lt; chunk-&gt;length(); index++) {
1160       compiledVFrame* vf = chunk-&gt;at(index);
1161       tty-&gt;print("       %2d - ", index);
1162       vf-&gt;print_value();
1163       int bci = chunk-&gt;at(index)-&gt;raw_bci();
1164       const char* code_name;
1165       if (bci == SynchronizationEntryBCI) {
1166         code_name = "sync entry";
1167       } else {
1168         Bytecodes::Code code = vf-&gt;method()-&gt;code_at(bci);
1169         code_name = Bytecodes::name(code);
1170       }
1171       tty-&gt;print(" - %s", code_name);
1172       tty-&gt;print_cr(" @ bci %d ", bci);
1173       if (Verbose) {
1174         vf-&gt;print();
1175         tty-&gt;cr();
1176       }
1177     }
1178   }
1179 #endif
1180 
1181   // Register map for next frame (used for stack crawl).  We capture
1182   // the state of the deopt'ing frame's caller.  Thus if we need to
1183   // stuff a C2I adapter we can properly fill in the callee-save
1184   // register locations.
1185   frame caller = fr.sender(reg_map);
1186   int frame_size = caller.sp() - fr.sp();
1187 
1188   frame sender = caller;
1189 
1190   // Since the Java thread being deoptimized will eventually adjust it's own stack,
1191   // the vframeArray containing the unpacking information is allocated in the C heap.
1192   // For Compiler1, the caller of the deoptimized frame is saved for use by unpack_frames().
1193   vframeArray* array = vframeArray::allocate(thread, frame_size, chunk, reg_map, sender, caller, fr, realloc_failures);
1194 
1195   // Compare the vframeArray to the collected vframes
1196   assert(array-&gt;structural_compare(thread, chunk), "just checking");
1197 
1198 #ifndef PRODUCT
1199   if (PrintDeoptimizationDetails) {
1200     ttyLocker ttyl;
1201     tty-&gt;print_cr("     Created vframeArray " INTPTR_FORMAT, p2i(array));
1202   }
1203 #endif // PRODUCT
1204 
1205   return array;
1206 }
1207 
1208 #if COMPILER2_OR_JVMCI
1209 void Deoptimization::pop_frames_failed_reallocs(JavaThread* thread, vframeArray* array) {
1210   // Reallocation of some scalar replaced objects failed. Record
1211   // that we need to pop all the interpreter frames for the
1212   // deoptimized compiled frame.
1213   assert(thread-&gt;frames_to_pop_failed_realloc() == 0, "missed frames to pop?");
1214   thread-&gt;set_frames_to_pop_failed_realloc(array-&gt;frames());
1215   // Unlock all monitors here otherwise the interpreter will see a
1216   // mix of locked and unlocked monitors (because of failed
1217   // reallocations of synchronized objects) and be confused.
1218   for (int i = 0; i &lt; array-&gt;frames(); i++) {
1219     MonitorChunk* monitors = array-&gt;element(i)-&gt;monitors();
1220     if (monitors != NULL) {
1221       for (int j = 0; j &lt; monitors-&gt;number_of_monitors(); j++) {
1222         BasicObjectLock* src = monitors-&gt;at(j);
1223         if (src-&gt;obj() != NULL) {
1224           ObjectSynchronizer::fast_exit(src-&gt;obj(), src-&gt;lock(), thread);
1225         }
1226       }
1227       array-&gt;element(i)-&gt;free_monitors(thread);
1228 #ifdef ASSERT
1229       array-&gt;element(i)-&gt;set_removed_monitors();
1230 #endif
1231     }
1232   }
1233 }
1234 #endif
1235 
1236 static void collect_monitors(compiledVFrame* cvf, GrowableArray&lt;Handle&gt;* objects_to_revoke) {
1237   GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
1238   Thread* thread = Thread::current();
1239   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1240     MonitorInfo* mon_info = monitors-&gt;at(i);
1241     if (!mon_info-&gt;eliminated() &amp;&amp; mon_info-&gt;owner() != NULL) {
1242       objects_to_revoke-&gt;append(Handle(thread, mon_info-&gt;owner()));
1243     }
1244   }
1245 }
1246 
1247 
1248 void Deoptimization::revoke_biases_of_monitors(JavaThread* thread, frame fr, RegisterMap* map) {
1249   if (!UseBiasedLocking) {
1250     return;
1251   }
1252 
1253   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1254 
1255   // Unfortunately we don't have a RegisterMap available in most of
1256   // the places we want to call this routine so we need to walk the
1257   // stack again to update the register map.
1258   if (map == NULL || !map-&gt;update_map()) {
1259     StackFrameStream sfs(thread, true);
1260     bool found = false;
1261     while (!found &amp;&amp; !sfs.is_done()) {
1262       frame* cur = sfs.current();
1263       sfs.next();
1264       found = cur-&gt;id() == fr.id();
1265     }
1266     assert(found, "frame to be deoptimized not found on target thread's stack");
1267     map = sfs.register_map();
1268   }
1269 
1270   vframe* vf = vframe::new_vframe(&amp;fr, map, thread);
1271   compiledVFrame* cvf = compiledVFrame::cast(vf);
1272   // Revoke monitors' biases in all scopes
1273   while (!cvf-&gt;is_top()) {
1274     collect_monitors(cvf, objects_to_revoke);
1275     cvf = compiledVFrame::cast(cvf-&gt;sender());
1276   }
1277   collect_monitors(cvf, objects_to_revoke);
1278 
1279   if (SafepointSynchronize::is_at_safepoint()) {
1280     BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1281   } else {
1282     BiasedLocking::revoke(objects_to_revoke);
1283   }
1284 }
1285 
1286 
1287 void Deoptimization::revoke_biases_of_monitors(CodeBlob* cb) {
1288   if (!UseBiasedLocking) {
1289     return;
1290   }
1291 
1292   assert(SafepointSynchronize::is_at_safepoint(), "must only be called from safepoint");
1293   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1294   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
1295     if (jt-&gt;has_last_Java_frame()) {
1296       StackFrameStream sfs(jt, true);
1297       while (!sfs.is_done()) {
1298         frame* cur = sfs.current();
1299         if (cb-&gt;contains(cur-&gt;pc())) {
1300           vframe* vf = vframe::new_vframe(cur, sfs.register_map(), jt);
1301           compiledVFrame* cvf = compiledVFrame::cast(vf);
1302           // Revoke monitors' biases in all scopes
1303           while (!cvf-&gt;is_top()) {
1304             collect_monitors(cvf, objects_to_revoke);
1305             cvf = compiledVFrame::cast(cvf-&gt;sender());
1306           }
1307           collect_monitors(cvf, objects_to_revoke);
1308         }
1309         sfs.next();
1310       }
1311     }
1312   }
1313   BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1314 }
1315 
1316 
1317 void Deoptimization::deoptimize_single_frame(JavaThread* thread, frame fr, Deoptimization::DeoptReason reason) {
1318   assert(fr.can_be_deoptimized(), "checking frame type");
1319 
1320   gather_statistics(reason, Action_none, Bytecodes::_illegal);
1321 
1322   if (LogCompilation &amp;&amp; xtty != NULL) {
1323     CompiledMethod* cm = fr.cb()-&gt;as_compiled_method_or_null();
1324     assert(cm != NULL, "only compiled methods can deopt");
1325 
1326     ttyLocker ttyl;
1327     xtty-&gt;begin_head("deoptimized thread='" UINTX_FORMAT "' reason='%s' pc='" INTPTR_FORMAT "'",(uintx)thread-&gt;osthread()-&gt;thread_id(), trap_reason_name(reason), p2i(fr.pc()));
1328     cm-&gt;log_identity(xtty);
1329     xtty-&gt;end_head();
1330     for (ScopeDesc* sd = cm-&gt;scope_desc_at(fr.pc()); ; sd = sd-&gt;sender()) {
1331       xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1332       xtty-&gt;method(sd-&gt;method());
1333       xtty-&gt;end_elem();
1334       if (sd-&gt;is_top())  break;
1335     }
1336     xtty-&gt;tail("deoptimized");
1337   }
1338 
1339   // Patch the compiled method so that when execution returns to it we will
1340   // deopt the execution state and return to the interpreter.
1341   fr.deoptimize(thread);
1342 }
1343 
1344 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map) {
1345   deoptimize(thread, fr, map, Reason_constraint);
1346 }
1347 
1348 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map, DeoptReason reason) {
1349   // Deoptimize only if the frame comes from compile code.
1350   // Do not deoptimize the frame which is already patched
1351   // during the execution of the loops below.
1352   if (!fr.is_compiled_frame() || fr.is_deoptimized_frame()) {
1353     return;
1354   }
1355   ResourceMark rm;
1356   DeoptimizationMarker dm;
1357   if (UseBiasedLocking) {
1358     revoke_biases_of_monitors(thread, fr, map);
1359   }
1360   deoptimize_single_frame(thread, fr, reason);
1361 
1362 }
1363 
1364 #if INCLUDE_JVMCI
1365 address Deoptimization::deoptimize_for_missing_exception_handler(CompiledMethod* cm) {
1366   // there is no exception handler for this pc =&gt; deoptimize
1367   cm-&gt;make_not_entrant();
1368 
1369   // Use Deoptimization::deoptimize for all of its side-effects:
1370   // revoking biases of monitors, gathering traps statistics, logging...
1371   // it also patches the return pc but we do not care about that
1372   // since we return a continuation to the deopt_blob below.
1373   JavaThread* thread = JavaThread::current();
1374   RegisterMap reg_map(thread, UseBiasedLocking);
1375   frame runtime_frame = thread-&gt;last_frame();
1376   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1377   assert(caller_frame.cb()-&gt;as_compiled_method_or_null() == cm, "expect top frame compiled method");
1378   Deoptimization::deoptimize(thread, caller_frame, &amp;reg_map, Deoptimization::Reason_not_compiled_exception_handler);
1379 
1380   MethodData* trap_mdo = get_method_data(thread, cm-&gt;method(), true);
1381   if (trap_mdo != NULL) {
1382     trap_mdo-&gt;inc_trap_count(Deoptimization::Reason_not_compiled_exception_handler);
1383   }
1384 
1385   return SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
1386 }
1387 #endif
1388 
1389 void Deoptimization::deoptimize_frame_internal(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1390   assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),
1391          "can only deoptimize other thread at a safepoint");
1392   // Compute frame and register map based on thread and sp.
1393   RegisterMap reg_map(thread, UseBiasedLocking);
1394   frame fr = thread-&gt;last_frame();
1395   while (fr.id() != id) {
1396     fr = fr.sender(&amp;reg_map);
1397   }
1398   deoptimize(thread, fr, &amp;reg_map, reason);
1399 }
1400 
1401 
1402 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1403   if (thread == Thread::current()) {
1404     Deoptimization::deoptimize_frame_internal(thread, id, reason);
1405   } else {
1406     VM_DeoptimizeFrame deopt(thread, id, reason);
1407     VMThread::execute(&amp;deopt);
1408   }
1409 }
1410 
1411 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id) {
1412   deoptimize_frame(thread, id, Reason_constraint);
1413 }
1414 
1415 // JVMTI PopFrame support
1416 JRT_LEAF(void, Deoptimization::popframe_preserve_args(JavaThread* thread, int bytes_to_save, void* start_address))
1417 {
1418   thread-&gt;popframe_preserve_args(in_ByteSize(bytes_to_save), start_address);
1419 }
1420 JRT_END
1421 
1422 MethodData*
1423 Deoptimization::get_method_data(JavaThread* thread, const methodHandle&amp; m,
1424                                 bool create_if_missing) {
1425   Thread* THREAD = thread;
1426   MethodData* mdo = m()-&gt;method_data();
1427   if (mdo == NULL &amp;&amp; create_if_missing &amp;&amp; !HAS_PENDING_EXCEPTION) {
1428     // Build an MDO.  Ignore errors like OutOfMemory;
1429     // that simply means we won't have an MDO to update.
1430     Method::build_interpreter_method_data(m, THREAD);
1431     if (HAS_PENDING_EXCEPTION) {
1432       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), "we expect only an OOM error here");
1433       CLEAR_PENDING_EXCEPTION;
1434     }
1435     mdo = m()-&gt;method_data();
1436   }
1437   return mdo;
1438 }
1439 
1440 #if COMPILER2_OR_JVMCI
1441 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index, TRAPS) {
1442   // in case of an unresolved klass entry, load the class.
1443   if (constant_pool-&gt;tag_at(index).is_unresolved_klass()) {
1444     Klass* tk = constant_pool-&gt;klass_at_ignore_error(index, CHECK);
1445     return;
1446   }
1447 
1448   if (!constant_pool-&gt;tag_at(index).is_symbol()) return;
1449 
1450   Handle class_loader (THREAD, constant_pool-&gt;pool_holder()-&gt;class_loader());
1451   Symbol*  symbol  = constant_pool-&gt;symbol_at(index);
1452 
1453   // class name?
1454   if (symbol-&gt;char_at(0) != '(') {
1455     Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1456     SystemDictionary::resolve_or_null(symbol, class_loader, protection_domain, CHECK);
1457     return;
1458   }
1459 
1460   // then it must be a signature!
1461   ResourceMark rm(THREAD);
1462   for (SignatureStream ss(symbol); !ss.is_done(); ss.next()) {
1463     if (ss.is_object()) {
1464       Symbol* class_name = ss.as_symbol(CHECK);
1465       Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1466       SystemDictionary::resolve_or_null(class_name, class_loader, protection_domain, CHECK);
1467     }
1468   }
1469 }
1470 
1471 
1472 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index) {
1473   EXCEPTION_MARK;
1474   load_class_by_index(constant_pool, index, THREAD);
1475   if (HAS_PENDING_EXCEPTION) {
1476     // Exception happened during classloading. We ignore the exception here, since it
1477     // is going to be rethrown since the current activation is going to be deoptimized and
1478     // the interpreter will re-execute the bytecode.
1479     CLEAR_PENDING_EXCEPTION;
1480     // Class loading called java code which may have caused a stack
1481     // overflow. If the exception was thrown right before the return
1482     // to the runtime the stack is no longer guarded. Reguard the
1483     // stack otherwise if we return to the uncommon trap blob and the
1484     // stack bang causes a stack overflow we crash.
1485     assert(THREAD-&gt;is_Java_thread(), "only a java thread can be here");
1486     JavaThread* thread = (JavaThread*)THREAD;
1487     bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
1488     if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
1489     assert(guard_pages_enabled, "stack banging in uncommon trap blob may cause crash");
1490   }
1491 }
1492 
1493 JRT_ENTRY(void, Deoptimization::uncommon_trap_inner(JavaThread* thread, jint trap_request)) {
1494   HandleMark hm;
1495 
1496   // uncommon_trap() is called at the beginning of the uncommon trap
1497   // handler. Note this fact before we start generating temporary frames
1498   // that can confuse an asynchronous stack walker. This counter is
1499   // decremented at the end of unpack_frames().
1500   thread-&gt;inc_in_deopt_handler();
1501 
1502   // We need to update the map if we have biased locking.
1503 #if INCLUDE_JVMCI
1504   // JVMCI might need to get an exception from the stack, which in turn requires the register map to be valid
1505   RegisterMap reg_map(thread, true);
1506 #else
1507   RegisterMap reg_map(thread, UseBiasedLocking);
1508 #endif
1509   frame stub_frame = thread-&gt;last_frame();
1510   frame fr = stub_frame.sender(&amp;reg_map);
1511   // Make sure the calling nmethod is not getting deoptimized and removed
1512   // before we are done with it.
1513   nmethodLocker nl(fr.pc());
1514 
1515   // Log a message
1516   Events::log(thread, "Uncommon trap: trap_request=" PTR32_FORMAT " fr.pc=" INTPTR_FORMAT " relative=" INTPTR_FORMAT,
1517               trap_request, p2i(fr.pc()), fr.pc() - fr.cb()-&gt;code_begin());
1518 
1519   {
1520     ResourceMark rm;
1521 
1522     // Revoke biases of any monitors in the frame to ensure we can migrate them
1523     revoke_biases_of_monitors(thread, fr, &amp;reg_map);
1524 
1525     DeoptReason reason = trap_request_reason(trap_request);
1526     DeoptAction action = trap_request_action(trap_request);
1527 #if INCLUDE_JVMCI
1528     int debug_id = trap_request_debug_id(trap_request);
1529 #endif
1530     jint unloaded_class_index = trap_request_index(trap_request); // CP idx or -1
1531 
1532     vframe*  vf  = vframe::new_vframe(&amp;fr, &amp;reg_map, thread);
1533     compiledVFrame* cvf = compiledVFrame::cast(vf);
1534 
1535     CompiledMethod* nm = cvf-&gt;code();
1536 
1537     ScopeDesc*      trap_scope  = cvf-&gt;scope();
1538 
1539     if (TraceDeoptimization) {
1540       ttyLocker ttyl;
1541       tty-&gt;print_cr("  bci=%d pc=" INTPTR_FORMAT ", relative_pc=" INTPTR_FORMAT ", method=%s" JVMCI_ONLY(", debug_id=%d"), trap_scope-&gt;bci(), p2i(fr.pc()), fr.pc() - nm-&gt;code_begin(), trap_scope-&gt;method()-&gt;name_and_sig_as_C_string()
1542 #if INCLUDE_JVMCI
1543           , debug_id
1544 #endif
1545           );
1546     }
1547 
1548     methodHandle    trap_method = trap_scope-&gt;method();
1549     int             trap_bci    = trap_scope-&gt;bci();
1550 #if INCLUDE_JVMCI
1551     long speculation = thread-&gt;pending_failed_speculation();
1552     if (nm-&gt;is_compiled_by_jvmci()) {
1553       if (speculation != 0) {
1554         oop speculation_log = nm-&gt;as_nmethod()-&gt;speculation_log();
1555         if (speculation_log != NULL) {
1556           if (TraceDeoptimization || TraceUncollectedSpeculations) {
1557             if (HotSpotSpeculationLog::lastFailed(speculation_log) != 0) {
1558               tty-&gt;print_cr("A speculation that was not collected by the compiler is being overwritten");
1559             }
1560           }
1561           if (TraceDeoptimization) {
1562             tty-&gt;print_cr("Saving speculation to speculation log");
1563           }
1564           HotSpotSpeculationLog::set_lastFailed(speculation_log, speculation);
1565         } else {
1566           if (TraceDeoptimization) {
1567             tty-&gt;print_cr("Speculation present but no speculation log");
1568           }
1569         }
1570         thread-&gt;set_pending_failed_speculation(0);
1571       } else {
1572         if (TraceDeoptimization) {
1573           tty-&gt;print_cr("No speculation");
1574         }
1575       }
1576     } else {
1577       assert(speculation == 0, "There should not be a speculation for method compiled by non-JVMCI compilers");
1578     }
1579 
1580     if (trap_bci == SynchronizationEntryBCI) {
1581       trap_bci = 0;
1582       thread-&gt;set_pending_monitorenter(true);
1583     }
1584 
1585     if (reason == Deoptimization::Reason_transfer_to_interpreter) {
1586       thread-&gt;set_pending_transfer_to_interpreter(true);
1587     }
1588 #endif
1589 
1590     Bytecodes::Code trap_bc     = trap_method-&gt;java_code_at(trap_bci);
1591     // Record this event in the histogram.
1592     gather_statistics(reason, action, trap_bc);
1593 
1594     // Ensure that we can record deopt. history:
1595     // Need MDO to record RTM code generation state.
1596     bool create_if_missing = ProfileTraps || UseCodeAging RTM_OPT_ONLY( || UseRTMLocking );
1597 
1598     methodHandle profiled_method;
1599 #if INCLUDE_JVMCI
1600     if (nm-&gt;is_compiled_by_jvmci()) {
1601       profiled_method = nm-&gt;method();
1602     } else {
1603       profiled_method = trap_method;
1604     }
1605 #else
1606     profiled_method = trap_method;
1607 #endif
1608 
1609     MethodData* trap_mdo =
1610       get_method_data(thread, profiled_method, create_if_missing);
1611 
1612     // Log a message
1613     Events::log_deopt_message(thread, "Uncommon trap: reason=%s action=%s pc=" INTPTR_FORMAT " method=%s @ %d %s",
1614                               trap_reason_name(reason), trap_action_name(action), p2i(fr.pc()),
1615                               trap_method-&gt;name_and_sig_as_C_string(), trap_bci, nm-&gt;compiler_name());
1616 
1617     // Print a bunch of diagnostics, if requested.
1618     if (TraceDeoptimization || LogCompilation) {
1619       ResourceMark rm;
1620       ttyLocker ttyl;
1621       char buf[100];
1622       if (xtty != NULL) {
1623         xtty-&gt;begin_head("uncommon_trap thread='" UINTX_FORMAT "' %s",
1624                          os::current_thread_id(),
1625                          format_trap_request(buf, sizeof(buf), trap_request));
1626         nm-&gt;log_identity(xtty);
1627       }
1628       Symbol* class_name = NULL;
1629       bool unresolved = false;
1630       if (unloaded_class_index &gt;= 0) {
1631         constantPoolHandle constants (THREAD, trap_method-&gt;constants());
1632         if (constants-&gt;tag_at(unloaded_class_index).is_unresolved_klass()) {
1633           class_name = constants-&gt;klass_name_at(unloaded_class_index);
1634           unresolved = true;
1635           if (xtty != NULL)
1636             xtty-&gt;print(" unresolved='1'");
1637         } else if (constants-&gt;tag_at(unloaded_class_index).is_symbol()) {
1638           class_name = constants-&gt;symbol_at(unloaded_class_index);
1639         }
1640         if (xtty != NULL)
1641           xtty-&gt;name(class_name);
1642       }
1643       if (xtty != NULL &amp;&amp; trap_mdo != NULL &amp;&amp; (int)reason &lt; (int)MethodData::_trap_hist_limit) {
1644         // Dump the relevant MDO state.
1645         // This is the deopt count for the current reason, any previous
1646         // reasons or recompiles seen at this point.
1647         int dcnt = trap_mdo-&gt;trap_count(reason);
1648         if (dcnt != 0)
1649           xtty-&gt;print(" count='%d'", dcnt);
1650         ProfileData* pdata = trap_mdo-&gt;bci_to_data(trap_bci);
1651         int dos = (pdata == NULL)? 0: pdata-&gt;trap_state();
1652         if (dos != 0) {
1653           xtty-&gt;print(" state='%s'", format_trap_state(buf, sizeof(buf), dos));
1654           if (trap_state_is_recompiled(dos)) {
1655             int recnt2 = trap_mdo-&gt;overflow_recompile_count();
1656             if (recnt2 != 0)
1657               xtty-&gt;print(" recompiles2='%d'", recnt2);
1658           }
1659         }
1660       }
1661       if (xtty != NULL) {
1662         xtty-&gt;stamp();
1663         xtty-&gt;end_head();
1664       }
1665       if (TraceDeoptimization) {  // make noise on the tty
1666         tty-&gt;print("Uncommon trap occurred in");
1667         nm-&gt;method()-&gt;print_short_name(tty);
1668         tty-&gt;print(" compiler=%s compile_id=%d", nm-&gt;compiler_name(), nm-&gt;compile_id());
1669 #if INCLUDE_JVMCI
1670         if (nm-&gt;is_nmethod()) {
1671           char* installed_code_name = nm-&gt;as_nmethod()-&gt;jvmci_installed_code_name(buf, sizeof(buf));
1672           if (installed_code_name != NULL) {
1673             tty-&gt;print(" (JVMCI: installed code name=%s) ", installed_code_name);
1674           }
1675         }
1676 #endif
1677         tty-&gt;print(" (@" INTPTR_FORMAT ") thread=" UINTX_FORMAT " reason=%s action=%s unloaded_class_index=%d" JVMCI_ONLY(" debug_id=%d"),
1678                    p2i(fr.pc()),
1679                    os::current_thread_id(),
1680                    trap_reason_name(reason),
1681                    trap_action_name(action),
1682                    unloaded_class_index
1683 #if INCLUDE_JVMCI
1684                    , debug_id
1685 #endif
1686                    );
1687         if (class_name != NULL) {
1688           tty-&gt;print(unresolved ? " unresolved class: " : " symbol: ");
1689           class_name-&gt;print_symbol_on(tty);
1690         }
1691         tty-&gt;cr();
1692       }
1693       if (xtty != NULL) {
1694         // Log the precise location of the trap.
1695         for (ScopeDesc* sd = trap_scope; ; sd = sd-&gt;sender()) {
1696           xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1697           xtty-&gt;method(sd-&gt;method());
1698           xtty-&gt;end_elem();
1699           if (sd-&gt;is_top())  break;
1700         }
1701         xtty-&gt;tail("uncommon_trap");
1702       }
1703     }
1704     // (End diagnostic printout.)
1705 
1706     // Load class if necessary
1707     if (unloaded_class_index &gt;= 0) {
1708       constantPoolHandle constants(THREAD, trap_method-&gt;constants());
1709       load_class_by_index(constants, unloaded_class_index);
1710     }
1711 
1712     // Flush the nmethod if necessary and desirable.
1713     //
1714     // We need to avoid situations where we are re-flushing the nmethod
1715     // because of a hot deoptimization site.  Repeated flushes at the same
1716     // point need to be detected by the compiler and avoided.  If the compiler
1717     // cannot avoid them (or has a bug and "refuses" to avoid them), this
1718     // module must take measures to avoid an infinite cycle of recompilation
1719     // and deoptimization.  There are several such measures:
1720     //
1721     //   1. If a recompilation is ordered a second time at some site X
1722     //   and for the same reason R, the action is adjusted to 'reinterpret',
1723     //   to give the interpreter time to exercise the method more thoroughly.
1724     //   If this happens, the method's overflow_recompile_count is incremented.
1725     //
1726     //   2. If the compiler fails to reduce the deoptimization rate, then
1727     //   the method's overflow_recompile_count will begin to exceed the set
1728     //   limit PerBytecodeRecompilationCutoff.  If this happens, the action
1729     //   is adjusted to 'make_not_compilable', and the method is abandoned
1730     //   to the interpreter.  This is a performance hit for hot methods,
1731     //   but is better than a disastrous infinite cycle of recompilations.
1732     //   (Actually, only the method containing the site X is abandoned.)
1733     //
1734     //   3. In parallel with the previous measures, if the total number of
1735     //   recompilations of a method exceeds the much larger set limit
1736     //   PerMethodRecompilationCutoff, the method is abandoned.
1737     //   This should only happen if the method is very large and has
1738     //   many "lukewarm" deoptimizations.  The code which enforces this
1739     //   limit is elsewhere (class nmethod, class Method).
1740     //
1741     // Note that the per-BCI 'is_recompiled' bit gives the compiler one chance
1742     // to recompile at each bytecode independently of the per-BCI cutoff.
1743     //
1744     // The decision to update code is up to the compiler, and is encoded
1745     // in the Action_xxx code.  If the compiler requests Action_none
1746     // no trap state is changed, no compiled code is changed, and the
1747     // computation suffers along in the interpreter.
1748     //
1749     // The other action codes specify various tactics for decompilation
1750     // and recompilation.  Action_maybe_recompile is the loosest, and
1751     // allows the compiled code to stay around until enough traps are seen,
1752     // and until the compiler gets around to recompiling the trapping method.
1753     //
1754     // The other actions cause immediate removal of the present code.
1755 
1756     // Traps caused by injected profile shouldn't pollute trap counts.
1757     bool injected_profile_trap = trap_method-&gt;has_injected_profile() &amp;&amp;
1758                                  (reason == Reason_intrinsic || reason == Reason_unreached);
1759 
1760     bool update_trap_state = (reason != Reason_tenured) &amp;&amp; !injected_profile_trap;
1761     bool make_not_entrant = false;
1762     bool make_not_compilable = false;
1763     bool reprofile = false;
1764     switch (action) {
1765     case Action_none:
1766       // Keep the old code.
1767       update_trap_state = false;
1768       break;
1769     case Action_maybe_recompile:
1770       // Do not need to invalidate the present code, but we can
1771       // initiate another
1772       // Start compiler without (necessarily) invalidating the nmethod.
1773       // The system will tolerate the old code, but new code should be
1774       // generated when possible.
1775       break;
1776     case Action_reinterpret:
1777       // Go back into the interpreter for a while, and then consider
1778       // recompiling form scratch.
1779       make_not_entrant = true;
1780       // Reset invocation counter for outer most method.
1781       // This will allow the interpreter to exercise the bytecodes
1782       // for a while before recompiling.
1783       // By contrast, Action_make_not_entrant is immediate.
1784       //
1785       // Note that the compiler will track null_check, null_assert,
1786       // range_check, and class_check events and log them as if they
1787       // had been traps taken from compiled code.  This will update
1788       // the MDO trap history so that the next compilation will
1789       // properly detect hot trap sites.
1790       reprofile = true;
1791       break;
1792     case Action_make_not_entrant:
1793       // Request immediate recompilation, and get rid of the old code.
1794       // Make them not entrant, so next time they are called they get
1795       // recompiled.  Unloaded classes are loaded now so recompile before next
1796       // time they are called.  Same for uninitialized.  The interpreter will
1797       // link the missing class, if any.
1798       make_not_entrant = true;
1799       break;
1800     case Action_make_not_compilable:
1801       // Give up on compiling this method at all.
1802       make_not_entrant = true;
1803       make_not_compilable = true;
1804       break;
1805     default:
1806       ShouldNotReachHere();
1807     }
1808 
1809     // Setting +ProfileTraps fixes the following, on all platforms:
1810     // 4852688: ProfileInterpreter is off by default for ia64.  The result is
1811     // infinite heroic-opt-uncommon-trap/deopt/recompile cycles, since the
1812     // recompile relies on a MethodData* to record heroic opt failures.
1813 
1814     // Whether the interpreter is producing MDO data or not, we also need
1815     // to use the MDO to detect hot deoptimization points and control
1816     // aggressive optimization.
1817     bool inc_recompile_count = false;
1818     ProfileData* pdata = NULL;
1819     if (ProfileTraps &amp;&amp; !is_client_compilation_mode_vm() &amp;&amp; update_trap_state &amp;&amp; trap_mdo != NULL) {
1820       assert(trap_mdo == get_method_data(thread, profiled_method, false), "sanity");
1821       uint this_trap_count = 0;
1822       bool maybe_prior_trap = false;
1823       bool maybe_prior_recompile = false;
1824       pdata = query_update_method_data(trap_mdo, trap_bci, reason, true,
1825 #if INCLUDE_JVMCI
1826                                    nm-&gt;is_compiled_by_jvmci() &amp;&amp; nm-&gt;is_osr_method(),
1827 #endif
1828                                    nm-&gt;method(),
1829                                    //outputs:
1830                                    this_trap_count,
1831                                    maybe_prior_trap,
1832                                    maybe_prior_recompile);
1833       // Because the interpreter also counts null, div0, range, and class
1834       // checks, these traps from compiled code are double-counted.
1835       // This is harmless; it just means that the PerXTrapLimit values
1836       // are in effect a little smaller than they look.
1837 
1838       DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1839       if (per_bc_reason != Reason_none) {
1840         // Now take action based on the partially known per-BCI history.
1841         if (maybe_prior_trap
1842             &amp;&amp; this_trap_count &gt;= (uint)PerBytecodeTrapLimit) {
1843           // If there are too many traps at this BCI, force a recompile.
1844           // This will allow the compiler to see the limit overflow, and
1845           // take corrective action, if possible.  The compiler generally
1846           // does not use the exact PerBytecodeTrapLimit value, but instead
1847           // changes its tactics if it sees any traps at all.  This provides
1848           // a little hysteresis, delaying a recompile until a trap happens
1849           // several times.
1850           //
1851           // Actually, since there is only one bit of counter per BCI,
1852           // the possible per-BCI counts are {0,1,(per-method count)}.
1853           // This produces accurate results if in fact there is only
1854           // one hot trap site, but begins to get fuzzy if there are
1855           // many sites.  For example, if there are ten sites each
1856           // trapping two or more times, they each get the blame for
1857           // all of their traps.
1858           make_not_entrant = true;
1859         }
1860 
1861         // Detect repeated recompilation at the same BCI, and enforce a limit.
1862         if (make_not_entrant &amp;&amp; maybe_prior_recompile) {
1863           // More than one recompile at this point.
1864           inc_recompile_count = maybe_prior_trap;
1865         }
1866       } else {
1867         // For reasons which are not recorded per-bytecode, we simply
1868         // force recompiles unconditionally.
1869         // (Note that PerMethodRecompilationCutoff is enforced elsewhere.)
1870         make_not_entrant = true;
1871       }
1872 
1873       // Go back to the compiler if there are too many traps in this method.
1874       if (this_trap_count &gt;= per_method_trap_limit(reason)) {
1875         // If there are too many traps in this method, force a recompile.
1876         // This will allow the compiler to see the limit overflow, and
1877         // take corrective action, if possible.
1878         // (This condition is an unlikely backstop only, because the
1879         // PerBytecodeTrapLimit is more likely to take effect first,
1880         // if it is applicable.)
1881         make_not_entrant = true;
1882       }
1883 
1884       // Here's more hysteresis:  If there has been a recompile at
1885       // this trap point already, run the method in the interpreter
1886       // for a while to exercise it more thoroughly.
1887       if (make_not_entrant &amp;&amp; maybe_prior_recompile &amp;&amp; maybe_prior_trap) {
1888         reprofile = true;
1889       }
1890     }
1891 
1892     // Take requested actions on the method:
1893 
1894     // Recompile
1895     if (make_not_entrant) {
1896       if (!nm-&gt;make_not_entrant()) {
1897         return; // the call did not change nmethod's state
1898       }
1899 
1900       if (pdata != NULL) {
1901         // Record the recompilation event, if any.
1902         int tstate0 = pdata-&gt;trap_state();
1903         int tstate1 = trap_state_set_recompiled(tstate0, true);
1904         if (tstate1 != tstate0)
1905           pdata-&gt;set_trap_state(tstate1);
1906       }
1907 
1908 #if INCLUDE_RTM_OPT
1909       // Restart collecting RTM locking abort statistic if the method
1910       // is recompiled for a reason other than RTM state change.
1911       // Assume that in new recompiled code the statistic could be different,
1912       // for example, due to different inlining.
1913       if ((reason != Reason_rtm_state_change) &amp;&amp; (trap_mdo != NULL) &amp;&amp;
1914           UseRTMDeopt &amp;&amp; (nm-&gt;as_nmethod()-&gt;rtm_state() != ProfileRTM)) {
1915         trap_mdo-&gt;atomic_set_rtm_state(ProfileRTM);
1916       }
1917 #endif
1918       // For code aging we count traps separately here, using make_not_entrant()
1919       // as a guard against simultaneous deopts in multiple threads.
1920       if (reason == Reason_tenured &amp;&amp; trap_mdo != NULL) {
1921         trap_mdo-&gt;inc_tenure_traps();
1922       }
1923     }
1924 
1925     if (inc_recompile_count) {
1926       trap_mdo-&gt;inc_overflow_recompile_count();
1927       if ((uint)trap_mdo-&gt;overflow_recompile_count() &gt;
1928           (uint)PerBytecodeRecompilationCutoff) {
1929         // Give up on the method containing the bad BCI.
1930         if (trap_method() == nm-&gt;method()) {
1931           make_not_compilable = true;
1932         } else {
1933           trap_method-&gt;set_not_compilable(CompLevel_full_optimization, true, "overflow_recompile_count &gt; PerBytecodeRecompilationCutoff");
1934           // But give grace to the enclosing nm-&gt;method().
1935         }
1936       }
1937     }
1938 
1939     // Reprofile
1940     if (reprofile) {
1941       CompilationPolicy::policy()-&gt;reprofile(trap_scope, nm-&gt;is_osr_method());
1942     }
1943 
1944     // Give up compiling
1945     if (make_not_compilable &amp;&amp; !nm-&gt;method()-&gt;is_not_compilable(CompLevel_full_optimization)) {
1946       assert(make_not_entrant, "consistent");
1947       nm-&gt;method()-&gt;set_not_compilable(CompLevel_full_optimization);
1948     }
1949 
1950   } // Free marked resources
1951 
1952 }
1953 JRT_END
1954 
1955 ProfileData*
1956 Deoptimization::query_update_method_data(MethodData* trap_mdo,
1957                                          int trap_bci,
1958                                          Deoptimization::DeoptReason reason,
1959                                          bool update_total_trap_count,
1960 #if INCLUDE_JVMCI
1961                                          bool is_osr,
1962 #endif
1963                                          Method* compiled_method,
1964                                          //outputs:
1965                                          uint&amp; ret_this_trap_count,
1966                                          bool&amp; ret_maybe_prior_trap,
1967                                          bool&amp; ret_maybe_prior_recompile) {
1968   bool maybe_prior_trap = false;
1969   bool maybe_prior_recompile = false;
1970   uint this_trap_count = 0;
1971   if (update_total_trap_count) {
1972     uint idx = reason;
1973 #if INCLUDE_JVMCI
1974     if (is_osr) {
1975       idx += Reason_LIMIT;
1976     }
1977 #endif
1978     uint prior_trap_count = trap_mdo-&gt;trap_count(idx);
1979     this_trap_count  = trap_mdo-&gt;inc_trap_count(idx);
1980 
1981     // If the runtime cannot find a place to store trap history,
1982     // it is estimated based on the general condition of the method.
1983     // If the method has ever been recompiled, or has ever incurred
1984     // a trap with the present reason , then this BCI is assumed
1985     // (pessimistically) to be the culprit.
1986     maybe_prior_trap      = (prior_trap_count != 0);
1987     maybe_prior_recompile = (trap_mdo-&gt;decompile_count() != 0);
1988   }
1989   ProfileData* pdata = NULL;
1990 
1991 
1992   // For reasons which are recorded per bytecode, we check per-BCI data.
1993   DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1994   assert(per_bc_reason != Reason_none || update_total_trap_count, "must be");
1995   if (per_bc_reason != Reason_none) {
1996     // Find the profile data for this BCI.  If there isn't one,
1997     // try to allocate one from the MDO's set of spares.
1998     // This will let us detect a repeated trap at this point.
1999     pdata = trap_mdo-&gt;allocate_bci_to_data(trap_bci, reason_is_speculate(reason) ? compiled_method : NULL);
2000 
2001     if (pdata != NULL) {
2002       if (reason_is_speculate(reason) &amp;&amp; !pdata-&gt;is_SpeculativeTrapData()) {
2003         if (LogCompilation &amp;&amp; xtty != NULL) {
2004           ttyLocker ttyl;
2005           // no more room for speculative traps in this MDO
2006           xtty-&gt;elem("speculative_traps_oom");
2007         }
2008       }
2009       // Query the trap state of this profile datum.
2010       int tstate0 = pdata-&gt;trap_state();
2011       if (!trap_state_has_reason(tstate0, per_bc_reason))
2012         maybe_prior_trap = false;
2013       if (!trap_state_is_recompiled(tstate0))
2014         maybe_prior_recompile = false;
2015 
2016       // Update the trap state of this profile datum.
2017       int tstate1 = tstate0;
2018       // Record the reason.
2019       tstate1 = trap_state_add_reason(tstate1, per_bc_reason);
2020       // Store the updated state on the MDO, for next time.
2021       if (tstate1 != tstate0)
2022         pdata-&gt;set_trap_state(tstate1);
2023     } else {
2024       if (LogCompilation &amp;&amp; xtty != NULL) {
2025         ttyLocker ttyl;
2026         // Missing MDP?  Leave a small complaint in the log.
2027         xtty-&gt;elem("missing_mdp bci='%d'", trap_bci);
2028       }
2029     }
2030   }
2031 
2032   // Return results:
2033   ret_this_trap_count = this_trap_count;
2034   ret_maybe_prior_trap = maybe_prior_trap;
2035   ret_maybe_prior_recompile = maybe_prior_recompile;
2036   return pdata;
2037 }
2038 
2039 void
2040 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2041   ResourceMark rm;
2042   // Ignored outputs:
2043   uint ignore_this_trap_count;
2044   bool ignore_maybe_prior_trap;
2045   bool ignore_maybe_prior_recompile;
2046   assert(!reason_is_speculate(reason), "reason speculate only used by compiler");
2047   // JVMCI uses the total counts to determine if deoptimizations are happening too frequently -&gt; do not adjust total counts
2048   bool update_total_counts = true JVMCI_ONLY( &amp;&amp; !UseJVMCICompiler);
2049   query_update_method_data(trap_mdo, trap_bci,
2050                            (DeoptReason)reason,
2051                            update_total_counts,
2052 #if INCLUDE_JVMCI
2053                            false,
2054 #endif
2055                            NULL,
2056                            ignore_this_trap_count,
2057                            ignore_maybe_prior_trap,
2058                            ignore_maybe_prior_recompile);
2059 }
2060 
2061 Deoptimization::UnrollBlock* Deoptimization::uncommon_trap(JavaThread* thread, jint trap_request, jint exec_mode) {
2062   if (TraceDeoptimization) {
2063     tty-&gt;print("Uncommon trap ");
2064   }
2065   // Still in Java no safepoints
2066   {
2067     // This enters VM and may safepoint
2068     uncommon_trap_inner(thread, trap_request);
2069   }
2070   return fetch_unroll_info_helper(thread, exec_mode);
2071 }
2072 
2073 // Local derived constants.
2074 // Further breakdown of DataLayout::trap_state, as promised by DataLayout.
2075 const int DS_REASON_MASK   = ((uint)DataLayout::trap_mask) &gt;&gt; 1;
2076 const int DS_RECOMPILE_BIT = DataLayout::trap_mask - DS_REASON_MASK;
2077 
2078 //---------------------------trap_state_reason---------------------------------
2079 Deoptimization::DeoptReason
2080 Deoptimization::trap_state_reason(int trap_state) {
2081   // This assert provides the link between the width of DataLayout::trap_bits
2082   // and the encoding of "recorded" reasons.  It ensures there are enough
2083   // bits to store all needed reasons in the per-BCI MDO profile.
2084   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2085   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2086   trap_state -= recompile_bit;
2087   if (trap_state == DS_REASON_MASK) {
2088     return Reason_many;
2089   } else {
2090     assert((int)Reason_none == 0, "state=0 =&gt; Reason_none");
2091     return (DeoptReason)trap_state;
2092   }
2093 }
2094 //-------------------------trap_state_has_reason-------------------------------
2095 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2096   assert(reason_is_recorded_per_bytecode((DeoptReason)reason), "valid reason");
2097   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2098   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2099   trap_state -= recompile_bit;
2100   if (trap_state == DS_REASON_MASK) {
2101     return -1;  // true, unspecifically (bottom of state lattice)
2102   } else if (trap_state == reason) {
2103     return 1;   // true, definitely
2104   } else if (trap_state == 0) {
2105     return 0;   // false, definitely (top of state lattice)
2106   } else {
2107     return 0;   // false, definitely
2108   }
2109 }
2110 //-------------------------trap_state_add_reason-------------------------------
2111 int Deoptimization::trap_state_add_reason(int trap_state, int reason) {
2112   assert(reason_is_recorded_per_bytecode((DeoptReason)reason) || reason == Reason_many, "valid reason");
2113   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2114   trap_state -= recompile_bit;
2115   if (trap_state == DS_REASON_MASK) {
2116     return trap_state + recompile_bit;     // already at state lattice bottom
2117   } else if (trap_state == reason) {
2118     return trap_state + recompile_bit;     // the condition is already true
2119   } else if (trap_state == 0) {
2120     return reason + recompile_bit;          // no condition has yet been true
2121   } else {
2122     return DS_REASON_MASK + recompile_bit;  // fall to state lattice bottom
2123   }
2124 }
2125 //-----------------------trap_state_is_recompiled------------------------------
2126 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2127   return (trap_state &amp; DS_RECOMPILE_BIT) != 0;
2128 }
2129 //-----------------------trap_state_set_recompiled-----------------------------
2130 int Deoptimization::trap_state_set_recompiled(int trap_state, bool z) {
2131   if (z)  return trap_state |  DS_RECOMPILE_BIT;
2132   else    return trap_state &amp; ~DS_RECOMPILE_BIT;
2133 }
2134 //---------------------------format_trap_state---------------------------------
2135 // This is used for debugging and diagnostics, including LogFile output.
2136 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2137                                               int trap_state) {
2138   assert(buflen &gt; 0, "sanity");
2139   DeoptReason reason      = trap_state_reason(trap_state);
2140   bool        recomp_flag = trap_state_is_recompiled(trap_state);
2141   // Re-encode the state from its decoded components.
2142   int decoded_state = 0;
2143   if (reason_is_recorded_per_bytecode(reason) || reason == Reason_many)
2144     decoded_state = trap_state_add_reason(decoded_state, reason);
2145   if (recomp_flag)
2146     decoded_state = trap_state_set_recompiled(decoded_state, recomp_flag);
2147   // If the state re-encodes properly, format it symbolically.
2148   // Because this routine is used for debugging and diagnostics,
2149   // be robust even if the state is a strange value.
2150   size_t len;
2151   if (decoded_state != trap_state) {
2152     // Random buggy state that doesn't decode??
2153     len = jio_snprintf(buf, buflen, "#%d", trap_state);
2154   } else {
2155     len = jio_snprintf(buf, buflen, "%s%s",
2156                        trap_reason_name(reason),
2157                        recomp_flag ? " recompiled" : "");
2158   }
2159   return buf;
2160 }
2161 
2162 
2163 //--------------------------------statics--------------------------------------
2164 const char* Deoptimization::_trap_reason_name[] = {
2165   // Note:  Keep this in sync. with enum DeoptReason.
2166   "none",
2167   "null_check",
2168   "null_assert" JVMCI_ONLY("_or_unreached0"),
2169   "range_check",
2170   "class_check",
2171   "array_check",
2172   "intrinsic" JVMCI_ONLY("_or_type_checked_inlining"),
2173   "bimorphic" JVMCI_ONLY("_or_optimized_type_check"),
2174   "profile_predicate",
2175   "unloaded",
2176   "uninitialized",
2177   "unreached",
2178   "unhandled",
2179   "constraint",
2180   "div0_check",
2181   "age",
2182   "predicate",
2183   "loop_limit_check",
2184   "speculate_class_check",
2185   "speculate_null_check",
2186   "speculate_null_assert",
2187   "rtm_state_change",
2188   "unstable_if",
2189   "unstable_fused_if",
2190 #if INCLUDE_JVMCI
2191   "aliasing",
2192   "transfer_to_interpreter",
2193   "not_compiled_exception_handler",
2194   "unresolved",
2195   "jsr_mismatch",
2196 #endif
2197   "tenured"
2198 };
2199 const char* Deoptimization::_trap_action_name[] = {
2200   // Note:  Keep this in sync. with enum DeoptAction.
2201   "none",
2202   "maybe_recompile",
2203   "reinterpret",
2204   "make_not_entrant",
2205   "make_not_compilable"
2206 };
2207 
2208 const char* Deoptimization::trap_reason_name(int reason) {
2209   // Check that every reason has a name
2210   STATIC_ASSERT(sizeof(_trap_reason_name)/sizeof(const char*) == Reason_LIMIT);
2211 
2212   if (reason == Reason_many)  return "many";
2213   if ((uint)reason &lt; Reason_LIMIT)
2214     return _trap_reason_name[reason];
2215   static char buf[20];
2216   sprintf(buf, "reason%d", reason);
2217   return buf;
2218 }
2219 const char* Deoptimization::trap_action_name(int action) {
2220   // Check that every action has a name
2221   STATIC_ASSERT(sizeof(_trap_action_name)/sizeof(const char*) == Action_LIMIT);
2222 
2223   if ((uint)action &lt; Action_LIMIT)
2224     return _trap_action_name[action];
2225   static char buf[20];
2226   sprintf(buf, "action%d", action);
2227   return buf;
2228 }
2229 
2230 // This is used for debugging and diagnostics, including LogFile output.
2231 const char* Deoptimization::format_trap_request(char* buf, size_t buflen,
2232                                                 int trap_request) {
2233   jint unloaded_class_index = trap_request_index(trap_request);
2234   const char* reason = trap_reason_name(trap_request_reason(trap_request));
2235   const char* action = trap_action_name(trap_request_action(trap_request));
2236 #if INCLUDE_JVMCI
2237   int debug_id = trap_request_debug_id(trap_request);
2238 #endif
2239   size_t len;
2240   if (unloaded_class_index &lt; 0) {
2241     len = jio_snprintf(buf, buflen, "reason='%s' action='%s'" JVMCI_ONLY(" debug_id='%d'"),
2242                        reason, action
2243 #if INCLUDE_JVMCI
2244                        ,debug_id
2245 #endif
2246                        );
2247   } else {
2248     len = jio_snprintf(buf, buflen, "reason='%s' action='%s' index='%d'" JVMCI_ONLY(" debug_id='%d'"),
2249                        reason, action, unloaded_class_index
2250 #if INCLUDE_JVMCI
2251                        ,debug_id
2252 #endif
2253                        );
2254   }
2255   return buf;
2256 }
2257 
2258 juint Deoptimization::_deoptimization_hist
2259         [Deoptimization::Reason_LIMIT]
2260     [1 + Deoptimization::Action_LIMIT]
2261         [Deoptimization::BC_CASE_LIMIT]
2262   = {0};
2263 
2264 enum {
2265   LSB_BITS = 8,
2266   LSB_MASK = right_n_bits(LSB_BITS)
2267 };
2268 
2269 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2270                                        Bytecodes::Code bc) {
2271   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2272   assert(action &gt;= 0 &amp;&amp; action &lt; Action_LIMIT, "oob");
2273   _deoptimization_hist[Reason_none][0][0] += 1;  // total
2274   _deoptimization_hist[reason][0][0]      += 1;  // per-reason total
2275   juint* cases = _deoptimization_hist[reason][1+action];
2276   juint* bc_counter_addr = NULL;
2277   juint  bc_counter      = 0;
2278   // Look for an unused counter, or an exact match to this BC.
2279   if (bc != Bytecodes::_illegal) {
2280     for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2281       juint* counter_addr = &amp;cases[bc_case];
2282       juint  counter = *counter_addr;
2283       if ((counter == 0 &amp;&amp; bc_counter_addr == NULL)
2284           || (Bytecodes::Code)(counter &amp; LSB_MASK) == bc) {
2285         // this counter is either free or is already devoted to this BC
2286         bc_counter_addr = counter_addr;
2287         bc_counter = counter | bc;
2288       }
2289     }
2290   }
2291   if (bc_counter_addr == NULL) {
2292     // Overflow, or no given bytecode.
2293     bc_counter_addr = &amp;cases[BC_CASE_LIMIT-1];
2294     bc_counter = (*bc_counter_addr &amp; ~LSB_MASK);  // clear LSB
2295   }
2296   *bc_counter_addr = bc_counter + (1 &lt;&lt; LSB_BITS);
2297 }
2298 
2299 jint Deoptimization::total_deoptimization_count() {
2300   return _deoptimization_hist[Reason_none][0][0];
2301 }
2302 
2303 jint Deoptimization::deoptimization_count(DeoptReason reason) {
2304   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2305   return _deoptimization_hist[reason][0][0];
2306 }
2307 
2308 void Deoptimization::print_statistics() {
2309   juint total = total_deoptimization_count();
2310   juint account = total;
2311   if (total != 0) {
2312     ttyLocker ttyl;
2313     if (xtty != NULL)  xtty-&gt;head("statistics type='deoptimization'");
2314     tty-&gt;print_cr("Deoptimization traps recorded:");
2315     #define PRINT_STAT_LINE(name, r) \
2316       tty-&gt;print_cr("  %4d (%4.1f%%) %s", (int)(r), ((r) * 100.0) / total, name);
2317     PRINT_STAT_LINE("total", total);
2318     // For each non-zero entry in the histogram, print the reason,
2319     // the action, and (if specifically known) the type of bytecode.
2320     for (int reason = 0; reason &lt; Reason_LIMIT; reason++) {
2321       for (int action = 0; action &lt; Action_LIMIT; action++) {
2322         juint* cases = _deoptimization_hist[reason][1+action];
2323         for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2324           juint counter = cases[bc_case];
2325           if (counter != 0) {
2326             char name[1*K];
2327             Bytecodes::Code bc = (Bytecodes::Code)(counter &amp; LSB_MASK);
2328             if (bc_case == BC_CASE_LIMIT &amp;&amp; (int)bc == 0)
2329               bc = Bytecodes::_illegal;
2330             sprintf(name, "%s/%s/%s",
2331                     trap_reason_name(reason),
2332                     trap_action_name(action),
2333                     Bytecodes::is_defined(bc)? Bytecodes::name(bc): "other");
2334             juint r = counter &gt;&gt; LSB_BITS;
2335             tty-&gt;print_cr("  %40s: " UINT32_FORMAT " (%.1f%%)", name, r, (r * 100.0) / total);
2336             account -= r;
2337           }
2338         }
2339       }
2340     }
2341     if (account != 0) {
2342       PRINT_STAT_LINE("unaccounted", account);
2343     }
2344     #undef PRINT_STAT_LINE
2345     if (xtty != NULL)  xtty-&gt;tail("statistics");
2346   }
2347 }
2348 #else // COMPILER2_OR_JVMCI
2349 
2350 
2351 // Stubs for C1 only system.
2352 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2353   return false;
2354 }
2355 
2356 const char* Deoptimization::trap_reason_name(int reason) {
2357   return "unknown";
2358 }
2359 
2360 void Deoptimization::print_statistics() {
2361   // no output
2362 }
2363 
2364 void
2365 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2366   // no udpate
2367 }
2368 
2369 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2370   return 0;
2371 }
2372 
2373 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2374                                        Bytecodes::Code bc) {
2375   // no update
2376 }
2377 
2378 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2379                                               int trap_state) {
2380   jio_snprintf(buf, buflen, "#%d", trap_state);
2381   return buf;
2382 }
2383 
2384 #endif // COMPILER2_OR_JVMCI
<a name="9" id="anc9"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="9" type="hidden" /></form></body></html>
