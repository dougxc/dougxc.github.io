<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "jvm.h"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "code/codeCache.hpp"
  29 #include "code/debugInfoRec.hpp"
  30 #include "code/nmethod.hpp"
  31 #include "code/pcDesc.hpp"
  32 #include "code/scopeDesc.hpp"
  33 #include "interpreter/bytecode.hpp"
  34 #include "interpreter/interpreter.hpp"
  35 #include "interpreter/oopMapCache.hpp"
  36 #include "memory/allocation.inline.hpp"
  37 #include "memory/oopFactory.hpp"
  38 #include "memory/resourceArea.hpp"
  39 #include "oops/method.hpp"
  40 #include "oops/objArrayOop.inline.hpp"
  41 #include "oops/oop.inline.hpp"
  42 #include "oops/fieldStreams.hpp"
  43 #include "oops/typeArrayOop.inline.hpp"
  44 #include "oops/verifyOopClosure.hpp"
  45 #include "prims/jvmtiThreadState.hpp"
  46 #include "runtime/biasedLocking.hpp"
  47 #include "runtime/compilationPolicy.hpp"
  48 #include "runtime/deoptimization.hpp"
  49 #include "runtime/frame.inline.hpp"
  50 #include "runtime/interfaceSupport.inline.hpp"
  51 #include "runtime/safepointVerifiers.hpp"
  52 #include "runtime/sharedRuntime.hpp"
  53 #include "runtime/signature.hpp"
  54 #include "runtime/stubRoutines.hpp"
  55 #include "runtime/thread.hpp"
  56 #include "runtime/threadSMR.hpp"
  57 #include "runtime/vframe.hpp"
  58 #include "runtime/vframeArray.hpp"
  59 #include "runtime/vframe_hp.hpp"
  60 #include "utilities/events.hpp"
  61 #include "utilities/preserveException.hpp"
  62 #include "utilities/xmlstream.hpp"
  63 
  64 #if INCLUDE_JVMCI
  65 #include "jvmci/jvmciRuntime.hpp"
  66 #include "jvmci/jvmciJavaClasses.hpp"
  67 #endif
  68 
  69 
  70 bool DeoptimizationMarker::_is_active = false;
  71 
  72 Deoptimization::UnrollBlock::UnrollBlock(int  size_of_deoptimized_frame,
  73                                          int  caller_adjustment,
  74                                          int  caller_actual_parameters,
  75                                          int  number_of_frames,
  76                                          intptr_t* frame_sizes,
  77                                          address* frame_pcs,
  78                                          BasicType return_type,
  79                                          int exec_mode) {
  80   _size_of_deoptimized_frame = size_of_deoptimized_frame;
  81   _caller_adjustment         = caller_adjustment;
  82   _caller_actual_parameters  = caller_actual_parameters;
  83   _number_of_frames          = number_of_frames;
  84   _frame_sizes               = frame_sizes;
  85   _frame_pcs                 = frame_pcs;
  86   _register_block            = NEW_C_HEAP_ARRAY(intptr_t, RegisterMap::reg_count * 2, mtCompiler);
  87   _return_type               = return_type;
  88   _initial_info              = 0;
  89   // PD (x86 only)
  90   _counter_temp              = 0;
  91   _unpack_kind               = exec_mode;
  92   _sender_sp_temp            = 0;
  93 
  94   _total_frame_sizes         = size_of_frames();
  95   assert(exec_mode &gt;= 0 &amp;&amp; exec_mode &lt; Unpack_LIMIT, "Unexpected exec_mode");
  96 }
  97 
  98 
  99 Deoptimization::UnrollBlock::~UnrollBlock() {
 100   FREE_C_HEAP_ARRAY(intptr_t, _frame_sizes);
 101   FREE_C_HEAP_ARRAY(intptr_t, _frame_pcs);
 102   FREE_C_HEAP_ARRAY(intptr_t, _register_block);
 103 }
 104 
 105 
 106 intptr_t* Deoptimization::UnrollBlock::value_addr_at(int register_number) const {
 107   assert(register_number &lt; RegisterMap::reg_count, "checking register number");
 108   return &amp;_register_block[register_number * 2];
 109 }
 110 
 111 
 112 
 113 int Deoptimization::UnrollBlock::size_of_frames() const {
 114   // Acount first for the adjustment of the initial frame
 115   int result = _caller_adjustment;
 116   for (int index = 0; index &lt; number_of_frames(); index++) {
 117     result += frame_sizes()[index];
 118   }
 119   return result;
 120 }
 121 
 122 
 123 void Deoptimization::UnrollBlock::print() {
 124   ttyLocker ttyl;
 125   tty-&gt;print_cr("UnrollBlock");
 126   tty-&gt;print_cr("  size_of_deoptimized_frame = %d", _size_of_deoptimized_frame);
 127   tty-&gt;print(   "  frame_sizes: ");
 128   for (int index = 0; index &lt; number_of_frames(); index++) {
 129     tty-&gt;print(INTX_FORMAT " ", frame_sizes()[index]);
 130   }
 131   tty-&gt;cr();
 132 }
 133 
 134 
 135 // In order to make fetch_unroll_info work properly with escape
 136 // analysis, The method was changed from JRT_LEAF to JRT_BLOCK_ENTRY and
 137 // ResetNoHandleMark and HandleMark were removed from it. The actual reallocation
 138 // of previously eliminated objects occurs in realloc_objects, which is
 139 // called from the method fetch_unroll_info_helper below.
 140 JRT_BLOCK_ENTRY(Deoptimization::UnrollBlock*, Deoptimization::fetch_unroll_info(JavaThread* thread, int exec_mode))
 141   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 142   // but makes the entry a little slower. There is however a little dance we have to
 143   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 144 
 145   // fetch_unroll_info() is called at the beginning of the deoptimization
 146   // handler. Note this fact before we start generating temporary frames
 147   // that can confuse an asynchronous stack walker. This counter is
 148   // decremented at the end of unpack_frames().
 149   if (TraceDeoptimization) {
 150     tty-&gt;print_cr("Deoptimizing thread " INTPTR_FORMAT, p2i(thread));
 151   }
 152   thread-&gt;inc_in_deopt_handler();
 153 
 154   return fetch_unroll_info_helper(thread, exec_mode);
 155 JRT_END
 156 
 157 
 158 // This is factored, since it is both called from a JRT_LEAF (deoptimization) and a JRT_ENTRY (uncommon_trap)
 159 Deoptimization::UnrollBlock* Deoptimization::fetch_unroll_info_helper(JavaThread* thread, int exec_mode) {
 160 
 161   // Note: there is a safepoint safety issue here. No matter whether we enter
 162   // via vanilla deopt or uncommon trap we MUST NOT stop at a safepoint once
 163   // the vframeArray is created.
 164   //
 165 
 166   // Allocate our special deoptimization ResourceMark
 167   DeoptResourceMark* dmark = new DeoptResourceMark(thread);
 168   assert(thread-&gt;deopt_mark() == NULL, "Pending deopt!");
 169   thread-&gt;set_deopt_mark(dmark);
 170 
 171   frame stub_frame = thread-&gt;last_frame(); // Makes stack walkable as side effect
 172   RegisterMap map(thread, true);
 173   RegisterMap dummy_map(thread, false);
 174   // Now get the deoptee with a valid map
 175   frame deoptee = stub_frame.sender(&amp;map);
 176   // Set the deoptee nmethod
 177   assert(thread-&gt;deopt_compiled_method() == NULL, "Pending deopt!");
 178   CompiledMethod* cm = deoptee.cb()-&gt;as_compiled_method_or_null();
 179   thread-&gt;set_deopt_compiled_method(cm);
 180 
 181   if (VerifyStack) {
 182     thread-&gt;validate_frame_layout();
 183   }
 184 
 185   // Create a growable array of VFrames where each VFrame represents an inlined
 186   // Java frame.  This storage is allocated with the usual system arena.
 187   assert(deoptee.is_compiled_frame(), "Wrong frame type");
 188   GrowableArray&lt;compiledVFrame*&gt;* chunk = new GrowableArray&lt;compiledVFrame*&gt;(10);
 189   vframe* vf = vframe::new_vframe(&amp;deoptee, &amp;map, thread);
 190   while (!vf-&gt;is_top()) {
 191     assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 192     chunk-&gt;push(compiledVFrame::cast(vf));
 193     vf = vf-&gt;sender();
 194   }
 195   assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 196   chunk-&gt;push(compiledVFrame::cast(vf));
 197 
 198   bool realloc_failures = false;
 199 
 200 #if COMPILER2_OR_JVMCI
 201   // Reallocate the non-escaping objects and restore their fields. Then
 202   // relock objects if synchronization on them was eliminated.
 203 #if !INCLUDE_JVMCI
 204   if (DoEscapeAnalysis || EliminateNestedLocks) {
 205     if (EliminateAllocations) {
 206 #endif // INCLUDE_JVMCI
 207       assert (chunk-&gt;at(0)-&gt;scope() != NULL,"expect only compiled java frames");
 208       GrowableArray&lt;ScopeValue*&gt;* objects = chunk-&gt;at(0)-&gt;scope()-&gt;objects();
 209 
 210       // The flag return_oop() indicates call sites which return oop
 211       // in compiled code. Such sites include java method calls,
 212       // runtime calls (for example, used to allocate new objects/arrays
 213       // on slow code path) and any other calls generated in compiled code.
 214       // It is not guaranteed that we can get such information here only
 215       // by analyzing bytecode in deoptimized frames. This is why this flag
 216       // is set during method compilation (see Compile::Process_OopMap_Node()).
 217       // If the previous frame was popped or if we are dispatching an exception,
 218       // we don't have an oop result.
 219       bool save_oop_result = chunk-&gt;at(0)-&gt;scope()-&gt;return_oop() &amp;&amp; !thread-&gt;popframe_forcing_deopt_reexecution() &amp;&amp; (exec_mode == Unpack_deopt);
 220       Handle return_value;
 221       if (save_oop_result) {
 222         // Reallocation may trigger GC. If deoptimization happened on return from
 223         // call which returns oop we need to save it since it is not in oopmap.
 224         oop result = deoptee.saved_oop_result(&amp;map);
 225         assert(oopDesc::is_oop_or_null(result), "must be oop");
 226         return_value = Handle(thread, result);
 227         assert(Universe::heap()-&gt;is_in_or_null(result), "must be heap pointer");
 228         if (TraceDeoptimization) {
 229           ttyLocker ttyl;
 230           tty-&gt;print_cr("SAVED OOP RESULT " INTPTR_FORMAT " in thread " INTPTR_FORMAT, p2i(result), p2i(thread));
 231         }
 232       }
 233       if (objects != NULL) {
 234         JRT_BLOCK
<a name="1" id="anc1"></a><span class="changed"> 235           realloc_failures = realloc_objects(thread, &amp;deoptee, &amp;map, objects, THREAD);</span>
 236         JRT_END
 237         bool skip_internal = (cm != NULL) &amp;&amp; !cm-&gt;is_compiled_by_jvmci();
 238         reassign_fields(&amp;deoptee, &amp;map, objects, realloc_failures, skip_internal);
 239 #ifndef PRODUCT
 240         if (TraceDeoptimization) {
 241           ttyLocker ttyl;
 242           tty-&gt;print_cr("REALLOC OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 243           print_objects(objects, realloc_failures);
 244         }
 245 #endif
 246       }
 247       if (save_oop_result) {
 248         // Restore result.
 249         deoptee.set_saved_oop_result(&amp;map, return_value());
 250       }
 251 #if !INCLUDE_JVMCI
 252     }
 253     if (EliminateLocks) {
 254 #endif // INCLUDE_JVMCI
 255 #ifndef PRODUCT
 256       bool first = true;
 257 #endif
 258       for (int i = 0; i &lt; chunk-&gt;length(); i++) {
 259         compiledVFrame* cvf = chunk-&gt;at(i);
 260         assert (cvf-&gt;scope() != NULL,"expect only compiled java frames");
 261         GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
 262         if (monitors-&gt;is_nonempty()) {
 263           relock_objects(monitors, thread, realloc_failures);
 264 #ifndef PRODUCT
 265           if (PrintDeoptimizationDetails) {
 266             ttyLocker ttyl;
 267             for (int j = 0; j &lt; monitors-&gt;length(); j++) {
 268               MonitorInfo* mi = monitors-&gt;at(j);
 269               if (mi-&gt;eliminated()) {
 270                 if (first) {
 271                   first = false;
 272                   tty-&gt;print_cr("RELOCK OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 273                 }
 274                 if (mi-&gt;owner_is_scalar_replaced()) {
 275                   Klass* k = java_lang_Class::as_Klass(mi-&gt;owner_klass());
 276                   tty-&gt;print_cr("     failed reallocation for klass %s", k-&gt;external_name());
 277                 } else {
 278                   tty-&gt;print_cr("     object &lt;" INTPTR_FORMAT "&gt; locked", p2i(mi-&gt;owner()));
 279                 }
 280               }
 281             }
 282           }
 283 #endif // !PRODUCT
 284         }
 285       }
 286 #if !INCLUDE_JVMCI
 287     }
 288   }
 289 #endif // INCLUDE_JVMCI
 290 #endif // COMPILER2_OR_JVMCI
 291 
 292   ScopeDesc* trap_scope = chunk-&gt;at(0)-&gt;scope();
 293   Handle exceptionObject;
 294   if (trap_scope-&gt;rethrow_exception()) {
 295     if (PrintDeoptimizationDetails) {
 296       tty-&gt;print_cr("Exception to be rethrown in the interpreter for method %s::%s at bci %d", trap_scope-&gt;method()-&gt;method_holder()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;method()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;bci());
 297     }
 298     GrowableArray&lt;ScopeValue*&gt;* expressions = trap_scope-&gt;expressions();
 299     guarantee(expressions != NULL &amp;&amp; expressions-&gt;length() &gt; 0, "must have exception to throw");
 300     ScopeValue* topOfStack = expressions-&gt;top();
 301     exceptionObject = StackValue::create_stack_value(&amp;deoptee, &amp;map, topOfStack)-&gt;get_obj();
 302     guarantee(exceptionObject() != NULL, "exception oop can not be null");
 303   }
 304 
 305   // Ensure that no safepoint is taken after pointers have been stored
 306   // in fields of rematerialized objects.  If a safepoint occurs from here on
 307   // out the java state residing in the vframeArray will be missed.
 308   NoSafepointVerifier no_safepoint;
 309 
 310   vframeArray* array = create_vframeArray(thread, deoptee, &amp;map, chunk, realloc_failures);
 311 #if COMPILER2_OR_JVMCI
 312   if (realloc_failures) {
 313     pop_frames_failed_reallocs(thread, array);
 314   }
 315 #endif
 316 
 317   assert(thread-&gt;vframe_array_head() == NULL, "Pending deopt!");
 318   thread-&gt;set_vframe_array_head(array);
 319 
 320   // Now that the vframeArray has been created if we have any deferred local writes
 321   // added by jvmti then we can free up that structure as the data is now in the
 322   // vframeArray
 323 
 324   if (thread-&gt;deferred_locals() != NULL) {
 325     GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* list = thread-&gt;deferred_locals();
 326     int i = 0;
 327     do {
 328       // Because of inlining we could have multiple vframes for a single frame
 329       // and several of the vframes could have deferred writes. Find them all.
 330       if (list-&gt;at(i)-&gt;id() == array-&gt;original().id()) {
 331         jvmtiDeferredLocalVariableSet* dlv = list-&gt;at(i);
 332         list-&gt;remove_at(i);
 333         // individual jvmtiDeferredLocalVariableSet are CHeapObj's
 334         delete dlv;
 335       } else {
 336         i++;
 337       }
 338     } while ( i &lt; list-&gt;length() );
 339     if (list-&gt;length() == 0) {
 340       thread-&gt;set_deferred_locals(NULL);
 341       // free the list and elements back to C heap.
 342       delete list;
 343     }
 344 
 345   }
 346 
 347   // Compute the caller frame based on the sender sp of stub_frame and stored frame sizes info.
 348   CodeBlob* cb = stub_frame.cb();
 349   // Verify we have the right vframeArray
 350   assert(cb-&gt;frame_size() &gt;= 0, "Unexpected frame size");
 351   intptr_t* unpack_sp = stub_frame.sp() + cb-&gt;frame_size();
 352 
 353   // If the deopt call site is a MethodHandle invoke call site we have
 354   // to adjust the unpack_sp.
 355   nmethod* deoptee_nm = deoptee.cb()-&gt;as_nmethod_or_null();
 356   if (deoptee_nm != NULL &amp;&amp; deoptee_nm-&gt;is_method_handle_return(deoptee.pc()))
 357     unpack_sp = deoptee.unextended_sp();
 358 
 359 #ifdef ASSERT
 360   assert(cb-&gt;is_deoptimization_stub() ||
 361          cb-&gt;is_uncommon_trap_stub() ||
 362          strcmp("Stub&lt;DeoptimizationStub.deoptimizationHandler&gt;", cb-&gt;name()) == 0 ||
 363          strcmp("Stub&lt;UncommonTrapStub.uncommonTrapHandler&gt;", cb-&gt;name()) == 0,
 364          "unexpected code blob: %s", cb-&gt;name());
 365 #endif
 366 
 367   // This is a guarantee instead of an assert because if vframe doesn't match
 368   // we will unpack the wrong deoptimized frame and wind up in strange places
 369   // where it will be very difficult to figure out what went wrong. Better
 370   // to die an early death here than some very obscure death later when the
 371   // trail is cold.
 372   // Note: on ia64 this guarantee can be fooled by frames with no memory stack
 373   // in that it will fail to detect a problem when there is one. This needs
 374   // more work in tiger timeframe.
 375   guarantee(array-&gt;unextended_sp() == unpack_sp, "vframe_array_head must contain the vframeArray to unpack");
 376 
 377   int number_of_frames = array-&gt;frames();
 378 
 379   // Compute the vframes' sizes.  Note that frame_sizes[] entries are ordered from outermost to innermost
 380   // virtual activation, which is the reverse of the elements in the vframes array.
 381   intptr_t* frame_sizes = NEW_C_HEAP_ARRAY(intptr_t, number_of_frames, mtCompiler);
 382   // +1 because we always have an interpreter return address for the final slot.
 383   address* frame_pcs = NEW_C_HEAP_ARRAY(address, number_of_frames + 1, mtCompiler);
 384   int popframe_extra_args = 0;
 385   // Create an interpreter return address for the stub to use as its return
 386   // address so the skeletal frames are perfectly walkable
 387   frame_pcs[number_of_frames] = Interpreter::deopt_entry(vtos, 0);
 388 
 389   // PopFrame requires that the preserved incoming arguments from the recently-popped topmost
 390   // activation be put back on the expression stack of the caller for reexecution
 391   if (JvmtiExport::can_pop_frame() &amp;&amp; thread-&gt;popframe_forcing_deopt_reexecution()) {
 392     popframe_extra_args = in_words(thread-&gt;popframe_preserved_args_size_in_words());
 393   }
 394 
 395   // Find the current pc for sender of the deoptee. Since the sender may have been deoptimized
 396   // itself since the deoptee vframeArray was created we must get a fresh value of the pc rather
 397   // than simply use array-&gt;sender.pc(). This requires us to walk the current set of frames
 398   //
 399   frame deopt_sender = stub_frame.sender(&amp;dummy_map); // First is the deoptee frame
 400   deopt_sender = deopt_sender.sender(&amp;dummy_map);     // Now deoptee caller
 401 
 402   // It's possible that the number of parameters at the call site is
 403   // different than number of arguments in the callee when method
 404   // handles are used.  If the caller is interpreted get the real
 405   // value so that the proper amount of space can be added to it's
 406   // frame.
 407   bool caller_was_method_handle = false;
 408   if (deopt_sender.is_interpreted_frame()) {
 409     methodHandle method = deopt_sender.interpreter_frame_method();
 410     Bytecode_invoke cur = Bytecode_invoke_check(method, deopt_sender.interpreter_frame_bci());
 411     if (cur.is_invokedynamic() || cur.is_invokehandle()) {
 412       // Method handle invokes may involve fairly arbitrary chains of
 413       // calls so it's impossible to know how much actual space the
 414       // caller has for locals.
 415       caller_was_method_handle = true;
 416     }
 417   }
 418 
 419   //
 420   // frame_sizes/frame_pcs[0] oldest frame (int or c2i)
 421   // frame_sizes/frame_pcs[1] next oldest frame (int)
 422   // frame_sizes/frame_pcs[n] youngest frame (int)
 423   //
 424   // Now a pc in frame_pcs is actually the return address to the frame's caller (a frame
 425   // owns the space for the return address to it's caller).  Confusing ain't it.
 426   //
 427   // The vframe array can address vframes with indices running from
 428   // 0.._frames-1. Index  0 is the youngest frame and _frame - 1 is the oldest (root) frame.
 429   // When we create the skeletal frames we need the oldest frame to be in the zero slot
 430   // in the frame_sizes/frame_pcs so the assembly code can do a trivial walk.
 431   // so things look a little strange in this loop.
 432   //
 433   int callee_parameters = 0;
 434   int callee_locals = 0;
 435   for (int index = 0; index &lt; array-&gt;frames(); index++ ) {
 436     // frame[number_of_frames - 1 ] = on_stack_size(youngest)
 437     // frame[number_of_frames - 2 ] = on_stack_size(sender(youngest))
 438     // frame[number_of_frames - 3 ] = on_stack_size(sender(sender(youngest)))
 439     frame_sizes[number_of_frames - 1 - index] = BytesPerWord * array-&gt;element(index)-&gt;on_stack_size(callee_parameters,
 440                                                                                                     callee_locals,
 441                                                                                                     index == 0,
 442                                                                                                     popframe_extra_args);
 443     // This pc doesn't have to be perfect just good enough to identify the frame
 444     // as interpreted so the skeleton frame will be walkable
 445     // The correct pc will be set when the skeleton frame is completely filled out
 446     // The final pc we store in the loop is wrong and will be overwritten below
 447     frame_pcs[number_of_frames - 1 - index ] = Interpreter::deopt_entry(vtos, 0) - frame::pc_return_offset;
 448 
 449     callee_parameters = array-&gt;element(index)-&gt;method()-&gt;size_of_parameters();
 450     callee_locals = array-&gt;element(index)-&gt;method()-&gt;max_locals();
 451     popframe_extra_args = 0;
 452   }
 453 
 454   // Compute whether the root vframe returns a float or double value.
 455   BasicType return_type;
 456   {
 457     methodHandle method(thread, array-&gt;element(0)-&gt;method());
 458     Bytecode_invoke invoke = Bytecode_invoke_check(method, array-&gt;element(0)-&gt;bci());
 459     return_type = invoke.is_valid() ? invoke.result_type() : T_ILLEGAL;
 460   }
 461 
 462   // Compute information for handling adapters and adjusting the frame size of the caller.
 463   int caller_adjustment = 0;
 464 
 465   // Compute the amount the oldest interpreter frame will have to adjust
 466   // its caller's stack by. If the caller is a compiled frame then
 467   // we pretend that the callee has no parameters so that the
 468   // extension counts for the full amount of locals and not just
 469   // locals-parms. This is because without a c2i adapter the parm
 470   // area as created by the compiled frame will not be usable by
 471   // the interpreter. (Depending on the calling convention there
 472   // may not even be enough space).
 473 
 474   // QQQ I'd rather see this pushed down into last_frame_adjust
 475   // and have it take the sender (aka caller).
 476 
 477   if (deopt_sender.is_compiled_frame() || caller_was_method_handle) {
 478     caller_adjustment = last_frame_adjust(0, callee_locals);
 479   } else if (callee_locals &gt; callee_parameters) {
 480     // The caller frame may need extending to accommodate
 481     // non-parameter locals of the first unpacked interpreted frame.
 482     // Compute that adjustment.
 483     caller_adjustment = last_frame_adjust(callee_parameters, callee_locals);
 484   }
 485 
 486   // If the sender is deoptimized the we must retrieve the address of the handler
 487   // since the frame will "magically" show the original pc before the deopt
 488   // and we'd undo the deopt.
 489 
 490   frame_pcs[0] = deopt_sender.raw_pc();
 491 
 492   assert(CodeCache::find_blob_unsafe(frame_pcs[0]) != NULL, "bad pc");
 493 
 494 #if INCLUDE_JVMCI
 495   if (exceptionObject() != NULL) {
 496     thread-&gt;set_exception_oop(exceptionObject());
 497     exec_mode = Unpack_exception;
 498   }
 499 #endif
 500 
 501   if (thread-&gt;frames_to_pop_failed_realloc() &gt; 0 &amp;&amp; exec_mode != Unpack_uncommon_trap) {
 502     assert(thread-&gt;has_pending_exception(), "should have thrown OOME");
 503     thread-&gt;set_exception_oop(thread-&gt;pending_exception());
 504     thread-&gt;clear_pending_exception();
 505     exec_mode = Unpack_exception;
 506   }
 507 
 508 #if INCLUDE_JVMCI
 509   if (thread-&gt;frames_to_pop_failed_realloc() &gt; 0) {
 510     thread-&gt;set_pending_monitorenter(false);
 511   }
 512 #endif
 513 
 514   UnrollBlock* info = new UnrollBlock(array-&gt;frame_size() * BytesPerWord,
 515                                       caller_adjustment * BytesPerWord,
 516                                       caller_was_method_handle ? 0 : callee_parameters,
 517                                       number_of_frames,
 518                                       frame_sizes,
 519                                       frame_pcs,
 520                                       return_type,
 521                                       exec_mode);
 522   // On some platforms, we need a way to pass some platform dependent
 523   // information to the unpacking code so the skeletal frames come out
 524   // correct (initial fp value, unextended sp, ...)
 525   info-&gt;set_initial_info((intptr_t) array-&gt;sender().initial_deoptimization_info());
 526 
 527   if (array-&gt;frames() &gt; 1) {
 528     if (VerifyStack &amp;&amp; TraceDeoptimization) {
 529       ttyLocker ttyl;
 530       tty-&gt;print_cr("Deoptimizing method containing inlining");
 531     }
 532   }
 533 
 534   array-&gt;set_unroll_block(info);
 535   return info;
 536 }
 537 
 538 // Called to cleanup deoptimization data structures in normal case
 539 // after unpacking to stack and when stack overflow error occurs
 540 void Deoptimization::cleanup_deopt_info(JavaThread *thread,
 541                                         vframeArray *array) {
 542 
 543   // Get array if coming from exception
 544   if (array == NULL) {
 545     array = thread-&gt;vframe_array_head();
 546   }
 547   thread-&gt;set_vframe_array_head(NULL);
 548 
 549   // Free the previous UnrollBlock
 550   vframeArray* old_array = thread-&gt;vframe_array_last();
 551   thread-&gt;set_vframe_array_last(array);
 552 
 553   if (old_array != NULL) {
 554     UnrollBlock* old_info = old_array-&gt;unroll_block();
 555     old_array-&gt;set_unroll_block(NULL);
 556     delete old_info;
 557     delete old_array;
 558   }
 559 
 560   // Deallocate any resource creating in this routine and any ResourceObjs allocated
 561   // inside the vframeArray (StackValueCollections)
 562 
 563   delete thread-&gt;deopt_mark();
 564   thread-&gt;set_deopt_mark(NULL);
 565   thread-&gt;set_deopt_compiled_method(NULL);
 566 
 567 
 568   if (JvmtiExport::can_pop_frame()) {
 569 #ifndef CC_INTERP
 570     // Regardless of whether we entered this routine with the pending
 571     // popframe condition bit set, we should always clear it now
 572     thread-&gt;clear_popframe_condition();
 573 #else
 574     // C++ interpreter will clear has_pending_popframe when it enters
 575     // with method_resume. For deopt_resume2 we clear it now.
 576     if (thread-&gt;popframe_forcing_deopt_reexecution())
 577         thread-&gt;clear_popframe_condition();
 578 #endif /* CC_INTERP */
 579   }
 580 
 581   // unpack_frames() is called at the end of the deoptimization handler
 582   // and (in C2) at the end of the uncommon trap handler. Note this fact
 583   // so that an asynchronous stack walker can work again. This counter is
 584   // incremented at the beginning of fetch_unroll_info() and (in C2) at
 585   // the beginning of uncommon_trap().
 586   thread-&gt;dec_in_deopt_handler();
 587 }
 588 
 589 // Moved from cpu directories because none of the cpus has callee save values.
 590 // If a cpu implements callee save values, move this to deoptimization_&lt;cpu&gt;.cpp.
 591 void Deoptimization::unwind_callee_save_values(frame* f, vframeArray* vframe_array) {
 592 
 593   // This code is sort of the equivalent of C2IAdapter::setup_stack_frame back in
 594   // the days we had adapter frames. When we deoptimize a situation where a
 595   // compiled caller calls a compiled caller will have registers it expects
 596   // to survive the call to the callee. If we deoptimize the callee the only
 597   // way we can restore these registers is to have the oldest interpreter
 598   // frame that we create restore these values. That is what this routine
 599   // will accomplish.
 600 
 601   // At the moment we have modified c2 to not have any callee save registers
 602   // so this problem does not exist and this routine is just a place holder.
 603 
 604   assert(f-&gt;is_interpreted_frame(), "must be interpreted");
 605 }
 606 
 607 // Return BasicType of value being returned
 608 JRT_LEAF(BasicType, Deoptimization::unpack_frames(JavaThread* thread, int exec_mode))
 609 
 610   // We are already active in the special DeoptResourceMark any ResourceObj's we
 611   // allocate will be freed at the end of the routine.
 612 
 613   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 614   // but makes the entry a little slower. There is however a little dance we have to
 615   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 616   ResetNoHandleMark rnhm; // No-op in release/product versions
 617   HandleMark hm;
 618 
 619   frame stub_frame = thread-&gt;last_frame();
 620 
 621   // Since the frame to unpack is the top frame of this thread, the vframe_array_head
 622   // must point to the vframeArray for the unpack frame.
 623   vframeArray* array = thread-&gt;vframe_array_head();
 624 
 625 #ifndef PRODUCT
 626   if (TraceDeoptimization) {
 627     ttyLocker ttyl;
 628     tty-&gt;print_cr("DEOPT UNPACKING thread " INTPTR_FORMAT " vframeArray " INTPTR_FORMAT " mode %d",
 629                   p2i(thread), p2i(array), exec_mode);
 630   }
 631 #endif
 632   Events::log(thread, "DEOPT UNPACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT " mode %d",
 633               p2i(stub_frame.pc()), p2i(stub_frame.sp()), exec_mode);
 634 
 635   UnrollBlock* info = array-&gt;unroll_block();
 636 
 637   // Unpack the interpreter frames and any adapter frame (c2 only) we might create.
 638   array-&gt;unpack_to_stack(stub_frame, exec_mode, info-&gt;caller_actual_parameters());
 639 
 640   BasicType bt = info-&gt;return_type();
 641 
 642   // If we have an exception pending, claim that the return type is an oop
 643   // so the deopt_blob does not overwrite the exception_oop.
 644 
 645   if (exec_mode == Unpack_exception)
 646     bt = T_OBJECT;
 647 
 648   // Cleanup thread deopt data
 649   cleanup_deopt_info(thread, array);
 650 
 651 #ifndef PRODUCT
 652   if (VerifyStack) {
 653     ResourceMark res_mark;
 654     // Clear pending exception to not break verification code (restored afterwards)
 655     PRESERVE_EXCEPTION_MARK;
 656 
 657     thread-&gt;validate_frame_layout();
 658 
 659     // Verify that the just-unpacked frames match the interpreter's
 660     // notions of expression stack and locals
 661     vframeArray* cur_array = thread-&gt;vframe_array_last();
 662     RegisterMap rm(thread, false);
 663     rm.set_include_argument_oops(false);
 664     bool is_top_frame = true;
 665     int callee_size_of_parameters = 0;
 666     int callee_max_locals = 0;
 667     for (int i = 0; i &lt; cur_array-&gt;frames(); i++) {
 668       vframeArrayElement* el = cur_array-&gt;element(i);
 669       frame* iframe = el-&gt;iframe();
 670       guarantee(iframe-&gt;is_interpreted_frame(), "Wrong frame type");
 671 
 672       // Get the oop map for this bci
 673       InterpreterOopMap mask;
 674       int cur_invoke_parameter_size = 0;
 675       bool try_next_mask = false;
 676       int next_mask_expression_stack_size = -1;
 677       int top_frame_expression_stack_adjustment = 0;
 678       methodHandle mh(thread, iframe-&gt;interpreter_frame_method());
 679       OopMapCache::compute_one_oop_map(mh, iframe-&gt;interpreter_frame_bci(), &amp;mask);
 680       BytecodeStream str(mh);
 681       str.set_start(iframe-&gt;interpreter_frame_bci());
 682       int max_bci = mh-&gt;code_size();
 683       // Get to the next bytecode if possible
 684       assert(str.bci() &lt; max_bci, "bci in interpreter frame out of bounds");
 685       // Check to see if we can grab the number of outgoing arguments
 686       // at an uncommon trap for an invoke (where the compiler
 687       // generates debug info before the invoke has executed)
 688       Bytecodes::Code cur_code = str.next();
 689       if (Bytecodes::is_invoke(cur_code)) {
 690         Bytecode_invoke invoke(mh, iframe-&gt;interpreter_frame_bci());
 691         cur_invoke_parameter_size = invoke.size_of_parameters();
 692         if (i != 0 &amp;&amp; !invoke.is_invokedynamic() &amp;&amp; MethodHandles::has_member_arg(invoke.klass(), invoke.name())) {
 693           callee_size_of_parameters++;
 694         }
 695       }
 696       if (str.bci() &lt; max_bci) {
 697         Bytecodes::Code next_code = str.next();
 698         if (next_code &gt;= 0) {
 699           // The interpreter oop map generator reports results before
 700           // the current bytecode has executed except in the case of
 701           // calls. It seems to be hard to tell whether the compiler
 702           // has emitted debug information matching the "state before"
 703           // a given bytecode or the state after, so we try both
 704           if (!Bytecodes::is_invoke(cur_code) &amp;&amp; cur_code != Bytecodes::_athrow) {
 705             // Get expression stack size for the next bytecode
 706             InterpreterOopMap next_mask;
 707             OopMapCache::compute_one_oop_map(mh, str.bci(), &amp;next_mask);
 708             next_mask_expression_stack_size = next_mask.expression_stack_size();
 709             if (Bytecodes::is_invoke(next_code)) {
 710               Bytecode_invoke invoke(mh, str.bci());
 711               next_mask_expression_stack_size += invoke.size_of_parameters();
 712             }
 713             // Need to subtract off the size of the result type of
 714             // the bytecode because this is not described in the
 715             // debug info but returned to the interpreter in the TOS
 716             // caching register
 717             BasicType bytecode_result_type = Bytecodes::result_type(cur_code);
 718             if (bytecode_result_type != T_ILLEGAL) {
 719               top_frame_expression_stack_adjustment = type2size[bytecode_result_type];
 720             }
 721             assert(top_frame_expression_stack_adjustment &gt;= 0, "stack adjustment must be positive");
 722             try_next_mask = true;
 723           }
 724         }
 725       }
 726 
 727       // Verify stack depth and oops in frame
 728       // This assertion may be dependent on the platform we're running on and may need modification (tested on x86 and sparc)
 729       if (!(
 730             /* SPARC */
 731             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_size_of_parameters) ||
 732             /* x86 */
 733             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_max_locals) ||
 734             (try_next_mask &amp;&amp;
 735              (iframe-&gt;interpreter_frame_expression_stack_size() == (next_mask_expression_stack_size -
 736                                                                     top_frame_expression_stack_adjustment))) ||
 737             (is_top_frame &amp;&amp; (exec_mode == Unpack_exception) &amp;&amp; iframe-&gt;interpreter_frame_expression_stack_size() == 0) ||
 738             (is_top_frame &amp;&amp; (exec_mode == Unpack_uncommon_trap || exec_mode == Unpack_reexecute || el-&gt;should_reexecute()) &amp;&amp;
 739              (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + cur_invoke_parameter_size))
 740             )) {
 741         {
 742           ttyLocker ttyl;
 743 
 744           // Print out some information that will help us debug the problem
 745           tty-&gt;print_cr("Wrong number of expression stack elements during deoptimization");
 746           tty-&gt;print_cr("  Error occurred while verifying frame %d (0..%d, 0 is topmost)", i, cur_array-&gt;frames() - 1);
 747           tty-&gt;print_cr("  Fabricated interpreter frame had %d expression stack elements",
 748                         iframe-&gt;interpreter_frame_expression_stack_size());
 749           tty-&gt;print_cr("  Interpreter oop map had %d expression stack elements", mask.expression_stack_size());
 750           tty-&gt;print_cr("  try_next_mask = %d", try_next_mask);
 751           tty-&gt;print_cr("  next_mask_expression_stack_size = %d", next_mask_expression_stack_size);
 752           tty-&gt;print_cr("  callee_size_of_parameters = %d", callee_size_of_parameters);
 753           tty-&gt;print_cr("  callee_max_locals = %d", callee_max_locals);
 754           tty-&gt;print_cr("  top_frame_expression_stack_adjustment = %d", top_frame_expression_stack_adjustment);
 755           tty-&gt;print_cr("  exec_mode = %d", exec_mode);
 756           tty-&gt;print_cr("  cur_invoke_parameter_size = %d", cur_invoke_parameter_size);
 757           tty-&gt;print_cr("  Thread = " INTPTR_FORMAT ", thread ID = %d", p2i(thread), thread-&gt;osthread()-&gt;thread_id());
 758           tty-&gt;print_cr("  Interpreted frames:");
 759           for (int k = 0; k &lt; cur_array-&gt;frames(); k++) {
 760             vframeArrayElement* el = cur_array-&gt;element(k);
 761             tty-&gt;print_cr("    %s (bci %d)", el-&gt;method()-&gt;name_and_sig_as_C_string(), el-&gt;bci());
 762           }
 763           cur_array-&gt;print_on_2(tty);
 764         } // release tty lock before calling guarantee
 765         guarantee(false, "wrong number of expression stack elements during deopt");
 766       }
 767       VerifyOopClosure verify;
 768       iframe-&gt;oops_interpreted_do(&amp;verify, &amp;rm, false);
 769       callee_size_of_parameters = mh-&gt;size_of_parameters();
 770       callee_max_locals = mh-&gt;max_locals();
 771       is_top_frame = false;
 772     }
 773   }
 774 #endif /* !PRODUCT */
 775 
 776 
 777   return bt;
 778 JRT_END
 779 
 780 
 781 int Deoptimization::deoptimize_dependents() {
 782   Threads::deoptimized_wrt_marked_nmethods();
 783   return 0;
 784 }
 785 
 786 Deoptimization::DeoptAction Deoptimization::_unloaded_action
 787   = Deoptimization::Action_reinterpret;
 788 
 789 #if COMPILER2_OR_JVMCI
<a name="2" id="anc2"></a><span class="changed"> 790 bool Deoptimization::realloc_objects(JavaThread* thread, frame* fr, RegisterMap* reg_map, GrowableArray&lt;ScopeValue*&gt;* objects, TRAPS) {</span>
 791   Handle pending_exception(THREAD, thread-&gt;pending_exception());
 792   const char* exception_file = thread-&gt;exception_file();
 793   int exception_line = thread-&gt;exception_line();
 794   thread-&gt;clear_pending_exception();
 795 
 796   bool failures = false;
 797 
 798   for (int i = 0; i &lt; objects-&gt;length(); i++) {
 799     assert(objects-&gt;at(i)-&gt;is_object(), "invalid debug information");
 800     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
 801 
 802     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
<a name="3" id="anc3"></a><span class="changed"> 803     if (reg_map == NULL &amp;&amp; (!sv-&gt;base_object()-&gt;is_constant_oop() || !sv-&gt;base_object()-&gt;as_ConstantOopReadValue()-&gt;value().is_null())) {</span>
<span class="changed"> 804       // skip element with base object if we don't have a register map</span>
<span class="changed"> 805       continue;</span>
<span class="changed"> 806     }</span>
<span class="changed"> 807     oop obj = StackValue::create_stack_value(fr, reg_map, sv-&gt;base_object())-&gt;get_obj()();</span>
 808 
<a name="4" id="anc4"></a><span class="new"> 809     if (obj == NULL) {</span>
 810       if (k-&gt;is_instance_klass()) {
 811         InstanceKlass* ik = InstanceKlass::cast(k);
 812         obj = ik-&gt;allocate_instance(THREAD);
 813       } else if (k-&gt;is_typeArray_klass()) {
 814         TypeArrayKlass* ak = TypeArrayKlass::cast(k);
 815         assert(sv-&gt;field_size() % type2size[ak-&gt;element_type()] == 0, "non-integral array length");
 816         int len = sv-&gt;field_size() / type2size[ak-&gt;element_type()];
 817         obj = ak-&gt;allocate(len, THREAD);
 818       } else if (k-&gt;is_objArray_klass()) {
 819         ObjArrayKlass* ak = ObjArrayKlass::cast(k);
 820         obj = ak-&gt;allocate(sv-&gt;field_size(), THREAD);
 821       }
<a name="5" id="anc5"></a><span class="new"> 822     }</span>
 823 
 824     if (obj == NULL) {
 825       failures = true;
 826     }
 827 
 828     assert(sv-&gt;value().is_null(), "redundant reallocation");
 829     assert(obj != NULL || HAS_PENDING_EXCEPTION, "allocation should succeed or we should get an exception");
 830     CLEAR_PENDING_EXCEPTION;
 831     sv-&gt;set_value(obj);
 832   }
 833 
 834   if (failures) {
 835     THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), failures);
 836   } else if (pending_exception.not_null()) {
 837     thread-&gt;set_pending_exception(pending_exception(), exception_file, exception_line);
 838   }
 839 
 840   return failures;
 841 }
 842 
 843 // restore elements of an eliminated type array
 844 void Deoptimization::reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type) {
 845   int index = 0;
 846   intptr_t val;
 847 
 848   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 849     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
<a name="6" id="anc6"></a><span class="new"> 850     if (value-&gt;type() == T_CONFLICT) {</span>
<span class="new"> 851       // skip fields with no values</span>
<span class="new"> 852       index += (type == T_LONG || type == T_DOUBLE) ? 2 : 1;</span>
<span class="new"> 853       continue;</span>
<span class="new"> 854     }</span>
 855     switch(type) {
 856     case T_LONG: case T_DOUBLE: {
 857       assert(value-&gt;type() == T_INT, "Agreement.");
 858       StackValue* low =
 859         StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 860 #ifdef _LP64
 861       jlong res = (jlong)low-&gt;get_int();
 862 #else
 863 #ifdef SPARC
 864       // For SPARC we have to swap high and low words.
 865       jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 866 #else
 867       jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 868 #endif //SPARC
 869 #endif
 870       obj-&gt;long_at_put(index, res);
 871       break;
 872     }
 873 
 874     // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 875     case T_INT: case T_FLOAT: { // 4 bytes.
 876       assert(value-&gt;type() == T_INT, "Agreement.");
 877       bool big_value = false;
 878       if (i + 1 &lt; sv-&gt;field_size() &amp;&amp; type == T_INT) {
 879         if (sv-&gt;field_at(i)-&gt;is_location()) {
 880           Location::Type type = ((LocationValue*) sv-&gt;field_at(i))-&gt;location().type();
 881           if (type == Location::dbl || type == Location::lng) {
 882             big_value = true;
 883           }
 884         } else if (sv-&gt;field_at(i)-&gt;is_constant_int()) {
 885           ScopeValue* next_scope_field = sv-&gt;field_at(i + 1);
 886           if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 887             big_value = true;
 888           }
 889         }
 890       }
 891 
 892       if (big_value) {
 893         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 894   #ifdef _LP64
 895         jlong res = (jlong)low-&gt;get_int();
 896   #else
 897   #ifdef SPARC
 898         // For SPARC we have to swap high and low words.
 899         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 900   #else
 901         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 902   #endif //SPARC
 903   #endif
 904         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;res));
 905         obj-&gt;int_at_put(++index, (jint)*(((jint*)&amp;res) + 1));
 906       } else {
 907         val = value-&gt;get_int();
 908         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;val));
 909       }
 910       break;
 911     }
 912 
 913     case T_SHORT:
 914       assert(value-&gt;type() == T_INT, "Agreement.");
 915       val = value-&gt;get_int();
 916       obj-&gt;short_at_put(index, (jshort)*((jint*)&amp;val));
 917       break;
 918 
 919     case T_CHAR:
 920       assert(value-&gt;type() == T_INT, "Agreement.");
 921       val = value-&gt;get_int();
 922       obj-&gt;char_at_put(index, (jchar)*((jint*)&amp;val));
 923       break;
 924 
 925     case T_BYTE:
 926       assert(value-&gt;type() == T_INT, "Agreement.");
 927       val = value-&gt;get_int();
 928       obj-&gt;byte_at_put(index, (jbyte)*((jint*)&amp;val));
 929       break;
 930 
 931     case T_BOOLEAN:
 932       assert(value-&gt;type() == T_INT, "Agreement.");
 933       val = value-&gt;get_int();
 934       obj-&gt;bool_at_put(index, (jboolean)*((jint*)&amp;val));
 935       break;
 936 
 937       default:
 938         ShouldNotReachHere();
 939     }
 940     index++;
 941   }
 942 }
 943 
 944 
 945 // restore fields of an eliminated object array
 946 void Deoptimization::reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj) {
 947   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 948     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
<a name="7" id="anc7"></a><span class="new"> 949     if (value-&gt;type() == T_CONFLICT) {</span>
<span class="new"> 950       // skip fields with no values</span>
<span class="new"> 951       continue;</span>
<span class="new"> 952     }</span>
 953     assert(value-&gt;type() == T_OBJECT, "object element expected");
 954     obj-&gt;obj_at_put(i, value-&gt;get_obj()());
 955   }
 956 }
 957 
 958 class ReassignedField {
 959 public:
 960   int _offset;
 961   BasicType _type;
 962 public:
 963   ReassignedField() {
 964     _offset = 0;
 965     _type = T_ILLEGAL;
 966   }
 967 };
 968 
 969 int compare(ReassignedField* left, ReassignedField* right) {
 970   return left-&gt;_offset - right-&gt;_offset;
 971 }
 972 
 973 // Restore fields of an eliminated instance object using the same field order
 974 // returned by HotSpotResolvedObjectTypeImpl.getInstanceFields(true)
 975 static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {
 976   if (klass-&gt;superklass() != NULL) {
 977     svIndex = reassign_fields_by_klass(klass-&gt;superklass(), fr, reg_map, sv, svIndex, obj, skip_internal);
 978   }
 979 
 980   GrowableArray&lt;ReassignedField&gt;* fields = new GrowableArray&lt;ReassignedField&gt;();
 981   for (AllFieldStream fs(klass); !fs.done(); fs.next()) {
 982     if (!fs.access_flags().is_static() &amp;&amp; (!skip_internal || !fs.access_flags().is_internal())) {
 983       ReassignedField field;
 984       field._offset = fs.offset();
 985       field._type = FieldType::basic_type(fs.signature());
 986       fields-&gt;append(field);
 987     }
 988   }
 989   fields-&gt;sort(compare);
 990   for (int i = 0; i &lt; fields-&gt;length(); i++) {
 991     intptr_t val;
 992     ScopeValue* scope_field = sv-&gt;field_at(svIndex);
 993     StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);
 994     int offset = fields-&gt;at(i)._offset;
 995     BasicType type = fields-&gt;at(i)._type;
<a name="8" id="anc8"></a><span class="new"> 996     if (value-&gt;type() == T_CONFLICT) {</span>
<span class="new"> 997       // skip fields with no values</span>
<span class="new"> 998       svIndex += (type == T_LONG || type == T_DOUBLE) ? 2 : 1;</span>
<span class="new"> 999       continue;</span>
<span class="new">1000     }</span>
1001     switch (type) {
1002       case T_OBJECT: case T_ARRAY:
1003         assert(value-&gt;type() == T_OBJECT, "Agreement.");
1004         obj-&gt;obj_field_put(offset, value-&gt;get_obj()());
1005         break;
1006 
1007       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
1008       case T_INT: case T_FLOAT: { // 4 bytes.
1009         assert(value-&gt;type() == T_INT, "Agreement.");
1010         bool big_value = false;
1011         if (i+1 &lt; fields-&gt;length() &amp;&amp; fields-&gt;at(i+1)._type == T_INT) {
1012           if (scope_field-&gt;is_location()) {
1013             Location::Type type = ((LocationValue*) scope_field)-&gt;location().type();
1014             if (type == Location::dbl || type == Location::lng) {
1015               big_value = true;
1016             }
1017           }
1018           if (scope_field-&gt;is_constant_int()) {
1019             ScopeValue* next_scope_field = sv-&gt;field_at(svIndex + 1);
1020             if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
1021               big_value = true;
1022             }
1023           }
1024         }
1025 
1026         if (big_value) {
1027           i++;
1028           assert(i &lt; fields-&gt;length(), "second T_INT field needed");
1029           assert(fields-&gt;at(i)._type == T_INT, "T_INT field needed");
1030         } else {
1031           val = value-&gt;get_int();
1032           obj-&gt;int_field_put(offset, (jint)*((jint*)&amp;val));
1033           break;
1034         }
1035       }
1036         /* no break */
1037 
1038       case T_LONG: case T_DOUBLE: {
1039         assert(value-&gt;type() == T_INT, "Agreement.");
1040         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++svIndex));
1041 #ifdef _LP64
1042         jlong res = (jlong)low-&gt;get_int();
1043 #else
1044 #ifdef SPARC
1045         // For SPARC we have to swap high and low words.
1046         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
1047 #else
1048         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
1049 #endif //SPARC
1050 #endif
1051         obj-&gt;long_field_put(offset, res);
1052         break;
1053       }
1054 
1055       case T_SHORT:
1056         assert(value-&gt;type() == T_INT, "Agreement.");
1057         val = value-&gt;get_int();
1058         obj-&gt;short_field_put(offset, (jshort)*((jint*)&amp;val));
1059         break;
1060 
1061       case T_CHAR:
1062         assert(value-&gt;type() == T_INT, "Agreement.");
1063         val = value-&gt;get_int();
1064         obj-&gt;char_field_put(offset, (jchar)*((jint*)&amp;val));
1065         break;
1066 
1067       case T_BYTE:
1068         assert(value-&gt;type() == T_INT, "Agreement.");
1069         val = value-&gt;get_int();
1070         obj-&gt;byte_field_put(offset, (jbyte)*((jint*)&amp;val));
1071         break;
1072 
1073       case T_BOOLEAN:
1074         assert(value-&gt;type() == T_INT, "Agreement.");
1075         val = value-&gt;get_int();
1076         obj-&gt;bool_field_put(offset, (jboolean)*((jint*)&amp;val));
1077         break;
1078 
1079       default:
1080         ShouldNotReachHere();
1081     }
1082     svIndex++;
1083   }
1084   return svIndex;
1085 }
1086 
1087 // restore fields of all eliminated objects and arrays
1088 void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures, bool skip_internal) {
1089   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1090     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1091     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
1092     Handle obj = sv-&gt;value();
1093     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1094     if (PrintDeoptimizationDetails) {
1095       tty-&gt;print_cr("reassign fields for object of type %s!", k-&gt;name()-&gt;as_C_string());
1096     }
1097     if (obj.is_null()) {
1098       continue;
1099     }
1100 
1101     if (k-&gt;is_instance_klass()) {
1102       InstanceKlass* ik = InstanceKlass::cast(k);
1103       reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);
1104     } else if (k-&gt;is_typeArray_klass()) {
1105       TypeArrayKlass* ak = TypeArrayKlass::cast(k);
1106       reassign_type_array_elements(fr, reg_map, sv, (typeArrayOop) obj(), ak-&gt;element_type());
1107     } else if (k-&gt;is_objArray_klass()) {
1108       reassign_object_array_elements(fr, reg_map, sv, (objArrayOop) obj());
1109     }
1110   }
1111 }
1112 
1113 
1114 // relock objects for which synchronization was eliminated
1115 void Deoptimization::relock_objects(GrowableArray&lt;MonitorInfo*&gt;* monitors, JavaThread* thread, bool realloc_failures) {
1116   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1117     MonitorInfo* mon_info = monitors-&gt;at(i);
1118     if (mon_info-&gt;eliminated()) {
1119       assert(!mon_info-&gt;owner_is_scalar_replaced() || realloc_failures, "reallocation was missed");
1120       if (!mon_info-&gt;owner_is_scalar_replaced()) {
1121         Handle obj(thread, mon_info-&gt;owner());
1122         markOop mark = obj-&gt;mark();
1123         if (UseBiasedLocking &amp;&amp; mark-&gt;has_bias_pattern()) {
1124           // New allocated objects may have the mark set to anonymously biased.
1125           // Also the deoptimized method may called methods with synchronization
1126           // where the thread-local object is bias locked to the current thread.
1127           assert(mark-&gt;is_biased_anonymously() ||
1128                  mark-&gt;biased_locker() == thread, "should be locked to current thread");
1129           // Reset mark word to unbiased prototype.
1130           markOop unbiased_prototype = markOopDesc::prototype()-&gt;set_age(mark-&gt;age());
1131           obj-&gt;set_mark(unbiased_prototype);
1132         }
1133         BasicLock* lock = mon_info-&gt;lock();
1134         ObjectSynchronizer::slow_enter(obj, lock, thread);
1135         assert(mon_info-&gt;owner()-&gt;is_locked(), "object must be locked now");
1136       }
1137     }
1138   }
1139 }
1140 
1141 
1142 #ifndef PRODUCT
1143 // print information about reallocated objects
1144 void Deoptimization::print_objects(GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures) {
1145   fieldDescriptor fd;
1146 
1147   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1148     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1149     Klass* k = java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()());
1150     Handle obj = sv-&gt;value();
1151 
1152     tty-&gt;print("     object &lt;" INTPTR_FORMAT "&gt; of type ", p2i(sv-&gt;value()()));
1153     k-&gt;print_value();
1154     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1155     if (obj.is_null()) {
1156       tty-&gt;print(" allocation failed");
1157     } else {
1158       tty-&gt;print(" allocated (%d bytes)", obj-&gt;size() * HeapWordSize);
1159     }
1160     tty-&gt;cr();
1161 
1162     if (Verbose &amp;&amp; !obj.is_null()) {
1163       k-&gt;oop_print_on(obj(), tty);
1164     }
1165   }
1166 }
1167 #endif
1168 #endif // COMPILER2_OR_JVMCI
1169 
1170 vframeArray* Deoptimization::create_vframeArray(JavaThread* thread, frame fr, RegisterMap *reg_map, GrowableArray&lt;compiledVFrame*&gt;* chunk, bool realloc_failures) {
1171   Events::log(thread, "DEOPT PACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT, p2i(fr.pc()), p2i(fr.sp()));
1172 
1173 #ifndef PRODUCT
1174   if (PrintDeoptimizationDetails) {
1175     ttyLocker ttyl;
1176     tty-&gt;print("DEOPT PACKING thread " INTPTR_FORMAT " ", p2i(thread));
1177     fr.print_on(tty);
1178     tty-&gt;print_cr("     Virtual frames (innermost first):");
1179     for (int index = 0; index &lt; chunk-&gt;length(); index++) {
1180       compiledVFrame* vf = chunk-&gt;at(index);
1181       tty-&gt;print("       %2d - ", index);
1182       vf-&gt;print_value();
1183       int bci = chunk-&gt;at(index)-&gt;raw_bci();
1184       const char* code_name;
1185       if (bci == SynchronizationEntryBCI) {
1186         code_name = "sync entry";
1187       } else {
1188         Bytecodes::Code code = vf-&gt;method()-&gt;code_at(bci);
1189         code_name = Bytecodes::name(code);
1190       }
1191       tty-&gt;print(" - %s", code_name);
1192       tty-&gt;print_cr(" @ bci %d ", bci);
1193       if (Verbose) {
1194         vf-&gt;print();
1195         tty-&gt;cr();
1196       }
1197     }
1198   }
1199 #endif
1200 
1201   // Register map for next frame (used for stack crawl).  We capture
1202   // the state of the deopt'ing frame's caller.  Thus if we need to
1203   // stuff a C2I adapter we can properly fill in the callee-save
1204   // register locations.
1205   frame caller = fr.sender(reg_map);
1206   int frame_size = caller.sp() - fr.sp();
1207 
1208   frame sender = caller;
1209 
1210   // Since the Java thread being deoptimized will eventually adjust it's own stack,
1211   // the vframeArray containing the unpacking information is allocated in the C heap.
1212   // For Compiler1, the caller of the deoptimized frame is saved for use by unpack_frames().
1213   vframeArray* array = vframeArray::allocate(thread, frame_size, chunk, reg_map, sender, caller, fr, realloc_failures);
1214 
1215   // Compare the vframeArray to the collected vframes
1216   assert(array-&gt;structural_compare(thread, chunk), "just checking");
1217 
1218 #ifndef PRODUCT
1219   if (PrintDeoptimizationDetails) {
1220     ttyLocker ttyl;
1221     tty-&gt;print_cr("     Created vframeArray " INTPTR_FORMAT, p2i(array));
1222   }
1223 #endif // PRODUCT
1224 
1225   return array;
1226 }
1227 
1228 #if COMPILER2_OR_JVMCI
1229 void Deoptimization::pop_frames_failed_reallocs(JavaThread* thread, vframeArray* array) {
1230   // Reallocation of some scalar replaced objects failed. Record
1231   // that we need to pop all the interpreter frames for the
1232   // deoptimized compiled frame.
1233   assert(thread-&gt;frames_to_pop_failed_realloc() == 0, "missed frames to pop?");
1234   thread-&gt;set_frames_to_pop_failed_realloc(array-&gt;frames());
1235   // Unlock all monitors here otherwise the interpreter will see a
1236   // mix of locked and unlocked monitors (because of failed
1237   // reallocations of synchronized objects) and be confused.
1238   for (int i = 0; i &lt; array-&gt;frames(); i++) {
1239     MonitorChunk* monitors = array-&gt;element(i)-&gt;monitors();
1240     if (monitors != NULL) {
1241       for (int j = 0; j &lt; monitors-&gt;number_of_monitors(); j++) {
1242         BasicObjectLock* src = monitors-&gt;at(j);
1243         if (src-&gt;obj() != NULL) {
1244           ObjectSynchronizer::fast_exit(src-&gt;obj(), src-&gt;lock(), thread);
1245         }
1246       }
1247       array-&gt;element(i)-&gt;free_monitors(thread);
1248 #ifdef ASSERT
1249       array-&gt;element(i)-&gt;set_removed_monitors();
1250 #endif
1251     }
1252   }
1253 }
1254 #endif
1255 
1256 static void collect_monitors(compiledVFrame* cvf, GrowableArray&lt;Handle&gt;* objects_to_revoke) {
1257   GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
1258   Thread* thread = Thread::current();
1259   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1260     MonitorInfo* mon_info = monitors-&gt;at(i);
1261     if (!mon_info-&gt;eliminated() &amp;&amp; mon_info-&gt;owner() != NULL) {
1262       objects_to_revoke-&gt;append(Handle(thread, mon_info-&gt;owner()));
1263     }
1264   }
1265 }
1266 
1267 
1268 void Deoptimization::revoke_biases_of_monitors(JavaThread* thread, frame fr, RegisterMap* map) {
1269   if (!UseBiasedLocking) {
1270     return;
1271   }
1272 
1273   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1274 
1275   // Unfortunately we don't have a RegisterMap available in most of
1276   // the places we want to call this routine so we need to walk the
1277   // stack again to update the register map.
1278   if (map == NULL || !map-&gt;update_map()) {
1279     StackFrameStream sfs(thread, true);
1280     bool found = false;
1281     while (!found &amp;&amp; !sfs.is_done()) {
1282       frame* cur = sfs.current();
1283       sfs.next();
1284       found = cur-&gt;id() == fr.id();
1285     }
1286     assert(found, "frame to be deoptimized not found on target thread's stack");
1287     map = sfs.register_map();
1288   }
1289 
1290   vframe* vf = vframe::new_vframe(&amp;fr, map, thread);
1291   compiledVFrame* cvf = compiledVFrame::cast(vf);
1292   // Revoke monitors' biases in all scopes
1293   while (!cvf-&gt;is_top()) {
1294     collect_monitors(cvf, objects_to_revoke);
1295     cvf = compiledVFrame::cast(cvf-&gt;sender());
1296   }
1297   collect_monitors(cvf, objects_to_revoke);
1298 
1299   if (SafepointSynchronize::is_at_safepoint()) {
1300     BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1301   } else {
1302     BiasedLocking::revoke(objects_to_revoke);
1303   }
1304 }
1305 
1306 
1307 void Deoptimization::revoke_biases_of_monitors(CodeBlob* cb) {
1308   if (!UseBiasedLocking) {
1309     return;
1310   }
1311 
1312   assert(SafepointSynchronize::is_at_safepoint(), "must only be called from safepoint");
1313   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1314   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
1315     if (jt-&gt;has_last_Java_frame()) {
1316       StackFrameStream sfs(jt, true);
1317       while (!sfs.is_done()) {
1318         frame* cur = sfs.current();
1319         if (cb-&gt;contains(cur-&gt;pc())) {
1320           vframe* vf = vframe::new_vframe(cur, sfs.register_map(), jt);
1321           compiledVFrame* cvf = compiledVFrame::cast(vf);
1322           // Revoke monitors' biases in all scopes
1323           while (!cvf-&gt;is_top()) {
1324             collect_monitors(cvf, objects_to_revoke);
1325             cvf = compiledVFrame::cast(cvf-&gt;sender());
1326           }
1327           collect_monitors(cvf, objects_to_revoke);
1328         }
1329         sfs.next();
1330       }
1331     }
1332   }
1333   BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1334 }
1335 
1336 
1337 void Deoptimization::deoptimize_single_frame(JavaThread* thread, frame fr, Deoptimization::DeoptReason reason) {
1338   assert(fr.can_be_deoptimized(), "checking frame type");
1339 
1340   gather_statistics(reason, Action_none, Bytecodes::_illegal);
1341 
1342   if (LogCompilation &amp;&amp; xtty != NULL) {
1343     CompiledMethod* cm = fr.cb()-&gt;as_compiled_method_or_null();
1344     assert(cm != NULL, "only compiled methods can deopt");
1345 
1346     ttyLocker ttyl;
1347     xtty-&gt;begin_head("deoptimized thread='" UINTX_FORMAT "' reason='%s' pc='" INTPTR_FORMAT "'",(uintx)thread-&gt;osthread()-&gt;thread_id(), trap_reason_name(reason), p2i(fr.pc()));
1348     cm-&gt;log_identity(xtty);
1349     xtty-&gt;end_head();
1350     for (ScopeDesc* sd = cm-&gt;scope_desc_at(fr.pc()); ; sd = sd-&gt;sender()) {
1351       xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1352       xtty-&gt;method(sd-&gt;method());
1353       xtty-&gt;end_elem();
1354       if (sd-&gt;is_top())  break;
1355     }
1356     xtty-&gt;tail("deoptimized");
1357   }
1358 
1359   // Patch the compiled method so that when execution returns to it we will
1360   // deopt the execution state and return to the interpreter.
1361   fr.deoptimize(thread);
1362 }
1363 
1364 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map) {
1365   deoptimize(thread, fr, map, Reason_constraint);
1366 }
1367 
1368 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map, DeoptReason reason) {
1369   // Deoptimize only if the frame comes from compile code.
1370   // Do not deoptimize the frame which is already patched
1371   // during the execution of the loops below.
1372   if (!fr.is_compiled_frame() || fr.is_deoptimized_frame()) {
1373     return;
1374   }
1375   ResourceMark rm;
1376   DeoptimizationMarker dm;
1377   if (UseBiasedLocking) {
1378     revoke_biases_of_monitors(thread, fr, map);
1379   }
1380   deoptimize_single_frame(thread, fr, reason);
1381 
1382 }
1383 
1384 #if INCLUDE_JVMCI
1385 address Deoptimization::deoptimize_for_missing_exception_handler(CompiledMethod* cm) {
1386   // there is no exception handler for this pc =&gt; deoptimize
1387   cm-&gt;make_not_entrant();
1388 
1389   // Use Deoptimization::deoptimize for all of its side-effects:
1390   // revoking biases of monitors, gathering traps statistics, logging...
1391   // it also patches the return pc but we do not care about that
1392   // since we return a continuation to the deopt_blob below.
1393   JavaThread* thread = JavaThread::current();
1394   RegisterMap reg_map(thread, UseBiasedLocking);
1395   frame runtime_frame = thread-&gt;last_frame();
1396   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1397   assert(caller_frame.cb()-&gt;as_compiled_method_or_null() == cm, "expect top frame compiled method");
1398   Deoptimization::deoptimize(thread, caller_frame, &amp;reg_map, Deoptimization::Reason_not_compiled_exception_handler);
1399 
1400   MethodData* trap_mdo = get_method_data(thread, cm-&gt;method(), true);
1401   if (trap_mdo != NULL) {
1402     trap_mdo-&gt;inc_trap_count(Deoptimization::Reason_not_compiled_exception_handler);
1403   }
1404 
1405   return SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
1406 }
1407 #endif
1408 
1409 void Deoptimization::deoptimize_frame_internal(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1410   assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),
1411          "can only deoptimize other thread at a safepoint");
1412   // Compute frame and register map based on thread and sp.
1413   RegisterMap reg_map(thread, UseBiasedLocking);
1414   frame fr = thread-&gt;last_frame();
1415   while (fr.id() != id) {
1416     fr = fr.sender(&amp;reg_map);
1417   }
1418   deoptimize(thread, fr, &amp;reg_map, reason);
1419 }
1420 
1421 
1422 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1423   if (thread == Thread::current()) {
1424     Deoptimization::deoptimize_frame_internal(thread, id, reason);
1425   } else {
1426     VM_DeoptimizeFrame deopt(thread, id, reason);
1427     VMThread::execute(&amp;deopt);
1428   }
1429 }
1430 
1431 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id) {
1432   deoptimize_frame(thread, id, Reason_constraint);
1433 }
1434 
1435 // JVMTI PopFrame support
1436 JRT_LEAF(void, Deoptimization::popframe_preserve_args(JavaThread* thread, int bytes_to_save, void* start_address))
1437 {
1438   thread-&gt;popframe_preserve_args(in_ByteSize(bytes_to_save), start_address);
1439 }
1440 JRT_END
1441 
1442 MethodData*
1443 Deoptimization::get_method_data(JavaThread* thread, const methodHandle&amp; m,
1444                                 bool create_if_missing) {
1445   Thread* THREAD = thread;
1446   MethodData* mdo = m()-&gt;method_data();
1447   if (mdo == NULL &amp;&amp; create_if_missing &amp;&amp; !HAS_PENDING_EXCEPTION) {
1448     // Build an MDO.  Ignore errors like OutOfMemory;
1449     // that simply means we won't have an MDO to update.
1450     Method::build_interpreter_method_data(m, THREAD);
1451     if (HAS_PENDING_EXCEPTION) {
1452       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), "we expect only an OOM error here");
1453       CLEAR_PENDING_EXCEPTION;
1454     }
1455     mdo = m()-&gt;method_data();
1456   }
1457   return mdo;
1458 }
1459 
1460 #if COMPILER2_OR_JVMCI
1461 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index, TRAPS) {
1462   // in case of an unresolved klass entry, load the class.
1463   if (constant_pool-&gt;tag_at(index).is_unresolved_klass()) {
1464     Klass* tk = constant_pool-&gt;klass_at_ignore_error(index, CHECK);
1465     return;
1466   }
1467 
1468   if (!constant_pool-&gt;tag_at(index).is_symbol()) return;
1469 
1470   Handle class_loader (THREAD, constant_pool-&gt;pool_holder()-&gt;class_loader());
1471   Symbol*  symbol  = constant_pool-&gt;symbol_at(index);
1472 
1473   // class name?
1474   if (symbol-&gt;char_at(0) != '(') {
1475     Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1476     SystemDictionary::resolve_or_null(symbol, class_loader, protection_domain, CHECK);
1477     return;
1478   }
1479 
1480   // then it must be a signature!
1481   ResourceMark rm(THREAD);
1482   for (SignatureStream ss(symbol); !ss.is_done(); ss.next()) {
1483     if (ss.is_object()) {
1484       Symbol* class_name = ss.as_symbol(CHECK);
1485       Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1486       SystemDictionary::resolve_or_null(class_name, class_loader, protection_domain, CHECK);
1487     }
1488   }
1489 }
1490 
1491 
1492 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index) {
1493   EXCEPTION_MARK;
1494   load_class_by_index(constant_pool, index, THREAD);
1495   if (HAS_PENDING_EXCEPTION) {
1496     // Exception happened during classloading. We ignore the exception here, since it
1497     // is going to be rethrown since the current activation is going to be deoptimized and
1498     // the interpreter will re-execute the bytecode.
1499     CLEAR_PENDING_EXCEPTION;
1500     // Class loading called java code which may have caused a stack
1501     // overflow. If the exception was thrown right before the return
1502     // to the runtime the stack is no longer guarded. Reguard the
1503     // stack otherwise if we return to the uncommon trap blob and the
1504     // stack bang causes a stack overflow we crash.
1505     assert(THREAD-&gt;is_Java_thread(), "only a java thread can be here");
1506     JavaThread* thread = (JavaThread*)THREAD;
1507     bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
1508     if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
1509     assert(guard_pages_enabled, "stack banging in uncommon trap blob may cause crash");
1510   }
1511 }
1512 
1513 JRT_ENTRY(void, Deoptimization::uncommon_trap_inner(JavaThread* thread, jint trap_request)) {
1514   HandleMark hm;
1515 
1516   // uncommon_trap() is called at the beginning of the uncommon trap
1517   // handler. Note this fact before we start generating temporary frames
1518   // that can confuse an asynchronous stack walker. This counter is
1519   // decremented at the end of unpack_frames().
1520   thread-&gt;inc_in_deopt_handler();
1521 
1522   // We need to update the map if we have biased locking.
1523 #if INCLUDE_JVMCI
1524   // JVMCI might need to get an exception from the stack, which in turn requires the register map to be valid
1525   RegisterMap reg_map(thread, true);
1526 #else
1527   RegisterMap reg_map(thread, UseBiasedLocking);
1528 #endif
1529   frame stub_frame = thread-&gt;last_frame();
1530   frame fr = stub_frame.sender(&amp;reg_map);
1531   // Make sure the calling nmethod is not getting deoptimized and removed
1532   // before we are done with it.
1533   nmethodLocker nl(fr.pc());
1534 
1535   // Log a message
1536   Events::log(thread, "Uncommon trap: trap_request=" PTR32_FORMAT " fr.pc=" INTPTR_FORMAT " relative=" INTPTR_FORMAT,
1537               trap_request, p2i(fr.pc()), fr.pc() - fr.cb()-&gt;code_begin());
1538 
1539   {
1540     ResourceMark rm;
1541 
1542     // Revoke biases of any monitors in the frame to ensure we can migrate them
1543     revoke_biases_of_monitors(thread, fr, &amp;reg_map);
1544 
1545     DeoptReason reason = trap_request_reason(trap_request);
1546     DeoptAction action = trap_request_action(trap_request);
1547 #if INCLUDE_JVMCI
1548     int debug_id = trap_request_debug_id(trap_request);
1549 #endif
1550     jint unloaded_class_index = trap_request_index(trap_request); // CP idx or -1
1551 
1552     vframe*  vf  = vframe::new_vframe(&amp;fr, &amp;reg_map, thread);
1553     compiledVFrame* cvf = compiledVFrame::cast(vf);
1554 
1555     CompiledMethod* nm = cvf-&gt;code();
1556 
1557     ScopeDesc*      trap_scope  = cvf-&gt;scope();
1558 
1559     if (TraceDeoptimization) {
1560       ttyLocker ttyl;
1561       tty-&gt;print_cr("  bci=%d pc=" INTPTR_FORMAT ", relative_pc=" INTPTR_FORMAT ", method=%s" JVMCI_ONLY(", debug_id=%d"), trap_scope-&gt;bci(), p2i(fr.pc()), fr.pc() - nm-&gt;code_begin(), trap_scope-&gt;method()-&gt;name_and_sig_as_C_string()
1562 #if INCLUDE_JVMCI
1563           , debug_id
1564 #endif
1565           );
1566     }
1567 
1568     methodHandle    trap_method = trap_scope-&gt;method();
1569     int             trap_bci    = trap_scope-&gt;bci();
1570 #if INCLUDE_JVMCI
1571     long speculation = thread-&gt;pending_failed_speculation();
1572     if (nm-&gt;is_compiled_by_jvmci()) {
1573       if (speculation != 0) {
1574         oop speculation_log = nm-&gt;as_nmethod()-&gt;speculation_log();
1575         if (speculation_log != NULL) {
1576           if (TraceDeoptimization || TraceUncollectedSpeculations) {
1577             if (HotSpotSpeculationLog::lastFailed(speculation_log) != 0) {
1578               tty-&gt;print_cr("A speculation that was not collected by the compiler is being overwritten");
1579             }
1580           }
1581           if (TraceDeoptimization) {
1582             tty-&gt;print_cr("Saving speculation to speculation log");
1583           }
1584           HotSpotSpeculationLog::set_lastFailed(speculation_log, speculation);
1585         } else {
1586           if (TraceDeoptimization) {
1587             tty-&gt;print_cr("Speculation present but no speculation log");
1588           }
1589         }
1590         thread-&gt;set_pending_failed_speculation(0);
1591       } else {
1592         if (TraceDeoptimization) {
1593           tty-&gt;print_cr("No speculation");
1594         }
1595       }
1596     } else {
1597       assert(speculation == 0, "There should not be a speculation for method compiled by non-JVMCI compilers");
1598     }
1599 
1600     if (trap_bci == SynchronizationEntryBCI) {
1601       trap_bci = 0;
1602       thread-&gt;set_pending_monitorenter(true);
1603     }
1604 
1605     if (reason == Deoptimization::Reason_transfer_to_interpreter) {
1606       thread-&gt;set_pending_transfer_to_interpreter(true);
1607     }
1608 #endif
1609 
1610     Bytecodes::Code trap_bc     = trap_method-&gt;java_code_at(trap_bci);
1611     // Record this event in the histogram.
1612     gather_statistics(reason, action, trap_bc);
1613 
1614     // Ensure that we can record deopt. history:
1615     // Need MDO to record RTM code generation state.
1616     bool create_if_missing = ProfileTraps || UseCodeAging RTM_OPT_ONLY( || UseRTMLocking );
1617 
1618     methodHandle profiled_method;
1619 #if INCLUDE_JVMCI
1620     if (nm-&gt;is_compiled_by_jvmci()) {
1621       profiled_method = nm-&gt;method();
1622     } else {
1623       profiled_method = trap_method;
1624     }
1625 #else
1626     profiled_method = trap_method;
1627 #endif
1628 
1629     MethodData* trap_mdo =
1630       get_method_data(thread, profiled_method, create_if_missing);
1631 
1632     // Log a message
1633     Events::log_deopt_message(thread, "Uncommon trap: reason=%s action=%s pc=" INTPTR_FORMAT " method=%s @ %d %s",
1634                               trap_reason_name(reason), trap_action_name(action), p2i(fr.pc()),
1635                               trap_method-&gt;name_and_sig_as_C_string(), trap_bci, nm-&gt;compiler_name());
1636 
1637     // Print a bunch of diagnostics, if requested.
1638     if (TraceDeoptimization || LogCompilation) {
1639       ResourceMark rm;
1640       ttyLocker ttyl;
1641       char buf[100];
1642       if (xtty != NULL) {
1643         xtty-&gt;begin_head("uncommon_trap thread='" UINTX_FORMAT "' %s",
1644                          os::current_thread_id(),
1645                          format_trap_request(buf, sizeof(buf), trap_request));
1646         nm-&gt;log_identity(xtty);
1647       }
1648       Symbol* class_name = NULL;
1649       bool unresolved = false;
1650       if (unloaded_class_index &gt;= 0) {
1651         constantPoolHandle constants (THREAD, trap_method-&gt;constants());
1652         if (constants-&gt;tag_at(unloaded_class_index).is_unresolved_klass()) {
1653           class_name = constants-&gt;klass_name_at(unloaded_class_index);
1654           unresolved = true;
1655           if (xtty != NULL)
1656             xtty-&gt;print(" unresolved='1'");
1657         } else if (constants-&gt;tag_at(unloaded_class_index).is_symbol()) {
1658           class_name = constants-&gt;symbol_at(unloaded_class_index);
1659         }
1660         if (xtty != NULL)
1661           xtty-&gt;name(class_name);
1662       }
1663       if (xtty != NULL &amp;&amp; trap_mdo != NULL &amp;&amp; (int)reason &lt; (int)MethodData::_trap_hist_limit) {
1664         // Dump the relevant MDO state.
1665         // This is the deopt count for the current reason, any previous
1666         // reasons or recompiles seen at this point.
1667         int dcnt = trap_mdo-&gt;trap_count(reason);
1668         if (dcnt != 0)
1669           xtty-&gt;print(" count='%d'", dcnt);
1670         ProfileData* pdata = trap_mdo-&gt;bci_to_data(trap_bci);
1671         int dos = (pdata == NULL)? 0: pdata-&gt;trap_state();
1672         if (dos != 0) {
1673           xtty-&gt;print(" state='%s'", format_trap_state(buf, sizeof(buf), dos));
1674           if (trap_state_is_recompiled(dos)) {
1675             int recnt2 = trap_mdo-&gt;overflow_recompile_count();
1676             if (recnt2 != 0)
1677               xtty-&gt;print(" recompiles2='%d'", recnt2);
1678           }
1679         }
1680       }
1681       if (xtty != NULL) {
1682         xtty-&gt;stamp();
1683         xtty-&gt;end_head();
1684       }
1685       if (TraceDeoptimization) {  // make noise on the tty
1686         tty-&gt;print("Uncommon trap occurred in");
1687         nm-&gt;method()-&gt;print_short_name(tty);
1688         tty-&gt;print(" compiler=%s compile_id=%d", nm-&gt;compiler_name(), nm-&gt;compile_id());
1689 #if INCLUDE_JVMCI
1690         if (nm-&gt;is_nmethod()) {
1691           char* installed_code_name = nm-&gt;as_nmethod()-&gt;jvmci_installed_code_name(buf, sizeof(buf));
1692           if (installed_code_name != NULL) {
1693             tty-&gt;print(" (JVMCI: installed code name=%s) ", installed_code_name);
1694           }
1695         }
1696 #endif
1697         tty-&gt;print(" (@" INTPTR_FORMAT ") thread=" UINTX_FORMAT " reason=%s action=%s unloaded_class_index=%d" JVMCI_ONLY(" debug_id=%d"),
1698                    p2i(fr.pc()),
1699                    os::current_thread_id(),
1700                    trap_reason_name(reason),
1701                    trap_action_name(action),
1702                    unloaded_class_index
1703 #if INCLUDE_JVMCI
1704                    , debug_id
1705 #endif
1706                    );
1707         if (class_name != NULL) {
1708           tty-&gt;print(unresolved ? " unresolved class: " : " symbol: ");
1709           class_name-&gt;print_symbol_on(tty);
1710         }
1711         tty-&gt;cr();
1712       }
1713       if (xtty != NULL) {
1714         // Log the precise location of the trap.
1715         for (ScopeDesc* sd = trap_scope; ; sd = sd-&gt;sender()) {
1716           xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1717           xtty-&gt;method(sd-&gt;method());
1718           xtty-&gt;end_elem();
1719           if (sd-&gt;is_top())  break;
1720         }
1721         xtty-&gt;tail("uncommon_trap");
1722       }
1723     }
1724     // (End diagnostic printout.)
1725 
1726     // Load class if necessary
1727     if (unloaded_class_index &gt;= 0) {
1728       constantPoolHandle constants(THREAD, trap_method-&gt;constants());
1729       load_class_by_index(constants, unloaded_class_index);
1730     }
1731 
1732     // Flush the nmethod if necessary and desirable.
1733     //
1734     // We need to avoid situations where we are re-flushing the nmethod
1735     // because of a hot deoptimization site.  Repeated flushes at the same
1736     // point need to be detected by the compiler and avoided.  If the compiler
1737     // cannot avoid them (or has a bug and "refuses" to avoid them), this
1738     // module must take measures to avoid an infinite cycle of recompilation
1739     // and deoptimization.  There are several such measures:
1740     //
1741     //   1. If a recompilation is ordered a second time at some site X
1742     //   and for the same reason R, the action is adjusted to 'reinterpret',
1743     //   to give the interpreter time to exercise the method more thoroughly.
1744     //   If this happens, the method's overflow_recompile_count is incremented.
1745     //
1746     //   2. If the compiler fails to reduce the deoptimization rate, then
1747     //   the method's overflow_recompile_count will begin to exceed the set
1748     //   limit PerBytecodeRecompilationCutoff.  If this happens, the action
1749     //   is adjusted to 'make_not_compilable', and the method is abandoned
1750     //   to the interpreter.  This is a performance hit for hot methods,
1751     //   but is better than a disastrous infinite cycle of recompilations.
1752     //   (Actually, only the method containing the site X is abandoned.)
1753     //
1754     //   3. In parallel with the previous measures, if the total number of
1755     //   recompilations of a method exceeds the much larger set limit
1756     //   PerMethodRecompilationCutoff, the method is abandoned.
1757     //   This should only happen if the method is very large and has
1758     //   many "lukewarm" deoptimizations.  The code which enforces this
1759     //   limit is elsewhere (class nmethod, class Method).
1760     //
1761     // Note that the per-BCI 'is_recompiled' bit gives the compiler one chance
1762     // to recompile at each bytecode independently of the per-BCI cutoff.
1763     //
1764     // The decision to update code is up to the compiler, and is encoded
1765     // in the Action_xxx code.  If the compiler requests Action_none
1766     // no trap state is changed, no compiled code is changed, and the
1767     // computation suffers along in the interpreter.
1768     //
1769     // The other action codes specify various tactics for decompilation
1770     // and recompilation.  Action_maybe_recompile is the loosest, and
1771     // allows the compiled code to stay around until enough traps are seen,
1772     // and until the compiler gets around to recompiling the trapping method.
1773     //
1774     // The other actions cause immediate removal of the present code.
1775 
1776     // Traps caused by injected profile shouldn't pollute trap counts.
1777     bool injected_profile_trap = trap_method-&gt;has_injected_profile() &amp;&amp;
1778                                  (reason == Reason_intrinsic || reason == Reason_unreached);
1779 
1780     bool update_trap_state = (reason != Reason_tenured) &amp;&amp; !injected_profile_trap;
1781     bool make_not_entrant = false;
1782     bool make_not_compilable = false;
1783     bool reprofile = false;
1784     switch (action) {
1785     case Action_none:
1786       // Keep the old code.
1787       update_trap_state = false;
1788       break;
1789     case Action_maybe_recompile:
1790       // Do not need to invalidate the present code, but we can
1791       // initiate another
1792       // Start compiler without (necessarily) invalidating the nmethod.
1793       // The system will tolerate the old code, but new code should be
1794       // generated when possible.
1795       break;
1796     case Action_reinterpret:
1797       // Go back into the interpreter for a while, and then consider
1798       // recompiling form scratch.
1799       make_not_entrant = true;
1800       // Reset invocation counter for outer most method.
1801       // This will allow the interpreter to exercise the bytecodes
1802       // for a while before recompiling.
1803       // By contrast, Action_make_not_entrant is immediate.
1804       //
1805       // Note that the compiler will track null_check, null_assert,
1806       // range_check, and class_check events and log them as if they
1807       // had been traps taken from compiled code.  This will update
1808       // the MDO trap history so that the next compilation will
1809       // properly detect hot trap sites.
1810       reprofile = true;
1811       break;
1812     case Action_make_not_entrant:
1813       // Request immediate recompilation, and get rid of the old code.
1814       // Make them not entrant, so next time they are called they get
1815       // recompiled.  Unloaded classes are loaded now so recompile before next
1816       // time they are called.  Same for uninitialized.  The interpreter will
1817       // link the missing class, if any.
1818       make_not_entrant = true;
1819       break;
1820     case Action_make_not_compilable:
1821       // Give up on compiling this method at all.
1822       make_not_entrant = true;
1823       make_not_compilable = true;
1824       break;
1825     default:
1826       ShouldNotReachHere();
1827     }
1828 
1829     // Setting +ProfileTraps fixes the following, on all platforms:
1830     // 4852688: ProfileInterpreter is off by default for ia64.  The result is
1831     // infinite heroic-opt-uncommon-trap/deopt/recompile cycles, since the
1832     // recompile relies on a MethodData* to record heroic opt failures.
1833 
1834     // Whether the interpreter is producing MDO data or not, we also need
1835     // to use the MDO to detect hot deoptimization points and control
1836     // aggressive optimization.
1837     bool inc_recompile_count = false;
1838     ProfileData* pdata = NULL;
1839     if (ProfileTraps &amp;&amp; !is_client_compilation_mode_vm() &amp;&amp; update_trap_state &amp;&amp; trap_mdo != NULL) {
1840       assert(trap_mdo == get_method_data(thread, profiled_method, false), "sanity");
1841       uint this_trap_count = 0;
1842       bool maybe_prior_trap = false;
1843       bool maybe_prior_recompile = false;
1844       pdata = query_update_method_data(trap_mdo, trap_bci, reason, true,
1845 #if INCLUDE_JVMCI
1846                                    nm-&gt;is_compiled_by_jvmci() &amp;&amp; nm-&gt;is_osr_method(),
1847 #endif
1848                                    nm-&gt;method(),
1849                                    //outputs:
1850                                    this_trap_count,
1851                                    maybe_prior_trap,
1852                                    maybe_prior_recompile);
1853       // Because the interpreter also counts null, div0, range, and class
1854       // checks, these traps from compiled code are double-counted.
1855       // This is harmless; it just means that the PerXTrapLimit values
1856       // are in effect a little smaller than they look.
1857 
1858       DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1859       if (per_bc_reason != Reason_none) {
1860         // Now take action based on the partially known per-BCI history.
1861         if (maybe_prior_trap
1862             &amp;&amp; this_trap_count &gt;= (uint)PerBytecodeTrapLimit) {
1863           // If there are too many traps at this BCI, force a recompile.
1864           // This will allow the compiler to see the limit overflow, and
1865           // take corrective action, if possible.  The compiler generally
1866           // does not use the exact PerBytecodeTrapLimit value, but instead
1867           // changes its tactics if it sees any traps at all.  This provides
1868           // a little hysteresis, delaying a recompile until a trap happens
1869           // several times.
1870           //
1871           // Actually, since there is only one bit of counter per BCI,
1872           // the possible per-BCI counts are {0,1,(per-method count)}.
1873           // This produces accurate results if in fact there is only
1874           // one hot trap site, but begins to get fuzzy if there are
1875           // many sites.  For example, if there are ten sites each
1876           // trapping two or more times, they each get the blame for
1877           // all of their traps.
1878           make_not_entrant = true;
1879         }
1880 
1881         // Detect repeated recompilation at the same BCI, and enforce a limit.
1882         if (make_not_entrant &amp;&amp; maybe_prior_recompile) {
1883           // More than one recompile at this point.
1884           inc_recompile_count = maybe_prior_trap;
1885         }
1886       } else {
1887         // For reasons which are not recorded per-bytecode, we simply
1888         // force recompiles unconditionally.
1889         // (Note that PerMethodRecompilationCutoff is enforced elsewhere.)
1890         make_not_entrant = true;
1891       }
1892 
1893       // Go back to the compiler if there are too many traps in this method.
1894       if (this_trap_count &gt;= per_method_trap_limit(reason)) {
1895         // If there are too many traps in this method, force a recompile.
1896         // This will allow the compiler to see the limit overflow, and
1897         // take corrective action, if possible.
1898         // (This condition is an unlikely backstop only, because the
1899         // PerBytecodeTrapLimit is more likely to take effect first,
1900         // if it is applicable.)
1901         make_not_entrant = true;
1902       }
1903 
1904       // Here's more hysteresis:  If there has been a recompile at
1905       // this trap point already, run the method in the interpreter
1906       // for a while to exercise it more thoroughly.
1907       if (make_not_entrant &amp;&amp; maybe_prior_recompile &amp;&amp; maybe_prior_trap) {
1908         reprofile = true;
1909       }
1910     }
1911 
1912     // Take requested actions on the method:
1913 
1914     // Recompile
1915     if (make_not_entrant) {
1916       if (!nm-&gt;make_not_entrant()) {
1917         return; // the call did not change nmethod's state
1918       }
1919 
1920       if (pdata != NULL) {
1921         // Record the recompilation event, if any.
1922         int tstate0 = pdata-&gt;trap_state();
1923         int tstate1 = trap_state_set_recompiled(tstate0, true);
1924         if (tstate1 != tstate0)
1925           pdata-&gt;set_trap_state(tstate1);
1926       }
1927 
1928 #if INCLUDE_RTM_OPT
1929       // Restart collecting RTM locking abort statistic if the method
1930       // is recompiled for a reason other than RTM state change.
1931       // Assume that in new recompiled code the statistic could be different,
1932       // for example, due to different inlining.
1933       if ((reason != Reason_rtm_state_change) &amp;&amp; (trap_mdo != NULL) &amp;&amp;
1934           UseRTMDeopt &amp;&amp; (nm-&gt;as_nmethod()-&gt;rtm_state() != ProfileRTM)) {
1935         trap_mdo-&gt;atomic_set_rtm_state(ProfileRTM);
1936       }
1937 #endif
1938       // For code aging we count traps separately here, using make_not_entrant()
1939       // as a guard against simultaneous deopts in multiple threads.
1940       if (reason == Reason_tenured &amp;&amp; trap_mdo != NULL) {
1941         trap_mdo-&gt;inc_tenure_traps();
1942       }
1943     }
1944 
1945     if (inc_recompile_count) {
1946       trap_mdo-&gt;inc_overflow_recompile_count();
1947       if ((uint)trap_mdo-&gt;overflow_recompile_count() &gt;
1948           (uint)PerBytecodeRecompilationCutoff) {
1949         // Give up on the method containing the bad BCI.
1950         if (trap_method() == nm-&gt;method()) {
1951           make_not_compilable = true;
1952         } else {
1953           trap_method-&gt;set_not_compilable(CompLevel_full_optimization, true, "overflow_recompile_count &gt; PerBytecodeRecompilationCutoff");
1954           // But give grace to the enclosing nm-&gt;method().
1955         }
1956       }
1957     }
1958 
1959     // Reprofile
1960     if (reprofile) {
1961       CompilationPolicy::policy()-&gt;reprofile(trap_scope, nm-&gt;is_osr_method());
1962     }
1963 
1964     // Give up compiling
1965     if (make_not_compilable &amp;&amp; !nm-&gt;method()-&gt;is_not_compilable(CompLevel_full_optimization)) {
1966       assert(make_not_entrant, "consistent");
1967       nm-&gt;method()-&gt;set_not_compilable(CompLevel_full_optimization);
1968     }
1969 
1970   } // Free marked resources
1971 
1972 }
1973 JRT_END
1974 
1975 ProfileData*
1976 Deoptimization::query_update_method_data(MethodData* trap_mdo,
1977                                          int trap_bci,
1978                                          Deoptimization::DeoptReason reason,
1979                                          bool update_total_trap_count,
1980 #if INCLUDE_JVMCI
1981                                          bool is_osr,
1982 #endif
1983                                          Method* compiled_method,
1984                                          //outputs:
1985                                          uint&amp; ret_this_trap_count,
1986                                          bool&amp; ret_maybe_prior_trap,
1987                                          bool&amp; ret_maybe_prior_recompile) {
1988   bool maybe_prior_trap = false;
1989   bool maybe_prior_recompile = false;
1990   uint this_trap_count = 0;
1991   if (update_total_trap_count) {
1992     uint idx = reason;
1993 #if INCLUDE_JVMCI
1994     if (is_osr) {
1995       idx += Reason_LIMIT;
1996     }
1997 #endif
1998     uint prior_trap_count = trap_mdo-&gt;trap_count(idx);
1999     this_trap_count  = trap_mdo-&gt;inc_trap_count(idx);
2000 
2001     // If the runtime cannot find a place to store trap history,
2002     // it is estimated based on the general condition of the method.
2003     // If the method has ever been recompiled, or has ever incurred
2004     // a trap with the present reason , then this BCI is assumed
2005     // (pessimistically) to be the culprit.
2006     maybe_prior_trap      = (prior_trap_count != 0);
2007     maybe_prior_recompile = (trap_mdo-&gt;decompile_count() != 0);
2008   }
2009   ProfileData* pdata = NULL;
2010 
2011 
2012   // For reasons which are recorded per bytecode, we check per-BCI data.
2013   DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
2014   assert(per_bc_reason != Reason_none || update_total_trap_count, "must be");
2015   if (per_bc_reason != Reason_none) {
2016     // Find the profile data for this BCI.  If there isn't one,
2017     // try to allocate one from the MDO's set of spares.
2018     // This will let us detect a repeated trap at this point.
2019     pdata = trap_mdo-&gt;allocate_bci_to_data(trap_bci, reason_is_speculate(reason) ? compiled_method : NULL);
2020 
2021     if (pdata != NULL) {
2022       if (reason_is_speculate(reason) &amp;&amp; !pdata-&gt;is_SpeculativeTrapData()) {
2023         if (LogCompilation &amp;&amp; xtty != NULL) {
2024           ttyLocker ttyl;
2025           // no more room for speculative traps in this MDO
2026           xtty-&gt;elem("speculative_traps_oom");
2027         }
2028       }
2029       // Query the trap state of this profile datum.
2030       int tstate0 = pdata-&gt;trap_state();
2031       if (!trap_state_has_reason(tstate0, per_bc_reason))
2032         maybe_prior_trap = false;
2033       if (!trap_state_is_recompiled(tstate0))
2034         maybe_prior_recompile = false;
2035 
2036       // Update the trap state of this profile datum.
2037       int tstate1 = tstate0;
2038       // Record the reason.
2039       tstate1 = trap_state_add_reason(tstate1, per_bc_reason);
2040       // Store the updated state on the MDO, for next time.
2041       if (tstate1 != tstate0)
2042         pdata-&gt;set_trap_state(tstate1);
2043     } else {
2044       if (LogCompilation &amp;&amp; xtty != NULL) {
2045         ttyLocker ttyl;
2046         // Missing MDP?  Leave a small complaint in the log.
2047         xtty-&gt;elem("missing_mdp bci='%d'", trap_bci);
2048       }
2049     }
2050   }
2051 
2052   // Return results:
2053   ret_this_trap_count = this_trap_count;
2054   ret_maybe_prior_trap = maybe_prior_trap;
2055   ret_maybe_prior_recompile = maybe_prior_recompile;
2056   return pdata;
2057 }
2058 
2059 void
2060 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2061   ResourceMark rm;
2062   // Ignored outputs:
2063   uint ignore_this_trap_count;
2064   bool ignore_maybe_prior_trap;
2065   bool ignore_maybe_prior_recompile;
2066   assert(!reason_is_speculate(reason), "reason speculate only used by compiler");
2067   // JVMCI uses the total counts to determine if deoptimizations are happening too frequently -&gt; do not adjust total counts
2068   bool update_total_counts = true JVMCI_ONLY( &amp;&amp; !UseJVMCICompiler);
2069   query_update_method_data(trap_mdo, trap_bci,
2070                            (DeoptReason)reason,
2071                            update_total_counts,
2072 #if INCLUDE_JVMCI
2073                            false,
2074 #endif
2075                            NULL,
2076                            ignore_this_trap_count,
2077                            ignore_maybe_prior_trap,
2078                            ignore_maybe_prior_recompile);
2079 }
2080 
2081 Deoptimization::UnrollBlock* Deoptimization::uncommon_trap(JavaThread* thread, jint trap_request, jint exec_mode) {
2082   if (TraceDeoptimization) {
2083     tty-&gt;print("Uncommon trap ");
2084   }
2085   // Still in Java no safepoints
2086   {
2087     // This enters VM and may safepoint
2088     uncommon_trap_inner(thread, trap_request);
2089   }
2090   return fetch_unroll_info_helper(thread, exec_mode);
2091 }
2092 
2093 // Local derived constants.
2094 // Further breakdown of DataLayout::trap_state, as promised by DataLayout.
2095 const int DS_REASON_MASK   = ((uint)DataLayout::trap_mask) &gt;&gt; 1;
2096 const int DS_RECOMPILE_BIT = DataLayout::trap_mask - DS_REASON_MASK;
2097 
2098 //---------------------------trap_state_reason---------------------------------
2099 Deoptimization::DeoptReason
2100 Deoptimization::trap_state_reason(int trap_state) {
2101   // This assert provides the link between the width of DataLayout::trap_bits
2102   // and the encoding of "recorded" reasons.  It ensures there are enough
2103   // bits to store all needed reasons in the per-BCI MDO profile.
2104   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2105   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2106   trap_state -= recompile_bit;
2107   if (trap_state == DS_REASON_MASK) {
2108     return Reason_many;
2109   } else {
2110     assert((int)Reason_none == 0, "state=0 =&gt; Reason_none");
2111     return (DeoptReason)trap_state;
2112   }
2113 }
2114 //-------------------------trap_state_has_reason-------------------------------
2115 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2116   assert(reason_is_recorded_per_bytecode((DeoptReason)reason), "valid reason");
2117   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2118   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2119   trap_state -= recompile_bit;
2120   if (trap_state == DS_REASON_MASK) {
2121     return -1;  // true, unspecifically (bottom of state lattice)
2122   } else if (trap_state == reason) {
2123     return 1;   // true, definitely
2124   } else if (trap_state == 0) {
2125     return 0;   // false, definitely (top of state lattice)
2126   } else {
2127     return 0;   // false, definitely
2128   }
2129 }
2130 //-------------------------trap_state_add_reason-------------------------------
2131 int Deoptimization::trap_state_add_reason(int trap_state, int reason) {
2132   assert(reason_is_recorded_per_bytecode((DeoptReason)reason) || reason == Reason_many, "valid reason");
2133   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2134   trap_state -= recompile_bit;
2135   if (trap_state == DS_REASON_MASK) {
2136     return trap_state + recompile_bit;     // already at state lattice bottom
2137   } else if (trap_state == reason) {
2138     return trap_state + recompile_bit;     // the condition is already true
2139   } else if (trap_state == 0) {
2140     return reason + recompile_bit;          // no condition has yet been true
2141   } else {
2142     return DS_REASON_MASK + recompile_bit;  // fall to state lattice bottom
2143   }
2144 }
2145 //-----------------------trap_state_is_recompiled------------------------------
2146 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2147   return (trap_state &amp; DS_RECOMPILE_BIT) != 0;
2148 }
2149 //-----------------------trap_state_set_recompiled-----------------------------
2150 int Deoptimization::trap_state_set_recompiled(int trap_state, bool z) {
2151   if (z)  return trap_state |  DS_RECOMPILE_BIT;
2152   else    return trap_state &amp; ~DS_RECOMPILE_BIT;
2153 }
2154 //---------------------------format_trap_state---------------------------------
2155 // This is used for debugging and diagnostics, including LogFile output.
2156 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2157                                               int trap_state) {
2158   assert(buflen &gt; 0, "sanity");
2159   DeoptReason reason      = trap_state_reason(trap_state);
2160   bool        recomp_flag = trap_state_is_recompiled(trap_state);
2161   // Re-encode the state from its decoded components.
2162   int decoded_state = 0;
2163   if (reason_is_recorded_per_bytecode(reason) || reason == Reason_many)
2164     decoded_state = trap_state_add_reason(decoded_state, reason);
2165   if (recomp_flag)
2166     decoded_state = trap_state_set_recompiled(decoded_state, recomp_flag);
2167   // If the state re-encodes properly, format it symbolically.
2168   // Because this routine is used for debugging and diagnostics,
2169   // be robust even if the state is a strange value.
2170   size_t len;
2171   if (decoded_state != trap_state) {
2172     // Random buggy state that doesn't decode??
2173     len = jio_snprintf(buf, buflen, "#%d", trap_state);
2174   } else {
2175     len = jio_snprintf(buf, buflen, "%s%s",
2176                        trap_reason_name(reason),
2177                        recomp_flag ? " recompiled" : "");
2178   }
2179   return buf;
2180 }
2181 
2182 
2183 //--------------------------------statics--------------------------------------
2184 const char* Deoptimization::_trap_reason_name[] = {
2185   // Note:  Keep this in sync. with enum DeoptReason.
2186   "none",
2187   "null_check",
2188   "null_assert" JVMCI_ONLY("_or_unreached0"),
2189   "range_check",
2190   "class_check",
2191   "array_check",
2192   "intrinsic" JVMCI_ONLY("_or_type_checked_inlining"),
2193   "bimorphic" JVMCI_ONLY("_or_optimized_type_check"),
2194   "profile_predicate",
2195   "unloaded",
2196   "uninitialized",
2197   "unreached",
2198   "unhandled",
2199   "constraint",
2200   "div0_check",
2201   "age",
2202   "predicate",
2203   "loop_limit_check",
2204   "speculate_class_check",
2205   "speculate_null_check",
2206   "speculate_null_assert",
2207   "rtm_state_change",
2208   "unstable_if",
2209   "unstable_fused_if",
2210 #if INCLUDE_JVMCI
2211   "aliasing",
2212   "transfer_to_interpreter",
2213   "not_compiled_exception_handler",
2214   "unresolved",
2215   "jsr_mismatch",
2216 #endif
2217   "tenured"
2218 };
2219 const char* Deoptimization::_trap_action_name[] = {
2220   // Note:  Keep this in sync. with enum DeoptAction.
2221   "none",
2222   "maybe_recompile",
2223   "reinterpret",
2224   "make_not_entrant",
2225   "make_not_compilable"
2226 };
2227 
2228 const char* Deoptimization::trap_reason_name(int reason) {
2229   // Check that every reason has a name
2230   STATIC_ASSERT(sizeof(_trap_reason_name)/sizeof(const char*) == Reason_LIMIT);
2231 
2232   if (reason == Reason_many)  return "many";
2233   if ((uint)reason &lt; Reason_LIMIT)
2234     return _trap_reason_name[reason];
2235   static char buf[20];
2236   sprintf(buf, "reason%d", reason);
2237   return buf;
2238 }
2239 const char* Deoptimization::trap_action_name(int action) {
2240   // Check that every action has a name
2241   STATIC_ASSERT(sizeof(_trap_action_name)/sizeof(const char*) == Action_LIMIT);
2242 
2243   if ((uint)action &lt; Action_LIMIT)
2244     return _trap_action_name[action];
2245   static char buf[20];
2246   sprintf(buf, "action%d", action);
2247   return buf;
2248 }
2249 
2250 // This is used for debugging and diagnostics, including LogFile output.
2251 const char* Deoptimization::format_trap_request(char* buf, size_t buflen,
2252                                                 int trap_request) {
2253   jint unloaded_class_index = trap_request_index(trap_request);
2254   const char* reason = trap_reason_name(trap_request_reason(trap_request));
2255   const char* action = trap_action_name(trap_request_action(trap_request));
2256 #if INCLUDE_JVMCI
2257   int debug_id = trap_request_debug_id(trap_request);
2258 #endif
2259   size_t len;
2260   if (unloaded_class_index &lt; 0) {
2261     len = jio_snprintf(buf, buflen, "reason='%s' action='%s'" JVMCI_ONLY(" debug_id='%d'"),
2262                        reason, action
2263 #if INCLUDE_JVMCI
2264                        ,debug_id
2265 #endif
2266                        );
2267   } else {
2268     len = jio_snprintf(buf, buflen, "reason='%s' action='%s' index='%d'" JVMCI_ONLY(" debug_id='%d'"),
2269                        reason, action, unloaded_class_index
2270 #if INCLUDE_JVMCI
2271                        ,debug_id
2272 #endif
2273                        );
2274   }
2275   return buf;
2276 }
2277 
2278 juint Deoptimization::_deoptimization_hist
2279         [Deoptimization::Reason_LIMIT]
2280     [1 + Deoptimization::Action_LIMIT]
2281         [Deoptimization::BC_CASE_LIMIT]
2282   = {0};
2283 
2284 enum {
2285   LSB_BITS = 8,
2286   LSB_MASK = right_n_bits(LSB_BITS)
2287 };
2288 
2289 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2290                                        Bytecodes::Code bc) {
2291   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2292   assert(action &gt;= 0 &amp;&amp; action &lt; Action_LIMIT, "oob");
2293   _deoptimization_hist[Reason_none][0][0] += 1;  // total
2294   _deoptimization_hist[reason][0][0]      += 1;  // per-reason total
2295   juint* cases = _deoptimization_hist[reason][1+action];
2296   juint* bc_counter_addr = NULL;
2297   juint  bc_counter      = 0;
2298   // Look for an unused counter, or an exact match to this BC.
2299   if (bc != Bytecodes::_illegal) {
2300     for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2301       juint* counter_addr = &amp;cases[bc_case];
2302       juint  counter = *counter_addr;
2303       if ((counter == 0 &amp;&amp; bc_counter_addr == NULL)
2304           || (Bytecodes::Code)(counter &amp; LSB_MASK) == bc) {
2305         // this counter is either free or is already devoted to this BC
2306         bc_counter_addr = counter_addr;
2307         bc_counter = counter | bc;
2308       }
2309     }
2310   }
2311   if (bc_counter_addr == NULL) {
2312     // Overflow, or no given bytecode.
2313     bc_counter_addr = &amp;cases[BC_CASE_LIMIT-1];
2314     bc_counter = (*bc_counter_addr &amp; ~LSB_MASK);  // clear LSB
2315   }
2316   *bc_counter_addr = bc_counter + (1 &lt;&lt; LSB_BITS);
2317 }
2318 
2319 jint Deoptimization::total_deoptimization_count() {
2320   return _deoptimization_hist[Reason_none][0][0];
2321 }
2322 
2323 jint Deoptimization::deoptimization_count(DeoptReason reason) {
2324   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2325   return _deoptimization_hist[reason][0][0];
2326 }
2327 
2328 void Deoptimization::print_statistics() {
2329   juint total = total_deoptimization_count();
2330   juint account = total;
2331   if (total != 0) {
2332     ttyLocker ttyl;
2333     if (xtty != NULL)  xtty-&gt;head("statistics type='deoptimization'");
2334     tty-&gt;print_cr("Deoptimization traps recorded:");
2335     #define PRINT_STAT_LINE(name, r) \
2336       tty-&gt;print_cr("  %4d (%4.1f%%) %s", (int)(r), ((r) * 100.0) / total, name);
2337     PRINT_STAT_LINE("total", total);
2338     // For each non-zero entry in the histogram, print the reason,
2339     // the action, and (if specifically known) the type of bytecode.
2340     for (int reason = 0; reason &lt; Reason_LIMIT; reason++) {
2341       for (int action = 0; action &lt; Action_LIMIT; action++) {
2342         juint* cases = _deoptimization_hist[reason][1+action];
2343         for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2344           juint counter = cases[bc_case];
2345           if (counter != 0) {
2346             char name[1*K];
2347             Bytecodes::Code bc = (Bytecodes::Code)(counter &amp; LSB_MASK);
2348             if (bc_case == BC_CASE_LIMIT &amp;&amp; (int)bc == 0)
2349               bc = Bytecodes::_illegal;
2350             sprintf(name, "%s/%s/%s",
2351                     trap_reason_name(reason),
2352                     trap_action_name(action),
2353                     Bytecodes::is_defined(bc)? Bytecodes::name(bc): "other");
2354             juint r = counter &gt;&gt; LSB_BITS;
2355             tty-&gt;print_cr("  %40s: " UINT32_FORMAT " (%.1f%%)", name, r, (r * 100.0) / total);
2356             account -= r;
2357           }
2358         }
2359       }
2360     }
2361     if (account != 0) {
2362       PRINT_STAT_LINE("unaccounted", account);
2363     }
2364     #undef PRINT_STAT_LINE
2365     if (xtty != NULL)  xtty-&gt;tail("statistics");
2366   }
2367 }
2368 #else // COMPILER2_OR_JVMCI
2369 
2370 
2371 // Stubs for C1 only system.
2372 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2373   return false;
2374 }
2375 
2376 const char* Deoptimization::trap_reason_name(int reason) {
2377   return "unknown";
2378 }
2379 
2380 void Deoptimization::print_statistics() {
2381   // no output
2382 }
2383 
2384 void
2385 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2386   // no udpate
2387 }
2388 
2389 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2390   return 0;
2391 }
2392 
2393 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2394                                        Bytecodes::Code bc) {
2395   // no update
2396 }
2397 
2398 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2399                                               int trap_state) {
2400   jio_snprintf(buf, buflen, "#%d", trap_state);
2401   return buf;
2402 }
2403 
2404 #endif // COMPILER2_OR_JVMCI
<a name="9" id="anc9"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="9" type="hidden" /></form></body></html>
