<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1998, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/assembler.inline.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "code/compiledIC.hpp"
  29 #include "code/debugInfo.hpp"
  30 #include "code/debugInfoRec.hpp"
  31 #include "compiler/compileBroker.hpp"
  32 #include "compiler/compilerDirectives.hpp"
  33 #include "compiler/oopMap.hpp"
  34 #include "memory/allocation.inline.hpp"
  35 #include "opto/ad.hpp"
  36 #include "opto/callnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/locknode.hpp"
  39 #include "opto/machnode.hpp"
  40 #include "opto/optoreg.hpp"
  41 #include "opto/output.hpp"
  42 #include "opto/regalloc.hpp"
  43 #include "opto/runtime.hpp"
  44 #include "opto/subnode.hpp"
  45 #include "opto/type.hpp"
  46 #include "runtime/handles.inline.hpp"
  47 #include "utilities/xmlstream.hpp"
  48 
  49 #ifndef PRODUCT
  50 #define DEBUG_ARG(x) , x
  51 #else
  52 #define DEBUG_ARG(x)
  53 #endif
  54 
  55 // Convert Nodes to instruction bits and pass off to the VM
  56 void Compile::Output() {
  57   // RootNode goes
  58   assert( _cfg-&gt;get_root_block()-&gt;number_of_nodes() == 0, "" );
  59 
  60   // The number of new nodes (mostly MachNop) is proportional to
  61   // the number of java calls and inner loops which are aligned.
  62   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
  63                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
  64                            "out of nodes before code generation" ) ) {
  65     return;
  66   }
  67   // Make sure I can find the Start Node
  68   Block *entry = _cfg-&gt;get_block(1);
  69   Block *broot = _cfg-&gt;get_root_block();
  70 
  71   const StartNode *start = entry-&gt;head()-&gt;as_Start();
  72 
  73   // Replace StartNode with prolog
  74   MachPrologNode *prolog = new MachPrologNode();
  75   entry-&gt;map_node(prolog, 0);
  76   _cfg-&gt;map_node_to_block(prolog, entry);
  77   _cfg-&gt;unmap_node_from_block(start); // start is no longer in any block
  78 
  79   // Virtual methods need an unverified entry point
  80 
  81   if( is_osr_compilation() ) {
  82     if( PoisonOSREntry ) {
  83       // TODO: Should use a ShouldNotReachHereNode...
  84       _cfg-&gt;insert( broot, 0, new MachBreakpointNode() );
  85     }
  86   } else {
  87     if( _method &amp;&amp; !_method-&gt;flags().is_static() ) {
  88       // Insert unvalidated entry point
  89       _cfg-&gt;insert( broot, 0, new MachUEPNode() );
  90     }
  91 
  92   }
  93 
  94   // Break before main entry point
  95   if ((_method &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
  96       (OptoBreakpoint &amp;&amp; is_method_compilation())       ||
  97       (OptoBreakpointOSR &amp;&amp; is_osr_compilation())       ||
  98       (OptoBreakpointC2R &amp;&amp; !_method)                   ) {
  99     // checking for _method means that OptoBreakpoint does not apply to
 100     // runtime stubs or frame converters
 101     _cfg-&gt;insert( entry, 1, new MachBreakpointNode() );
 102   }
 103 
 104   // Insert epilogs before every return
 105   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 106     Block* block = _cfg-&gt;get_block(i);
 107     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == _cfg-&gt;get_root_block()) { // Found a program exit point?
 108       Node* m = block-&gt;end();
 109       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 110         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 111         block-&gt;add_inst(epilog);
 112         _cfg-&gt;map_node_to_block(epilog, block);
 113       }
 114     }
 115   }
 116 
 117   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, _cfg-&gt;number_of_blocks() + 1);
 118   blk_starts[0] = 0;
 119 
 120   // Initialize code buffer and process short branches.
 121   CodeBuffer* cb = init_buffer(blk_starts);
 122 
 123   if (cb == NULL || failing()) {
 124     return;
 125   }
 126 
 127   ScheduleAndBundle();
 128 
 129 #ifndef PRODUCT
 130   if (trace_opto_output()) {
 131     tty-&gt;print("\n---- After ScheduleAndBundle ----\n");
 132     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 133       tty-&gt;print("\nBB#%03d:\n", i);
 134       Block* block = _cfg-&gt;get_block(i);
 135       for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
 136         Node* n = block-&gt;get_node(j);
 137         OptoReg::Name reg = _regalloc-&gt;get_reg_first(n);
 138         tty-&gt;print(" %-6s ", reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : "");
 139         n-&gt;dump();
 140       }
 141     }
 142   }
 143 #endif
 144 
 145   if (failing()) {
 146     return;
 147   }
 148 
 149   BuildOopMaps();
 150 
 151   if (failing())  {
 152     return;
 153   }
 154 
 155   fill_buffer(cb, blk_starts);
 156 }
 157 
 158 bool Compile::need_stack_bang(int frame_size_in_bytes) const {
 159   // Determine if we need to generate a stack overflow check.
 160   // Do it if the method is not a stub function and
 161   // has java calls or has frame size &gt; vm_page_size/8.
 162   // The debug VM checks that deoptimization doesn't trigger an
 163   // unexpected stack overflow (compiled method stack banging should
 164   // guarantee it doesn't happen) so we always need the stack bang in
 165   // a debug VM.
 166   return (UseStackBanging &amp;&amp; stub_function() == NULL &amp;&amp;
 167           (has_java_calls() || frame_size_in_bytes &gt; os::vm_page_size()&gt;&gt;3
 168            DEBUG_ONLY(|| true)));
 169 }
 170 
 171 bool Compile::need_register_stack_bang() const {
 172   // Determine if we need to generate a register stack overflow check.
 173   // This is only used on architectures which have split register
 174   // and memory stacks (ie. IA64).
 175   // Bang if the method is not a stub function and has java calls
 176   return (stub_function() == NULL &amp;&amp; has_java_calls());
 177 }
 178 
 179 
 180 // Compute the size of first NumberOfLoopInstrToAlign instructions at the top
 181 // of a loop. When aligning a loop we need to provide enough instructions
 182 // in cpu's fetch buffer to feed decoders. The loop alignment could be
 183 // avoided if we have enough instructions in fetch buffer at the head of a loop.
 184 // By default, the size is set to 999999 by Block's constructor so that
 185 // a loop will be aligned if the size is not reset here.
 186 //
 187 // Note: Mach instructions could contain several HW instructions
 188 // so the size is estimated only.
 189 //
 190 void Compile::compute_loop_first_inst_sizes() {
 191   // The next condition is used to gate the loop alignment optimization.
 192   // Don't aligned a loop if there are enough instructions at the head of a loop
 193   // or alignment padding is larger then MaxLoopPad. By default, MaxLoopPad
 194   // is equal to OptoLoopAlignment-1 except on new Intel cpus, where it is
 195   // equal to 11 bytes which is the largest address NOP instruction.
 196   if (MaxLoopPad &lt; OptoLoopAlignment - 1) {
 197     uint last_block = _cfg-&gt;number_of_blocks() - 1;
 198     for (uint i = 1; i &lt;= last_block; i++) {
 199       Block* block = _cfg-&gt;get_block(i);
 200       // Check the first loop's block which requires an alignment.
 201       if (block-&gt;loop_alignment() &gt; (uint)relocInfo::addr_unit()) {
 202         uint sum_size = 0;
 203         uint inst_cnt = NumberOfLoopInstrToAlign;
 204         inst_cnt = block-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 205 
 206         // Check subsequent fallthrough blocks if the loop's first
 207         // block(s) does not have enough instructions.
 208         Block *nb = block;
 209         while(inst_cnt &gt; 0 &amp;&amp;
 210               i &lt; last_block &amp;&amp;
 211               !_cfg-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 212               !nb-&gt;has_successor(block)) {
 213           i++;
 214           nb = _cfg-&gt;get_block(i);
 215           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 216         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 217 
 218         block-&gt;set_first_inst_size(sum_size);
 219       } // f( b-&gt;head()-&gt;is_Loop() )
 220     } // for( i &lt;= last_block )
 221   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 222 }
 223 
 224 // The architecture description provides short branch variants for some long
 225 // branch instructions. Replace eligible long branches with short branches.
 226 void Compile::shorten_branches(uint* blk_starts, int&amp; code_size, int&amp; reloc_size, int&amp; stub_size) {
 227   // Compute size of each block, method size, and relocation information size
 228   uint nblocks  = _cfg-&gt;number_of_blocks();
 229 
 230   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 231   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 232   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 233 
 234   // Collect worst case block paddings
 235   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 236   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 237 
 238   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 239   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 240 
 241   bool has_short_branch_candidate = false;
 242 
 243   // Initialize the sizes to 0
 244   code_size  = 0;          // Size in bytes of generated code
 245   stub_size  = 0;          // Size in bytes of all stub entries
 246   // Size in bytes of all relocation entries, including those in local stubs.
 247   // Start with 2-bytes of reloc info for the unvalidated entry point
 248   reloc_size = 1;          // Number of relocation entries
 249 
 250   // Make three passes.  The first computes pessimistic blk_starts,
 251   // relative jmp_offset and reloc_size information.  The second performs
 252   // short branch substitution using the pessimistic sizing.  The
 253   // third inserts nops where needed.
 254 
 255   // Step one, perform a pessimistic sizing pass.
 256   uint last_call_adr = max_juint;
 257   uint last_avoid_back_to_back_adr = max_juint;
 258   uint nop_size = (new MachNopNode())-&gt;size(_regalloc);
 259   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 260     Block* block = _cfg-&gt;get_block(i);
 261 
 262     // During short branch replacement, we store the relative (to blk_starts)
 263     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 264     // This is so that we do not need to recompute sizes of all nodes when
 265     // we compute correct blk_starts in our next sizing pass.
 266     jmp_offset[i] = 0;
 267     jmp_size[i]   = 0;
 268     jmp_nidx[i]   = -1;
 269     DEBUG_ONLY( jmp_target[i] = 0; )
 270     DEBUG_ONLY( jmp_rule[i]   = 0; )
 271 
 272     // Sum all instruction sizes to compute block size
 273     uint last_inst = block-&gt;number_of_nodes();
 274     uint blk_size = 0;
 275     for (uint j = 0; j &lt; last_inst; j++) {
 276       Node* nj = block-&gt;get_node(j);
 277       // Handle machine instruction nodes
 278       if (nj-&gt;is_Mach()) {
 279         MachNode *mach = nj-&gt;as_Mach();
 280         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 281         reloc_size += mach-&gt;reloc();
 282         if (mach-&gt;is_MachCall()) {
 283           // add size information for trampoline stub
 284           // class CallStubImpl is platform-specific and defined in the *.ad files.
 285           stub_size  += CallStubImpl::size_call_trampoline();
 286           reloc_size += CallStubImpl::reloc_call_trampoline();
 287 
 288           MachCallNode *mcall = mach-&gt;as_MachCall();
 289           // This destination address is NOT PC-relative
 290 
 291           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
 292 
 293           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 294             stub_size  += CompiledStaticCall::to_interp_stub_size();
 295             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 296 #if INCLUDE_AOT
 297             stub_size  += CompiledStaticCall::to_aot_stub_size();
 298             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 299 #endif
 300           }
 301         } else if (mach-&gt;is_MachSafePoint()) {
 302           // If call/safepoint are adjacent, account for possible
 303           // nop to disambiguate the two safepoints.
 304           // ScheduleAndBundle() can rearrange nodes in a block,
 305           // check for all offsets inside this block.
 306           if (last_call_adr &gt;= blk_starts[i]) {
 307             blk_size += nop_size;
 308           }
 309         }
 310         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 311           // Nop is inserted between "avoid back to back" instructions.
 312           // ScheduleAndBundle() can rearrange nodes in a block,
 313           // check for all offsets inside this block.
 314           if (last_avoid_back_to_back_adr &gt;= blk_starts[i]) {
 315             blk_size += nop_size;
 316           }
 317         }
 318         if (mach-&gt;may_be_short_branch()) {
 319           if (!nj-&gt;is_MachBranch()) {
 320 #ifndef PRODUCT
 321             nj-&gt;dump(3);
 322 #endif
 323             Unimplemented();
 324           }
 325           assert(jmp_nidx[i] == -1, "block should have only one branch");
 326           jmp_offset[i] = blk_size;
 327           jmp_size[i]   = nj-&gt;size(_regalloc);
 328           jmp_nidx[i]   = j;
 329           has_short_branch_candidate = true;
 330         }
 331       }
 332       blk_size += nj-&gt;size(_regalloc);
 333       // Remember end of call offset
 334       if (nj-&gt;is_MachCall() &amp;&amp; !nj-&gt;is_MachCallLeaf()) {
 335         last_call_adr = blk_starts[i]+blk_size;
 336       }
 337       // Remember end of avoid_back_to_back offset
 338       if (nj-&gt;is_Mach() &amp;&amp; nj-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
 339         last_avoid_back_to_back_adr = blk_starts[i]+blk_size;
 340       }
 341     }
 342 
 343     // When the next block starts a loop, we may insert pad NOP
 344     // instructions.  Since we cannot know our future alignment,
 345     // assume the worst.
 346     if (i &lt; nblocks - 1) {
 347       Block* nb = _cfg-&gt;get_block(i + 1);
 348       int max_loop_pad = nb-&gt;code_alignment()-relocInfo::addr_unit();
 349       if (max_loop_pad &gt; 0) {
 350         assert(is_power_of_2(max_loop_pad+relocInfo::addr_unit()), "");
 351         // Adjust last_call_adr and/or last_avoid_back_to_back_adr.
 352         // If either is the last instruction in this block, bump by
 353         // max_loop_pad in lock-step with blk_size, so sizing
 354         // calculations in subsequent blocks still can conservatively
 355         // detect that it may the last instruction in this block.
 356         if (last_call_adr == blk_starts[i]+blk_size) {
 357           last_call_adr += max_loop_pad;
 358         }
 359         if (last_avoid_back_to_back_adr == blk_starts[i]+blk_size) {
 360           last_avoid_back_to_back_adr += max_loop_pad;
 361         }
 362         blk_size += max_loop_pad;
 363         block_worst_case_pad[i + 1] = max_loop_pad;
 364       }
 365     }
 366 
 367     // Save block size; update total method size
 368     blk_starts[i+1] = blk_starts[i]+blk_size;
 369   }
 370 
 371   // Step two, replace eligible long jumps.
 372   bool progress = true;
 373   uint last_may_be_short_branch_adr = max_juint;
 374   while (has_short_branch_candidate &amp;&amp; progress) {
 375     progress = false;
 376     has_short_branch_candidate = false;
 377     int adjust_block_start = 0;
 378     for (uint i = 0; i &lt; nblocks; i++) {
 379       Block* block = _cfg-&gt;get_block(i);
 380       int idx = jmp_nidx[i];
 381       MachNode* mach = (idx == -1) ? NULL: block-&gt;get_node(idx)-&gt;as_Mach();
 382       if (mach != NULL &amp;&amp; mach-&gt;may_be_short_branch()) {
 383 #ifdef ASSERT
 384         assert(jmp_size[i] &gt; 0 &amp;&amp; mach-&gt;is_MachBranch(), "sanity");
 385         int j;
 386         // Find the branch; ignore trailing NOPs.
 387         for (j = block-&gt;number_of_nodes()-1; j&gt;=0; j--) {
 388           Node* n = block-&gt;get_node(j);
 389           if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con)
 390             break;
 391         }
 392         assert(j &gt;= 0 &amp;&amp; j == idx &amp;&amp; block-&gt;get_node(j) == (Node*)mach, "sanity");
 393 #endif
 394         int br_size = jmp_size[i];
 395         int br_offs = blk_starts[i] + jmp_offset[i];
 396 
 397         // This requires the TRUE branch target be in succs[0]
 398         uint bnum = block-&gt;non_connector_successor(0)-&gt;_pre_order;
 399         int offset = blk_starts[bnum] - br_offs;
 400         if (bnum &gt; i) { // adjust following block's offset
 401           offset -= adjust_block_start;
 402         }
 403 
 404         // This block can be a loop header, account for the padding
 405         // in the previous block.
 406         int block_padding = block_worst_case_pad[i];
 407         assert(i == 0 || block_padding == 0 || br_offs &gt;= block_padding, "Should have at least a padding on top");
 408         // In the following code a nop could be inserted before
 409         // the branch which will increase the backward distance.
 410         bool needs_padding = ((uint)(br_offs - block_padding) == last_may_be_short_branch_adr);
 411         assert(!needs_padding || jmp_offset[i] == 0, "padding only branches at the beginning of block");
 412 
 413         if (needs_padding &amp;&amp; offset &lt;= 0)
 414           offset -= nop_size;
 415 
 416         if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
 417           // We've got a winner.  Replace this branch.
 418           MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
 419 
 420           // Update the jmp_size.
 421           int new_size = replacement-&gt;size(_regalloc);
 422           int diff     = br_size - new_size;
 423           assert(diff &gt;= (int)nop_size, "short_branch size should be smaller");
 424           // Conservatively take into account padding between
 425           // avoid_back_to_back branches. Previous branch could be
 426           // converted into avoid_back_to_back branch during next
 427           // rounds.
 428           if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 429             jmp_offset[i] += nop_size;
 430             diff -= nop_size;
 431           }
 432           adjust_block_start += diff;
 433           block-&gt;map_node(replacement, idx);
 434           mach-&gt;subsume_by(replacement, C);
 435           mach = replacement;
 436           progress = true;
 437 
 438           jmp_size[i] = new_size;
 439           DEBUG_ONLY( jmp_target[i] = bnum; );
 440           DEBUG_ONLY( jmp_rule[i] = mach-&gt;rule(); );
 441         } else {
 442           // The jump distance is not short, try again during next iteration.
 443           has_short_branch_candidate = true;
 444         }
 445       } // (mach-&gt;may_be_short_branch())
 446       if (mach != NULL &amp;&amp; (mach-&gt;may_be_short_branch() ||
 447                            mach-&gt;avoid_back_to_back(MachNode::AVOID_AFTER))) {
 448         last_may_be_short_branch_adr = blk_starts[i] + jmp_offset[i] + jmp_size[i];
 449       }
 450       blk_starts[i+1] -= adjust_block_start;
 451     }
 452   }
 453 
 454 #ifdef ASSERT
 455   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 456     if (jmp_target[i] != 0) {
 457       int br_size = jmp_size[i];
 458       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
 459       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
 460         tty-&gt;print_cr("target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d", blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
 461       }
 462       assert(_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), "Displacement too large for short jmp");
 463     }
 464   }
 465 #endif
 466 
 467   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 468   // after ScheduleAndBundle().
 469 
 470   // ------------------
 471   // Compute size for code buffer
 472   code_size = blk_starts[nblocks];
 473 
 474   // Relocation records
 475   reloc_size += 1;              // Relo entry for exception handler
 476 
 477   // Adjust reloc_size to number of record of relocation info
 478   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 479   // a relocation index.
 480   // The CodeBuffer will expand the locs array if this estimate is too low.
 481   reloc_size *= 10 / sizeof(relocInfo);
 482 }
 483 
 484 //------------------------------FillLocArray-----------------------------------
 485 // Create a bit of debug info and append it to the array.  The mapping is from
 486 // Java local or expression stack to constant, register or stack-slot.  For
 487 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 488 // entry has been taken care of and caller should skip it).
 489 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 490   // This should never have accepted Bad before
 491   assert(OptoReg::is_valid(regnum), "location must be valid");
 492   return (OptoReg::is_reg(regnum))
 493     ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )
 494     : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
 495 }
 496 
 497 
 498 ObjectValue*
 499 Compile::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 500   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 501     assert(objs-&gt;at(i)-&gt;is_object(), "corrupt object cache");
 502     ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
 503     if (sv-&gt;id() == id) {
 504       return sv;
 505     }
 506   }
 507   // Otherwise..
 508   return NULL;
 509 }
 510 
 511 void Compile::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,
 512                                      ObjectValue* sv ) {
 513   assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, "Precondition");
 514   objs-&gt;append(sv);
 515 }
 516 
 517 
 518 void Compile::FillLocArray( int idx, MachSafePointNode* sfpt, Node *local,
 519                             GrowableArray&lt;ScopeValue*&gt; *array,
 520                             GrowableArray&lt;ScopeValue*&gt; *objs ) {
 521   assert( local, "use _top instead of null" );
 522   if (array-&gt;length() != idx) {
 523     assert(array-&gt;length() == idx + 1, "Unexpected array count");
 524     // Old functionality:
 525     //   return
 526     // New functionality:
 527     //   Assert if the local is not top. In product mode let the new node
 528     //   override the old entry.
 529     assert(local == top(), "LocArray collision");
 530     if (local == top()) {
 531       return;
 532     }
 533     array-&gt;pop();
 534   }
 535   const Type *t = local-&gt;bottom_type();
 536 
 537   // Is it a safepoint scalar object node?
 538   if (local-&gt;is_SafePointScalarObject()) {
 539     SafePointScalarObjectNode* spobj = local-&gt;as_SafePointScalarObject();
 540 
 541     ObjectValue* sv = Compile::sv_for_node_id(objs, spobj-&gt;_idx);
 542     if (sv == NULL) {
 543       ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
 544       assert(cik-&gt;is_instance_klass() ||
 545              cik-&gt;is_array_klass(), "Not supported allocation.");
 546       sv = new ObjectValue(spobj-&gt;_idx,
<a name="1" id="anc1"></a><span class="changed"> 547                            new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));</span>

 548       Compile::set_sv_for_object_node(objs, sv);
 549 
 550       uint first_ind = spobj-&gt;first_index(sfpt-&gt;jvms());
 551       for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
 552         Node* fld_node = sfpt-&gt;in(first_ind+i);
 553         (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfpt, fld_node, sv-&gt;field_values(), objs);
 554       }
 555     }
 556     array-&gt;append(sv);
 557     return;
 558   }
 559 
 560   // Grab the register number for the local
 561   OptoReg::Name regnum = _regalloc-&gt;get_reg_first(local);
 562   if( OptoReg::is_valid(regnum) ) {// Got a register/stack?
 563     // Record the double as two float registers.
 564     // The register mask for such a value always specifies two adjacent
 565     // float registers, with the lower register number even.
 566     // Normally, the allocation of high and low words to these registers
 567     // is irrelevant, because nearly all operations on register pairs
 568     // (e.g., StoreD) treat them as a single unit.
 569     // Here, we assume in addition that the words in these two registers
 570     // stored "naturally" (by operations like StoreD and double stores
 571     // within the interpreter) such that the lower-numbered register
 572     // is written to the lower memory address.  This may seem like
 573     // a machine dependency, but it is not--it is a requirement on
 574     // the author of the &lt;arch&gt;.ad file to ensure that, for every
 575     // even/odd double-register pair to which a double may be allocated,
 576     // the word in the even single-register is stored to the first
 577     // memory word.  (Note that register numbers are completely
 578     // arbitrary, and are not tied to any machine-level encodings.)
 579 #ifdef _LP64
 580     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon ) {
 581       array-&gt;append(new ConstantIntValue((jint)0));
 582       array-&gt;append(new_loc_value( _regalloc, regnum, Location::dbl ));
 583     } else if ( t-&gt;base() == Type::Long ) {
 584       array-&gt;append(new ConstantIntValue((jint)0));
 585       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));
 586     } else if ( t-&gt;base() == Type::RawPtr ) {
 587       // jsr/ret return address which must be restored into a the full
 588       // width 64-bit stack slot.
 589       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));
 590     }
 591 #else //_LP64
 592 #ifdef SPARC
 593     if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
 594       // For SPARC we have to swap high and low words for
 595       // long values stored in a single-register (g0-g7).
 596       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 597       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 598     } else
 599 #endif //SPARC
 600     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
 601       // Repack the double/long as two jints.
 602       // The convention the interpreter uses is that the second local
 603       // holds the first raw word of the native double representation.
 604       // This is actually reasonable, since locals and stack arrays
 605       // grow downwards in all implementations.
 606       // (If, on some machine, the interpreter's Java locals or stack
 607       // were to grow upwards, the embedded doubles would be word-swapped.)
 608       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 609       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 610     }
 611 #endif //_LP64
 612     else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
 613                OptoReg::is_reg(regnum) ) {
 614       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::float_in_double()
 615                                    ? Location::float_in_dbl : Location::normal ));
 616     } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
 617       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::int_in_long
 618                                    ? Location::int_in_long : Location::normal ));
 619     } else if( t-&gt;base() == Type::NarrowOop ) {
 620       array-&gt;append(new_loc_value( _regalloc, regnum, Location::narrowoop ));
 621     } else {
 622       array-&gt;append(new_loc_value( _regalloc, regnum, _regalloc-&gt;is_oop(local) ? Location::oop : Location::normal ));
 623     }
 624     return;
 625   }
 626 
 627   // No register.  It must be constant data.
 628   switch (t-&gt;base()) {
 629   case Type::Half:              // Second half of a double
 630     ShouldNotReachHere();       // Caller should skip 2nd halves
 631     break;
 632   case Type::AnyPtr:
 633     array-&gt;append(new ConstantOopWriteValue(NULL));
 634     break;
 635   case Type::AryPtr:
 636   case Type::InstPtr:          // fall through
 637     array-&gt;append(new ConstantOopWriteValue(t-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));
 638     break;
 639   case Type::NarrowOop:
 640     if (t == TypeNarrowOop::NULL_PTR) {
 641       array-&gt;append(new ConstantOopWriteValue(NULL));
 642     } else {
 643       array-&gt;append(new ConstantOopWriteValue(t-&gt;make_ptr()-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));
 644     }
 645     break;
 646   case Type::Int:
 647     array-&gt;append(new ConstantIntValue(t-&gt;is_int()-&gt;get_con()));
 648     break;
 649   case Type::RawPtr:
 650     // A return address (T_ADDRESS).
 651     assert((intptr_t)t-&gt;is_ptr()-&gt;get_con() &lt; (intptr_t)0x10000, "must be a valid BCI");
 652 #ifdef _LP64
 653     // Must be restored to the full-width 64-bit stack slot.
 654     array-&gt;append(new ConstantLongValue(t-&gt;is_ptr()-&gt;get_con()));
 655 #else
 656     array-&gt;append(new ConstantIntValue(t-&gt;is_ptr()-&gt;get_con()));
 657 #endif
 658     break;
 659   case Type::FloatCon: {
 660     float f = t-&gt;is_float_constant()-&gt;getf();
 661     array-&gt;append(new ConstantIntValue(jint_cast(f)));
 662     break;
 663   }
 664   case Type::DoubleCon: {
 665     jdouble d = t-&gt;is_double_constant()-&gt;getd();
 666 #ifdef _LP64
 667     array-&gt;append(new ConstantIntValue((jint)0));
 668     array-&gt;append(new ConstantDoubleValue(d));
 669 #else
 670     // Repack the double as two jints.
 671     // The convention the interpreter uses is that the second local
 672     // holds the first raw word of the native double representation.
 673     // This is actually reasonable, since locals and stack arrays
 674     // grow downwards in all implementations.
 675     // (If, on some machine, the interpreter's Java locals or stack
 676     // were to grow upwards, the embedded doubles would be word-swapped.)
 677     jlong_accessor acc;
 678     acc.long_value = jlong_cast(d);
 679     array-&gt;append(new ConstantIntValue(acc.words[1]));
 680     array-&gt;append(new ConstantIntValue(acc.words[0]));
 681 #endif
 682     break;
 683   }
 684   case Type::Long: {
 685     jlong d = t-&gt;is_long()-&gt;get_con();
 686 #ifdef _LP64
 687     array-&gt;append(new ConstantIntValue((jint)0));
 688     array-&gt;append(new ConstantLongValue(d));
 689 #else
 690     // Repack the long as two jints.
 691     // The convention the interpreter uses is that the second local
 692     // holds the first raw word of the native double representation.
 693     // This is actually reasonable, since locals and stack arrays
 694     // grow downwards in all implementations.
 695     // (If, on some machine, the interpreter's Java locals or stack
 696     // were to grow upwards, the embedded doubles would be word-swapped.)
 697     jlong_accessor acc;
 698     acc.long_value = d;
 699     array-&gt;append(new ConstantIntValue(acc.words[1]));
 700     array-&gt;append(new ConstantIntValue(acc.words[0]));
 701 #endif
 702     break;
 703   }
 704   case Type::Top:               // Add an illegal value here
 705     array-&gt;append(new LocationValue(Location()));
 706     break;
 707   default:
 708     ShouldNotReachHere();
 709     break;
 710   }
 711 }
 712 
 713 // Determine if this node starts a bundle
 714 bool Compile::starts_bundle(const Node *n) const {
 715   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 716           _node_bundling_base[n-&gt;_idx].starts_bundle());
 717 }
 718 
 719 //--------------------------Process_OopMap_Node--------------------------------
 720 void Compile::Process_OopMap_Node(MachNode *mach, int current_offset) {
 721 
 722   // Handle special safepoint nodes for synchronization
 723   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 724   MachCallNode      *mcall;
 725 
 726   int safepoint_pc_offset = current_offset;
 727   bool is_method_handle_invoke = false;
 728   bool return_oop = false;
 729 
 730   // Add the safepoint in the DebugInfoRecorder
 731   if( !mach-&gt;is_MachCall() ) {
 732     mcall = NULL;
 733     debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 734   } else {
 735     mcall = mach-&gt;as_MachCall();
 736 
 737     // Is the call a MethodHandle call?
 738     if (mcall-&gt;is_MachCallJava()) {
 739       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 740         assert(has_method_handle_invokes(), "must have been set during call generation");
 741         is_method_handle_invoke = true;
 742       }
 743     }
 744 
 745     // Check if a call returns an object.
 746     if (mcall-&gt;returns_pointer()) {
 747       return_oop = true;
 748     }
 749     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
 750     debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
 751   }
 752 
 753   // Loop over the JVMState list to add scope information
 754   // Do not skip safepoints with a NULL method, they need monitor info
 755   JVMState* youngest_jvms = sfn-&gt;jvms();
 756   int max_depth = youngest_jvms-&gt;depth();
 757 
 758   // Allocate the object pool for scalar-replaced objects -- the map from
 759   // small-integer keys (which can be recorded in the local and ostack
 760   // arrays) to descriptions of the object state.
 761   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
 762 
 763   // Visit scopes from oldest to youngest.
 764   for (int depth = 1; depth &lt;= max_depth; depth++) {
 765     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 766     int idx;
 767     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 768     // Safepoints that do not have method() set only provide oop-map and monitor info
 769     // to support GC; these do not support deoptimization.
 770     int num_locs = (method == NULL) ? 0 : jvms-&gt;loc_size();
 771     int num_exps = (method == NULL) ? 0 : jvms-&gt;stk_size();
 772     int num_mon  = jvms-&gt;nof_monitors();
 773     assert(method == NULL || jvms-&gt;bci() &lt; 0 || num_locs == method-&gt;max_locals(),
 774            "JVMS local count must match that of the method");
 775 
 776     // Add Local and Expression Stack Information
 777 
 778     // Insert locals into the locarray
 779     GrowableArray&lt;ScopeValue*&gt; *locarray = new GrowableArray&lt;ScopeValue*&gt;(num_locs);
 780     for( idx = 0; idx &lt; num_locs; idx++ ) {
 781       FillLocArray( idx, sfn, sfn-&gt;local(jvms, idx), locarray, objs );
 782     }
 783 
 784     // Insert expression stack entries into the exparray
 785     GrowableArray&lt;ScopeValue*&gt; *exparray = new GrowableArray&lt;ScopeValue*&gt;(num_exps);
 786     for( idx = 0; idx &lt; num_exps; idx++ ) {
 787       FillLocArray( idx,  sfn, sfn-&gt;stack(jvms, idx), exparray, objs );
 788     }
 789 
 790     // Add in mappings of the monitors
 791     assert( !method ||
 792             !method-&gt;is_synchronized() ||
 793             method-&gt;is_native() ||
 794             num_mon &gt; 0 ||
 795             !GenerateSynchronizationCode,
 796             "monitors must always exist for synchronized methods");
 797 
 798     // Build the growable array of ScopeValues for exp stack
 799     GrowableArray&lt;MonitorValue*&gt; *monarray = new GrowableArray&lt;MonitorValue*&gt;(num_mon);
 800 
 801     // Loop over monitors and insert into array
 802     for (idx = 0; idx &lt; num_mon; idx++) {
 803       // Grab the node that defines this monitor
 804       Node* box_node = sfn-&gt;monitor_box(jvms, idx);
 805       Node* obj_node = sfn-&gt;monitor_obj(jvms, idx);
 806 
 807       // Create ScopeValue for object
 808       ScopeValue *scval = NULL;
 809 
 810       if (obj_node-&gt;is_SafePointScalarObject()) {
 811         SafePointScalarObjectNode* spobj = obj_node-&gt;as_SafePointScalarObject();
 812         scval = Compile::sv_for_node_id(objs, spobj-&gt;_idx);
 813         if (scval == NULL) {
 814           const Type *t = spobj-&gt;bottom_type();
 815           ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
 816           assert(cik-&gt;is_instance_klass() ||
 817                  cik-&gt;is_array_klass(), "Not supported allocation.");
 818           ObjectValue* sv = new ObjectValue(spobj-&gt;_idx,
<a name="2" id="anc2"></a><span class="changed"> 819                                             new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));</span>

 820           Compile::set_sv_for_object_node(objs, sv);
 821 
 822           uint first_ind = spobj-&gt;first_index(youngest_jvms);
 823           for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
 824             Node* fld_node = sfn-&gt;in(first_ind+i);
 825             (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfn, fld_node, sv-&gt;field_values(), objs);
 826           }
 827           scval = sv;
 828         }
 829       } else if (!obj_node-&gt;is_Con()) {
 830         OptoReg::Name obj_reg = _regalloc-&gt;get_reg_first(obj_node);
 831         if( obj_node-&gt;bottom_type()-&gt;base() == Type::NarrowOop ) {
 832           scval = new_loc_value( _regalloc, obj_reg, Location::narrowoop );
 833         } else {
 834           scval = new_loc_value( _regalloc, obj_reg, Location::oop );
 835         }
 836       } else {
 837         const TypePtr *tp = obj_node-&gt;get_ptr_type();
 838         scval = new ConstantOopWriteValue(tp-&gt;is_oopptr()-&gt;const_oop()-&gt;constant_encoding());
 839       }
 840 
 841       OptoReg::Name box_reg = BoxLockNode::reg(box_node);
 842       Location basic_lock = Location::new_stk_loc(Location::normal,_regalloc-&gt;reg2offset(box_reg));
 843       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
 844       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
 845     }
 846 
 847     // We dump the object pool first, since deoptimization reads it in first.
 848     debug_info()-&gt;dump_object_pool(objs);
 849 
 850     // Build first class objects to pass to scope
 851     DebugToken *locvals = debug_info()-&gt;create_scope_values(locarray);
 852     DebugToken *expvals = debug_info()-&gt;create_scope_values(exparray);
 853     DebugToken *monvals = debug_info()-&gt;create_monitor_values(monarray);
 854 
 855     // Make method available for all Safepoints
 856     ciMethod* scope_method = method ? method : _method;
 857     // Describe the scope here
 858     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, "must be a valid or entry BCI");
 859     assert(!jvms-&gt;should_reexecute() || depth == max_depth, "reexecute allowed only for the youngest");
 860     // Now we can describe the scope.
 861     methodHandle null_mh;
 862     bool rethrow_exception = false;
 863     debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);
 864   } // End jvms loop
 865 
 866   // Mark the end of the scope set.
 867   debug_info()-&gt;end_safepoint(safepoint_pc_offset);
 868 }
 869 
 870 
 871 
 872 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
 873 class NonSafepointEmitter {
 874   Compile*  C;
 875   JVMState* _pending_jvms;
 876   int       _pending_offset;
 877 
 878   void emit_non_safepoint();
 879 
 880  public:
 881   NonSafepointEmitter(Compile* compile) {
 882     this-&gt;C = compile;
 883     _pending_jvms = NULL;
 884     _pending_offset = 0;
 885   }
 886 
 887   void observe_instruction(Node* n, int pc_offset) {
 888     if (!C-&gt;debug_info()-&gt;recording_non_safepoints())  return;
 889 
 890     Node_Notes* nn = C-&gt;node_notes_at(n-&gt;_idx);
 891     if (nn == NULL || nn-&gt;jvms() == NULL)  return;
 892     if (_pending_jvms != NULL &amp;&amp;
 893         _pending_jvms-&gt;same_calls_as(nn-&gt;jvms())) {
 894       // Repeated JVMS?  Stretch it up here.
 895       _pending_offset = pc_offset;
 896     } else {
 897       if (_pending_jvms != NULL &amp;&amp;
 898           _pending_offset &lt; pc_offset) {
 899         emit_non_safepoint();
 900       }
 901       _pending_jvms = NULL;
 902       if (pc_offset &gt; C-&gt;debug_info()-&gt;last_pc_offset()) {
 903         // This is the only way _pending_jvms can become non-NULL:
 904         _pending_jvms = nn-&gt;jvms();
 905         _pending_offset = pc_offset;
 906       }
 907     }
 908   }
 909 
 910   // Stay out of the way of real safepoints:
 911   void observe_safepoint(JVMState* jvms, int pc_offset) {
 912     if (_pending_jvms != NULL &amp;&amp;
 913         !_pending_jvms-&gt;same_calls_as(jvms) &amp;&amp;
 914         _pending_offset &lt; pc_offset) {
 915       emit_non_safepoint();
 916     }
 917     _pending_jvms = NULL;
 918   }
 919 
 920   void flush_at_end() {
 921     if (_pending_jvms != NULL) {
 922       emit_non_safepoint();
 923     }
 924     _pending_jvms = NULL;
 925   }
 926 };
 927 
 928 void NonSafepointEmitter::emit_non_safepoint() {
 929   JVMState* youngest_jvms = _pending_jvms;
 930   int       pc_offset     = _pending_offset;
 931 
 932   // Clear it now:
 933   _pending_jvms = NULL;
 934 
 935   DebugInformationRecorder* debug_info = C-&gt;debug_info();
 936   assert(debug_info-&gt;recording_non_safepoints(), "sanity");
 937 
 938   debug_info-&gt;add_non_safepoint(pc_offset);
 939   int max_depth = youngest_jvms-&gt;depth();
 940 
 941   // Visit scopes from oldest to youngest.
 942   for (int depth = 1; depth &lt;= max_depth; depth++) {
 943     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 944     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 945     assert(!jvms-&gt;should_reexecute() || depth==max_depth, "reexecute allowed only for the youngest");
 946     methodHandle null_mh;
 947     debug_info-&gt;describe_scope(pc_offset, null_mh, method, jvms-&gt;bci(), jvms-&gt;should_reexecute());
 948   }
 949 
 950   // Mark the end of the scope set.
 951   debug_info-&gt;end_non_safepoint(pc_offset);
 952 }
 953 
 954 //------------------------------init_buffer------------------------------------
 955 CodeBuffer* Compile::init_buffer(uint* blk_starts) {
 956 
 957   // Set the initially allocated size
 958   int  code_req   = initial_code_capacity;
 959   int  locs_req   = initial_locs_capacity;
 960   int  stub_req   = initial_stub_capacity;
 961   int  const_req  = initial_const_capacity;
 962 
 963   int  pad_req    = NativeCall::instruction_size;
 964   // The extra spacing after the code is necessary on some platforms.
 965   // Sometimes we need to patch in a jump after the last instruction,
 966   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
 967 
 968   // Compute the byte offset where we can store the deopt pc.
 969   if (fixed_slots() != 0) {
 970     _orig_pc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
 971   }
 972 
 973   // Compute prolog code size
 974   _method_size = 0;
 975   _frame_slots = OptoReg::reg2stack(_matcher-&gt;_old_SP)+_regalloc-&gt;_framesize;
 976 #if defined(IA64) &amp;&amp; !defined(AIX)
 977   if (save_argument_registers()) {
 978     // 4815101: this is a stub with implicit and unknown precision fp args.
 979     // The usual spill mechanism can only generate stfd's in this case, which
 980     // doesn't work if the fp reg to spill contains a single-precision denorm.
 981     // Instead, we hack around the normal spill mechanism using stfspill's and
 982     // ldffill's in the MachProlog and MachEpilog emit methods.  We allocate
 983     // space here for the fp arg regs (f8-f15) we're going to thusly spill.
 984     //
 985     // If we ever implement 16-byte 'registers' == stack slots, we can
 986     // get rid of this hack and have SpillCopy generate stfspill/ldffill
 987     // instead of stfd/stfs/ldfd/ldfs.
 988     _frame_slots += 8*(16/BytesPerInt);
 989   }
 990 #endif
 991   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, "sanity check");
 992 
 993   if (has_mach_constant_base_node()) {
 994     uint add_size = 0;
 995     // Fill the constant table.
 996     // Note:  This must happen before shorten_branches.
 997     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 998       Block* b = _cfg-&gt;get_block(i);
 999 
1000       for (uint j = 0; j &lt; b-&gt;number_of_nodes(); j++) {
1001         Node* n = b-&gt;get_node(j);
1002 
1003         // If the node is a MachConstantNode evaluate the constant
1004         // value section.
1005         if (n-&gt;is_MachConstant()) {
1006           MachConstantNode* machcon = n-&gt;as_MachConstant();
1007           machcon-&gt;eval_constant(C);
1008         } else if (n-&gt;is_Mach()) {
1009           // On Power there are more nodes that issue constants.
1010           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1011         }
1012       }
1013     }
1014 
1015     // Calculate the offsets of the constants and the size of the
1016     // constant table (including the padding to the next section).
1017     constant_table().calculate_offsets_and_size();
1018     const_req = constant_table().size() + add_size;
1019   }
1020 
1021   // Initialize the space for the BufferBlob used to find and verify
1022   // instruction size in MachNode::emit_size()
1023   init_scratch_buffer_blob(const_req);
1024   if (failing())  return NULL; // Out of memory
1025 
1026   // Pre-compute the length of blocks and replace
1027   // long branches with short if machine supports it.
1028   shorten_branches(blk_starts, code_req, locs_req, stub_req);
1029 
1030   // nmethod and CodeBuffer count stubs &amp; constants as part of method's code.
1031   // class HandlerImpl is platform-specific and defined in the *.ad files.
1032   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1033   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1034   stub_req += MAX_stubs_size;   // ensure per-stub margin
1035   code_req += MAX_inst_size;    // ensure per-instruction margin
1036 
1037   if (StressCodeBuffers)
1038     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1039 
1040   int total_req =
1041     const_req +
1042     code_req +
1043     pad_req +
1044     stub_req +
1045     exception_handler_req +
1046     deopt_handler_req;               // deopt handler
1047 
1048   if (has_method_handle_invokes())
1049     total_req += deopt_handler_req;  // deopt MH handler
1050 
1051   CodeBuffer* cb = code_buffer();
1052   cb-&gt;initialize(total_req, locs_req);
1053 
1054   // Have we run out of code space?
1055   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1056     C-&gt;record_failure("CodeCache is full");
1057     return NULL;
1058   }
1059   // Configure the code buffer.
1060   cb-&gt;initialize_consts_size(const_req);
1061   cb-&gt;initialize_stubs_size(stub_req);
1062   cb-&gt;initialize_oop_recorder(env()-&gt;oop_recorder());
1063 
1064   // fill in the nop array for bundling computations
1065   MachNode *_nop_list[Bundle::_nop_count];
1066   Bundle::initialize_nops(_nop_list);
1067 
1068   return cb;
1069 }
1070 
1071 //------------------------------fill_buffer------------------------------------
1072 void Compile::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
1073   // blk_starts[] contains offsets calculated during short branches processing,
1074   // offsets should not be increased during following steps.
1075 
1076   // Compute the size of first NumberOfLoopInstrToAlign instructions at head
1077   // of a loop. It is used to determine the padding for loop alignment.
1078   compute_loop_first_inst_sizes();
1079 
1080   // Create oopmap set.
1081   _oop_map_set = new OopMapSet();
1082 
1083   // !!!!! This preserves old handling of oopmaps for now
1084   debug_info()-&gt;set_oopmaps(_oop_map_set);
1085 
1086   uint nblocks  = _cfg-&gt;number_of_blocks();
1087   // Count and start of implicit null check instructions
1088   uint inct_cnt = 0;
1089   uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1090 
1091   // Count and start of calls
1092   uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1093 
1094   uint  return_offset = 0;
1095   int nop_size = (new MachNopNode())-&gt;size(_regalloc);
1096 
1097   int previous_offset = 0;
1098   int current_offset  = 0;
1099   int last_call_offset = -1;
1100   int last_avoid_back_to_back_offset = -1;
1101 #ifdef ASSERT
1102   uint* jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks);
1103   uint* jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
1104   uint* jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
1105   uint* jmp_rule   = NEW_RESOURCE_ARRAY(uint,nblocks);
1106 #endif
1107 
1108   // Create an array of unused labels, one for each basic block, if printing is enabled
1109 #ifndef PRODUCT
1110   int *node_offsets      = NULL;
1111   uint node_offset_limit = unique();
1112 
1113   if (print_assembly())
1114     node_offsets         = NEW_RESOURCE_ARRAY(int, node_offset_limit);
1115 #endif
1116 
1117   NonSafepointEmitter non_safepoints(this);  // emit non-safepoints lazily
1118 
1119   // Emit the constant table.
1120   if (has_mach_constant_base_node()) {
1121     constant_table().emit(*cb);
1122   }
1123 
1124   // Create an array of labels, one for each basic block
1125   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1126   for (uint i=0; i &lt;= nblocks; i++) {
1127     blk_labels[i].init();
1128   }
1129 
1130   // ------------------
1131   // Now fill in the code buffer
1132   Node *delay_slot = NULL;
1133 
1134   for (uint i = 0; i &lt; nblocks; i++) {
1135     Block* block = _cfg-&gt;get_block(i);
1136     Node* head = block-&gt;head();
1137 
1138     // If this block needs to start aligned (i.e, can be reached other
1139     // than by falling-thru from the previous block), then force the
1140     // start of a new bundle.
1141     if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(head)) {
1142       cb-&gt;flush_bundle(true);
1143     }
1144 
1145 #ifdef ASSERT
1146     if (!block-&gt;is_connector()) {
1147       stringStream st;
1148       block-&gt;dump_head(_cfg, &amp;st);
1149       MacroAssembler(cb).block_comment(st.as_string());
1150     }
1151     jmp_target[i] = 0;
1152     jmp_offset[i] = 0;
1153     jmp_size[i]   = 0;
1154     jmp_rule[i]   = 0;
1155 #endif
1156     int blk_offset = current_offset;
1157 
1158     // Define the label at the beginning of the basic block
1159     MacroAssembler(cb).bind(blk_labels[block-&gt;_pre_order]);
1160 
1161     uint last_inst = block-&gt;number_of_nodes();
1162 
1163     // Emit block normally, except for last instruction.
1164     // Emit means "dump code bits into code buffer".
1165     for (uint j = 0; j&lt;last_inst; j++) {
1166 
1167       // Get the node
1168       Node* n = block-&gt;get_node(j);
1169 
1170       // See if delay slots are supported
1171       if (valid_bundle_info(n) &amp;&amp;
1172           node_bundling(n)-&gt;used_in_unconditional_delay()) {
1173         assert(delay_slot == NULL, "no use of delay slot node");
1174         assert(n-&gt;size(_regalloc) == Pipeline::instr_unit_size(), "delay slot instruction wrong size");
1175 
1176         delay_slot = n;
1177         continue;
1178       }
1179 
1180       // If this starts a new instruction group, then flush the current one
1181       // (but allow split bundles)
1182       if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(n))
1183         cb-&gt;flush_bundle(false);
1184 
1185       // Special handling for SafePoint/Call Nodes
1186       bool is_mcall = false;
1187       if (n-&gt;is_Mach()) {
1188         MachNode *mach = n-&gt;as_Mach();
1189         is_mcall = n-&gt;is_MachCall();
1190         bool is_sfn = n-&gt;is_MachSafePoint();
1191 
1192         // If this requires all previous instructions be flushed, then do so
1193         if (is_sfn || is_mcall || mach-&gt;alignment_required() != 1) {
1194           cb-&gt;flush_bundle(true);
1195           current_offset = cb-&gt;insts_size();
1196         }
1197 
1198         // A padding may be needed again since a previous instruction
1199         // could be moved to delay slot.
1200 
1201         // align the instruction if necessary
1202         int padding = mach-&gt;compute_padding(current_offset);
1203         // Make sure safepoint node for polling is distinct from a call's
1204         // return by adding a nop if needed.
1205         if (is_sfn &amp;&amp; !is_mcall &amp;&amp; padding == 0 &amp;&amp; current_offset == last_call_offset) {
1206           padding = nop_size;
1207         }
1208         if (padding == 0 &amp;&amp; mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE) &amp;&amp;
1209             current_offset == last_avoid_back_to_back_offset) {
1210           // Avoid back to back some instructions.
1211           padding = nop_size;
1212         }
1213 
1214         if (padding &gt; 0) {
1215           assert((padding % nop_size) == 0, "padding is not a multiple of NOP size");
1216           int nops_cnt = padding / nop_size;
1217           MachNode *nop = new MachNopNode(nops_cnt);
1218           block-&gt;insert_node(nop, j++);
1219           last_inst++;
1220           _cfg-&gt;map_node_to_block(nop, block);
1221           // Ensure enough space.
1222           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1223           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1224             C-&gt;record_failure("CodeCache is full");
1225             return;
1226           }
1227           nop-&gt;emit(*cb, _regalloc);
1228           cb-&gt;flush_bundle(true);
1229           current_offset = cb-&gt;insts_size();
1230         }
1231 
1232         // Remember the start of the last call in a basic block
1233         if (is_mcall) {
1234           MachCallNode *mcall = mach-&gt;as_MachCall();
1235 
1236           // This destination address is NOT PC-relative
1237           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
1238 
1239           // Save the return address
1240           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1241 
1242           if (mcall-&gt;is_MachCallLeaf()) {
1243             is_mcall = false;
1244             is_sfn = false;
1245           }
1246         }
1247 
1248         // sfn will be valid whenever mcall is valid now because of inheritance
1249         if (is_sfn || is_mcall) {
1250 
1251           // Handle special safepoint nodes for synchronization
1252           if (!is_mcall) {
1253             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1254             // !!!!! Stubs only need an oopmap right now, so bail out
1255             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1256               // Write the oopmap directly to the code blob??!!
1257               continue;
1258             }
1259           } // End synchronization
1260 
1261           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1262                                            current_offset);
1263           Process_OopMap_Node(mach, current_offset);
1264         } // End if safepoint
1265 
1266         // If this is a null check, then add the start of the previous instruction to the list
1267         else if( mach-&gt;is_MachNullCheck() ) {
1268           inct_starts[inct_cnt++] = previous_offset;
1269         }
1270 
1271         // If this is a branch, then fill in the label with the target BB's label
1272         else if (mach-&gt;is_MachBranch()) {
1273           // This requires the TRUE branch target be in succs[0]
1274           uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1275 
1276           // Try to replace long branch if delay slot is not used,
1277           // it is mostly for back branches since forward branch's
1278           // distance is not updated yet.
1279           bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
1280                                     node_bundling(n)-&gt;use_unconditional_delay();
1281           if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
1282            assert(delay_slot == NULL, "not expecting delay slot node");
1283            int br_size = n-&gt;size(_regalloc);
1284             int offset = blk_starts[block_num] - current_offset;
1285             if (block_num &gt;= i) {
1286               // Current and following block's offset are not
1287               // finalized yet, adjust distance by the difference
1288               // between calculated and final offsets of current block.
1289               offset -= (blk_starts[i] - blk_offset);
1290             }
1291             // In the following code a nop could be inserted before
1292             // the branch which will increase the backward distance.
1293             bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
1294             if (needs_padding &amp;&amp; offset &lt;= 0)
1295               offset -= nop_size;
1296 
1297             if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
1298               // We've got a winner.  Replace this branch.
1299               MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
1300 
1301               // Update the jmp_size.
1302               int new_size = replacement-&gt;size(_regalloc);
1303               assert((br_size - new_size) &gt;= (int)nop_size, "short_branch size should be smaller");
1304               // Insert padding between avoid_back_to_back branches.
1305               if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
1306                 MachNode *nop = new MachNopNode();
1307                 block-&gt;insert_node(nop, j++);
1308                 _cfg-&gt;map_node_to_block(nop, block);
1309                 last_inst++;
1310                 nop-&gt;emit(*cb, _regalloc);
1311                 cb-&gt;flush_bundle(true);
1312                 current_offset = cb-&gt;insts_size();
1313               }
1314 #ifdef ASSERT
1315               jmp_target[i] = block_num;
1316               jmp_offset[i] = current_offset - blk_offset;
1317               jmp_size[i]   = new_size;
1318               jmp_rule[i]   = mach-&gt;rule();
1319 #endif
1320               block-&gt;map_node(replacement, j);
1321               mach-&gt;subsume_by(replacement, C);
1322               n    = replacement;
1323               mach = replacement;
1324             }
1325           }
1326           mach-&gt;as_MachBranch()-&gt;label_set( &amp;blk_labels[block_num], block_num );
1327         } else if (mach-&gt;ideal_Opcode() == Op_Jump) {
1328           for (uint h = 0; h &lt; block-&gt;_num_succs; h++) {
1329             Block* succs_block = block-&gt;_succs[h];
1330             for (uint j = 1; j &lt; succs_block-&gt;num_preds(); j++) {
1331               Node* jpn = succs_block-&gt;pred(j);
1332               if (jpn-&gt;is_JumpProj() &amp;&amp; jpn-&gt;in(0) == mach) {
1333                 uint block_num = succs_block-&gt;non_connector()-&gt;_pre_order;
1334                 Label *blkLabel = &amp;blk_labels[block_num];
1335                 mach-&gt;add_case_label(jpn-&gt;as_JumpProj()-&gt;proj_no(), blkLabel);
1336               }
1337             }
1338           }
1339         }
1340 #ifdef ASSERT
1341         // Check that oop-store precedes the card-mark
1342         else if (mach-&gt;ideal_Opcode() == Op_StoreCM) {
1343           uint storeCM_idx = j;
1344           int count = 0;
1345           for (uint prec = mach-&gt;req(); prec &lt; mach-&gt;len(); prec++) {
1346             Node *oop_store = mach-&gt;in(prec);  // Precedence edge
1347             if (oop_store == NULL) continue;
1348             count++;
1349             uint i4;
1350             for (i4 = 0; i4 &lt; last_inst; ++i4) {
1351               if (block-&gt;get_node(i4) == oop_store) {
1352                 break;
1353               }
1354             }
1355             // Note: This test can provide a false failure if other precedence
1356             // edges have been added to the storeCMNode.
1357             assert(i4 == last_inst || i4 &lt; storeCM_idx, "CM card-mark executes before oop-store");
1358           }
1359           assert(count &gt; 0, "storeCM expects at least one precedence edge");
1360         }
1361 #endif
1362         else if (!n-&gt;is_Proj()) {
1363           // Remember the beginning of the previous instruction, in case
1364           // it's followed by a flag-kill and a null-check.  Happens on
1365           // Intel all the time, with add-to-memory kind of opcodes.
1366           previous_offset = current_offset;
1367         }
1368 
1369         // Not an else-if!
1370         // If this is a trap based cmp then add its offset to the list.
1371         if (mach-&gt;is_TrapBasedCheckNode()) {
1372           inct_starts[inct_cnt++] = current_offset;
1373         }
1374       }
1375 
1376       // Verify that there is sufficient space remaining
1377       cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1378       if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1379         C-&gt;record_failure("CodeCache is full");
1380         return;
1381       }
1382 
1383       // Save the offset for the listing
1384 #ifndef PRODUCT
1385       if (node_offsets &amp;&amp; n-&gt;_idx &lt; node_offset_limit)
1386         node_offsets[n-&gt;_idx] = cb-&gt;insts_size();
1387 #endif
1388 
1389       // "Normal" instruction case
1390       DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
1391       n-&gt;emit(*cb, _regalloc);
1392       current_offset  = cb-&gt;insts_size();
1393 
1394       // Above we only verified that there is enough space in the instruction section.
1395       // However, the instruction may emit stubs that cause code buffer expansion.
1396       // Bail out here if expansion failed due to a lack of code cache space.
1397       if (failing()) {
1398         return;
1399       }
1400 
1401 #ifdef ASSERT
1402       if (n-&gt;size(_regalloc) &lt; (current_offset-instr_offset)) {
1403         n-&gt;dump();
1404         assert(false, "wrong size of mach node");
1405       }
1406 #endif
1407       non_safepoints.observe_instruction(n, current_offset);
1408 
1409       // mcall is last "call" that can be a safepoint
1410       // record it so we can see if a poll will directly follow it
1411       // in which case we'll need a pad to make the PcDesc sites unique
1412       // see  5010568. This can be slightly inaccurate but conservative
1413       // in the case that return address is not actually at current_offset.
1414       // This is a small price to pay.
1415 
1416       if (is_mcall) {
1417         last_call_offset = current_offset;
1418       }
1419 
1420       if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
1421         // Avoid back to back some instructions.
1422         last_avoid_back_to_back_offset = current_offset;
1423       }
1424 
1425       // See if this instruction has a delay slot
1426       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
1427         guarantee(delay_slot != NULL, "expecting delay slot node");
1428 
1429         // Back up 1 instruction
1430         cb-&gt;set_insts_end(cb-&gt;insts_end() - Pipeline::instr_unit_size());
1431 
1432         // Save the offset for the listing
1433 #ifndef PRODUCT
1434         if (node_offsets &amp;&amp; delay_slot-&gt;_idx &lt; node_offset_limit)
1435           node_offsets[delay_slot-&gt;_idx] = cb-&gt;insts_size();
1436 #endif
1437 
1438         // Support a SafePoint in the delay slot
1439         if (delay_slot-&gt;is_MachSafePoint()) {
1440           MachNode *mach = delay_slot-&gt;as_Mach();
1441           // !!!!! Stubs only need an oopmap right now, so bail out
1442           if (!mach-&gt;is_MachCall() &amp;&amp; mach-&gt;as_MachSafePoint()-&gt;jvms()-&gt;method() == NULL) {
1443             // Write the oopmap directly to the code blob??!!
1444             delay_slot = NULL;
1445             continue;
1446           }
1447 
1448           int adjusted_offset = current_offset - Pipeline::instr_unit_size();
1449           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1450                                            adjusted_offset);
1451           // Generate an OopMap entry
1452           Process_OopMap_Node(mach, adjusted_offset);
1453         }
1454 
1455         // Insert the delay slot instruction
1456         delay_slot-&gt;emit(*cb, _regalloc);
1457 
1458         // Don't reuse it
1459         delay_slot = NULL;
1460       }
1461 
1462     } // End for all instructions in block
1463 
1464     // If the next block is the top of a loop, pad this block out to align
1465     // the loop top a little. Helps prevent pipe stalls at loop back branches.
1466     if (i &lt; nblocks-1) {
1467       Block *nb = _cfg-&gt;get_block(i + 1);
1468       int padding = nb-&gt;alignment_padding(current_offset);
1469       if( padding &gt; 0 ) {
1470         MachNode *nop = new MachNopNode(padding / nop_size);
1471         block-&gt;insert_node(nop, block-&gt;number_of_nodes());
1472         _cfg-&gt;map_node_to_block(nop, block);
1473         nop-&gt;emit(*cb, _regalloc);
1474         current_offset = cb-&gt;insts_size();
1475       }
1476     }
1477     // Verify that the distance for generated before forward
1478     // short branches is still valid.
1479     guarantee((int)(blk_starts[i+1] - blk_starts[i]) &gt;= (current_offset - blk_offset), "shouldn't increase block size");
1480 
1481     // Save new block start offset
1482     blk_starts[i] = blk_offset;
1483   } // End of for all blocks
1484   blk_starts[nblocks] = current_offset;
1485 
1486   non_safepoints.flush_at_end();
1487 
1488   // Offset too large?
1489   if (failing())  return;
1490 
1491   // Define a pseudo-label at the end of the code
1492   MacroAssembler(cb).bind( blk_labels[nblocks] );
1493 
1494   // Compute the size of the first block
1495   _first_block_size = blk_labels[1].loc_pos() - blk_labels[0].loc_pos();
1496 
1497 #ifdef ASSERT
1498   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
1499     if (jmp_target[i] != 0) {
1500       int br_size = jmp_size[i];
1501       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
1502       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
1503         tty-&gt;print_cr("target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d", blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
1504         assert(false, "Displacement too large for short jmp");
1505       }
1506     }
1507   }
1508 #endif
1509 
1510 #ifndef PRODUCT
1511   // Information on the size of the method, without the extraneous code
1512   Scheduling::increment_method_size(cb-&gt;insts_size());
1513 #endif
1514 
1515   // ------------------
1516   // Fill in exception table entries.
1517   FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
1518 
1519   // Only java methods have exception handlers and deopt handlers
1520   // class HandlerImpl is platform-specific and defined in the *.ad files.
1521   if (_method) {
1522     // Emit the exception handler code.
1523     _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
1524     if (failing()) {
1525       return; // CodeBuffer::expand failed
1526     }
1527     // Emit the deopt handler code.
1528     _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
1529 
1530     // Emit the MethodHandle deopt handler code (if required).
1531     if (has_method_handle_invokes() &amp;&amp; !failing()) {
1532       // We can use the same code as for the normal deopt handler, we
1533       // just need a different entry point address.
1534       _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
1535     }
1536   }
1537 
1538   // One last check for failed CodeBuffer::expand:
1539   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1540     C-&gt;record_failure("CodeCache is full");
1541     return;
1542   }
1543 
1544 #ifndef PRODUCT
1545   // Dump the assembly code, including basic-block numbers
1546   if (print_assembly()) {
1547     ttyLocker ttyl;  // keep the following output all in one block
1548     if (!VMThread::should_terminate()) {  // test this under the tty lock
1549       // This output goes directly to the tty, not the compiler log.
1550       // To enable tools to match it up with the compilation activity,
1551       // be sure to tag this tty output with the compile ID.
1552       if (xtty != NULL) {
1553         xtty-&gt;head("opto_assembly compile_id='%d'%s", compile_id(),
1554                    is_osr_compilation()    ? " compile_kind='osr'" :
1555                    "");
1556       }
1557       if (method() != NULL) {
1558         method()-&gt;print_metadata();
1559       }
1560       dump_asm(node_offsets, node_offset_limit);
1561       if (xtty != NULL) {
1562         // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
1563         // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
1564         // thread safe
1565         ttyLocker ttyl2;
1566         xtty-&gt;tail("opto_assembly");
1567       }
1568     }
1569   }
1570 #endif
1571 
1572 }
1573 
1574 void Compile::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {
1575   _inc_table.set_size(cnt);
1576 
1577   uint inct_cnt = 0;
1578   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
1579     Block* block = _cfg-&gt;get_block(i);
1580     Node *n = NULL;
1581     int j;
1582 
1583     // Find the branch; ignore trailing NOPs.
1584     for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
1585       n = block-&gt;get_node(j);
1586       if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con) {
1587         break;
1588       }
1589     }
1590 
1591     // If we didn't find anything, continue
1592     if (j &lt; 0) {
1593       continue;
1594     }
1595 
1596     // Compute ExceptionHandlerTable subtable entry and add it
1597     // (skip empty blocks)
1598     if (n-&gt;is_Catch()) {
1599 
1600       // Get the offset of the return from the call
1601       uint call_return = call_returns[block-&gt;_pre_order];
1602 #ifdef ASSERT
1603       assert( call_return &gt; 0, "no call seen for this basic block" );
1604       while (block-&gt;get_node(--j)-&gt;is_MachProj()) ;
1605       assert(block-&gt;get_node(j)-&gt;is_MachCall(), "CatchProj must follow call");
1606 #endif
1607       // last instruction is a CatchNode, find it's CatchProjNodes
1608       int nof_succs = block-&gt;_num_succs;
1609       // allocate space
1610       GrowableArray&lt;intptr_t&gt; handler_bcis(nof_succs);
1611       GrowableArray&lt;intptr_t&gt; handler_pcos(nof_succs);
1612       // iterate through all successors
1613       for (int j = 0; j &lt; nof_succs; j++) {
1614         Block* s = block-&gt;_succs[j];
1615         bool found_p = false;
1616         for (uint k = 1; k &lt; s-&gt;num_preds(); k++) {
1617           Node* pk = s-&gt;pred(k);
1618           if (pk-&gt;is_CatchProj() &amp;&amp; pk-&gt;in(0) == n) {
1619             const CatchProjNode* p = pk-&gt;as_CatchProj();
1620             found_p = true;
1621             // add the corresponding handler bci &amp; pco information
1622             if (p-&gt;_con != CatchProjNode::fall_through_index) {
1623               // p leads to an exception handler (and is not fall through)
1624               assert(s == _cfg-&gt;get_block(s-&gt;_pre_order), "bad numbering");
1625               // no duplicates, please
1626               if (!handler_bcis.contains(p-&gt;handler_bci())) {
1627                 uint block_num = s-&gt;non_connector()-&gt;_pre_order;
1628                 handler_bcis.append(p-&gt;handler_bci());
1629                 handler_pcos.append(blk_labels[block_num].loc_pos());
1630               }
1631             }
1632           }
1633         }
1634         assert(found_p, "no matching predecessor found");
1635         // Note:  Due to empty block removal, one block may have
1636         // several CatchProj inputs, from the same Catch.
1637       }
1638 
1639       // Set the offset of the return from the call
1640       assert(handler_bcis.find(-1) != -1, "must have default handler");
1641       _handler_table.add_subtable(call_return, &amp;handler_bcis, NULL, &amp;handler_pcos);
1642       continue;
1643     }
1644 
1645     // Handle implicit null exception table updates
1646     if (n-&gt;is_MachNullCheck()) {
1647       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1648       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1649       continue;
1650     }
1651     // Handle implicit exception table updates: trap instructions.
1652     if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;is_TrapBasedCheckNode()) {
1653       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1654       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1655       continue;
1656     }
1657   } // End of for all blocks fill in exception table entries
1658 }
1659 
1660 // Static Variables
1661 #ifndef PRODUCT
1662 uint Scheduling::_total_nop_size = 0;
1663 uint Scheduling::_total_method_size = 0;
1664 uint Scheduling::_total_branches = 0;
1665 uint Scheduling::_total_unconditional_delays = 0;
1666 uint Scheduling::_total_instructions_per_bundle[Pipeline::_max_instrs_per_cycle+1];
1667 #endif
1668 
1669 // Initializer for class Scheduling
1670 
1671 Scheduling::Scheduling(Arena *arena, Compile &amp;compile)
1672   : _arena(arena),
1673     _cfg(compile.cfg()),
1674     _regalloc(compile.regalloc()),
1675     _scheduled(arena),
1676     _available(arena),
1677     _reg_node(arena),
1678     _pinch_free_list(arena),
1679     _next_node(NULL),
1680     _bundle_instr_count(0),
1681     _bundle_cycle_number(0),
1682     _bundle_use(0, 0, resource_count, &amp;_bundle_use_elements[0])
1683 #ifndef PRODUCT
1684   , _branches(0)
1685   , _unconditional_delays(0)
1686 #endif
1687 {
1688   // Create a MachNopNode
1689   _nop = new MachNopNode();
1690 
1691   // Now that the nops are in the array, save the count
1692   // (but allow entries for the nops)
1693   _node_bundling_limit = compile.unique();
1694   uint node_max = _regalloc-&gt;node_regs_max_index();
1695 
1696   compile.set_node_bundling_limit(_node_bundling_limit);
1697 
1698   // This one is persistent within the Compile class
1699   _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
1700 
1701   // Allocate space for fixed-size arrays
1702   _node_latency    = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1703   _uses            = NEW_ARENA_ARRAY(arena, short,          node_max);
1704   _current_latency = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1705 
1706   // Clear the arrays
1707   memset(_node_bundling_base, 0, node_max * sizeof(Bundle));
1708   memset(_node_latency,       0, node_max * sizeof(unsigned short));
1709   memset(_uses,               0, node_max * sizeof(short));
1710   memset(_current_latency,    0, node_max * sizeof(unsigned short));
1711 
1712   // Clear the bundling information
1713   memcpy(_bundle_use_elements, Pipeline_Use::elaborated_elements, sizeof(Pipeline_Use::elaborated_elements));
1714 
1715   // Get the last node
1716   Block* block = _cfg-&gt;get_block(_cfg-&gt;number_of_blocks() - 1);
1717 
1718   _next_node = block-&gt;get_node(block-&gt;number_of_nodes() - 1);
1719 }
1720 
1721 #ifndef PRODUCT
1722 // Scheduling destructor
1723 Scheduling::~Scheduling() {
1724   _total_branches             += _branches;
1725   _total_unconditional_delays += _unconditional_delays;
1726 }
1727 #endif
1728 
1729 // Step ahead "i" cycles
1730 void Scheduling::step(uint i) {
1731 
1732   Bundle *bundle = node_bundling(_next_node);
1733   bundle-&gt;set_starts_bundle();
1734 
1735   // Update the bundle record, but leave the flags information alone
1736   if (_bundle_instr_count &gt; 0) {
1737     bundle-&gt;set_instr_count(_bundle_instr_count);
1738     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1739   }
1740 
1741   // Update the state information
1742   _bundle_instr_count = 0;
1743   _bundle_cycle_number += i;
1744   _bundle_use.step(i);
1745 }
1746 
1747 void Scheduling::step_and_clear() {
1748   Bundle *bundle = node_bundling(_next_node);
1749   bundle-&gt;set_starts_bundle();
1750 
1751   // Update the bundle record
1752   if (_bundle_instr_count &gt; 0) {
1753     bundle-&gt;set_instr_count(_bundle_instr_count);
1754     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1755 
1756     _bundle_cycle_number += 1;
1757   }
1758 
1759   // Clear the bundling information
1760   _bundle_instr_count = 0;
1761   _bundle_use.reset();
1762 
1763   memcpy(_bundle_use_elements,
1764     Pipeline_Use::elaborated_elements,
1765     sizeof(Pipeline_Use::elaborated_elements));
1766 }
1767 
1768 // Perform instruction scheduling and bundling over the sequence of
1769 // instructions in backwards order.
1770 void Compile::ScheduleAndBundle() {
1771 
1772   // Don't optimize this if it isn't a method
1773   if (!_method)
1774     return;
1775 
1776   // Don't optimize this if scheduling is disabled
1777   if (!do_scheduling())
1778     return;
1779 
1780   // Scheduling code works only with pairs (16 bytes) maximum.
1781   if (max_vector_size() &gt; 16)
1782     return;
1783 
1784   TracePhase tp("isched", &amp;timers[_t_instrSched]);
1785 
1786   // Create a data structure for all the scheduling information
1787   Scheduling scheduling(Thread::current()-&gt;resource_area(), *this);
1788 
1789   // Walk backwards over each basic block, computing the needed alignment
1790   // Walk over all the basic blocks
1791   scheduling.DoScheduling();
1792 }
1793 
1794 // Compute the latency of all the instructions.  This is fairly simple,
1795 // because we already have a legal ordering.  Walk over the instructions
1796 // from first to last, and compute the latency of the instruction based
1797 // on the latency of the preceding instruction(s).
1798 void Scheduling::ComputeLocalLatenciesForward(const Block *bb) {
1799 #ifndef PRODUCT
1800   if (_cfg-&gt;C-&gt;trace_opto_output())
1801     tty-&gt;print("# -&gt; ComputeLocalLatenciesForward\n");
1802 #endif
1803 
1804   // Walk over all the schedulable instructions
1805   for( uint j=_bb_start; j &lt; _bb_end; j++ ) {
1806 
1807     // This is a kludge, forcing all latency calculations to start at 1.
1808     // Used to allow latency 0 to force an instruction to the beginning
1809     // of the bb
1810     uint latency = 1;
1811     Node *use = bb-&gt;get_node(j);
1812     uint nlen = use-&gt;len();
1813 
1814     // Walk over all the inputs
1815     for ( uint k=0; k &lt; nlen; k++ ) {
1816       Node *def = use-&gt;in(k);
1817       if (!def)
1818         continue;
1819 
1820       uint l = _node_latency[def-&gt;_idx] + use-&gt;latency(k);
1821       if (latency &lt; l)
1822         latency = l;
1823     }
1824 
1825     _node_latency[use-&gt;_idx] = latency;
1826 
1827 #ifndef PRODUCT
1828     if (_cfg-&gt;C-&gt;trace_opto_output()) {
1829       tty-&gt;print("# latency %4d: ", latency);
1830       use-&gt;dump();
1831     }
1832 #endif
1833   }
1834 
1835 #ifndef PRODUCT
1836   if (_cfg-&gt;C-&gt;trace_opto_output())
1837     tty-&gt;print("# &lt;- ComputeLocalLatenciesForward\n");
1838 #endif
1839 
1840 } // end ComputeLocalLatenciesForward
1841 
1842 // See if this node fits into the present instruction bundle
1843 bool Scheduling::NodeFitsInBundle(Node *n) {
1844   uint n_idx = n-&gt;_idx;
1845 
1846   // If this is the unconditional delay instruction, then it fits
1847   if (n == _unconditional_delay_slot) {
1848 #ifndef PRODUCT
1849     if (_cfg-&gt;C-&gt;trace_opto_output())
1850       tty-&gt;print("#     NodeFitsInBundle [%4d]: TRUE; is in unconditional delay slot\n", n-&gt;_idx);
1851 #endif
1852     return (true);
1853   }
1854 
1855   // If the node cannot be scheduled this cycle, skip it
1856   if (_current_latency[n_idx] &gt; _bundle_cycle_number) {
1857 #ifndef PRODUCT
1858     if (_cfg-&gt;C-&gt;trace_opto_output())
1859       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; latency %4d &gt; %d\n",
1860         n-&gt;_idx, _current_latency[n_idx], _bundle_cycle_number);
1861 #endif
1862     return (false);
1863   }
1864 
1865   const Pipeline *node_pipeline = n-&gt;pipeline();
1866 
1867   uint instruction_count = node_pipeline-&gt;instructionCount();
1868   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
1869     instruction_count = 0;
1870   else if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
1871     instruction_count++;
1872 
1873   if (_bundle_instr_count + instruction_count &gt; Pipeline::_max_instrs_per_cycle) {
1874 #ifndef PRODUCT
1875     if (_cfg-&gt;C-&gt;trace_opto_output())
1876       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; too many instructions: %d &gt; %d\n",
1877         n-&gt;_idx, _bundle_instr_count + instruction_count, Pipeline::_max_instrs_per_cycle);
1878 #endif
1879     return (false);
1880   }
1881 
1882   // Don't allow non-machine nodes to be handled this way
1883   if (!n-&gt;is_Mach() &amp;&amp; instruction_count == 0)
1884     return (false);
1885 
1886   // See if there is any overlap
1887   uint delay = _bundle_use.full_latency(0, node_pipeline-&gt;resourceUse());
1888 
1889   if (delay &gt; 0) {
1890 #ifndef PRODUCT
1891     if (_cfg-&gt;C-&gt;trace_opto_output())
1892       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; functional units overlap\n", n_idx);
1893 #endif
1894     return false;
1895   }
1896 
1897 #ifndef PRODUCT
1898   if (_cfg-&gt;C-&gt;trace_opto_output())
1899     tty-&gt;print("#     NodeFitsInBundle [%4d]:  TRUE\n", n_idx);
1900 #endif
1901 
1902   return true;
1903 }
1904 
1905 Node * Scheduling::ChooseNodeToBundle() {
1906   uint siz = _available.size();
1907 
1908   if (siz == 0) {
1909 
1910 #ifndef PRODUCT
1911     if (_cfg-&gt;C-&gt;trace_opto_output())
1912       tty-&gt;print("#   ChooseNodeToBundle: NULL\n");
1913 #endif
1914     return (NULL);
1915   }
1916 
1917   // Fast path, if only 1 instruction in the bundle
1918   if (siz == 1) {
1919 #ifndef PRODUCT
1920     if (_cfg-&gt;C-&gt;trace_opto_output()) {
1921       tty-&gt;print("#   ChooseNodeToBundle (only 1): ");
1922       _available[0]-&gt;dump();
1923     }
1924 #endif
1925     return (_available[0]);
1926   }
1927 
1928   // Don't bother, if the bundle is already full
1929   if (_bundle_instr_count &lt; Pipeline::_max_instrs_per_cycle) {
1930     for ( uint i = 0; i &lt; siz; i++ ) {
1931       Node *n = _available[i];
1932 
1933       // Skip projections, we'll handle them another way
1934       if (n-&gt;is_Proj())
1935         continue;
1936 
1937       // This presupposed that instructions are inserted into the
1938       // available list in a legality order; i.e. instructions that
1939       // must be inserted first are at the head of the list
1940       if (NodeFitsInBundle(n)) {
1941 #ifndef PRODUCT
1942         if (_cfg-&gt;C-&gt;trace_opto_output()) {
1943           tty-&gt;print("#   ChooseNodeToBundle: ");
1944           n-&gt;dump();
1945         }
1946 #endif
1947         return (n);
1948       }
1949     }
1950   }
1951 
1952   // Nothing fits in this bundle, choose the highest priority
1953 #ifndef PRODUCT
1954   if (_cfg-&gt;C-&gt;trace_opto_output()) {
1955     tty-&gt;print("#   ChooseNodeToBundle: ");
1956     _available[0]-&gt;dump();
1957   }
1958 #endif
1959 
1960   return _available[0];
1961 }
1962 
1963 void Scheduling::AddNodeToAvailableList(Node *n) {
1964   assert( !n-&gt;is_Proj(), "projections never directly made available" );
1965 #ifndef PRODUCT
1966   if (_cfg-&gt;C-&gt;trace_opto_output()) {
1967     tty-&gt;print("#   AddNodeToAvailableList: ");
1968     n-&gt;dump();
1969   }
1970 #endif
1971 
1972   int latency = _current_latency[n-&gt;_idx];
1973 
1974   // Insert in latency order (insertion sort)
1975   uint i;
1976   for ( i=0; i &lt; _available.size(); i++ )
1977     if (_current_latency[_available[i]-&gt;_idx] &gt; latency)
1978       break;
1979 
1980   // Special Check for compares following branches
1981   if( n-&gt;is_Mach() &amp;&amp; _scheduled.size() &gt; 0 ) {
1982     int op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1983     Node *last = _scheduled[0];
1984     if( last-&gt;is_MachIf() &amp;&amp; last-&gt;in(1) == n &amp;&amp;
1985         ( op == Op_CmpI ||
1986           op == Op_CmpU ||
1987           op == Op_CmpUL ||
1988           op == Op_CmpP ||
1989           op == Op_CmpF ||
1990           op == Op_CmpD ||
1991           op == Op_CmpL ) ) {
1992 
1993       // Recalculate position, moving to front of same latency
1994       for ( i=0 ; i &lt; _available.size(); i++ )
1995         if (_current_latency[_available[i]-&gt;_idx] &gt;= latency)
1996           break;
1997     }
1998   }
1999 
2000   // Insert the node in the available list
2001   _available.insert(i, n);
2002 
2003 #ifndef PRODUCT
2004   if (_cfg-&gt;C-&gt;trace_opto_output())
2005     dump_available();
2006 #endif
2007 }
2008 
2009 void Scheduling::DecrementUseCounts(Node *n, const Block *bb) {
2010   for ( uint i=0; i &lt; n-&gt;len(); i++ ) {
2011     Node *def = n-&gt;in(i);
2012     if (!def) continue;
2013     if( def-&gt;is_Proj() )        // If this is a machine projection, then
2014       def = def-&gt;in(0);         // propagate usage thru to the base instruction
2015 
2016     if(_cfg-&gt;get_block_for_node(def) != bb) { // Ignore if not block-local
2017       continue;
2018     }
2019 
2020     // Compute the latency
2021     uint l = _bundle_cycle_number + n-&gt;latency(i);
2022     if (_current_latency[def-&gt;_idx] &lt; l)
2023       _current_latency[def-&gt;_idx] = l;
2024 
2025     // If this does not have uses then schedule it
2026     if ((--_uses[def-&gt;_idx]) == 0)
2027       AddNodeToAvailableList(def);
2028   }
2029 }
2030 
2031 void Scheduling::AddNodeToBundle(Node *n, const Block *bb) {
2032 #ifndef PRODUCT
2033   if (_cfg-&gt;C-&gt;trace_opto_output()) {
2034     tty-&gt;print("#   AddNodeToBundle: ");
2035     n-&gt;dump();
2036   }
2037 #endif
2038 
2039   // Remove this from the available list
2040   uint i;
2041   for (i = 0; i &lt; _available.size(); i++)
2042     if (_available[i] == n)
2043       break;
2044   assert(i &lt; _available.size(), "entry in _available list not found");
2045   _available.remove(i);
2046 
2047   // See if this fits in the current bundle
2048   const Pipeline *node_pipeline = n-&gt;pipeline();
2049   const Pipeline_Use&amp; node_usage = node_pipeline-&gt;resourceUse();
2050 
2051   // Check for instructions to be placed in the delay slot. We
2052   // do this before we actually schedule the current instruction,
2053   // because the delay slot follows the current instruction.
2054   if (Pipeline::_branch_has_delay_slot &amp;&amp;
2055       node_pipeline-&gt;hasBranchDelay() &amp;&amp;
2056       !_unconditional_delay_slot) {
2057 
2058     uint siz = _available.size();
2059 
2060     // Conditional branches can support an instruction that
2061     // is unconditionally executed and not dependent by the
2062     // branch, OR a conditionally executed instruction if
2063     // the branch is taken.  In practice, this means that
2064     // the first instruction at the branch target is
2065     // copied to the delay slot, and the branch goes to
2066     // the instruction after that at the branch target
2067     if ( n-&gt;is_MachBranch() ) {
2068 
2069       assert( !n-&gt;is_MachNullCheck(), "should not look for delay slot for Null Check" );
2070       assert( !n-&gt;is_Catch(),         "should not look for delay slot for Catch" );
2071 
2072 #ifndef PRODUCT
2073       _branches++;
2074 #endif
2075 
2076       // At least 1 instruction is on the available list
2077       // that is not dependent on the branch
2078       for (uint i = 0; i &lt; siz; i++) {
2079         Node *d = _available[i];
2080         const Pipeline *avail_pipeline = d-&gt;pipeline();
2081 
2082         // Don't allow safepoints in the branch shadow, that will
2083         // cause a number of difficulties
2084         if ( avail_pipeline-&gt;instructionCount() == 1 &amp;&amp;
2085             !avail_pipeline-&gt;hasMultipleBundles() &amp;&amp;
2086             !avail_pipeline-&gt;hasBranchDelay() &amp;&amp;
2087             Pipeline::instr_has_unit_size() &amp;&amp;
2088             d-&gt;size(_regalloc) == Pipeline::instr_unit_size() &amp;&amp;
2089             NodeFitsInBundle(d) &amp;&amp;
2090             !node_bundling(d)-&gt;used_in_delay()) {
2091 
2092           if (d-&gt;is_Mach() &amp;&amp; !d-&gt;is_MachSafePoint()) {
2093             // A node that fits in the delay slot was found, so we need to
2094             // set the appropriate bits in the bundle pipeline information so
2095             // that it correctly indicates resource usage.  Later, when we
2096             // attempt to add this instruction to the bundle, we will skip
2097             // setting the resource usage.
2098             _unconditional_delay_slot = d;
2099             node_bundling(n)-&gt;set_use_unconditional_delay();
2100             node_bundling(d)-&gt;set_used_in_unconditional_delay();
2101             _bundle_use.add_usage(avail_pipeline-&gt;resourceUse());
2102             _current_latency[d-&gt;_idx] = _bundle_cycle_number;
2103             _next_node = d;
2104             ++_bundle_instr_count;
2105 #ifndef PRODUCT
2106             _unconditional_delays++;
2107 #endif
2108             break;
2109           }
2110         }
2111       }
2112     }
2113 
2114     // No delay slot, add a nop to the usage
2115     if (!_unconditional_delay_slot) {
2116       // See if adding an instruction in the delay slot will overflow
2117       // the bundle.
2118       if (!NodeFitsInBundle(_nop)) {
2119 #ifndef PRODUCT
2120         if (_cfg-&gt;C-&gt;trace_opto_output())
2121           tty-&gt;print("#  *** STEP(1 instruction for delay slot) ***\n");
2122 #endif
2123         step(1);
2124       }
2125 
2126       _bundle_use.add_usage(_nop-&gt;pipeline()-&gt;resourceUse());
2127       _next_node = _nop;
2128       ++_bundle_instr_count;
2129     }
2130 
2131     // See if the instruction in the delay slot requires a
2132     // step of the bundles
2133     if (!NodeFitsInBundle(n)) {
2134 #ifndef PRODUCT
2135         if (_cfg-&gt;C-&gt;trace_opto_output())
2136           tty-&gt;print("#  *** STEP(branch won't fit) ***\n");
2137 #endif
2138         // Update the state information
2139         _bundle_instr_count = 0;
2140         _bundle_cycle_number += 1;
2141         _bundle_use.step(1);
2142     }
2143   }
2144 
2145   // Get the number of instructions
2146   uint instruction_count = node_pipeline-&gt;instructionCount();
2147   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
2148     instruction_count = 0;
2149 
2150   // Compute the latency information
2151   uint delay = 0;
2152 
2153   if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode()) {
2154     int relative_latency = _current_latency[n-&gt;_idx] - _bundle_cycle_number;
2155     if (relative_latency &lt; 0)
2156       relative_latency = 0;
2157 
2158     delay = _bundle_use.full_latency(relative_latency, node_usage);
2159 
2160     // Does not fit in this bundle, start a new one
2161     if (delay &gt; 0) {
2162       step(delay);
2163 
2164 #ifndef PRODUCT
2165       if (_cfg-&gt;C-&gt;trace_opto_output())
2166         tty-&gt;print("#  *** STEP(%d) ***\n", delay);
2167 #endif
2168     }
2169   }
2170 
2171   // If this was placed in the delay slot, ignore it
2172   if (n != _unconditional_delay_slot) {
2173 
2174     if (delay == 0) {
2175       if (node_pipeline-&gt;hasMultipleBundles()) {
2176 #ifndef PRODUCT
2177         if (_cfg-&gt;C-&gt;trace_opto_output())
2178           tty-&gt;print("#  *** STEP(multiple instructions) ***\n");
2179 #endif
2180         step(1);
2181       }
2182 
2183       else if (instruction_count + _bundle_instr_count &gt; Pipeline::_max_instrs_per_cycle) {
2184 #ifndef PRODUCT
2185         if (_cfg-&gt;C-&gt;trace_opto_output())
2186           tty-&gt;print("#  *** STEP(%d &gt;= %d instructions) ***\n",
2187             instruction_count + _bundle_instr_count,
2188             Pipeline::_max_instrs_per_cycle);
2189 #endif
2190         step(1);
2191       }
2192     }
2193 
2194     if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
2195       _bundle_instr_count++;
2196 
2197     // Set the node's latency
2198     _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2199 
2200     // Now merge the functional unit information
2201     if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode())
2202       _bundle_use.add_usage(node_usage);
2203 
2204     // Increment the number of instructions in this bundle
2205     _bundle_instr_count += instruction_count;
2206 
2207     // Remember this node for later
2208     if (n-&gt;is_Mach())
2209       _next_node = n;
2210   }
2211 
2212   // It's possible to have a BoxLock in the graph and in the _bbs mapping but
2213   // not in the bb-&gt;_nodes array.  This happens for debug-info-only BoxLocks.
2214   // 'Schedule' them (basically ignore in the schedule) but do not insert them
2215   // into the block.  All other scheduled nodes get put in the schedule here.
2216   int op = n-&gt;Opcode();
2217   if( (op == Op_Node &amp;&amp; n-&gt;req() == 0) || // anti-dependence node OR
2218       (op != Op_Node &amp;&amp;         // Not an unused antidepedence node and
2219        // not an unallocated boxlock
2220        (OptoReg::is_valid(_regalloc-&gt;get_reg_first(n)) || op != Op_BoxLock)) ) {
2221 
2222     // Push any trailing projections
2223     if( bb-&gt;get_node(bb-&gt;number_of_nodes()-1) != n ) {
2224       for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
2225         Node *foi = n-&gt;fast_out(i);
2226         if( foi-&gt;is_Proj() )
2227           _scheduled.push(foi);
2228       }
2229     }
2230 
2231     // Put the instruction in the schedule list
2232     _scheduled.push(n);
2233   }
2234 
2235 #ifndef PRODUCT
2236   if (_cfg-&gt;C-&gt;trace_opto_output())
2237     dump_available();
2238 #endif
2239 
2240   // Walk all the definitions, decrementing use counts, and
2241   // if a definition has a 0 use count, place it in the available list.
2242   DecrementUseCounts(n,bb);
2243 }
2244 
2245 // This method sets the use count within a basic block.  We will ignore all
2246 // uses outside the current basic block.  As we are doing a backwards walk,
2247 // any node we reach that has a use count of 0 may be scheduled.  This also
2248 // avoids the problem of cyclic references from phi nodes, as long as phi
2249 // nodes are at the front of the basic block.  This method also initializes
2250 // the available list to the set of instructions that have no uses within this
2251 // basic block.
2252 void Scheduling::ComputeUseCount(const Block *bb) {
2253 #ifndef PRODUCT
2254   if (_cfg-&gt;C-&gt;trace_opto_output())
2255     tty-&gt;print("# -&gt; ComputeUseCount\n");
2256 #endif
2257 
2258   // Clear the list of available and scheduled instructions, just in case
2259   _available.clear();
2260   _scheduled.clear();
2261 
2262   // No delay slot specified
2263   _unconditional_delay_slot = NULL;
2264 
2265 #ifdef ASSERT
2266   for( uint i=0; i &lt; bb-&gt;number_of_nodes(); i++ )
2267     assert( _uses[bb-&gt;get_node(i)-&gt;_idx] == 0, "_use array not clean" );
2268 #endif
2269 
2270   // Force the _uses count to never go to zero for unscheduable pieces
2271   // of the block
2272   for( uint k = 0; k &lt; _bb_start; k++ )
2273     _uses[bb-&gt;get_node(k)-&gt;_idx] = 1;
2274   for( uint l = _bb_end; l &lt; bb-&gt;number_of_nodes(); l++ )
2275     _uses[bb-&gt;get_node(l)-&gt;_idx] = 1;
2276 
2277   // Iterate backwards over the instructions in the block.  Don't count the
2278   // branch projections at end or the block header instructions.
2279   for( uint j = _bb_end-1; j &gt;= _bb_start; j-- ) {
2280     Node *n = bb-&gt;get_node(j);
2281     if( n-&gt;is_Proj() ) continue; // Projections handled another way
2282 
2283     // Account for all uses
2284     for ( uint k = 0; k &lt; n-&gt;len(); k++ ) {
2285       Node *inp = n-&gt;in(k);
2286       if (!inp) continue;
2287       assert(inp != n, "no cycles allowed" );
2288       if (_cfg-&gt;get_block_for_node(inp) == bb) { // Block-local use?
2289         if (inp-&gt;is_Proj()) { // Skip through Proj's
2290           inp = inp-&gt;in(0);
2291         }
2292         ++_uses[inp-&gt;_idx];     // Count 1 block-local use
2293       }
2294     }
2295 
2296     // If this instruction has a 0 use count, then it is available
2297     if (!_uses[n-&gt;_idx]) {
2298       _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2299       AddNodeToAvailableList(n);
2300     }
2301 
2302 #ifndef PRODUCT
2303     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2304       tty-&gt;print("#   uses: %3d: ", _uses[n-&gt;_idx]);
2305       n-&gt;dump();
2306     }
2307 #endif
2308   }
2309 
2310 #ifndef PRODUCT
2311   if (_cfg-&gt;C-&gt;trace_opto_output())
2312     tty-&gt;print("# &lt;- ComputeUseCount\n");
2313 #endif
2314 }
2315 
2316 // This routine performs scheduling on each basic block in reverse order,
2317 // using instruction latencies and taking into account function unit
2318 // availability.
2319 void Scheduling::DoScheduling() {
2320 #ifndef PRODUCT
2321   if (_cfg-&gt;C-&gt;trace_opto_output())
2322     tty-&gt;print("# -&gt; DoScheduling\n");
2323 #endif
2324 
2325   Block *succ_bb = NULL;
2326   Block *bb;
2327 
2328   // Walk over all the basic blocks in reverse order
2329   for (int i = _cfg-&gt;number_of_blocks() - 1; i &gt;= 0; succ_bb = bb, i--) {
2330     bb = _cfg-&gt;get_block(i);
2331 
2332 #ifndef PRODUCT
2333     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2334       tty-&gt;print("#  Schedule BB#%03d (initial)\n", i);
2335       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2336         bb-&gt;get_node(j)-&gt;dump();
2337       }
2338     }
2339 #endif
2340 
2341     // On the head node, skip processing
2342     if (bb == _cfg-&gt;get_root_block()) {
2343       continue;
2344     }
2345 
2346     // Skip empty, connector blocks
2347     if (bb-&gt;is_connector())
2348       continue;
2349 
2350     // If the following block is not the sole successor of
2351     // this one, then reset the pipeline information
2352     if (bb-&gt;_num_succs != 1 || bb-&gt;non_connector_successor(0) != succ_bb) {
2353 #ifndef PRODUCT
2354       if (_cfg-&gt;C-&gt;trace_opto_output()) {
2355         tty-&gt;print("*** bundle start of next BB, node %d, for %d instructions\n",
2356                    _next_node-&gt;_idx, _bundle_instr_count);
2357       }
2358 #endif
2359       step_and_clear();
2360     }
2361 
2362     // Leave untouched the starting instruction, any Phis, a CreateEx node
2363     // or Top.  bb-&gt;get_node(_bb_start) is the first schedulable instruction.
2364     _bb_end = bb-&gt;number_of_nodes()-1;
2365     for( _bb_start=1; _bb_start &lt;= _bb_end; _bb_start++ ) {
2366       Node *n = bb-&gt;get_node(_bb_start);
2367       // Things not matched, like Phinodes and ProjNodes don't get scheduled.
2368       // Also, MachIdealNodes do not get scheduled
2369       if( !n-&gt;is_Mach() ) continue;     // Skip non-machine nodes
2370       MachNode *mach = n-&gt;as_Mach();
2371       int iop = mach-&gt;ideal_Opcode();
2372       if( iop == Op_CreateEx ) continue; // CreateEx is pinned
2373       if( iop == Op_Con ) continue;      // Do not schedule Top
2374       if( iop == Op_Node &amp;&amp;     // Do not schedule PhiNodes, ProjNodes
2375           mach-&gt;pipeline() == MachNode::pipeline_class() &amp;&amp;
2376           !n-&gt;is_SpillCopy() &amp;&amp; !n-&gt;is_MachMerge() )  // Breakpoints, Prolog, etc
2377         continue;
2378       break;                    // Funny loop structure to be sure...
2379     }
2380     // Compute last "interesting" instruction in block - last instruction we
2381     // might schedule.  _bb_end points just after last schedulable inst.  We
2382     // normally schedule conditional branches (despite them being forced last
2383     // in the block), because they have delay slots we can fill.  Calls all
2384     // have their delay slots filled in the template expansions, so we don't
2385     // bother scheduling them.
2386     Node *last = bb-&gt;get_node(_bb_end);
2387     // Ignore trailing NOPs.
2388     while (_bb_end &gt; 0 &amp;&amp; last-&gt;is_Mach() &amp;&amp;
2389            last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Con) {
2390       last = bb-&gt;get_node(--_bb_end);
2391     }
2392     assert(!last-&gt;is_Mach() || last-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con, "");
2393     if( last-&gt;is_Catch() ||
2394        // Exclude unreachable path case when Halt node is in a separate block.
2395        (_bb_end &gt; 1 &amp;&amp; last-&gt;is_Mach() &amp;&amp; last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Halt) ) {
2396       // There must be a prior call.  Skip it.
2397       while( !bb-&gt;get_node(--_bb_end)-&gt;is_MachCall() ) {
2398         assert( bb-&gt;get_node(_bb_end)-&gt;is_MachProj(), "skipping projections after expected call" );
2399       }
2400     } else if( last-&gt;is_MachNullCheck() ) {
2401       // Backup so the last null-checked memory instruction is
2402       // outside the schedulable range. Skip over the nullcheck,
2403       // projection, and the memory nodes.
2404       Node *mem = last-&gt;in(1);
2405       do {
2406         _bb_end--;
2407       } while (mem != bb-&gt;get_node(_bb_end));
2408     } else {
2409       // Set _bb_end to point after last schedulable inst.
2410       _bb_end++;
2411     }
2412 
2413     assert( _bb_start &lt;= _bb_end, "inverted block ends" );
2414 
2415     // Compute the register antidependencies for the basic block
2416     ComputeRegisterAntidependencies(bb);
2417     if (_cfg-&gt;C-&gt;failing())  return;  // too many D-U pinch points
2418 
2419     // Compute intra-bb latencies for the nodes
2420     ComputeLocalLatenciesForward(bb);
2421 
2422     // Compute the usage within the block, and set the list of all nodes
2423     // in the block that have no uses within the block.
2424     ComputeUseCount(bb);
2425 
2426     // Schedule the remaining instructions in the block
2427     while ( _available.size() &gt; 0 ) {
2428       Node *n = ChooseNodeToBundle();
2429       guarantee(n != NULL, "no nodes available");
2430       AddNodeToBundle(n,bb);
2431     }
2432 
2433     assert( _scheduled.size() == _bb_end - _bb_start, "wrong number of instructions" );
2434 #ifdef ASSERT
2435     for( uint l = _bb_start; l &lt; _bb_end; l++ ) {
2436       Node *n = bb-&gt;get_node(l);
2437       uint m;
2438       for( m = 0; m &lt; _bb_end-_bb_start; m++ )
2439         if( _scheduled[m] == n )
2440           break;
2441       assert( m &lt; _bb_end-_bb_start, "instruction missing in schedule" );
2442     }
2443 #endif
2444 
2445     // Now copy the instructions (in reverse order) back to the block
2446     for ( uint k = _bb_start; k &lt; _bb_end; k++ )
2447       bb-&gt;map_node(_scheduled[_bb_end-k-1], k);
2448 
2449 #ifndef PRODUCT
2450     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2451       tty-&gt;print("#  Schedule BB#%03d (final)\n", i);
2452       uint current = 0;
2453       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2454         Node *n = bb-&gt;get_node(j);
2455         if( valid_bundle_info(n) ) {
2456           Bundle *bundle = node_bundling(n);
2457           if (bundle-&gt;instr_count() &gt; 0 || bundle-&gt;flags() &gt; 0) {
2458             tty-&gt;print("*** Bundle: ");
2459             bundle-&gt;dump();
2460           }
2461           n-&gt;dump();
2462         }
2463       }
2464     }
2465 #endif
2466 #ifdef ASSERT
2467   verify_good_schedule(bb,"after block local scheduling");
2468 #endif
2469   }
2470 
2471 #ifndef PRODUCT
2472   if (_cfg-&gt;C-&gt;trace_opto_output())
2473     tty-&gt;print("# &lt;- DoScheduling\n");
2474 #endif
2475 
2476   // Record final node-bundling array location
2477   _regalloc-&gt;C-&gt;set_node_bundling_base(_node_bundling_base);
2478 
2479 } // end DoScheduling
2480 
2481 // Verify that no live-range used in the block is killed in the block by a
2482 // wrong DEF.  This doesn't verify live-ranges that span blocks.
2483 
2484 // Check for edge existence.  Used to avoid adding redundant precedence edges.
2485 static bool edge_from_to( Node *from, Node *to ) {
2486   for( uint i=0; i&lt;from-&gt;len(); i++ )
2487     if( from-&gt;in(i) == to )
2488       return true;
2489   return false;
2490 }
2491 
2492 #ifdef ASSERT
2493 void Scheduling::verify_do_def( Node *n, OptoReg::Name def, const char *msg ) {
2494   // Check for bad kills
2495   if( OptoReg::is_valid(def) ) { // Ignore stores &amp; control flow
2496     Node *prior_use = _reg_node[def];
2497     if( prior_use &amp;&amp; !edge_from_to(prior_use,n) ) {
2498       tty-&gt;print("%s = ",OptoReg::as_VMReg(def)-&gt;name());
2499       n-&gt;dump();
2500       tty-&gt;print_cr("...");
2501       prior_use-&gt;dump();
2502       assert(edge_from_to(prior_use,n), "%s", msg);
2503     }
2504     _reg_node.map(def,NULL); // Kill live USEs
2505   }
2506 }
2507 
2508 void Scheduling::verify_good_schedule( Block *b, const char *msg ) {
2509 
2510   // Zap to something reasonable for the verify code
2511   _reg_node.clear();
2512 
2513   // Walk over the block backwards.  Check to make sure each DEF doesn't
2514   // kill a live value (other than the one it's supposed to).  Add each
2515   // USE to the live set.
2516   for( uint i = b-&gt;number_of_nodes()-1; i &gt;= _bb_start; i-- ) {
2517     Node *n = b-&gt;get_node(i);
2518     int n_op = n-&gt;Opcode();
2519     if( n_op == Op_MachProj &amp;&amp; n-&gt;ideal_reg() == MachProjNode::fat_proj ) {
2520       // Fat-proj kills a slew of registers
2521       RegMask rm = n-&gt;out_RegMask();// Make local copy
2522       while( rm.is_NotEmpty() ) {
2523         OptoReg::Name kill = rm.find_first_elem();
2524         rm.Remove(kill);
2525         verify_do_def( n, kill, msg );
2526       }
2527     } else if( n_op != Op_Node ) { // Avoid brand new antidependence nodes
2528       // Get DEF'd registers the normal way
2529       verify_do_def( n, _regalloc-&gt;get_reg_first(n), msg );
2530       verify_do_def( n, _regalloc-&gt;get_reg_second(n), msg );
2531     }
2532 
2533     // Now make all USEs live
2534     for( uint i=1; i&lt;n-&gt;req(); i++ ) {
2535       Node *def = n-&gt;in(i);
2536       assert(def != 0, "input edge required");
2537       OptoReg::Name reg_lo = _regalloc-&gt;get_reg_first(def);
2538       OptoReg::Name reg_hi = _regalloc-&gt;get_reg_second(def);
2539       if( OptoReg::is_valid(reg_lo) ) {
2540         assert(!_reg_node[reg_lo] || edge_from_to(_reg_node[reg_lo],def), "%s", msg);
2541         _reg_node.map(reg_lo,n);
2542       }
2543       if( OptoReg::is_valid(reg_hi) ) {
2544         assert(!_reg_node[reg_hi] || edge_from_to(_reg_node[reg_hi],def), "%s", msg);
2545         _reg_node.map(reg_hi,n);
2546       }
2547     }
2548 
2549   }
2550 
2551   // Zap to something reasonable for the Antidependence code
2552   _reg_node.clear();
2553 }
2554 #endif
2555 
2556 // Conditionally add precedence edges.  Avoid putting edges on Projs.
2557 static void add_prec_edge_from_to( Node *from, Node *to ) {
2558   if( from-&gt;is_Proj() ) {       // Put precedence edge on Proj's input
2559     assert( from-&gt;req() == 1 &amp;&amp; (from-&gt;len() == 1 || from-&gt;in(1)==0), "no precedence edges on projections" );
2560     from = from-&gt;in(0);
2561   }
2562   if( from != to &amp;&amp;             // No cycles (for things like LD L0,[L0+4] )
2563       !edge_from_to( from, to ) ) // Avoid duplicate edge
2564     from-&gt;add_prec(to);
2565 }
2566 
2567 void Scheduling::anti_do_def( Block *b, Node *def, OptoReg::Name def_reg, int is_def ) {
2568   if( !OptoReg::is_valid(def_reg) ) // Ignore stores &amp; control flow
2569     return;
2570 
2571   Node *pinch = _reg_node[def_reg]; // Get pinch point
2572   if ((pinch == NULL) || _cfg-&gt;get_block_for_node(pinch) != b || // No pinch-point yet?
2573       is_def ) {    // Check for a true def (not a kill)
2574     _reg_node.map(def_reg,def); // Record def/kill as the optimistic pinch-point
2575     return;
2576   }
2577 
2578   Node *kill = def;             // Rename 'def' to more descriptive 'kill'
2579   debug_only( def = (Node*)((intptr_t)0xdeadbeef); )
2580 
2581   // After some number of kills there _may_ be a later def
2582   Node *later_def = NULL;
2583 
2584   // Finding a kill requires a real pinch-point.
2585   // Check for not already having a pinch-point.
2586   // Pinch points are Op_Node's.
2587   if( pinch-&gt;Opcode() != Op_Node ) { // Or later-def/kill as pinch-point?
2588     later_def = pinch;            // Must be def/kill as optimistic pinch-point
2589     if ( _pinch_free_list.size() &gt; 0) {
2590       pinch = _pinch_free_list.pop();
2591     } else {
2592       pinch = new Node(1); // Pinch point to-be
2593     }
2594     if (pinch-&gt;_idx &gt;= _regalloc-&gt;node_regs_max_index()) {
2595       _cfg-&gt;C-&gt;record_method_not_compilable("too many D-U pinch points");
2596       return;
2597     }
2598     _cfg-&gt;map_node_to_block(pinch, b);      // Pretend it's valid in this block (lazy init)
2599     _reg_node.map(def_reg,pinch); // Record pinch-point
2600     //_regalloc-&gt;set_bad(pinch-&gt;_idx); // Already initialized this way.
2601     if( later_def-&gt;outcnt() == 0 || later_def-&gt;ideal_reg() == MachProjNode::fat_proj ) { // Distinguish def from kill
2602       pinch-&gt;init_req(0, _cfg-&gt;C-&gt;top());     // set not NULL for the next call
2603       add_prec_edge_from_to(later_def,pinch); // Add edge from kill to pinch
2604       later_def = NULL;           // and no later def
2605     }
2606     pinch-&gt;set_req(0,later_def);  // Hook later def so we can find it
2607   } else {                        // Else have valid pinch point
2608     if( pinch-&gt;in(0) )            // If there is a later-def
2609       later_def = pinch-&gt;in(0);   // Get it
2610   }
2611 
2612   // Add output-dependence edge from later def to kill
2613   if( later_def )               // If there is some original def
2614     add_prec_edge_from_to(later_def,kill); // Add edge from def to kill
2615 
2616   // See if current kill is also a use, and so is forced to be the pinch-point.
2617   if( pinch-&gt;Opcode() == Op_Node ) {
2618     Node *uses = kill-&gt;is_Proj() ? kill-&gt;in(0) : kill;
2619     for( uint i=1; i&lt;uses-&gt;req(); i++ ) {
2620       if( _regalloc-&gt;get_reg_first(uses-&gt;in(i)) == def_reg ||
2621           _regalloc-&gt;get_reg_second(uses-&gt;in(i)) == def_reg ) {
2622         // Yes, found a use/kill pinch-point
2623         pinch-&gt;set_req(0,NULL);  //
2624         pinch-&gt;replace_by(kill); // Move anti-dep edges up
2625         pinch = kill;
2626         _reg_node.map(def_reg,pinch);
2627         return;
2628       }
2629     }
2630   }
2631 
2632   // Add edge from kill to pinch-point
2633   add_prec_edge_from_to(kill,pinch);
2634 }
2635 
2636 void Scheduling::anti_do_use( Block *b, Node *use, OptoReg::Name use_reg ) {
2637   if( !OptoReg::is_valid(use_reg) ) // Ignore stores &amp; control flow
2638     return;
2639   Node *pinch = _reg_node[use_reg]; // Get pinch point
2640   // Check for no later def_reg/kill in block
2641   if ((pinch != NULL) &amp;&amp; _cfg-&gt;get_block_for_node(pinch) == b &amp;&amp;
2642       // Use has to be block-local as well
2643       _cfg-&gt;get_block_for_node(use) == b) {
2644     if( pinch-&gt;Opcode() == Op_Node &amp;&amp; // Real pinch-point (not optimistic?)
2645         pinch-&gt;req() == 1 ) {   // pinch not yet in block?
2646       pinch-&gt;del_req(0);        // yank pointer to later-def, also set flag
2647       // Insert the pinch-point in the block just after the last use
2648       b-&gt;insert_node(pinch, b-&gt;find_node(use) + 1);
2649       _bb_end++;                // Increase size scheduled region in block
2650     }
2651 
2652     add_prec_edge_from_to(pinch,use);
2653   }
2654 }
2655 
2656 // We insert antidependences between the reads and following write of
2657 // allocated registers to prevent illegal code motion. Hopefully, the
2658 // number of added references should be fairly small, especially as we
2659 // are only adding references within the current basic block.
2660 void Scheduling::ComputeRegisterAntidependencies(Block *b) {
2661 
2662 #ifdef ASSERT
2663   verify_good_schedule(b,"before block local scheduling");
2664 #endif
2665 
2666   // A valid schedule, for each register independently, is an endless cycle
2667   // of: a def, then some uses (connected to the def by true dependencies),
2668   // then some kills (defs with no uses), finally the cycle repeats with a new
2669   // def.  The uses are allowed to float relative to each other, as are the
2670   // kills.  No use is allowed to slide past a kill (or def).  This requires
2671   // antidependencies between all uses of a single def and all kills that
2672   // follow, up to the next def.  More edges are redundant, because later defs
2673   // &amp; kills are already serialized with true or antidependencies.  To keep
2674   // the edge count down, we add a 'pinch point' node if there's more than
2675   // one use or more than one kill/def.
2676 
2677   // We add dependencies in one bottom-up pass.
2678 
2679   // For each instruction we handle it's DEFs/KILLs, then it's USEs.
2680 
2681   // For each DEF/KILL, we check to see if there's a prior DEF/KILL for this
2682   // register.  If not, we record the DEF/KILL in _reg_node, the
2683   // register-to-def mapping.  If there is a prior DEF/KILL, we insert a
2684   // "pinch point", a new Node that's in the graph but not in the block.
2685   // We put edges from the prior and current DEF/KILLs to the pinch point.
2686   // We put the pinch point in _reg_node.  If there's already a pinch point
2687   // we merely add an edge from the current DEF/KILL to the pinch point.
2688 
2689   // After doing the DEF/KILLs, we handle USEs.  For each used register, we
2690   // put an edge from the pinch point to the USE.
2691 
2692   // To be expedient, the _reg_node array is pre-allocated for the whole
2693   // compilation.  _reg_node is lazily initialized; it either contains a NULL,
2694   // or a valid def/kill/pinch-point, or a leftover node from some prior
2695   // block.  Leftover node from some prior block is treated like a NULL (no
2696   // prior def, so no anti-dependence needed).  Valid def is distinguished by
2697   // it being in the current block.
2698   bool fat_proj_seen = false;
2699   uint last_safept = _bb_end-1;
2700   Node* end_node         = (_bb_end-1 &gt;= _bb_start) ? b-&gt;get_node(last_safept) : NULL;
2701   Node* last_safept_node = end_node;
2702   for( uint i = _bb_end-1; i &gt;= _bb_start; i-- ) {
2703     Node *n = b-&gt;get_node(i);
2704     int is_def = n-&gt;outcnt();   // def if some uses prior to adding precedence edges
2705     if( n-&gt;is_MachProj() &amp;&amp; n-&gt;ideal_reg() == MachProjNode::fat_proj ) {
2706       // Fat-proj kills a slew of registers
2707       // This can add edges to 'n' and obscure whether or not it was a def,
2708       // hence the is_def flag.
2709       fat_proj_seen = true;
2710       RegMask rm = n-&gt;out_RegMask();// Make local copy
2711       while( rm.is_NotEmpty() ) {
2712         OptoReg::Name kill = rm.find_first_elem();
2713         rm.Remove(kill);
2714         anti_do_def( b, n, kill, is_def );
2715       }
2716     } else {
2717       // Get DEF'd registers the normal way
2718       anti_do_def( b, n, _regalloc-&gt;get_reg_first(n), is_def );
2719       anti_do_def( b, n, _regalloc-&gt;get_reg_second(n), is_def );
2720     }
2721 
2722     // Kill projections on a branch should appear to occur on the
2723     // branch, not afterwards, so grab the masks from the projections
2724     // and process them.
2725     if (n-&gt;is_MachBranch() || (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Jump)) {
2726       for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
2727         Node* use = n-&gt;fast_out(i);
2728         if (use-&gt;is_Proj()) {
2729           RegMask rm = use-&gt;out_RegMask();// Make local copy
2730           while( rm.is_NotEmpty() ) {
2731             OptoReg::Name kill = rm.find_first_elem();
2732             rm.Remove(kill);
2733             anti_do_def( b, n, kill, false );
2734           }
2735         }
2736       }
2737     }
2738 
2739     // Check each register used by this instruction for a following DEF/KILL
2740     // that must occur afterward and requires an anti-dependence edge.
2741     for( uint j=0; j&lt;n-&gt;req(); j++ ) {
2742       Node *def = n-&gt;in(j);
2743       if( def ) {
2744         assert( !def-&gt;is_MachProj() || def-&gt;ideal_reg() != MachProjNode::fat_proj, "" );
2745         anti_do_use( b, n, _regalloc-&gt;get_reg_first(def) );
2746         anti_do_use( b, n, _regalloc-&gt;get_reg_second(def) );
2747       }
2748     }
2749     // Do not allow defs of new derived values to float above GC
2750     // points unless the base is definitely available at the GC point.
2751 
2752     Node *m = b-&gt;get_node(i);
2753 
2754     // Add precedence edge from following safepoint to use of derived pointer
2755     if( last_safept_node != end_node &amp;&amp;
2756         m != last_safept_node) {
2757       for (uint k = 1; k &lt; m-&gt;req(); k++) {
2758         const Type *t = m-&gt;in(k)-&gt;bottom_type();
2759         if( t-&gt;isa_oop_ptr() &amp;&amp;
2760             t-&gt;is_ptr()-&gt;offset() != 0 ) {
2761           last_safept_node-&gt;add_prec( m );
2762           break;
2763         }
2764       }
2765     }
2766 
2767     if( n-&gt;jvms() ) {           // Precedence edge from derived to safept
2768       // Check if last_safept_node was moved by pinch-point insertion in anti_do_use()
2769       if( b-&gt;get_node(last_safept) != last_safept_node ) {
2770         last_safept = b-&gt;find_node(last_safept_node);
2771       }
2772       for( uint j=last_safept; j &gt; i; j-- ) {
2773         Node *mach = b-&gt;get_node(j);
2774         if( mach-&gt;is_Mach() &amp;&amp; mach-&gt;as_Mach()-&gt;ideal_Opcode() == Op_AddP )
2775           mach-&gt;add_prec( n );
2776       }
2777       last_safept = i;
2778       last_safept_node = m;
2779     }
2780   }
2781 
2782   if (fat_proj_seen) {
2783     // Garbage collect pinch nodes that were not consumed.
2784     // They are usually created by a fat kill MachProj for a call.
2785     garbage_collect_pinch_nodes();
2786   }
2787 }
2788 
2789 // Garbage collect pinch nodes for reuse by other blocks.
2790 //
2791 // The block scheduler's insertion of anti-dependence
2792 // edges creates many pinch nodes when the block contains
2793 // 2 or more Calls.  A pinch node is used to prevent a
2794 // combinatorial explosion of edges.  If a set of kills for a
2795 // register is anti-dependent on a set of uses (or defs), rather
2796 // than adding an edge in the graph between each pair of kill
2797 // and use (or def), a pinch is inserted between them:
2798 //
2799 //            use1   use2  use3
2800 //                \   |   /
2801 //                 \  |  /
2802 //                  pinch
2803 //                 /  |  \
2804 //                /   |   \
2805 //            kill1 kill2 kill3
2806 //
2807 // One pinch node is created per register killed when
2808 // the second call is encountered during a backwards pass
2809 // over the block.  Most of these pinch nodes are never
2810 // wired into the graph because the register is never
2811 // used or def'ed in the block.
2812 //
2813 void Scheduling::garbage_collect_pinch_nodes() {
2814 #ifndef PRODUCT
2815     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print("Reclaimed pinch nodes:");
2816 #endif
2817     int trace_cnt = 0;
2818     for (uint k = 0; k &lt; _reg_node.Size(); k++) {
2819       Node* pinch = _reg_node[k];
2820       if ((pinch != NULL) &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp;
2821           // no predecence input edges
2822           (pinch-&gt;req() == pinch-&gt;len() || pinch-&gt;in(pinch-&gt;req()) == NULL) ) {
2823         cleanup_pinch(pinch);
2824         _pinch_free_list.push(pinch);
2825         _reg_node.map(k, NULL);
2826 #ifndef PRODUCT
2827         if (_cfg-&gt;C-&gt;trace_opto_output()) {
2828           trace_cnt++;
2829           if (trace_cnt &gt; 40) {
2830             tty-&gt;print("\n");
2831             trace_cnt = 0;
2832           }
2833           tty-&gt;print(" %d", pinch-&gt;_idx);
2834         }
2835 #endif
2836       }
2837     }
2838 #ifndef PRODUCT
2839     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print("\n");
2840 #endif
2841 }
2842 
2843 // Clean up a pinch node for reuse.
2844 void Scheduling::cleanup_pinch( Node *pinch ) {
2845   assert (pinch &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp; pinch-&gt;req() == 1, "just checking");
2846 
2847   for (DUIterator_Last imin, i = pinch-&gt;last_outs(imin); i &gt;= imin; ) {
2848     Node* use = pinch-&gt;last_out(i);
2849     uint uses_found = 0;
2850     for (uint j = use-&gt;req(); j &lt; use-&gt;len(); j++) {
2851       if (use-&gt;in(j) == pinch) {
2852         use-&gt;rm_prec(j);
2853         uses_found++;
2854       }
2855     }
2856     assert(uses_found &gt; 0, "must be a precedence edge");
2857     i -= uses_found;    // we deleted 1 or more copies of this edge
2858   }
2859   // May have a later_def entry
2860   pinch-&gt;set_req(0, NULL);
2861 }
2862 
2863 #ifndef PRODUCT
2864 
2865 void Scheduling::dump_available() const {
2866   tty-&gt;print("#Availist  ");
2867   for (uint i = 0; i &lt; _available.size(); i++)
2868     tty-&gt;print(" N%d/l%d", _available[i]-&gt;_idx,_current_latency[_available[i]-&gt;_idx]);
2869   tty-&gt;cr();
2870 }
2871 
2872 // Print Scheduling Statistics
2873 void Scheduling::print_statistics() {
2874   // Print the size added by nops for bundling
2875   tty-&gt;print("Nops added %d bytes to total of %d bytes",
2876     _total_nop_size, _total_method_size);
2877   if (_total_method_size &gt; 0)
2878     tty-&gt;print(", for %.2f%%",
2879       ((double)_total_nop_size) / ((double) _total_method_size) * 100.0);
2880   tty-&gt;print("\n");
2881 
2882   // Print the number of branch shadows filled
2883   if (Pipeline::_branch_has_delay_slot) {
2884     tty-&gt;print("Of %d branches, %d had unconditional delay slots filled",
2885       _total_branches, _total_unconditional_delays);
2886     if (_total_branches &gt; 0)
2887       tty-&gt;print(", for %.2f%%",
2888         ((double)_total_unconditional_delays) / ((double)_total_branches) * 100.0);
2889     tty-&gt;print("\n");
2890   }
2891 
2892   uint total_instructions = 0, total_bundles = 0;
2893 
2894   for (uint i = 1; i &lt;= Pipeline::_max_instrs_per_cycle; i++) {
2895     uint bundle_count   = _total_instructions_per_bundle[i];
2896     total_instructions += bundle_count * i;
2897     total_bundles      += bundle_count;
2898   }
2899 
2900   if (total_bundles &gt; 0)
2901     tty-&gt;print("Average ILP (excluding nops) is %.2f\n",
2902       ((double)total_instructions) / ((double)total_bundles));
2903 }
2904 #endif
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
