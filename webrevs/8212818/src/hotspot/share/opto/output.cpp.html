<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/hotspot/share/opto/output.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1998, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/assembler.inline.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "code/compiledIC.hpp"
  29 #include "code/debugInfo.hpp"
  30 #include "code/debugInfoRec.hpp"
  31 #include "compiler/compileBroker.hpp"
  32 #include "compiler/compilerDirectives.hpp"
  33 #include "compiler/oopMap.hpp"
  34 #include "memory/allocation.inline.hpp"
  35 #include "opto/ad.hpp"
  36 #include "opto/callnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/locknode.hpp"
  39 #include "opto/machnode.hpp"
  40 #include "opto/optoreg.hpp"
  41 #include "opto/output.hpp"
  42 #include "opto/regalloc.hpp"
  43 #include "opto/runtime.hpp"
  44 #include "opto/subnode.hpp"
  45 #include "opto/type.hpp"
  46 #include "runtime/handles.inline.hpp"
  47 #include "utilities/xmlstream.hpp"
  48 
  49 #ifndef PRODUCT
  50 #define DEBUG_ARG(x) , x
  51 #else
  52 #define DEBUG_ARG(x)
  53 #endif
  54 
  55 // Convert Nodes to instruction bits and pass off to the VM
  56 void Compile::Output() {
  57   // RootNode goes
  58   assert( _cfg-&gt;get_root_block()-&gt;number_of_nodes() == 0, "" );
  59 
  60   // The number of new nodes (mostly MachNop) is proportional to
  61   // the number of java calls and inner loops which are aligned.
  62   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
  63                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
  64                            "out of nodes before code generation" ) ) {
  65     return;
  66   }
  67   // Make sure I can find the Start Node
  68   Block *entry = _cfg-&gt;get_block(1);
  69   Block *broot = _cfg-&gt;get_root_block();
  70 
  71   const StartNode *start = entry-&gt;head()-&gt;as_Start();
  72 
  73   // Replace StartNode with prolog
  74   MachPrologNode *prolog = new MachPrologNode();
  75   entry-&gt;map_node(prolog, 0);
  76   _cfg-&gt;map_node_to_block(prolog, entry);
  77   _cfg-&gt;unmap_node_from_block(start); // start is no longer in any block
  78 
  79   // Virtual methods need an unverified entry point
  80 
  81   if( is_osr_compilation() ) {
  82     if( PoisonOSREntry ) {
  83       // TODO: Should use a ShouldNotReachHereNode...
  84       _cfg-&gt;insert( broot, 0, new MachBreakpointNode() );
  85     }
  86   } else {
  87     if( _method &amp;&amp; !_method-&gt;flags().is_static() ) {
  88       // Insert unvalidated entry point
  89       _cfg-&gt;insert( broot, 0, new MachUEPNode() );
  90     }
  91 
  92   }
  93 
  94   // Break before main entry point
  95   if ((_method &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
  96       (OptoBreakpoint &amp;&amp; is_method_compilation())       ||
  97       (OptoBreakpointOSR &amp;&amp; is_osr_compilation())       ||
  98       (OptoBreakpointC2R &amp;&amp; !_method)                   ) {
  99     // checking for _method means that OptoBreakpoint does not apply to
 100     // runtime stubs or frame converters
 101     _cfg-&gt;insert( entry, 1, new MachBreakpointNode() );
 102   }
 103 
 104   // Insert epilogs before every return
 105   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 106     Block* block = _cfg-&gt;get_block(i);
 107     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == _cfg-&gt;get_root_block()) { // Found a program exit point?
 108       Node* m = block-&gt;end();
 109       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 110         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 111         block-&gt;add_inst(epilog);
 112         _cfg-&gt;map_node_to_block(epilog, block);
 113       }
 114     }
 115   }
 116 
 117   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, _cfg-&gt;number_of_blocks() + 1);
 118   blk_starts[0] = 0;
 119 
 120   // Initialize code buffer and process short branches.
 121   CodeBuffer* cb = init_buffer(blk_starts);
 122 
 123   if (cb == NULL || failing()) {
 124     return;
 125   }
 126 
 127   ScheduleAndBundle();
 128 
 129 #ifndef PRODUCT
 130   if (trace_opto_output()) {
 131     tty-&gt;print("\n---- After ScheduleAndBundle ----\n");
 132     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 133       tty-&gt;print("\nBB#%03d:\n", i);
 134       Block* block = _cfg-&gt;get_block(i);
 135       for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
 136         Node* n = block-&gt;get_node(j);
 137         OptoReg::Name reg = _regalloc-&gt;get_reg_first(n);
 138         tty-&gt;print(" %-6s ", reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : "");
 139         n-&gt;dump();
 140       }
 141     }
 142   }
 143 #endif
 144 
 145   if (failing()) {
 146     return;
 147   }
 148 
 149   BuildOopMaps();
 150 
 151   if (failing())  {
 152     return;
 153   }
 154 
 155   fill_buffer(cb, blk_starts);
 156 }
 157 
 158 bool Compile::need_stack_bang(int frame_size_in_bytes) const {
 159   // Determine if we need to generate a stack overflow check.
 160   // Do it if the method is not a stub function and
 161   // has java calls or has frame size &gt; vm_page_size/8.
 162   // The debug VM checks that deoptimization doesn't trigger an
 163   // unexpected stack overflow (compiled method stack banging should
 164   // guarantee it doesn't happen) so we always need the stack bang in
 165   // a debug VM.
 166   return (UseStackBanging &amp;&amp; stub_function() == NULL &amp;&amp;
 167           (has_java_calls() || frame_size_in_bytes &gt; os::vm_page_size()&gt;&gt;3
 168            DEBUG_ONLY(|| true)));
 169 }
 170 
 171 bool Compile::need_register_stack_bang() const {
 172   // Determine if we need to generate a register stack overflow check.
 173   // This is only used on architectures which have split register
 174   // and memory stacks (ie. IA64).
 175   // Bang if the method is not a stub function and has java calls
 176   return (stub_function() == NULL &amp;&amp; has_java_calls());
 177 }
 178 
 179 
 180 // Compute the size of first NumberOfLoopInstrToAlign instructions at the top
 181 // of a loop. When aligning a loop we need to provide enough instructions
 182 // in cpu's fetch buffer to feed decoders. The loop alignment could be
 183 // avoided if we have enough instructions in fetch buffer at the head of a loop.
 184 // By default, the size is set to 999999 by Block's constructor so that
 185 // a loop will be aligned if the size is not reset here.
 186 //
 187 // Note: Mach instructions could contain several HW instructions
 188 // so the size is estimated only.
 189 //
 190 void Compile::compute_loop_first_inst_sizes() {
 191   // The next condition is used to gate the loop alignment optimization.
 192   // Don't aligned a loop if there are enough instructions at the head of a loop
 193   // or alignment padding is larger then MaxLoopPad. By default, MaxLoopPad
 194   // is equal to OptoLoopAlignment-1 except on new Intel cpus, where it is
 195   // equal to 11 bytes which is the largest address NOP instruction.
 196   if (MaxLoopPad &lt; OptoLoopAlignment - 1) {
 197     uint last_block = _cfg-&gt;number_of_blocks() - 1;
 198     for (uint i = 1; i &lt;= last_block; i++) {
 199       Block* block = _cfg-&gt;get_block(i);
 200       // Check the first loop's block which requires an alignment.
 201       if (block-&gt;loop_alignment() &gt; (uint)relocInfo::addr_unit()) {
 202         uint sum_size = 0;
 203         uint inst_cnt = NumberOfLoopInstrToAlign;
 204         inst_cnt = block-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 205 
 206         // Check subsequent fallthrough blocks if the loop's first
 207         // block(s) does not have enough instructions.
 208         Block *nb = block;
 209         while(inst_cnt &gt; 0 &amp;&amp;
 210               i &lt; last_block &amp;&amp;
 211               !_cfg-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 212               !nb-&gt;has_successor(block)) {
 213           i++;
 214           nb = _cfg-&gt;get_block(i);
 215           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 216         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 217 
 218         block-&gt;set_first_inst_size(sum_size);
 219       } // f( b-&gt;head()-&gt;is_Loop() )
 220     } // for( i &lt;= last_block )
 221   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 222 }
 223 
 224 // The architecture description provides short branch variants for some long
 225 // branch instructions. Replace eligible long branches with short branches.
 226 void Compile::shorten_branches(uint* blk_starts, int&amp; code_size, int&amp; reloc_size, int&amp; stub_size) {
 227   // Compute size of each block, method size, and relocation information size
 228   uint nblocks  = _cfg-&gt;number_of_blocks();
 229 
 230   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 231   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 232   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 233 
 234   // Collect worst case block paddings
 235   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 236   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 237 
 238   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 239   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 240 
 241   bool has_short_branch_candidate = false;
 242 
 243   // Initialize the sizes to 0
 244   code_size  = 0;          // Size in bytes of generated code
 245   stub_size  = 0;          // Size in bytes of all stub entries
 246   // Size in bytes of all relocation entries, including those in local stubs.
 247   // Start with 2-bytes of reloc info for the unvalidated entry point
 248   reloc_size = 1;          // Number of relocation entries
 249 
 250   // Make three passes.  The first computes pessimistic blk_starts,
 251   // relative jmp_offset and reloc_size information.  The second performs
 252   // short branch substitution using the pessimistic sizing.  The
 253   // third inserts nops where needed.
 254 
 255   // Step one, perform a pessimistic sizing pass.
 256   uint last_call_adr = max_juint;
 257   uint last_avoid_back_to_back_adr = max_juint;
 258   uint nop_size = (new MachNopNode())-&gt;size(_regalloc);
 259   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 260     Block* block = _cfg-&gt;get_block(i);
 261 
 262     // During short branch replacement, we store the relative (to blk_starts)
 263     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 264     // This is so that we do not need to recompute sizes of all nodes when
 265     // we compute correct blk_starts in our next sizing pass.
 266     jmp_offset[i] = 0;
 267     jmp_size[i]   = 0;
 268     jmp_nidx[i]   = -1;
 269     DEBUG_ONLY( jmp_target[i] = 0; )
 270     DEBUG_ONLY( jmp_rule[i]   = 0; )
 271 
 272     // Sum all instruction sizes to compute block size
 273     uint last_inst = block-&gt;number_of_nodes();
 274     uint blk_size = 0;
 275     for (uint j = 0; j &lt; last_inst; j++) {
 276       Node* nj = block-&gt;get_node(j);
 277       // Handle machine instruction nodes
 278       if (nj-&gt;is_Mach()) {
 279         MachNode *mach = nj-&gt;as_Mach();
 280         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 281         reloc_size += mach-&gt;reloc();
 282         if (mach-&gt;is_MachCall()) {
 283           // add size information for trampoline stub
 284           // class CallStubImpl is platform-specific and defined in the *.ad files.
 285           stub_size  += CallStubImpl::size_call_trampoline();
 286           reloc_size += CallStubImpl::reloc_call_trampoline();
 287 
 288           MachCallNode *mcall = mach-&gt;as_MachCall();
 289           // This destination address is NOT PC-relative
 290 
 291           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
 292 
 293           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 294             stub_size  += CompiledStaticCall::to_interp_stub_size();
 295             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 296 #if INCLUDE_AOT
 297             stub_size  += CompiledStaticCall::to_aot_stub_size();
 298             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 299 #endif
 300           }
 301         } else if (mach-&gt;is_MachSafePoint()) {
 302           // If call/safepoint are adjacent, account for possible
 303           // nop to disambiguate the two safepoints.
 304           // ScheduleAndBundle() can rearrange nodes in a block,
 305           // check for all offsets inside this block.
 306           if (last_call_adr &gt;= blk_starts[i]) {
 307             blk_size += nop_size;
 308           }
 309         }
 310         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 311           // Nop is inserted between "avoid back to back" instructions.
 312           // ScheduleAndBundle() can rearrange nodes in a block,
 313           // check for all offsets inside this block.
 314           if (last_avoid_back_to_back_adr &gt;= blk_starts[i]) {
 315             blk_size += nop_size;
 316           }
 317         }
 318         if (mach-&gt;may_be_short_branch()) {
 319           if (!nj-&gt;is_MachBranch()) {
 320 #ifndef PRODUCT
 321             nj-&gt;dump(3);
 322 #endif
 323             Unimplemented();
 324           }
 325           assert(jmp_nidx[i] == -1, "block should have only one branch");
 326           jmp_offset[i] = blk_size;
 327           jmp_size[i]   = nj-&gt;size(_regalloc);
 328           jmp_nidx[i]   = j;
 329           has_short_branch_candidate = true;
 330         }
 331       }
 332       blk_size += nj-&gt;size(_regalloc);
 333       // Remember end of call offset
 334       if (nj-&gt;is_MachCall() &amp;&amp; !nj-&gt;is_MachCallLeaf()) {
 335         last_call_adr = blk_starts[i]+blk_size;
 336       }
 337       // Remember end of avoid_back_to_back offset
 338       if (nj-&gt;is_Mach() &amp;&amp; nj-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
 339         last_avoid_back_to_back_adr = blk_starts[i]+blk_size;
 340       }
 341     }
 342 
 343     // When the next block starts a loop, we may insert pad NOP
 344     // instructions.  Since we cannot know our future alignment,
 345     // assume the worst.
 346     if (i &lt; nblocks - 1) {
 347       Block* nb = _cfg-&gt;get_block(i + 1);
 348       int max_loop_pad = nb-&gt;code_alignment()-relocInfo::addr_unit();
 349       if (max_loop_pad &gt; 0) {
 350         assert(is_power_of_2(max_loop_pad+relocInfo::addr_unit()), "");
 351         // Adjust last_call_adr and/or last_avoid_back_to_back_adr.
 352         // If either is the last instruction in this block, bump by
 353         // max_loop_pad in lock-step with blk_size, so sizing
 354         // calculations in subsequent blocks still can conservatively
 355         // detect that it may the last instruction in this block.
 356         if (last_call_adr == blk_starts[i]+blk_size) {
 357           last_call_adr += max_loop_pad;
 358         }
 359         if (last_avoid_back_to_back_adr == blk_starts[i]+blk_size) {
 360           last_avoid_back_to_back_adr += max_loop_pad;
 361         }
 362         blk_size += max_loop_pad;
 363         block_worst_case_pad[i + 1] = max_loop_pad;
 364       }
 365     }
 366 
 367     // Save block size; update total method size
 368     blk_starts[i+1] = blk_starts[i]+blk_size;
 369   }
 370 
 371   // Step two, replace eligible long jumps.
 372   bool progress = true;
 373   uint last_may_be_short_branch_adr = max_juint;
 374   while (has_short_branch_candidate &amp;&amp; progress) {
 375     progress = false;
 376     has_short_branch_candidate = false;
 377     int adjust_block_start = 0;
 378     for (uint i = 0; i &lt; nblocks; i++) {
 379       Block* block = _cfg-&gt;get_block(i);
 380       int idx = jmp_nidx[i];
 381       MachNode* mach = (idx == -1) ? NULL: block-&gt;get_node(idx)-&gt;as_Mach();
 382       if (mach != NULL &amp;&amp; mach-&gt;may_be_short_branch()) {
 383 #ifdef ASSERT
 384         assert(jmp_size[i] &gt; 0 &amp;&amp; mach-&gt;is_MachBranch(), "sanity");
 385         int j;
 386         // Find the branch; ignore trailing NOPs.
 387         for (j = block-&gt;number_of_nodes()-1; j&gt;=0; j--) {
 388           Node* n = block-&gt;get_node(j);
 389           if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con)
 390             break;
 391         }
 392         assert(j &gt;= 0 &amp;&amp; j == idx &amp;&amp; block-&gt;get_node(j) == (Node*)mach, "sanity");
 393 #endif
 394         int br_size = jmp_size[i];
 395         int br_offs = blk_starts[i] + jmp_offset[i];
 396 
 397         // This requires the TRUE branch target be in succs[0]
 398         uint bnum = block-&gt;non_connector_successor(0)-&gt;_pre_order;
 399         int offset = blk_starts[bnum] - br_offs;
 400         if (bnum &gt; i) { // adjust following block's offset
 401           offset -= adjust_block_start;
 402         }
 403 
 404         // This block can be a loop header, account for the padding
 405         // in the previous block.
 406         int block_padding = block_worst_case_pad[i];
 407         assert(i == 0 || block_padding == 0 || br_offs &gt;= block_padding, "Should have at least a padding on top");
 408         // In the following code a nop could be inserted before
 409         // the branch which will increase the backward distance.
 410         bool needs_padding = ((uint)(br_offs - block_padding) == last_may_be_short_branch_adr);
 411         assert(!needs_padding || jmp_offset[i] == 0, "padding only branches at the beginning of block");
 412 
 413         if (needs_padding &amp;&amp; offset &lt;= 0)
 414           offset -= nop_size;
 415 
 416         if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
 417           // We've got a winner.  Replace this branch.
 418           MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
 419 
 420           // Update the jmp_size.
 421           int new_size = replacement-&gt;size(_regalloc);
 422           int diff     = br_size - new_size;
 423           assert(diff &gt;= (int)nop_size, "short_branch size should be smaller");
 424           // Conservatively take into account padding between
 425           // avoid_back_to_back branches. Previous branch could be
 426           // converted into avoid_back_to_back branch during next
 427           // rounds.
 428           if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 429             jmp_offset[i] += nop_size;
 430             diff -= nop_size;
 431           }
 432           adjust_block_start += diff;
 433           block-&gt;map_node(replacement, idx);
 434           mach-&gt;subsume_by(replacement, C);
 435           mach = replacement;
 436           progress = true;
 437 
 438           jmp_size[i] = new_size;
 439           DEBUG_ONLY( jmp_target[i] = bnum; );
 440           DEBUG_ONLY( jmp_rule[i] = mach-&gt;rule(); );
 441         } else {
 442           // The jump distance is not short, try again during next iteration.
 443           has_short_branch_candidate = true;
 444         }
 445       } // (mach-&gt;may_be_short_branch())
 446       if (mach != NULL &amp;&amp; (mach-&gt;may_be_short_branch() ||
 447                            mach-&gt;avoid_back_to_back(MachNode::AVOID_AFTER))) {
 448         last_may_be_short_branch_adr = blk_starts[i] + jmp_offset[i] + jmp_size[i];
 449       }
 450       blk_starts[i+1] -= adjust_block_start;
 451     }
 452   }
 453 
 454 #ifdef ASSERT
 455   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 456     if (jmp_target[i] != 0) {
 457       int br_size = jmp_size[i];
 458       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
 459       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
 460         tty-&gt;print_cr("target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d", blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
 461       }
 462       assert(_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), "Displacement too large for short jmp");
 463     }
 464   }
 465 #endif
 466 
 467   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 468   // after ScheduleAndBundle().
 469 
 470   // ------------------
 471   // Compute size for code buffer
 472   code_size = blk_starts[nblocks];
 473 
 474   // Relocation records
 475   reloc_size += 1;              // Relo entry for exception handler
 476 
 477   // Adjust reloc_size to number of record of relocation info
 478   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 479   // a relocation index.
 480   // The CodeBuffer will expand the locs array if this estimate is too low.
 481   reloc_size *= 10 / sizeof(relocInfo);
 482 }
 483 
 484 //------------------------------FillLocArray-----------------------------------
 485 // Create a bit of debug info and append it to the array.  The mapping is from
 486 // Java local or expression stack to constant, register or stack-slot.  For
 487 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 488 // entry has been taken care of and caller should skip it).
 489 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 490   // This should never have accepted Bad before
 491   assert(OptoReg::is_valid(regnum), "location must be valid");
 492   return (OptoReg::is_reg(regnum))
 493     ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )
 494     : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
 495 }
 496 
 497 
 498 ObjectValue*
 499 Compile::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 500   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 501     assert(objs-&gt;at(i)-&gt;is_object(), "corrupt object cache");
 502     ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
 503     if (sv-&gt;id() == id) {
 504       return sv;
 505     }
 506   }
 507   // Otherwise..
 508   return NULL;
 509 }
 510 
 511 void Compile::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,
 512                                      ObjectValue* sv ) {
 513   assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, "Precondition");
 514   objs-&gt;append(sv);
 515 }
 516 
 517 
 518 void Compile::FillLocArray( int idx, MachSafePointNode* sfpt, Node *local,
 519                             GrowableArray&lt;ScopeValue*&gt; *array,
 520                             GrowableArray&lt;ScopeValue*&gt; *objs ) {
 521   assert( local, "use _top instead of null" );
 522   if (array-&gt;length() != idx) {
 523     assert(array-&gt;length() == idx + 1, "Unexpected array count");
 524     // Old functionality:
 525     //   return
 526     // New functionality:
 527     //   Assert if the local is not top. In product mode let the new node
 528     //   override the old entry.
 529     assert(local == top(), "LocArray collision");
 530     if (local == top()) {
 531       return;
 532     }
 533     array-&gt;pop();
 534   }
 535   const Type *t = local-&gt;bottom_type();
 536 
 537   // Is it a safepoint scalar object node?
 538   if (local-&gt;is_SafePointScalarObject()) {
 539     SafePointScalarObjectNode* spobj = local-&gt;as_SafePointScalarObject();
 540 
 541     ObjectValue* sv = Compile::sv_for_node_id(objs, spobj-&gt;_idx);
 542     if (sv == NULL) {
 543       ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
 544       assert(cik-&gt;is_instance_klass() ||
 545              cik-&gt;is_array_klass(), "Not supported allocation.");
 546       sv = new ObjectValue(spobj-&gt;_idx,
 547                            new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()),
 548                            new ConstantOopWriteValue(NULL));
 549       Compile::set_sv_for_object_node(objs, sv);
 550 
 551       uint first_ind = spobj-&gt;first_index(sfpt-&gt;jvms());
 552       for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
 553         Node* fld_node = sfpt-&gt;in(first_ind+i);
 554         (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfpt, fld_node, sv-&gt;field_values(), objs);
 555       }
 556     }
 557     array-&gt;append(sv);
 558     return;
 559   }
 560 
 561   // Grab the register number for the local
 562   OptoReg::Name regnum = _regalloc-&gt;get_reg_first(local);
 563   if( OptoReg::is_valid(regnum) ) {// Got a register/stack?
 564     // Record the double as two float registers.
 565     // The register mask for such a value always specifies two adjacent
 566     // float registers, with the lower register number even.
 567     // Normally, the allocation of high and low words to these registers
 568     // is irrelevant, because nearly all operations on register pairs
 569     // (e.g., StoreD) treat them as a single unit.
 570     // Here, we assume in addition that the words in these two registers
 571     // stored "naturally" (by operations like StoreD and double stores
 572     // within the interpreter) such that the lower-numbered register
 573     // is written to the lower memory address.  This may seem like
 574     // a machine dependency, but it is not--it is a requirement on
 575     // the author of the &lt;arch&gt;.ad file to ensure that, for every
 576     // even/odd double-register pair to which a double may be allocated,
 577     // the word in the even single-register is stored to the first
 578     // memory word.  (Note that register numbers are completely
 579     // arbitrary, and are not tied to any machine-level encodings.)
 580 #ifdef _LP64
 581     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon ) {
 582       array-&gt;append(new ConstantIntValue((jint)0));
 583       array-&gt;append(new_loc_value( _regalloc, regnum, Location::dbl ));
 584     } else if ( t-&gt;base() == Type::Long ) {
 585       array-&gt;append(new ConstantIntValue((jint)0));
 586       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));
 587     } else if ( t-&gt;base() == Type::RawPtr ) {
 588       // jsr/ret return address which must be restored into a the full
 589       // width 64-bit stack slot.
 590       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));
 591     }
 592 #else //_LP64
 593 #ifdef SPARC
 594     if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
 595       // For SPARC we have to swap high and low words for
 596       // long values stored in a single-register (g0-g7).
 597       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 598       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 599     } else
 600 #endif //SPARC
 601     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
 602       // Repack the double/long as two jints.
 603       // The convention the interpreter uses is that the second local
 604       // holds the first raw word of the native double representation.
 605       // This is actually reasonable, since locals and stack arrays
 606       // grow downwards in all implementations.
 607       // (If, on some machine, the interpreter's Java locals or stack
 608       // were to grow upwards, the embedded doubles would be word-swapped.)
 609       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 610       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 611     }
 612 #endif //_LP64
 613     else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
 614                OptoReg::is_reg(regnum) ) {
 615       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::float_in_double()
 616                                    ? Location::float_in_dbl : Location::normal ));
 617     } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
 618       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::int_in_long
 619                                    ? Location::int_in_long : Location::normal ));
 620     } else if( t-&gt;base() == Type::NarrowOop ) {
 621       array-&gt;append(new_loc_value( _regalloc, regnum, Location::narrowoop ));
 622     } else {
 623       array-&gt;append(new_loc_value( _regalloc, regnum, _regalloc-&gt;is_oop(local) ? Location::oop : Location::normal ));
 624     }
 625     return;
 626   }
 627 
 628   // No register.  It must be constant data.
 629   switch (t-&gt;base()) {
 630   case Type::Half:              // Second half of a double
 631     ShouldNotReachHere();       // Caller should skip 2nd halves
 632     break;
 633   case Type::AnyPtr:
 634     array-&gt;append(new ConstantOopWriteValue(NULL));
 635     break;
 636   case Type::AryPtr:
 637   case Type::InstPtr:          // fall through
 638     array-&gt;append(new ConstantOopWriteValue(t-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));
 639     break;
 640   case Type::NarrowOop:
 641     if (t == TypeNarrowOop::NULL_PTR) {
 642       array-&gt;append(new ConstantOopWriteValue(NULL));
 643     } else {
 644       array-&gt;append(new ConstantOopWriteValue(t-&gt;make_ptr()-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));
 645     }
 646     break;
 647   case Type::Int:
 648     array-&gt;append(new ConstantIntValue(t-&gt;is_int()-&gt;get_con()));
 649     break;
 650   case Type::RawPtr:
 651     // A return address (T_ADDRESS).
 652     assert((intptr_t)t-&gt;is_ptr()-&gt;get_con() &lt; (intptr_t)0x10000, "must be a valid BCI");
 653 #ifdef _LP64
 654     // Must be restored to the full-width 64-bit stack slot.
 655     array-&gt;append(new ConstantLongValue(t-&gt;is_ptr()-&gt;get_con()));
 656 #else
 657     array-&gt;append(new ConstantIntValue(t-&gt;is_ptr()-&gt;get_con()));
 658 #endif
 659     break;
 660   case Type::FloatCon: {
 661     float f = t-&gt;is_float_constant()-&gt;getf();
 662     array-&gt;append(new ConstantIntValue(jint_cast(f)));
 663     break;
 664   }
 665   case Type::DoubleCon: {
 666     jdouble d = t-&gt;is_double_constant()-&gt;getd();
 667 #ifdef _LP64
 668     array-&gt;append(new ConstantIntValue((jint)0));
 669     array-&gt;append(new ConstantDoubleValue(d));
 670 #else
 671     // Repack the double as two jints.
 672     // The convention the interpreter uses is that the second local
 673     // holds the first raw word of the native double representation.
 674     // This is actually reasonable, since locals and stack arrays
 675     // grow downwards in all implementations.
 676     // (If, on some machine, the interpreter's Java locals or stack
 677     // were to grow upwards, the embedded doubles would be word-swapped.)
 678     jlong_accessor acc;
 679     acc.long_value = jlong_cast(d);
 680     array-&gt;append(new ConstantIntValue(acc.words[1]));
 681     array-&gt;append(new ConstantIntValue(acc.words[0]));
 682 #endif
 683     break;
 684   }
 685   case Type::Long: {
 686     jlong d = t-&gt;is_long()-&gt;get_con();
 687 #ifdef _LP64
 688     array-&gt;append(new ConstantIntValue((jint)0));
 689     array-&gt;append(new ConstantLongValue(d));
 690 #else
 691     // Repack the long as two jints.
 692     // The convention the interpreter uses is that the second local
 693     // holds the first raw word of the native double representation.
 694     // This is actually reasonable, since locals and stack arrays
 695     // grow downwards in all implementations.
 696     // (If, on some machine, the interpreter's Java locals or stack
 697     // were to grow upwards, the embedded doubles would be word-swapped.)
 698     jlong_accessor acc;
 699     acc.long_value = d;
 700     array-&gt;append(new ConstantIntValue(acc.words[1]));
 701     array-&gt;append(new ConstantIntValue(acc.words[0]));
 702 #endif
 703     break;
 704   }
 705   case Type::Top:               // Add an illegal value here
 706     array-&gt;append(new LocationValue(Location()));
 707     break;
 708   default:
 709     ShouldNotReachHere();
 710     break;
 711   }
 712 }
 713 
 714 // Determine if this node starts a bundle
 715 bool Compile::starts_bundle(const Node *n) const {
 716   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 717           _node_bundling_base[n-&gt;_idx].starts_bundle());
 718 }
 719 
 720 //--------------------------Process_OopMap_Node--------------------------------
 721 void Compile::Process_OopMap_Node(MachNode *mach, int current_offset) {
 722 
 723   // Handle special safepoint nodes for synchronization
 724   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 725   MachCallNode      *mcall;
 726 
 727   int safepoint_pc_offset = current_offset;
 728   bool is_method_handle_invoke = false;
 729   bool return_oop = false;
 730 
 731   // Add the safepoint in the DebugInfoRecorder
 732   if( !mach-&gt;is_MachCall() ) {
 733     mcall = NULL;
 734     debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 735   } else {
 736     mcall = mach-&gt;as_MachCall();
 737 
 738     // Is the call a MethodHandle call?
 739     if (mcall-&gt;is_MachCallJava()) {
 740       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 741         assert(has_method_handle_invokes(), "must have been set during call generation");
 742         is_method_handle_invoke = true;
 743       }
 744     }
 745 
 746     // Check if a call returns an object.
 747     if (mcall-&gt;returns_pointer()) {
 748       return_oop = true;
 749     }
 750     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
 751     debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
 752   }
 753 
 754   // Loop over the JVMState list to add scope information
 755   // Do not skip safepoints with a NULL method, they need monitor info
 756   JVMState* youngest_jvms = sfn-&gt;jvms();
 757   int max_depth = youngest_jvms-&gt;depth();
 758 
 759   // Allocate the object pool for scalar-replaced objects -- the map from
 760   // small-integer keys (which can be recorded in the local and ostack
 761   // arrays) to descriptions of the object state.
 762   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
 763 
 764   // Visit scopes from oldest to youngest.
 765   for (int depth = 1; depth &lt;= max_depth; depth++) {
 766     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 767     int idx;
 768     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 769     // Safepoints that do not have method() set only provide oop-map and monitor info
 770     // to support GC; these do not support deoptimization.
 771     int num_locs = (method == NULL) ? 0 : jvms-&gt;loc_size();
 772     int num_exps = (method == NULL) ? 0 : jvms-&gt;stk_size();
 773     int num_mon  = jvms-&gt;nof_monitors();
 774     assert(method == NULL || jvms-&gt;bci() &lt; 0 || num_locs == method-&gt;max_locals(),
 775            "JVMS local count must match that of the method");
 776 
 777     // Add Local and Expression Stack Information
 778 
 779     // Insert locals into the locarray
 780     GrowableArray&lt;ScopeValue*&gt; *locarray = new GrowableArray&lt;ScopeValue*&gt;(num_locs);
 781     for( idx = 0; idx &lt; num_locs; idx++ ) {
 782       FillLocArray( idx, sfn, sfn-&gt;local(jvms, idx), locarray, objs );
 783     }
 784 
 785     // Insert expression stack entries into the exparray
 786     GrowableArray&lt;ScopeValue*&gt; *exparray = new GrowableArray&lt;ScopeValue*&gt;(num_exps);
 787     for( idx = 0; idx &lt; num_exps; idx++ ) {
 788       FillLocArray( idx,  sfn, sfn-&gt;stack(jvms, idx), exparray, objs );
 789     }
 790 
 791     // Add in mappings of the monitors
 792     assert( !method ||
 793             !method-&gt;is_synchronized() ||
 794             method-&gt;is_native() ||
 795             num_mon &gt; 0 ||
 796             !GenerateSynchronizationCode,
 797             "monitors must always exist for synchronized methods");
 798 
 799     // Build the growable array of ScopeValues for exp stack
 800     GrowableArray&lt;MonitorValue*&gt; *monarray = new GrowableArray&lt;MonitorValue*&gt;(num_mon);
 801 
 802     // Loop over monitors and insert into array
 803     for (idx = 0; idx &lt; num_mon; idx++) {
 804       // Grab the node that defines this monitor
 805       Node* box_node = sfn-&gt;monitor_box(jvms, idx);
 806       Node* obj_node = sfn-&gt;monitor_obj(jvms, idx);
 807 
 808       // Create ScopeValue for object
 809       ScopeValue *scval = NULL;
 810 
 811       if (obj_node-&gt;is_SafePointScalarObject()) {
 812         SafePointScalarObjectNode* spobj = obj_node-&gt;as_SafePointScalarObject();
 813         scval = Compile::sv_for_node_id(objs, spobj-&gt;_idx);
 814         if (scval == NULL) {
 815           const Type *t = spobj-&gt;bottom_type();
 816           ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
 817           assert(cik-&gt;is_instance_klass() ||
 818                  cik-&gt;is_array_klass(), "Not supported allocation.");
 819           ObjectValue* sv = new ObjectValue(spobj-&gt;_idx,
 820                                             new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()),
 821                                             new ConstantOopWriteValue(NULL));
 822           Compile::set_sv_for_object_node(objs, sv);
 823 
 824           uint first_ind = spobj-&gt;first_index(youngest_jvms);
 825           for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
 826             Node* fld_node = sfn-&gt;in(first_ind+i);
 827             (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfn, fld_node, sv-&gt;field_values(), objs);
 828           }
 829           scval = sv;
 830         }
 831       } else if (!obj_node-&gt;is_Con()) {
 832         OptoReg::Name obj_reg = _regalloc-&gt;get_reg_first(obj_node);
 833         if( obj_node-&gt;bottom_type()-&gt;base() == Type::NarrowOop ) {
 834           scval = new_loc_value( _regalloc, obj_reg, Location::narrowoop );
 835         } else {
 836           scval = new_loc_value( _regalloc, obj_reg, Location::oop );
 837         }
 838       } else {
 839         const TypePtr *tp = obj_node-&gt;get_ptr_type();
 840         scval = new ConstantOopWriteValue(tp-&gt;is_oopptr()-&gt;const_oop()-&gt;constant_encoding());
 841       }
 842 
 843       OptoReg::Name box_reg = BoxLockNode::reg(box_node);
 844       Location basic_lock = Location::new_stk_loc(Location::normal,_regalloc-&gt;reg2offset(box_reg));
 845       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
 846       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
 847     }
 848 
 849     // We dump the object pool first, since deoptimization reads it in first.
 850     debug_info()-&gt;dump_object_pool(objs);
 851 
 852     // Build first class objects to pass to scope
 853     DebugToken *locvals = debug_info()-&gt;create_scope_values(locarray);
 854     DebugToken *expvals = debug_info()-&gt;create_scope_values(exparray);
 855     DebugToken *monvals = debug_info()-&gt;create_monitor_values(monarray);
 856 
 857     // Make method available for all Safepoints
 858     ciMethod* scope_method = method ? method : _method;
 859     // Describe the scope here
 860     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, "must be a valid or entry BCI");
 861     assert(!jvms-&gt;should_reexecute() || depth == max_depth, "reexecute allowed only for the youngest");
 862     // Now we can describe the scope.
 863     methodHandle null_mh;
 864     bool rethrow_exception = false;
 865     debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);
 866   } // End jvms loop
 867 
 868   // Mark the end of the scope set.
 869   debug_info()-&gt;end_safepoint(safepoint_pc_offset);
 870 }
 871 
 872 
 873 
 874 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
 875 class NonSafepointEmitter {
 876   Compile*  C;
 877   JVMState* _pending_jvms;
 878   int       _pending_offset;
 879 
 880   void emit_non_safepoint();
 881 
 882  public:
 883   NonSafepointEmitter(Compile* compile) {
 884     this-&gt;C = compile;
 885     _pending_jvms = NULL;
 886     _pending_offset = 0;
 887   }
 888 
 889   void observe_instruction(Node* n, int pc_offset) {
 890     if (!C-&gt;debug_info()-&gt;recording_non_safepoints())  return;
 891 
 892     Node_Notes* nn = C-&gt;node_notes_at(n-&gt;_idx);
 893     if (nn == NULL || nn-&gt;jvms() == NULL)  return;
 894     if (_pending_jvms != NULL &amp;&amp;
 895         _pending_jvms-&gt;same_calls_as(nn-&gt;jvms())) {
 896       // Repeated JVMS?  Stretch it up here.
 897       _pending_offset = pc_offset;
 898     } else {
 899       if (_pending_jvms != NULL &amp;&amp;
 900           _pending_offset &lt; pc_offset) {
 901         emit_non_safepoint();
 902       }
 903       _pending_jvms = NULL;
 904       if (pc_offset &gt; C-&gt;debug_info()-&gt;last_pc_offset()) {
 905         // This is the only way _pending_jvms can become non-NULL:
 906         _pending_jvms = nn-&gt;jvms();
 907         _pending_offset = pc_offset;
 908       }
 909     }
 910   }
 911 
 912   // Stay out of the way of real safepoints:
 913   void observe_safepoint(JVMState* jvms, int pc_offset) {
 914     if (_pending_jvms != NULL &amp;&amp;
 915         !_pending_jvms-&gt;same_calls_as(jvms) &amp;&amp;
 916         _pending_offset &lt; pc_offset) {
 917       emit_non_safepoint();
 918     }
 919     _pending_jvms = NULL;
 920   }
 921 
 922   void flush_at_end() {
 923     if (_pending_jvms != NULL) {
 924       emit_non_safepoint();
 925     }
 926     _pending_jvms = NULL;
 927   }
 928 };
 929 
 930 void NonSafepointEmitter::emit_non_safepoint() {
 931   JVMState* youngest_jvms = _pending_jvms;
 932   int       pc_offset     = _pending_offset;
 933 
 934   // Clear it now:
 935   _pending_jvms = NULL;
 936 
 937   DebugInformationRecorder* debug_info = C-&gt;debug_info();
 938   assert(debug_info-&gt;recording_non_safepoints(), "sanity");
 939 
 940   debug_info-&gt;add_non_safepoint(pc_offset);
 941   int max_depth = youngest_jvms-&gt;depth();
 942 
 943   // Visit scopes from oldest to youngest.
 944   for (int depth = 1; depth &lt;= max_depth; depth++) {
 945     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 946     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 947     assert(!jvms-&gt;should_reexecute() || depth==max_depth, "reexecute allowed only for the youngest");
 948     methodHandle null_mh;
 949     debug_info-&gt;describe_scope(pc_offset, null_mh, method, jvms-&gt;bci(), jvms-&gt;should_reexecute());
 950   }
 951 
 952   // Mark the end of the scope set.
 953   debug_info-&gt;end_non_safepoint(pc_offset);
 954 }
 955 
 956 //------------------------------init_buffer------------------------------------
 957 CodeBuffer* Compile::init_buffer(uint* blk_starts) {
 958 
 959   // Set the initially allocated size
 960   int  code_req   = initial_code_capacity;
 961   int  locs_req   = initial_locs_capacity;
 962   int  stub_req   = initial_stub_capacity;
 963   int  const_req  = initial_const_capacity;
 964 
 965   int  pad_req    = NativeCall::instruction_size;
 966   // The extra spacing after the code is necessary on some platforms.
 967   // Sometimes we need to patch in a jump after the last instruction,
 968   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
 969 
 970   // Compute the byte offset where we can store the deopt pc.
 971   if (fixed_slots() != 0) {
 972     _orig_pc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
 973   }
 974 
 975   // Compute prolog code size
 976   _method_size = 0;
 977   _frame_slots = OptoReg::reg2stack(_matcher-&gt;_old_SP)+_regalloc-&gt;_framesize;
 978 #if defined(IA64) &amp;&amp; !defined(AIX)
 979   if (save_argument_registers()) {
 980     // 4815101: this is a stub with implicit and unknown precision fp args.
 981     // The usual spill mechanism can only generate stfd's in this case, which
 982     // doesn't work if the fp reg to spill contains a single-precision denorm.
 983     // Instead, we hack around the normal spill mechanism using stfspill's and
 984     // ldffill's in the MachProlog and MachEpilog emit methods.  We allocate
 985     // space here for the fp arg regs (f8-f15) we're going to thusly spill.
 986     //
 987     // If we ever implement 16-byte 'registers' == stack slots, we can
 988     // get rid of this hack and have SpillCopy generate stfspill/ldffill
 989     // instead of stfd/stfs/ldfd/ldfs.
 990     _frame_slots += 8*(16/BytesPerInt);
 991   }
 992 #endif
 993   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, "sanity check");
 994 
 995   if (has_mach_constant_base_node()) {
 996     uint add_size = 0;
 997     // Fill the constant table.
 998     // Note:  This must happen before shorten_branches.
 999     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
1000       Block* b = _cfg-&gt;get_block(i);
1001 
1002       for (uint j = 0; j &lt; b-&gt;number_of_nodes(); j++) {
1003         Node* n = b-&gt;get_node(j);
1004 
1005         // If the node is a MachConstantNode evaluate the constant
1006         // value section.
1007         if (n-&gt;is_MachConstant()) {
1008           MachConstantNode* machcon = n-&gt;as_MachConstant();
1009           machcon-&gt;eval_constant(C);
1010         } else if (n-&gt;is_Mach()) {
1011           // On Power there are more nodes that issue constants.
1012           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1013         }
1014       }
1015     }
1016 
1017     // Calculate the offsets of the constants and the size of the
1018     // constant table (including the padding to the next section).
1019     constant_table().calculate_offsets_and_size();
1020     const_req = constant_table().size() + add_size;
1021   }
1022 
1023   // Initialize the space for the BufferBlob used to find and verify
1024   // instruction size in MachNode::emit_size()
1025   init_scratch_buffer_blob(const_req);
1026   if (failing())  return NULL; // Out of memory
1027 
1028   // Pre-compute the length of blocks and replace
1029   // long branches with short if machine supports it.
1030   shorten_branches(blk_starts, code_req, locs_req, stub_req);
1031 
1032   // nmethod and CodeBuffer count stubs &amp; constants as part of method's code.
1033   // class HandlerImpl is platform-specific and defined in the *.ad files.
1034   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1035   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1036   stub_req += MAX_stubs_size;   // ensure per-stub margin
1037   code_req += MAX_inst_size;    // ensure per-instruction margin
1038 
1039   if (StressCodeBuffers)
1040     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1041 
1042   int total_req =
1043     const_req +
1044     code_req +
1045     pad_req +
1046     stub_req +
1047     exception_handler_req +
1048     deopt_handler_req;               // deopt handler
1049 
1050   if (has_method_handle_invokes())
1051     total_req += deopt_handler_req;  // deopt MH handler
1052 
1053   CodeBuffer* cb = code_buffer();
1054   cb-&gt;initialize(total_req, locs_req);
1055 
1056   // Have we run out of code space?
1057   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1058     C-&gt;record_failure("CodeCache is full");
1059     return NULL;
1060   }
1061   // Configure the code buffer.
1062   cb-&gt;initialize_consts_size(const_req);
1063   cb-&gt;initialize_stubs_size(stub_req);
1064   cb-&gt;initialize_oop_recorder(env()-&gt;oop_recorder());
1065 
1066   // fill in the nop array for bundling computations
1067   MachNode *_nop_list[Bundle::_nop_count];
1068   Bundle::initialize_nops(_nop_list);
1069 
1070   return cb;
1071 }
1072 
1073 //------------------------------fill_buffer------------------------------------
1074 void Compile::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
1075   // blk_starts[] contains offsets calculated during short branches processing,
1076   // offsets should not be increased during following steps.
1077 
1078   // Compute the size of first NumberOfLoopInstrToAlign instructions at head
1079   // of a loop. It is used to determine the padding for loop alignment.
1080   compute_loop_first_inst_sizes();
1081 
1082   // Create oopmap set.
1083   _oop_map_set = new OopMapSet();
1084 
1085   // !!!!! This preserves old handling of oopmaps for now
1086   debug_info()-&gt;set_oopmaps(_oop_map_set);
1087 
1088   uint nblocks  = _cfg-&gt;number_of_blocks();
1089   // Count and start of implicit null check instructions
1090   uint inct_cnt = 0;
1091   uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1092 
1093   // Count and start of calls
1094   uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1095 
1096   uint  return_offset = 0;
1097   int nop_size = (new MachNopNode())-&gt;size(_regalloc);
1098 
1099   int previous_offset = 0;
1100   int current_offset  = 0;
1101   int last_call_offset = -1;
1102   int last_avoid_back_to_back_offset = -1;
1103 #ifdef ASSERT
1104   uint* jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks);
1105   uint* jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
1106   uint* jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
1107   uint* jmp_rule   = NEW_RESOURCE_ARRAY(uint,nblocks);
1108 #endif
1109 
1110   // Create an array of unused labels, one for each basic block, if printing is enabled
1111 #ifndef PRODUCT
1112   int *node_offsets      = NULL;
1113   uint node_offset_limit = unique();
1114 
1115   if (print_assembly())
1116     node_offsets         = NEW_RESOURCE_ARRAY(int, node_offset_limit);
1117 #endif
1118 
1119   NonSafepointEmitter non_safepoints(this);  // emit non-safepoints lazily
1120 
1121   // Emit the constant table.
1122   if (has_mach_constant_base_node()) {
1123     constant_table().emit(*cb);
1124   }
1125 
1126   // Create an array of labels, one for each basic block
1127   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1128   for (uint i=0; i &lt;= nblocks; i++) {
1129     blk_labels[i].init();
1130   }
1131 
1132   // ------------------
1133   // Now fill in the code buffer
1134   Node *delay_slot = NULL;
1135 
1136   for (uint i = 0; i &lt; nblocks; i++) {
1137     Block* block = _cfg-&gt;get_block(i);
1138     Node* head = block-&gt;head();
1139 
1140     // If this block needs to start aligned (i.e, can be reached other
1141     // than by falling-thru from the previous block), then force the
1142     // start of a new bundle.
1143     if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(head)) {
1144       cb-&gt;flush_bundle(true);
1145     }
1146 
1147 #ifdef ASSERT
1148     if (!block-&gt;is_connector()) {
1149       stringStream st;
1150       block-&gt;dump_head(_cfg, &amp;st);
1151       MacroAssembler(cb).block_comment(st.as_string());
1152     }
1153     jmp_target[i] = 0;
1154     jmp_offset[i] = 0;
1155     jmp_size[i]   = 0;
1156     jmp_rule[i]   = 0;
1157 #endif
1158     int blk_offset = current_offset;
1159 
1160     // Define the label at the beginning of the basic block
1161     MacroAssembler(cb).bind(blk_labels[block-&gt;_pre_order]);
1162 
1163     uint last_inst = block-&gt;number_of_nodes();
1164 
1165     // Emit block normally, except for last instruction.
1166     // Emit means "dump code bits into code buffer".
1167     for (uint j = 0; j&lt;last_inst; j++) {
1168 
1169       // Get the node
1170       Node* n = block-&gt;get_node(j);
1171 
1172       // See if delay slots are supported
1173       if (valid_bundle_info(n) &amp;&amp;
1174           node_bundling(n)-&gt;used_in_unconditional_delay()) {
1175         assert(delay_slot == NULL, "no use of delay slot node");
1176         assert(n-&gt;size(_regalloc) == Pipeline::instr_unit_size(), "delay slot instruction wrong size");
1177 
1178         delay_slot = n;
1179         continue;
1180       }
1181 
1182       // If this starts a new instruction group, then flush the current one
1183       // (but allow split bundles)
1184       if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(n))
1185         cb-&gt;flush_bundle(false);
1186 
1187       // Special handling for SafePoint/Call Nodes
1188       bool is_mcall = false;
1189       if (n-&gt;is_Mach()) {
1190         MachNode *mach = n-&gt;as_Mach();
1191         is_mcall = n-&gt;is_MachCall();
1192         bool is_sfn = n-&gt;is_MachSafePoint();
1193 
1194         // If this requires all previous instructions be flushed, then do so
1195         if (is_sfn || is_mcall || mach-&gt;alignment_required() != 1) {
1196           cb-&gt;flush_bundle(true);
1197           current_offset = cb-&gt;insts_size();
1198         }
1199 
1200         // A padding may be needed again since a previous instruction
1201         // could be moved to delay slot.
1202 
1203         // align the instruction if necessary
1204         int padding = mach-&gt;compute_padding(current_offset);
1205         // Make sure safepoint node for polling is distinct from a call's
1206         // return by adding a nop if needed.
1207         if (is_sfn &amp;&amp; !is_mcall &amp;&amp; padding == 0 &amp;&amp; current_offset == last_call_offset) {
1208           padding = nop_size;
1209         }
1210         if (padding == 0 &amp;&amp; mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE) &amp;&amp;
1211             current_offset == last_avoid_back_to_back_offset) {
1212           // Avoid back to back some instructions.
1213           padding = nop_size;
1214         }
1215 
1216         if (padding &gt; 0) {
1217           assert((padding % nop_size) == 0, "padding is not a multiple of NOP size");
1218           int nops_cnt = padding / nop_size;
1219           MachNode *nop = new MachNopNode(nops_cnt);
1220           block-&gt;insert_node(nop, j++);
1221           last_inst++;
1222           _cfg-&gt;map_node_to_block(nop, block);
1223           // Ensure enough space.
1224           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1225           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1226             C-&gt;record_failure("CodeCache is full");
1227             return;
1228           }
1229           nop-&gt;emit(*cb, _regalloc);
1230           cb-&gt;flush_bundle(true);
1231           current_offset = cb-&gt;insts_size();
1232         }
1233 
1234         // Remember the start of the last call in a basic block
1235         if (is_mcall) {
1236           MachCallNode *mcall = mach-&gt;as_MachCall();
1237 
1238           // This destination address is NOT PC-relative
1239           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
1240 
1241           // Save the return address
1242           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1243 
1244           if (mcall-&gt;is_MachCallLeaf()) {
1245             is_mcall = false;
1246             is_sfn = false;
1247           }
1248         }
1249 
1250         // sfn will be valid whenever mcall is valid now because of inheritance
1251         if (is_sfn || is_mcall) {
1252 
1253           // Handle special safepoint nodes for synchronization
1254           if (!is_mcall) {
1255             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1256             // !!!!! Stubs only need an oopmap right now, so bail out
1257             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1258               // Write the oopmap directly to the code blob??!!
1259               continue;
1260             }
1261           } // End synchronization
1262 
1263           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1264                                            current_offset);
1265           Process_OopMap_Node(mach, current_offset);
1266         } // End if safepoint
1267 
1268         // If this is a null check, then add the start of the previous instruction to the list
1269         else if( mach-&gt;is_MachNullCheck() ) {
1270           inct_starts[inct_cnt++] = previous_offset;
1271         }
1272 
1273         // If this is a branch, then fill in the label with the target BB's label
1274         else if (mach-&gt;is_MachBranch()) {
1275           // This requires the TRUE branch target be in succs[0]
1276           uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1277 
1278           // Try to replace long branch if delay slot is not used,
1279           // it is mostly for back branches since forward branch's
1280           // distance is not updated yet.
1281           bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
1282                                     node_bundling(n)-&gt;use_unconditional_delay();
1283           if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
1284            assert(delay_slot == NULL, "not expecting delay slot node");
1285            int br_size = n-&gt;size(_regalloc);
1286             int offset = blk_starts[block_num] - current_offset;
1287             if (block_num &gt;= i) {
1288               // Current and following block's offset are not
1289               // finalized yet, adjust distance by the difference
1290               // between calculated and final offsets of current block.
1291               offset -= (blk_starts[i] - blk_offset);
1292             }
1293             // In the following code a nop could be inserted before
1294             // the branch which will increase the backward distance.
1295             bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
1296             if (needs_padding &amp;&amp; offset &lt;= 0)
1297               offset -= nop_size;
1298 
1299             if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
1300               // We've got a winner.  Replace this branch.
1301               MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
1302 
1303               // Update the jmp_size.
1304               int new_size = replacement-&gt;size(_regalloc);
1305               assert((br_size - new_size) &gt;= (int)nop_size, "short_branch size should be smaller");
1306               // Insert padding between avoid_back_to_back branches.
1307               if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
1308                 MachNode *nop = new MachNopNode();
1309                 block-&gt;insert_node(nop, j++);
1310                 _cfg-&gt;map_node_to_block(nop, block);
1311                 last_inst++;
1312                 nop-&gt;emit(*cb, _regalloc);
1313                 cb-&gt;flush_bundle(true);
1314                 current_offset = cb-&gt;insts_size();
1315               }
1316 #ifdef ASSERT
1317               jmp_target[i] = block_num;
1318               jmp_offset[i] = current_offset - blk_offset;
1319               jmp_size[i]   = new_size;
1320               jmp_rule[i]   = mach-&gt;rule();
1321 #endif
1322               block-&gt;map_node(replacement, j);
1323               mach-&gt;subsume_by(replacement, C);
1324               n    = replacement;
1325               mach = replacement;
1326             }
1327           }
1328           mach-&gt;as_MachBranch()-&gt;label_set( &amp;blk_labels[block_num], block_num );
1329         } else if (mach-&gt;ideal_Opcode() == Op_Jump) {
1330           for (uint h = 0; h &lt; block-&gt;_num_succs; h++) {
1331             Block* succs_block = block-&gt;_succs[h];
1332             for (uint j = 1; j &lt; succs_block-&gt;num_preds(); j++) {
1333               Node* jpn = succs_block-&gt;pred(j);
1334               if (jpn-&gt;is_JumpProj() &amp;&amp; jpn-&gt;in(0) == mach) {
1335                 uint block_num = succs_block-&gt;non_connector()-&gt;_pre_order;
1336                 Label *blkLabel = &amp;blk_labels[block_num];
1337                 mach-&gt;add_case_label(jpn-&gt;as_JumpProj()-&gt;proj_no(), blkLabel);
1338               }
1339             }
1340           }
1341         }
1342 #ifdef ASSERT
1343         // Check that oop-store precedes the card-mark
1344         else if (mach-&gt;ideal_Opcode() == Op_StoreCM) {
1345           uint storeCM_idx = j;
1346           int count = 0;
1347           for (uint prec = mach-&gt;req(); prec &lt; mach-&gt;len(); prec++) {
1348             Node *oop_store = mach-&gt;in(prec);  // Precedence edge
1349             if (oop_store == NULL) continue;
1350             count++;
1351             uint i4;
1352             for (i4 = 0; i4 &lt; last_inst; ++i4) {
1353               if (block-&gt;get_node(i4) == oop_store) {
1354                 break;
1355               }
1356             }
1357             // Note: This test can provide a false failure if other precedence
1358             // edges have been added to the storeCMNode.
1359             assert(i4 == last_inst || i4 &lt; storeCM_idx, "CM card-mark executes before oop-store");
1360           }
1361           assert(count &gt; 0, "storeCM expects at least one precedence edge");
1362         }
1363 #endif
1364         else if (!n-&gt;is_Proj()) {
1365           // Remember the beginning of the previous instruction, in case
1366           // it's followed by a flag-kill and a null-check.  Happens on
1367           // Intel all the time, with add-to-memory kind of opcodes.
1368           previous_offset = current_offset;
1369         }
1370 
1371         // Not an else-if!
1372         // If this is a trap based cmp then add its offset to the list.
1373         if (mach-&gt;is_TrapBasedCheckNode()) {
1374           inct_starts[inct_cnt++] = current_offset;
1375         }
1376       }
1377 
1378       // Verify that there is sufficient space remaining
1379       cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1380       if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1381         C-&gt;record_failure("CodeCache is full");
1382         return;
1383       }
1384 
1385       // Save the offset for the listing
1386 #ifndef PRODUCT
1387       if (node_offsets &amp;&amp; n-&gt;_idx &lt; node_offset_limit)
1388         node_offsets[n-&gt;_idx] = cb-&gt;insts_size();
1389 #endif
1390 
1391       // "Normal" instruction case
1392       DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
1393       n-&gt;emit(*cb, _regalloc);
1394       current_offset  = cb-&gt;insts_size();
1395 
1396       // Above we only verified that there is enough space in the instruction section.
1397       // However, the instruction may emit stubs that cause code buffer expansion.
1398       // Bail out here if expansion failed due to a lack of code cache space.
1399       if (failing()) {
1400         return;
1401       }
1402 
1403 #ifdef ASSERT
1404       if (n-&gt;size(_regalloc) &lt; (current_offset-instr_offset)) {
1405         n-&gt;dump();
1406         assert(false, "wrong size of mach node");
1407       }
1408 #endif
1409       non_safepoints.observe_instruction(n, current_offset);
1410 
1411       // mcall is last "call" that can be a safepoint
1412       // record it so we can see if a poll will directly follow it
1413       // in which case we'll need a pad to make the PcDesc sites unique
1414       // see  5010568. This can be slightly inaccurate but conservative
1415       // in the case that return address is not actually at current_offset.
1416       // This is a small price to pay.
1417 
1418       if (is_mcall) {
1419         last_call_offset = current_offset;
1420       }
1421 
1422       if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
1423         // Avoid back to back some instructions.
1424         last_avoid_back_to_back_offset = current_offset;
1425       }
1426 
1427       // See if this instruction has a delay slot
1428       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
1429         guarantee(delay_slot != NULL, "expecting delay slot node");
1430 
1431         // Back up 1 instruction
1432         cb-&gt;set_insts_end(cb-&gt;insts_end() - Pipeline::instr_unit_size());
1433 
1434         // Save the offset for the listing
1435 #ifndef PRODUCT
1436         if (node_offsets &amp;&amp; delay_slot-&gt;_idx &lt; node_offset_limit)
1437           node_offsets[delay_slot-&gt;_idx] = cb-&gt;insts_size();
1438 #endif
1439 
1440         // Support a SafePoint in the delay slot
1441         if (delay_slot-&gt;is_MachSafePoint()) {
1442           MachNode *mach = delay_slot-&gt;as_Mach();
1443           // !!!!! Stubs only need an oopmap right now, so bail out
1444           if (!mach-&gt;is_MachCall() &amp;&amp; mach-&gt;as_MachSafePoint()-&gt;jvms()-&gt;method() == NULL) {
1445             // Write the oopmap directly to the code blob??!!
1446             delay_slot = NULL;
1447             continue;
1448           }
1449 
1450           int adjusted_offset = current_offset - Pipeline::instr_unit_size();
1451           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1452                                            adjusted_offset);
1453           // Generate an OopMap entry
1454           Process_OopMap_Node(mach, adjusted_offset);
1455         }
1456 
1457         // Insert the delay slot instruction
1458         delay_slot-&gt;emit(*cb, _regalloc);
1459 
1460         // Don't reuse it
1461         delay_slot = NULL;
1462       }
1463 
1464     } // End for all instructions in block
1465 
1466     // If the next block is the top of a loop, pad this block out to align
1467     // the loop top a little. Helps prevent pipe stalls at loop back branches.
1468     if (i &lt; nblocks-1) {
1469       Block *nb = _cfg-&gt;get_block(i + 1);
1470       int padding = nb-&gt;alignment_padding(current_offset);
1471       if( padding &gt; 0 ) {
1472         MachNode *nop = new MachNopNode(padding / nop_size);
1473         block-&gt;insert_node(nop, block-&gt;number_of_nodes());
1474         _cfg-&gt;map_node_to_block(nop, block);
1475         nop-&gt;emit(*cb, _regalloc);
1476         current_offset = cb-&gt;insts_size();
1477       }
1478     }
1479     // Verify that the distance for generated before forward
1480     // short branches is still valid.
1481     guarantee((int)(blk_starts[i+1] - blk_starts[i]) &gt;= (current_offset - blk_offset), "shouldn't increase block size");
1482 
1483     // Save new block start offset
1484     blk_starts[i] = blk_offset;
1485   } // End of for all blocks
1486   blk_starts[nblocks] = current_offset;
1487 
1488   non_safepoints.flush_at_end();
1489 
1490   // Offset too large?
1491   if (failing())  return;
1492 
1493   // Define a pseudo-label at the end of the code
1494   MacroAssembler(cb).bind( blk_labels[nblocks] );
1495 
1496   // Compute the size of the first block
1497   _first_block_size = blk_labels[1].loc_pos() - blk_labels[0].loc_pos();
1498 
1499 #ifdef ASSERT
1500   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
1501     if (jmp_target[i] != 0) {
1502       int br_size = jmp_size[i];
1503       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
1504       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
1505         tty-&gt;print_cr("target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d", blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
1506         assert(false, "Displacement too large for short jmp");
1507       }
1508     }
1509   }
1510 #endif
1511 
1512 #ifndef PRODUCT
1513   // Information on the size of the method, without the extraneous code
1514   Scheduling::increment_method_size(cb-&gt;insts_size());
1515 #endif
1516 
1517   // ------------------
1518   // Fill in exception table entries.
1519   FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
1520 
1521   // Only java methods have exception handlers and deopt handlers
1522   // class HandlerImpl is platform-specific and defined in the *.ad files.
1523   if (_method) {
1524     // Emit the exception handler code.
1525     _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
1526     if (failing()) {
1527       return; // CodeBuffer::expand failed
1528     }
1529     // Emit the deopt handler code.
1530     _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
1531 
1532     // Emit the MethodHandle deopt handler code (if required).
1533     if (has_method_handle_invokes() &amp;&amp; !failing()) {
1534       // We can use the same code as for the normal deopt handler, we
1535       // just need a different entry point address.
1536       _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
1537     }
1538   }
1539 
1540   // One last check for failed CodeBuffer::expand:
1541   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1542     C-&gt;record_failure("CodeCache is full");
1543     return;
1544   }
1545 
1546 #ifndef PRODUCT
1547   // Dump the assembly code, including basic-block numbers
1548   if (print_assembly()) {
1549     ttyLocker ttyl;  // keep the following output all in one block
1550     if (!VMThread::should_terminate()) {  // test this under the tty lock
1551       // This output goes directly to the tty, not the compiler log.
1552       // To enable tools to match it up with the compilation activity,
1553       // be sure to tag this tty output with the compile ID.
1554       if (xtty != NULL) {
1555         xtty-&gt;head("opto_assembly compile_id='%d'%s", compile_id(),
1556                    is_osr_compilation()    ? " compile_kind='osr'" :
1557                    "");
1558       }
1559       if (method() != NULL) {
1560         method()-&gt;print_metadata();
1561       }
1562       dump_asm(node_offsets, node_offset_limit);
1563       if (xtty != NULL) {
1564         // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
1565         // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
1566         // thread safe
1567         ttyLocker ttyl2;
1568         xtty-&gt;tail("opto_assembly");
1569       }
1570     }
1571   }
1572 #endif
1573 
1574 }
1575 
1576 void Compile::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {
1577   _inc_table.set_size(cnt);
1578 
1579   uint inct_cnt = 0;
1580   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
1581     Block* block = _cfg-&gt;get_block(i);
1582     Node *n = NULL;
1583     int j;
1584 
1585     // Find the branch; ignore trailing NOPs.
1586     for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
1587       n = block-&gt;get_node(j);
1588       if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con) {
1589         break;
1590       }
1591     }
1592 
1593     // If we didn't find anything, continue
1594     if (j &lt; 0) {
1595       continue;
1596     }
1597 
1598     // Compute ExceptionHandlerTable subtable entry and add it
1599     // (skip empty blocks)
1600     if (n-&gt;is_Catch()) {
1601 
1602       // Get the offset of the return from the call
1603       uint call_return = call_returns[block-&gt;_pre_order];
1604 #ifdef ASSERT
1605       assert( call_return &gt; 0, "no call seen for this basic block" );
1606       while (block-&gt;get_node(--j)-&gt;is_MachProj()) ;
1607       assert(block-&gt;get_node(j)-&gt;is_MachCall(), "CatchProj must follow call");
1608 #endif
1609       // last instruction is a CatchNode, find it's CatchProjNodes
1610       int nof_succs = block-&gt;_num_succs;
1611       // allocate space
1612       GrowableArray&lt;intptr_t&gt; handler_bcis(nof_succs);
1613       GrowableArray&lt;intptr_t&gt; handler_pcos(nof_succs);
1614       // iterate through all successors
1615       for (int j = 0; j &lt; nof_succs; j++) {
1616         Block* s = block-&gt;_succs[j];
1617         bool found_p = false;
1618         for (uint k = 1; k &lt; s-&gt;num_preds(); k++) {
1619           Node* pk = s-&gt;pred(k);
1620           if (pk-&gt;is_CatchProj() &amp;&amp; pk-&gt;in(0) == n) {
1621             const CatchProjNode* p = pk-&gt;as_CatchProj();
1622             found_p = true;
1623             // add the corresponding handler bci &amp; pco information
1624             if (p-&gt;_con != CatchProjNode::fall_through_index) {
1625               // p leads to an exception handler (and is not fall through)
1626               assert(s == _cfg-&gt;get_block(s-&gt;_pre_order), "bad numbering");
1627               // no duplicates, please
1628               if (!handler_bcis.contains(p-&gt;handler_bci())) {
1629                 uint block_num = s-&gt;non_connector()-&gt;_pre_order;
1630                 handler_bcis.append(p-&gt;handler_bci());
1631                 handler_pcos.append(blk_labels[block_num].loc_pos());
1632               }
1633             }
1634           }
1635         }
1636         assert(found_p, "no matching predecessor found");
1637         // Note:  Due to empty block removal, one block may have
1638         // several CatchProj inputs, from the same Catch.
1639       }
1640 
1641       // Set the offset of the return from the call
1642       assert(handler_bcis.find(-1) != -1, "must have default handler");
1643       _handler_table.add_subtable(call_return, &amp;handler_bcis, NULL, &amp;handler_pcos);
1644       continue;
1645     }
1646 
1647     // Handle implicit null exception table updates
1648     if (n-&gt;is_MachNullCheck()) {
1649       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1650       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1651       continue;
1652     }
1653     // Handle implicit exception table updates: trap instructions.
1654     if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;is_TrapBasedCheckNode()) {
1655       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1656       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1657       continue;
1658     }
1659   } // End of for all blocks fill in exception table entries
1660 }
1661 
1662 // Static Variables
1663 #ifndef PRODUCT
1664 uint Scheduling::_total_nop_size = 0;
1665 uint Scheduling::_total_method_size = 0;
1666 uint Scheduling::_total_branches = 0;
1667 uint Scheduling::_total_unconditional_delays = 0;
1668 uint Scheduling::_total_instructions_per_bundle[Pipeline::_max_instrs_per_cycle+1];
1669 #endif
1670 
1671 // Initializer for class Scheduling
1672 
1673 Scheduling::Scheduling(Arena *arena, Compile &amp;compile)
1674   : _arena(arena),
1675     _cfg(compile.cfg()),
1676     _regalloc(compile.regalloc()),
1677     _scheduled(arena),
1678     _available(arena),
1679     _reg_node(arena),
1680     _pinch_free_list(arena),
1681     _next_node(NULL),
1682     _bundle_instr_count(0),
1683     _bundle_cycle_number(0),
1684     _bundle_use(0, 0, resource_count, &amp;_bundle_use_elements[0])
1685 #ifndef PRODUCT
1686   , _branches(0)
1687   , _unconditional_delays(0)
1688 #endif
1689 {
1690   // Create a MachNopNode
1691   _nop = new MachNopNode();
1692 
1693   // Now that the nops are in the array, save the count
1694   // (but allow entries for the nops)
1695   _node_bundling_limit = compile.unique();
1696   uint node_max = _regalloc-&gt;node_regs_max_index();
1697 
1698   compile.set_node_bundling_limit(_node_bundling_limit);
1699 
1700   // This one is persistent within the Compile class
1701   _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
1702 
1703   // Allocate space for fixed-size arrays
1704   _node_latency    = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1705   _uses            = NEW_ARENA_ARRAY(arena, short,          node_max);
1706   _current_latency = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1707 
1708   // Clear the arrays
1709   memset(_node_bundling_base, 0, node_max * sizeof(Bundle));
1710   memset(_node_latency,       0, node_max * sizeof(unsigned short));
1711   memset(_uses,               0, node_max * sizeof(short));
1712   memset(_current_latency,    0, node_max * sizeof(unsigned short));
1713 
1714   // Clear the bundling information
1715   memcpy(_bundle_use_elements, Pipeline_Use::elaborated_elements, sizeof(Pipeline_Use::elaborated_elements));
1716 
1717   // Get the last node
1718   Block* block = _cfg-&gt;get_block(_cfg-&gt;number_of_blocks() - 1);
1719 
1720   _next_node = block-&gt;get_node(block-&gt;number_of_nodes() - 1);
1721 }
1722 
1723 #ifndef PRODUCT
1724 // Scheduling destructor
1725 Scheduling::~Scheduling() {
1726   _total_branches             += _branches;
1727   _total_unconditional_delays += _unconditional_delays;
1728 }
1729 #endif
1730 
1731 // Step ahead "i" cycles
1732 void Scheduling::step(uint i) {
1733 
1734   Bundle *bundle = node_bundling(_next_node);
1735   bundle-&gt;set_starts_bundle();
1736 
1737   // Update the bundle record, but leave the flags information alone
1738   if (_bundle_instr_count &gt; 0) {
1739     bundle-&gt;set_instr_count(_bundle_instr_count);
1740     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1741   }
1742 
1743   // Update the state information
1744   _bundle_instr_count = 0;
1745   _bundle_cycle_number += i;
1746   _bundle_use.step(i);
1747 }
1748 
1749 void Scheduling::step_and_clear() {
1750   Bundle *bundle = node_bundling(_next_node);
1751   bundle-&gt;set_starts_bundle();
1752 
1753   // Update the bundle record
1754   if (_bundle_instr_count &gt; 0) {
1755     bundle-&gt;set_instr_count(_bundle_instr_count);
1756     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1757 
1758     _bundle_cycle_number += 1;
1759   }
1760 
1761   // Clear the bundling information
1762   _bundle_instr_count = 0;
1763   _bundle_use.reset();
1764 
1765   memcpy(_bundle_use_elements,
1766     Pipeline_Use::elaborated_elements,
1767     sizeof(Pipeline_Use::elaborated_elements));
1768 }
1769 
1770 // Perform instruction scheduling and bundling over the sequence of
1771 // instructions in backwards order.
1772 void Compile::ScheduleAndBundle() {
1773 
1774   // Don't optimize this if it isn't a method
1775   if (!_method)
1776     return;
1777 
1778   // Don't optimize this if scheduling is disabled
1779   if (!do_scheduling())
1780     return;
1781 
1782   // Scheduling code works only with pairs (16 bytes) maximum.
1783   if (max_vector_size() &gt; 16)
1784     return;
1785 
1786   TracePhase tp("isched", &amp;timers[_t_instrSched]);
1787 
1788   // Create a data structure for all the scheduling information
1789   Scheduling scheduling(Thread::current()-&gt;resource_area(), *this);
1790 
1791   // Walk backwards over each basic block, computing the needed alignment
1792   // Walk over all the basic blocks
1793   scheduling.DoScheduling();
1794 }
1795 
1796 // Compute the latency of all the instructions.  This is fairly simple,
1797 // because we already have a legal ordering.  Walk over the instructions
1798 // from first to last, and compute the latency of the instruction based
1799 // on the latency of the preceding instruction(s).
1800 void Scheduling::ComputeLocalLatenciesForward(const Block *bb) {
1801 #ifndef PRODUCT
1802   if (_cfg-&gt;C-&gt;trace_opto_output())
1803     tty-&gt;print("# -&gt; ComputeLocalLatenciesForward\n");
1804 #endif
1805 
1806   // Walk over all the schedulable instructions
1807   for( uint j=_bb_start; j &lt; _bb_end; j++ ) {
1808 
1809     // This is a kludge, forcing all latency calculations to start at 1.
1810     // Used to allow latency 0 to force an instruction to the beginning
1811     // of the bb
1812     uint latency = 1;
1813     Node *use = bb-&gt;get_node(j);
1814     uint nlen = use-&gt;len();
1815 
1816     // Walk over all the inputs
1817     for ( uint k=0; k &lt; nlen; k++ ) {
1818       Node *def = use-&gt;in(k);
1819       if (!def)
1820         continue;
1821 
1822       uint l = _node_latency[def-&gt;_idx] + use-&gt;latency(k);
1823       if (latency &lt; l)
1824         latency = l;
1825     }
1826 
1827     _node_latency[use-&gt;_idx] = latency;
1828 
1829 #ifndef PRODUCT
1830     if (_cfg-&gt;C-&gt;trace_opto_output()) {
1831       tty-&gt;print("# latency %4d: ", latency);
1832       use-&gt;dump();
1833     }
1834 #endif
1835   }
1836 
1837 #ifndef PRODUCT
1838   if (_cfg-&gt;C-&gt;trace_opto_output())
1839     tty-&gt;print("# &lt;- ComputeLocalLatenciesForward\n");
1840 #endif
1841 
1842 } // end ComputeLocalLatenciesForward
1843 
1844 // See if this node fits into the present instruction bundle
1845 bool Scheduling::NodeFitsInBundle(Node *n) {
1846   uint n_idx = n-&gt;_idx;
1847 
1848   // If this is the unconditional delay instruction, then it fits
1849   if (n == _unconditional_delay_slot) {
1850 #ifndef PRODUCT
1851     if (_cfg-&gt;C-&gt;trace_opto_output())
1852       tty-&gt;print("#     NodeFitsInBundle [%4d]: TRUE; is in unconditional delay slot\n", n-&gt;_idx);
1853 #endif
1854     return (true);
1855   }
1856 
1857   // If the node cannot be scheduled this cycle, skip it
1858   if (_current_latency[n_idx] &gt; _bundle_cycle_number) {
1859 #ifndef PRODUCT
1860     if (_cfg-&gt;C-&gt;trace_opto_output())
1861       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; latency %4d &gt; %d\n",
1862         n-&gt;_idx, _current_latency[n_idx], _bundle_cycle_number);
1863 #endif
1864     return (false);
1865   }
1866 
1867   const Pipeline *node_pipeline = n-&gt;pipeline();
1868 
1869   uint instruction_count = node_pipeline-&gt;instructionCount();
1870   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
1871     instruction_count = 0;
1872   else if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
1873     instruction_count++;
1874 
1875   if (_bundle_instr_count + instruction_count &gt; Pipeline::_max_instrs_per_cycle) {
1876 #ifndef PRODUCT
1877     if (_cfg-&gt;C-&gt;trace_opto_output())
1878       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; too many instructions: %d &gt; %d\n",
1879         n-&gt;_idx, _bundle_instr_count + instruction_count, Pipeline::_max_instrs_per_cycle);
1880 #endif
1881     return (false);
1882   }
1883 
1884   // Don't allow non-machine nodes to be handled this way
1885   if (!n-&gt;is_Mach() &amp;&amp; instruction_count == 0)
1886     return (false);
1887 
1888   // See if there is any overlap
1889   uint delay = _bundle_use.full_latency(0, node_pipeline-&gt;resourceUse());
1890 
1891   if (delay &gt; 0) {
1892 #ifndef PRODUCT
1893     if (_cfg-&gt;C-&gt;trace_opto_output())
1894       tty-&gt;print("#     NodeFitsInBundle [%4d]: FALSE; functional units overlap\n", n_idx);
1895 #endif
1896     return false;
1897   }
1898 
1899 #ifndef PRODUCT
1900   if (_cfg-&gt;C-&gt;trace_opto_output())
1901     tty-&gt;print("#     NodeFitsInBundle [%4d]:  TRUE\n", n_idx);
1902 #endif
1903 
1904   return true;
1905 }
1906 
1907 Node * Scheduling::ChooseNodeToBundle() {
1908   uint siz = _available.size();
1909 
1910   if (siz == 0) {
1911 
1912 #ifndef PRODUCT
1913     if (_cfg-&gt;C-&gt;trace_opto_output())
1914       tty-&gt;print("#   ChooseNodeToBundle: NULL\n");
1915 #endif
1916     return (NULL);
1917   }
1918 
1919   // Fast path, if only 1 instruction in the bundle
1920   if (siz == 1) {
1921 #ifndef PRODUCT
1922     if (_cfg-&gt;C-&gt;trace_opto_output()) {
1923       tty-&gt;print("#   ChooseNodeToBundle (only 1): ");
1924       _available[0]-&gt;dump();
1925     }
1926 #endif
1927     return (_available[0]);
1928   }
1929 
1930   // Don't bother, if the bundle is already full
1931   if (_bundle_instr_count &lt; Pipeline::_max_instrs_per_cycle) {
1932     for ( uint i = 0; i &lt; siz; i++ ) {
1933       Node *n = _available[i];
1934 
1935       // Skip projections, we'll handle them another way
1936       if (n-&gt;is_Proj())
1937         continue;
1938 
1939       // This presupposed that instructions are inserted into the
1940       // available list in a legality order; i.e. instructions that
1941       // must be inserted first are at the head of the list
1942       if (NodeFitsInBundle(n)) {
1943 #ifndef PRODUCT
1944         if (_cfg-&gt;C-&gt;trace_opto_output()) {
1945           tty-&gt;print("#   ChooseNodeToBundle: ");
1946           n-&gt;dump();
1947         }
1948 #endif
1949         return (n);
1950       }
1951     }
1952   }
1953 
1954   // Nothing fits in this bundle, choose the highest priority
1955 #ifndef PRODUCT
1956   if (_cfg-&gt;C-&gt;trace_opto_output()) {
1957     tty-&gt;print("#   ChooseNodeToBundle: ");
1958     _available[0]-&gt;dump();
1959   }
1960 #endif
1961 
1962   return _available[0];
1963 }
1964 
1965 void Scheduling::AddNodeToAvailableList(Node *n) {
1966   assert( !n-&gt;is_Proj(), "projections never directly made available" );
1967 #ifndef PRODUCT
1968   if (_cfg-&gt;C-&gt;trace_opto_output()) {
1969     tty-&gt;print("#   AddNodeToAvailableList: ");
1970     n-&gt;dump();
1971   }
1972 #endif
1973 
1974   int latency = _current_latency[n-&gt;_idx];
1975 
1976   // Insert in latency order (insertion sort)
1977   uint i;
1978   for ( i=0; i &lt; _available.size(); i++ )
1979     if (_current_latency[_available[i]-&gt;_idx] &gt; latency)
1980       break;
1981 
1982   // Special Check for compares following branches
1983   if( n-&gt;is_Mach() &amp;&amp; _scheduled.size() &gt; 0 ) {
1984     int op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1985     Node *last = _scheduled[0];
1986     if( last-&gt;is_MachIf() &amp;&amp; last-&gt;in(1) == n &amp;&amp;
1987         ( op == Op_CmpI ||
1988           op == Op_CmpU ||
1989           op == Op_CmpUL ||
1990           op == Op_CmpP ||
1991           op == Op_CmpF ||
1992           op == Op_CmpD ||
1993           op == Op_CmpL ) ) {
1994 
1995       // Recalculate position, moving to front of same latency
1996       for ( i=0 ; i &lt; _available.size(); i++ )
1997         if (_current_latency[_available[i]-&gt;_idx] &gt;= latency)
1998           break;
1999     }
2000   }
2001 
2002   // Insert the node in the available list
2003   _available.insert(i, n);
2004 
2005 #ifndef PRODUCT
2006   if (_cfg-&gt;C-&gt;trace_opto_output())
2007     dump_available();
2008 #endif
2009 }
2010 
2011 void Scheduling::DecrementUseCounts(Node *n, const Block *bb) {
2012   for ( uint i=0; i &lt; n-&gt;len(); i++ ) {
2013     Node *def = n-&gt;in(i);
2014     if (!def) continue;
2015     if( def-&gt;is_Proj() )        // If this is a machine projection, then
2016       def = def-&gt;in(0);         // propagate usage thru to the base instruction
2017 
2018     if(_cfg-&gt;get_block_for_node(def) != bb) { // Ignore if not block-local
2019       continue;
2020     }
2021 
2022     // Compute the latency
2023     uint l = _bundle_cycle_number + n-&gt;latency(i);
2024     if (_current_latency[def-&gt;_idx] &lt; l)
2025       _current_latency[def-&gt;_idx] = l;
2026 
2027     // If this does not have uses then schedule it
2028     if ((--_uses[def-&gt;_idx]) == 0)
2029       AddNodeToAvailableList(def);
2030   }
2031 }
2032 
2033 void Scheduling::AddNodeToBundle(Node *n, const Block *bb) {
2034 #ifndef PRODUCT
2035   if (_cfg-&gt;C-&gt;trace_opto_output()) {
2036     tty-&gt;print("#   AddNodeToBundle: ");
2037     n-&gt;dump();
2038   }
2039 #endif
2040 
2041   // Remove this from the available list
2042   uint i;
2043   for (i = 0; i &lt; _available.size(); i++)
2044     if (_available[i] == n)
2045       break;
2046   assert(i &lt; _available.size(), "entry in _available list not found");
2047   _available.remove(i);
2048 
2049   // See if this fits in the current bundle
2050   const Pipeline *node_pipeline = n-&gt;pipeline();
2051   const Pipeline_Use&amp; node_usage = node_pipeline-&gt;resourceUse();
2052 
2053   // Check for instructions to be placed in the delay slot. We
2054   // do this before we actually schedule the current instruction,
2055   // because the delay slot follows the current instruction.
2056   if (Pipeline::_branch_has_delay_slot &amp;&amp;
2057       node_pipeline-&gt;hasBranchDelay() &amp;&amp;
2058       !_unconditional_delay_slot) {
2059 
2060     uint siz = _available.size();
2061 
2062     // Conditional branches can support an instruction that
2063     // is unconditionally executed and not dependent by the
2064     // branch, OR a conditionally executed instruction if
2065     // the branch is taken.  In practice, this means that
2066     // the first instruction at the branch target is
2067     // copied to the delay slot, and the branch goes to
2068     // the instruction after that at the branch target
2069     if ( n-&gt;is_MachBranch() ) {
2070 
2071       assert( !n-&gt;is_MachNullCheck(), "should not look for delay slot for Null Check" );
2072       assert( !n-&gt;is_Catch(),         "should not look for delay slot for Catch" );
2073 
2074 #ifndef PRODUCT
2075       _branches++;
2076 #endif
2077 
2078       // At least 1 instruction is on the available list
2079       // that is not dependent on the branch
2080       for (uint i = 0; i &lt; siz; i++) {
2081         Node *d = _available[i];
2082         const Pipeline *avail_pipeline = d-&gt;pipeline();
2083 
2084         // Don't allow safepoints in the branch shadow, that will
2085         // cause a number of difficulties
2086         if ( avail_pipeline-&gt;instructionCount() == 1 &amp;&amp;
2087             !avail_pipeline-&gt;hasMultipleBundles() &amp;&amp;
2088             !avail_pipeline-&gt;hasBranchDelay() &amp;&amp;
2089             Pipeline::instr_has_unit_size() &amp;&amp;
2090             d-&gt;size(_regalloc) == Pipeline::instr_unit_size() &amp;&amp;
2091             NodeFitsInBundle(d) &amp;&amp;
2092             !node_bundling(d)-&gt;used_in_delay()) {
2093 
2094           if (d-&gt;is_Mach() &amp;&amp; !d-&gt;is_MachSafePoint()) {
2095             // A node that fits in the delay slot was found, so we need to
2096             // set the appropriate bits in the bundle pipeline information so
2097             // that it correctly indicates resource usage.  Later, when we
2098             // attempt to add this instruction to the bundle, we will skip
2099             // setting the resource usage.
2100             _unconditional_delay_slot = d;
2101             node_bundling(n)-&gt;set_use_unconditional_delay();
2102             node_bundling(d)-&gt;set_used_in_unconditional_delay();
2103             _bundle_use.add_usage(avail_pipeline-&gt;resourceUse());
2104             _current_latency[d-&gt;_idx] = _bundle_cycle_number;
2105             _next_node = d;
2106             ++_bundle_instr_count;
2107 #ifndef PRODUCT
2108             _unconditional_delays++;
2109 #endif
2110             break;
2111           }
2112         }
2113       }
2114     }
2115 
2116     // No delay slot, add a nop to the usage
2117     if (!_unconditional_delay_slot) {
2118       // See if adding an instruction in the delay slot will overflow
2119       // the bundle.
2120       if (!NodeFitsInBundle(_nop)) {
2121 #ifndef PRODUCT
2122         if (_cfg-&gt;C-&gt;trace_opto_output())
2123           tty-&gt;print("#  *** STEP(1 instruction for delay slot) ***\n");
2124 #endif
2125         step(1);
2126       }
2127 
2128       _bundle_use.add_usage(_nop-&gt;pipeline()-&gt;resourceUse());
2129       _next_node = _nop;
2130       ++_bundle_instr_count;
2131     }
2132 
2133     // See if the instruction in the delay slot requires a
2134     // step of the bundles
2135     if (!NodeFitsInBundle(n)) {
2136 #ifndef PRODUCT
2137         if (_cfg-&gt;C-&gt;trace_opto_output())
2138           tty-&gt;print("#  *** STEP(branch won't fit) ***\n");
2139 #endif
2140         // Update the state information
2141         _bundle_instr_count = 0;
2142         _bundle_cycle_number += 1;
2143         _bundle_use.step(1);
2144     }
2145   }
2146 
2147   // Get the number of instructions
2148   uint instruction_count = node_pipeline-&gt;instructionCount();
2149   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
2150     instruction_count = 0;
2151 
2152   // Compute the latency information
2153   uint delay = 0;
2154 
2155   if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode()) {
2156     int relative_latency = _current_latency[n-&gt;_idx] - _bundle_cycle_number;
2157     if (relative_latency &lt; 0)
2158       relative_latency = 0;
2159 
2160     delay = _bundle_use.full_latency(relative_latency, node_usage);
2161 
2162     // Does not fit in this bundle, start a new one
2163     if (delay &gt; 0) {
2164       step(delay);
2165 
2166 #ifndef PRODUCT
2167       if (_cfg-&gt;C-&gt;trace_opto_output())
2168         tty-&gt;print("#  *** STEP(%d) ***\n", delay);
2169 #endif
2170     }
2171   }
2172 
2173   // If this was placed in the delay slot, ignore it
2174   if (n != _unconditional_delay_slot) {
2175 
2176     if (delay == 0) {
2177       if (node_pipeline-&gt;hasMultipleBundles()) {
2178 #ifndef PRODUCT
2179         if (_cfg-&gt;C-&gt;trace_opto_output())
2180           tty-&gt;print("#  *** STEP(multiple instructions) ***\n");
2181 #endif
2182         step(1);
2183       }
2184 
2185       else if (instruction_count + _bundle_instr_count &gt; Pipeline::_max_instrs_per_cycle) {
2186 #ifndef PRODUCT
2187         if (_cfg-&gt;C-&gt;trace_opto_output())
2188           tty-&gt;print("#  *** STEP(%d &gt;= %d instructions) ***\n",
2189             instruction_count + _bundle_instr_count,
2190             Pipeline::_max_instrs_per_cycle);
2191 #endif
2192         step(1);
2193       }
2194     }
2195 
2196     if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
2197       _bundle_instr_count++;
2198 
2199     // Set the node's latency
2200     _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2201 
2202     // Now merge the functional unit information
2203     if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode())
2204       _bundle_use.add_usage(node_usage);
2205 
2206     // Increment the number of instructions in this bundle
2207     _bundle_instr_count += instruction_count;
2208 
2209     // Remember this node for later
2210     if (n-&gt;is_Mach())
2211       _next_node = n;
2212   }
2213 
2214   // It's possible to have a BoxLock in the graph and in the _bbs mapping but
2215   // not in the bb-&gt;_nodes array.  This happens for debug-info-only BoxLocks.
2216   // 'Schedule' them (basically ignore in the schedule) but do not insert them
2217   // into the block.  All other scheduled nodes get put in the schedule here.
2218   int op = n-&gt;Opcode();
2219   if( (op == Op_Node &amp;&amp; n-&gt;req() == 0) || // anti-dependence node OR
2220       (op != Op_Node &amp;&amp;         // Not an unused antidepedence node and
2221        // not an unallocated boxlock
2222        (OptoReg::is_valid(_regalloc-&gt;get_reg_first(n)) || op != Op_BoxLock)) ) {
2223 
2224     // Push any trailing projections
2225     if( bb-&gt;get_node(bb-&gt;number_of_nodes()-1) != n ) {
2226       for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
2227         Node *foi = n-&gt;fast_out(i);
2228         if( foi-&gt;is_Proj() )
2229           _scheduled.push(foi);
2230       }
2231     }
2232 
2233     // Put the instruction in the schedule list
2234     _scheduled.push(n);
2235   }
2236 
2237 #ifndef PRODUCT
2238   if (_cfg-&gt;C-&gt;trace_opto_output())
2239     dump_available();
2240 #endif
2241 
2242   // Walk all the definitions, decrementing use counts, and
2243   // if a definition has a 0 use count, place it in the available list.
2244   DecrementUseCounts(n,bb);
2245 }
2246 
2247 // This method sets the use count within a basic block.  We will ignore all
2248 // uses outside the current basic block.  As we are doing a backwards walk,
2249 // any node we reach that has a use count of 0 may be scheduled.  This also
2250 // avoids the problem of cyclic references from phi nodes, as long as phi
2251 // nodes are at the front of the basic block.  This method also initializes
2252 // the available list to the set of instructions that have no uses within this
2253 // basic block.
2254 void Scheduling::ComputeUseCount(const Block *bb) {
2255 #ifndef PRODUCT
2256   if (_cfg-&gt;C-&gt;trace_opto_output())
2257     tty-&gt;print("# -&gt; ComputeUseCount\n");
2258 #endif
2259 
2260   // Clear the list of available and scheduled instructions, just in case
2261   _available.clear();
2262   _scheduled.clear();
2263 
2264   // No delay slot specified
2265   _unconditional_delay_slot = NULL;
2266 
2267 #ifdef ASSERT
2268   for( uint i=0; i &lt; bb-&gt;number_of_nodes(); i++ )
2269     assert( _uses[bb-&gt;get_node(i)-&gt;_idx] == 0, "_use array not clean" );
2270 #endif
2271 
2272   // Force the _uses count to never go to zero for unscheduable pieces
2273   // of the block
2274   for( uint k = 0; k &lt; _bb_start; k++ )
2275     _uses[bb-&gt;get_node(k)-&gt;_idx] = 1;
2276   for( uint l = _bb_end; l &lt; bb-&gt;number_of_nodes(); l++ )
2277     _uses[bb-&gt;get_node(l)-&gt;_idx] = 1;
2278 
2279   // Iterate backwards over the instructions in the block.  Don't count the
2280   // branch projections at end or the block header instructions.
2281   for( uint j = _bb_end-1; j &gt;= _bb_start; j-- ) {
2282     Node *n = bb-&gt;get_node(j);
2283     if( n-&gt;is_Proj() ) continue; // Projections handled another way
2284 
2285     // Account for all uses
2286     for ( uint k = 0; k &lt; n-&gt;len(); k++ ) {
2287       Node *inp = n-&gt;in(k);
2288       if (!inp) continue;
2289       assert(inp != n, "no cycles allowed" );
2290       if (_cfg-&gt;get_block_for_node(inp) == bb) { // Block-local use?
2291         if (inp-&gt;is_Proj()) { // Skip through Proj's
2292           inp = inp-&gt;in(0);
2293         }
2294         ++_uses[inp-&gt;_idx];     // Count 1 block-local use
2295       }
2296     }
2297 
2298     // If this instruction has a 0 use count, then it is available
2299     if (!_uses[n-&gt;_idx]) {
2300       _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2301       AddNodeToAvailableList(n);
2302     }
2303 
2304 #ifndef PRODUCT
2305     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2306       tty-&gt;print("#   uses: %3d: ", _uses[n-&gt;_idx]);
2307       n-&gt;dump();
2308     }
2309 #endif
2310   }
2311 
2312 #ifndef PRODUCT
2313   if (_cfg-&gt;C-&gt;trace_opto_output())
2314     tty-&gt;print("# &lt;- ComputeUseCount\n");
2315 #endif
2316 }
2317 
2318 // This routine performs scheduling on each basic block in reverse order,
2319 // using instruction latencies and taking into account function unit
2320 // availability.
2321 void Scheduling::DoScheduling() {
2322 #ifndef PRODUCT
2323   if (_cfg-&gt;C-&gt;trace_opto_output())
2324     tty-&gt;print("# -&gt; DoScheduling\n");
2325 #endif
2326 
2327   Block *succ_bb = NULL;
2328   Block *bb;
2329 
2330   // Walk over all the basic blocks in reverse order
2331   for (int i = _cfg-&gt;number_of_blocks() - 1; i &gt;= 0; succ_bb = bb, i--) {
2332     bb = _cfg-&gt;get_block(i);
2333 
2334 #ifndef PRODUCT
2335     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2336       tty-&gt;print("#  Schedule BB#%03d (initial)\n", i);
2337       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2338         bb-&gt;get_node(j)-&gt;dump();
2339       }
2340     }
2341 #endif
2342 
2343     // On the head node, skip processing
2344     if (bb == _cfg-&gt;get_root_block()) {
2345       continue;
2346     }
2347 
2348     // Skip empty, connector blocks
2349     if (bb-&gt;is_connector())
2350       continue;
2351 
2352     // If the following block is not the sole successor of
2353     // this one, then reset the pipeline information
2354     if (bb-&gt;_num_succs != 1 || bb-&gt;non_connector_successor(0) != succ_bb) {
2355 #ifndef PRODUCT
2356       if (_cfg-&gt;C-&gt;trace_opto_output()) {
2357         tty-&gt;print("*** bundle start of next BB, node %d, for %d instructions\n",
2358                    _next_node-&gt;_idx, _bundle_instr_count);
2359       }
2360 #endif
2361       step_and_clear();
2362     }
2363 
2364     // Leave untouched the starting instruction, any Phis, a CreateEx node
2365     // or Top.  bb-&gt;get_node(_bb_start) is the first schedulable instruction.
2366     _bb_end = bb-&gt;number_of_nodes()-1;
2367     for( _bb_start=1; _bb_start &lt;= _bb_end; _bb_start++ ) {
2368       Node *n = bb-&gt;get_node(_bb_start);
2369       // Things not matched, like Phinodes and ProjNodes don't get scheduled.
2370       // Also, MachIdealNodes do not get scheduled
2371       if( !n-&gt;is_Mach() ) continue;     // Skip non-machine nodes
2372       MachNode *mach = n-&gt;as_Mach();
2373       int iop = mach-&gt;ideal_Opcode();
2374       if( iop == Op_CreateEx ) continue; // CreateEx is pinned
2375       if( iop == Op_Con ) continue;      // Do not schedule Top
2376       if( iop == Op_Node &amp;&amp;     // Do not schedule PhiNodes, ProjNodes
2377           mach-&gt;pipeline() == MachNode::pipeline_class() &amp;&amp;
2378           !n-&gt;is_SpillCopy() &amp;&amp; !n-&gt;is_MachMerge() )  // Breakpoints, Prolog, etc
2379         continue;
2380       break;                    // Funny loop structure to be sure...
2381     }
2382     // Compute last "interesting" instruction in block - last instruction we
2383     // might schedule.  _bb_end points just after last schedulable inst.  We
2384     // normally schedule conditional branches (despite them being forced last
2385     // in the block), because they have delay slots we can fill.  Calls all
2386     // have their delay slots filled in the template expansions, so we don't
2387     // bother scheduling them.
2388     Node *last = bb-&gt;get_node(_bb_end);
2389     // Ignore trailing NOPs.
2390     while (_bb_end &gt; 0 &amp;&amp; last-&gt;is_Mach() &amp;&amp;
2391            last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Con) {
2392       last = bb-&gt;get_node(--_bb_end);
2393     }
2394     assert(!last-&gt;is_Mach() || last-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con, "");
2395     if( last-&gt;is_Catch() ||
2396        // Exclude unreachable path case when Halt node is in a separate block.
2397        (_bb_end &gt; 1 &amp;&amp; last-&gt;is_Mach() &amp;&amp; last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Halt) ) {
2398       // There must be a prior call.  Skip it.
2399       while( !bb-&gt;get_node(--_bb_end)-&gt;is_MachCall() ) {
2400         assert( bb-&gt;get_node(_bb_end)-&gt;is_MachProj(), "skipping projections after expected call" );
2401       }
2402     } else if( last-&gt;is_MachNullCheck() ) {
2403       // Backup so the last null-checked memory instruction is
2404       // outside the schedulable range. Skip over the nullcheck,
2405       // projection, and the memory nodes.
2406       Node *mem = last-&gt;in(1);
2407       do {
2408         _bb_end--;
2409       } while (mem != bb-&gt;get_node(_bb_end));
2410     } else {
2411       // Set _bb_end to point after last schedulable inst.
2412       _bb_end++;
2413     }
2414 
2415     assert( _bb_start &lt;= _bb_end, "inverted block ends" );
2416 
2417     // Compute the register antidependencies for the basic block
2418     ComputeRegisterAntidependencies(bb);
2419     if (_cfg-&gt;C-&gt;failing())  return;  // too many D-U pinch points
2420 
2421     // Compute intra-bb latencies for the nodes
2422     ComputeLocalLatenciesForward(bb);
2423 
2424     // Compute the usage within the block, and set the list of all nodes
2425     // in the block that have no uses within the block.
2426     ComputeUseCount(bb);
2427 
2428     // Schedule the remaining instructions in the block
2429     while ( _available.size() &gt; 0 ) {
2430       Node *n = ChooseNodeToBundle();
2431       guarantee(n != NULL, "no nodes available");
2432       AddNodeToBundle(n,bb);
2433     }
2434 
2435     assert( _scheduled.size() == _bb_end - _bb_start, "wrong number of instructions" );
2436 #ifdef ASSERT
2437     for( uint l = _bb_start; l &lt; _bb_end; l++ ) {
2438       Node *n = bb-&gt;get_node(l);
2439       uint m;
2440       for( m = 0; m &lt; _bb_end-_bb_start; m++ )
2441         if( _scheduled[m] == n )
2442           break;
2443       assert( m &lt; _bb_end-_bb_start, "instruction missing in schedule" );
2444     }
2445 #endif
2446 
2447     // Now copy the instructions (in reverse order) back to the block
2448     for ( uint k = _bb_start; k &lt; _bb_end; k++ )
2449       bb-&gt;map_node(_scheduled[_bb_end-k-1], k);
2450 
2451 #ifndef PRODUCT
2452     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2453       tty-&gt;print("#  Schedule BB#%03d (final)\n", i);
2454       uint current = 0;
2455       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2456         Node *n = bb-&gt;get_node(j);
2457         if( valid_bundle_info(n) ) {
2458           Bundle *bundle = node_bundling(n);
2459           if (bundle-&gt;instr_count() &gt; 0 || bundle-&gt;flags() &gt; 0) {
2460             tty-&gt;print("*** Bundle: ");
2461             bundle-&gt;dump();
2462           }
2463           n-&gt;dump();
2464         }
2465       }
2466     }
2467 #endif
2468 #ifdef ASSERT
2469   verify_good_schedule(bb,"after block local scheduling");
2470 #endif
2471   }
2472 
2473 #ifndef PRODUCT
2474   if (_cfg-&gt;C-&gt;trace_opto_output())
2475     tty-&gt;print("# &lt;- DoScheduling\n");
2476 #endif
2477 
2478   // Record final node-bundling array location
2479   _regalloc-&gt;C-&gt;set_node_bundling_base(_node_bundling_base);
2480 
2481 } // end DoScheduling
2482 
2483 // Verify that no live-range used in the block is killed in the block by a
2484 // wrong DEF.  This doesn't verify live-ranges that span blocks.
2485 
2486 // Check for edge existence.  Used to avoid adding redundant precedence edges.
2487 static bool edge_from_to( Node *from, Node *to ) {
2488   for( uint i=0; i&lt;from-&gt;len(); i++ )
2489     if( from-&gt;in(i) == to )
2490       return true;
2491   return false;
2492 }
2493 
2494 #ifdef ASSERT
2495 void Scheduling::verify_do_def( Node *n, OptoReg::Name def, const char *msg ) {
2496   // Check for bad kills
2497   if( OptoReg::is_valid(def) ) { // Ignore stores &amp; control flow
2498     Node *prior_use = _reg_node[def];
2499     if( prior_use &amp;&amp; !edge_from_to(prior_use,n) ) {
2500       tty-&gt;print("%s = ",OptoReg::as_VMReg(def)-&gt;name());
2501       n-&gt;dump();
2502       tty-&gt;print_cr("...");
2503       prior_use-&gt;dump();
2504       assert(edge_from_to(prior_use,n), "%s", msg);
2505     }
2506     _reg_node.map(def,NULL); // Kill live USEs
2507   }
2508 }
2509 
2510 void Scheduling::verify_good_schedule( Block *b, const char *msg ) {
2511 
2512   // Zap to something reasonable for the verify code
2513   _reg_node.clear();
2514 
2515   // Walk over the block backwards.  Check to make sure each DEF doesn't
2516   // kill a live value (other than the one it's supposed to).  Add each
2517   // USE to the live set.
2518   for( uint i = b-&gt;number_of_nodes()-1; i &gt;= _bb_start; i-- ) {
2519     Node *n = b-&gt;get_node(i);
2520     int n_op = n-&gt;Opcode();
2521     if( n_op == Op_MachProj &amp;&amp; n-&gt;ideal_reg() == MachProjNode::fat_proj ) {
2522       // Fat-proj kills a slew of registers
2523       RegMask rm = n-&gt;out_RegMask();// Make local copy
2524       while( rm.is_NotEmpty() ) {
2525         OptoReg::Name kill = rm.find_first_elem();
2526         rm.Remove(kill);
2527         verify_do_def( n, kill, msg );
2528       }
2529     } else if( n_op != Op_Node ) { // Avoid brand new antidependence nodes
2530       // Get DEF'd registers the normal way
2531       verify_do_def( n, _regalloc-&gt;get_reg_first(n), msg );
2532       verify_do_def( n, _regalloc-&gt;get_reg_second(n), msg );
2533     }
2534 
2535     // Now make all USEs live
2536     for( uint i=1; i&lt;n-&gt;req(); i++ ) {
2537       Node *def = n-&gt;in(i);
2538       assert(def != 0, "input edge required");
2539       OptoReg::Name reg_lo = _regalloc-&gt;get_reg_first(def);
2540       OptoReg::Name reg_hi = _regalloc-&gt;get_reg_second(def);
2541       if( OptoReg::is_valid(reg_lo) ) {
2542         assert(!_reg_node[reg_lo] || edge_from_to(_reg_node[reg_lo],def), "%s", msg);
2543         _reg_node.map(reg_lo,n);
2544       }
2545       if( OptoReg::is_valid(reg_hi) ) {
2546         assert(!_reg_node[reg_hi] || edge_from_to(_reg_node[reg_hi],def), "%s", msg);
2547         _reg_node.map(reg_hi,n);
2548       }
2549     }
2550 
2551   }
2552 
2553   // Zap to something reasonable for the Antidependence code
2554   _reg_node.clear();
2555 }
2556 #endif
2557 
2558 // Conditionally add precedence edges.  Avoid putting edges on Projs.
2559 static void add_prec_edge_from_to( Node *from, Node *to ) {
2560   if( from-&gt;is_Proj() ) {       // Put precedence edge on Proj's input
2561     assert( from-&gt;req() == 1 &amp;&amp; (from-&gt;len() == 1 || from-&gt;in(1)==0), "no precedence edges on projections" );
2562     from = from-&gt;in(0);
2563   }
2564   if( from != to &amp;&amp;             // No cycles (for things like LD L0,[L0+4] )
2565       !edge_from_to( from, to ) ) // Avoid duplicate edge
2566     from-&gt;add_prec(to);
2567 }
2568 
2569 void Scheduling::anti_do_def( Block *b, Node *def, OptoReg::Name def_reg, int is_def ) {
2570   if( !OptoReg::is_valid(def_reg) ) // Ignore stores &amp; control flow
2571     return;
2572 
2573   Node *pinch = _reg_node[def_reg]; // Get pinch point
2574   if ((pinch == NULL) || _cfg-&gt;get_block_for_node(pinch) != b || // No pinch-point yet?
2575       is_def ) {    // Check for a true def (not a kill)
2576     _reg_node.map(def_reg,def); // Record def/kill as the optimistic pinch-point
2577     return;
2578   }
2579 
2580   Node *kill = def;             // Rename 'def' to more descriptive 'kill'
2581   debug_only( def = (Node*)((intptr_t)0xdeadbeef); )
2582 
2583   // After some number of kills there _may_ be a later def
2584   Node *later_def = NULL;
2585 
2586   // Finding a kill requires a real pinch-point.
2587   // Check for not already having a pinch-point.
2588   // Pinch points are Op_Node's.
2589   if( pinch-&gt;Opcode() != Op_Node ) { // Or later-def/kill as pinch-point?
2590     later_def = pinch;            // Must be def/kill as optimistic pinch-point
2591     if ( _pinch_free_list.size() &gt; 0) {
2592       pinch = _pinch_free_list.pop();
2593     } else {
2594       pinch = new Node(1); // Pinch point to-be
2595     }
2596     if (pinch-&gt;_idx &gt;= _regalloc-&gt;node_regs_max_index()) {
2597       _cfg-&gt;C-&gt;record_method_not_compilable("too many D-U pinch points");
2598       return;
2599     }
2600     _cfg-&gt;map_node_to_block(pinch, b);      // Pretend it's valid in this block (lazy init)
2601     _reg_node.map(def_reg,pinch); // Record pinch-point
2602     //_regalloc-&gt;set_bad(pinch-&gt;_idx); // Already initialized this way.
2603     if( later_def-&gt;outcnt() == 0 || later_def-&gt;ideal_reg() == MachProjNode::fat_proj ) { // Distinguish def from kill
2604       pinch-&gt;init_req(0, _cfg-&gt;C-&gt;top());     // set not NULL for the next call
2605       add_prec_edge_from_to(later_def,pinch); // Add edge from kill to pinch
2606       later_def = NULL;           // and no later def
2607     }
2608     pinch-&gt;set_req(0,later_def);  // Hook later def so we can find it
2609   } else {                        // Else have valid pinch point
2610     if( pinch-&gt;in(0) )            // If there is a later-def
2611       later_def = pinch-&gt;in(0);   // Get it
2612   }
2613 
2614   // Add output-dependence edge from later def to kill
2615   if( later_def )               // If there is some original def
2616     add_prec_edge_from_to(later_def,kill); // Add edge from def to kill
2617 
2618   // See if current kill is also a use, and so is forced to be the pinch-point.
2619   if( pinch-&gt;Opcode() == Op_Node ) {
2620     Node *uses = kill-&gt;is_Proj() ? kill-&gt;in(0) : kill;
2621     for( uint i=1; i&lt;uses-&gt;req(); i++ ) {
2622       if( _regalloc-&gt;get_reg_first(uses-&gt;in(i)) == def_reg ||
2623           _regalloc-&gt;get_reg_second(uses-&gt;in(i)) == def_reg ) {
2624         // Yes, found a use/kill pinch-point
2625         pinch-&gt;set_req(0,NULL);  //
2626         pinch-&gt;replace_by(kill); // Move anti-dep edges up
2627         pinch = kill;
2628         _reg_node.map(def_reg,pinch);
2629         return;
2630       }
2631     }
2632   }
2633 
2634   // Add edge from kill to pinch-point
2635   add_prec_edge_from_to(kill,pinch);
2636 }
2637 
2638 void Scheduling::anti_do_use( Block *b, Node *use, OptoReg::Name use_reg ) {
2639   if( !OptoReg::is_valid(use_reg) ) // Ignore stores &amp; control flow
2640     return;
2641   Node *pinch = _reg_node[use_reg]; // Get pinch point
2642   // Check for no later def_reg/kill in block
2643   if ((pinch != NULL) &amp;&amp; _cfg-&gt;get_block_for_node(pinch) == b &amp;&amp;
2644       // Use has to be block-local as well
2645       _cfg-&gt;get_block_for_node(use) == b) {
2646     if( pinch-&gt;Opcode() == Op_Node &amp;&amp; // Real pinch-point (not optimistic?)
2647         pinch-&gt;req() == 1 ) {   // pinch not yet in block?
2648       pinch-&gt;del_req(0);        // yank pointer to later-def, also set flag
2649       // Insert the pinch-point in the block just after the last use
2650       b-&gt;insert_node(pinch, b-&gt;find_node(use) + 1);
2651       _bb_end++;                // Increase size scheduled region in block
2652     }
2653 
2654     add_prec_edge_from_to(pinch,use);
2655   }
2656 }
2657 
2658 // We insert antidependences between the reads and following write of
2659 // allocated registers to prevent illegal code motion. Hopefully, the
2660 // number of added references should be fairly small, especially as we
2661 // are only adding references within the current basic block.
2662 void Scheduling::ComputeRegisterAntidependencies(Block *b) {
2663 
2664 #ifdef ASSERT
2665   verify_good_schedule(b,"before block local scheduling");
2666 #endif
2667 
2668   // A valid schedule, for each register independently, is an endless cycle
2669   // of: a def, then some uses (connected to the def by true dependencies),
2670   // then some kills (defs with no uses), finally the cycle repeats with a new
2671   // def.  The uses are allowed to float relative to each other, as are the
2672   // kills.  No use is allowed to slide past a kill (or def).  This requires
2673   // antidependencies between all uses of a single def and all kills that
2674   // follow, up to the next def.  More edges are redundant, because later defs
2675   // &amp; kills are already serialized with true or antidependencies.  To keep
2676   // the edge count down, we add a 'pinch point' node if there's more than
2677   // one use or more than one kill/def.
2678 
2679   // We add dependencies in one bottom-up pass.
2680 
2681   // For each instruction we handle it's DEFs/KILLs, then it's USEs.
2682 
2683   // For each DEF/KILL, we check to see if there's a prior DEF/KILL for this
2684   // register.  If not, we record the DEF/KILL in _reg_node, the
2685   // register-to-def mapping.  If there is a prior DEF/KILL, we insert a
2686   // "pinch point", a new Node that's in the graph but not in the block.
2687   // We put edges from the prior and current DEF/KILLs to the pinch point.
2688   // We put the pinch point in _reg_node.  If there's already a pinch point
2689   // we merely add an edge from the current DEF/KILL to the pinch point.
2690 
2691   // After doing the DEF/KILLs, we handle USEs.  For each used register, we
2692   // put an edge from the pinch point to the USE.
2693 
2694   // To be expedient, the _reg_node array is pre-allocated for the whole
2695   // compilation.  _reg_node is lazily initialized; it either contains a NULL,
2696   // or a valid def/kill/pinch-point, or a leftover node from some prior
2697   // block.  Leftover node from some prior block is treated like a NULL (no
2698   // prior def, so no anti-dependence needed).  Valid def is distinguished by
2699   // it being in the current block.
2700   bool fat_proj_seen = false;
2701   uint last_safept = _bb_end-1;
2702   Node* end_node         = (_bb_end-1 &gt;= _bb_start) ? b-&gt;get_node(last_safept) : NULL;
2703   Node* last_safept_node = end_node;
2704   for( uint i = _bb_end-1; i &gt;= _bb_start; i-- ) {
2705     Node *n = b-&gt;get_node(i);
2706     int is_def = n-&gt;outcnt();   // def if some uses prior to adding precedence edges
2707     if( n-&gt;is_MachProj() &amp;&amp; n-&gt;ideal_reg() == MachProjNode::fat_proj ) {
2708       // Fat-proj kills a slew of registers
2709       // This can add edges to 'n' and obscure whether or not it was a def,
2710       // hence the is_def flag.
2711       fat_proj_seen = true;
2712       RegMask rm = n-&gt;out_RegMask();// Make local copy
2713       while( rm.is_NotEmpty() ) {
2714         OptoReg::Name kill = rm.find_first_elem();
2715         rm.Remove(kill);
2716         anti_do_def( b, n, kill, is_def );
2717       }
2718     } else {
2719       // Get DEF'd registers the normal way
2720       anti_do_def( b, n, _regalloc-&gt;get_reg_first(n), is_def );
2721       anti_do_def( b, n, _regalloc-&gt;get_reg_second(n), is_def );
2722     }
2723 
2724     // Kill projections on a branch should appear to occur on the
2725     // branch, not afterwards, so grab the masks from the projections
2726     // and process them.
2727     if (n-&gt;is_MachBranch() || (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Jump)) {
2728       for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
2729         Node* use = n-&gt;fast_out(i);
2730         if (use-&gt;is_Proj()) {
2731           RegMask rm = use-&gt;out_RegMask();// Make local copy
2732           while( rm.is_NotEmpty() ) {
2733             OptoReg::Name kill = rm.find_first_elem();
2734             rm.Remove(kill);
2735             anti_do_def( b, n, kill, false );
2736           }
2737         }
2738       }
2739     }
2740 
2741     // Check each register used by this instruction for a following DEF/KILL
2742     // that must occur afterward and requires an anti-dependence edge.
2743     for( uint j=0; j&lt;n-&gt;req(); j++ ) {
2744       Node *def = n-&gt;in(j);
2745       if( def ) {
2746         assert( !def-&gt;is_MachProj() || def-&gt;ideal_reg() != MachProjNode::fat_proj, "" );
2747         anti_do_use( b, n, _regalloc-&gt;get_reg_first(def) );
2748         anti_do_use( b, n, _regalloc-&gt;get_reg_second(def) );
2749       }
2750     }
2751     // Do not allow defs of new derived values to float above GC
2752     // points unless the base is definitely available at the GC point.
2753 
2754     Node *m = b-&gt;get_node(i);
2755 
2756     // Add precedence edge from following safepoint to use of derived pointer
2757     if( last_safept_node != end_node &amp;&amp;
2758         m != last_safept_node) {
2759       for (uint k = 1; k &lt; m-&gt;req(); k++) {
2760         const Type *t = m-&gt;in(k)-&gt;bottom_type();
2761         if( t-&gt;isa_oop_ptr() &amp;&amp;
2762             t-&gt;is_ptr()-&gt;offset() != 0 ) {
2763           last_safept_node-&gt;add_prec( m );
2764           break;
2765         }
2766       }
2767     }
2768 
2769     if( n-&gt;jvms() ) {           // Precedence edge from derived to safept
2770       // Check if last_safept_node was moved by pinch-point insertion in anti_do_use()
2771       if( b-&gt;get_node(last_safept) != last_safept_node ) {
2772         last_safept = b-&gt;find_node(last_safept_node);
2773       }
2774       for( uint j=last_safept; j &gt; i; j-- ) {
2775         Node *mach = b-&gt;get_node(j);
2776         if( mach-&gt;is_Mach() &amp;&amp; mach-&gt;as_Mach()-&gt;ideal_Opcode() == Op_AddP )
2777           mach-&gt;add_prec( n );
2778       }
2779       last_safept = i;
2780       last_safept_node = m;
2781     }
2782   }
2783 
2784   if (fat_proj_seen) {
2785     // Garbage collect pinch nodes that were not consumed.
2786     // They are usually created by a fat kill MachProj for a call.
2787     garbage_collect_pinch_nodes();
2788   }
2789 }
2790 
2791 // Garbage collect pinch nodes for reuse by other blocks.
2792 //
2793 // The block scheduler's insertion of anti-dependence
2794 // edges creates many pinch nodes when the block contains
2795 // 2 or more Calls.  A pinch node is used to prevent a
2796 // combinatorial explosion of edges.  If a set of kills for a
2797 // register is anti-dependent on a set of uses (or defs), rather
2798 // than adding an edge in the graph between each pair of kill
2799 // and use (or def), a pinch is inserted between them:
2800 //
2801 //            use1   use2  use3
2802 //                \   |   /
2803 //                 \  |  /
2804 //                  pinch
2805 //                 /  |  \
2806 //                /   |   \
2807 //            kill1 kill2 kill3
2808 //
2809 // One pinch node is created per register killed when
2810 // the second call is encountered during a backwards pass
2811 // over the block.  Most of these pinch nodes are never
2812 // wired into the graph because the register is never
2813 // used or def'ed in the block.
2814 //
2815 void Scheduling::garbage_collect_pinch_nodes() {
2816 #ifndef PRODUCT
2817     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print("Reclaimed pinch nodes:");
2818 #endif
2819     int trace_cnt = 0;
2820     for (uint k = 0; k &lt; _reg_node.Size(); k++) {
2821       Node* pinch = _reg_node[k];
2822       if ((pinch != NULL) &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp;
2823           // no predecence input edges
2824           (pinch-&gt;req() == pinch-&gt;len() || pinch-&gt;in(pinch-&gt;req()) == NULL) ) {
2825         cleanup_pinch(pinch);
2826         _pinch_free_list.push(pinch);
2827         _reg_node.map(k, NULL);
2828 #ifndef PRODUCT
2829         if (_cfg-&gt;C-&gt;trace_opto_output()) {
2830           trace_cnt++;
2831           if (trace_cnt &gt; 40) {
2832             tty-&gt;print("\n");
2833             trace_cnt = 0;
2834           }
2835           tty-&gt;print(" %d", pinch-&gt;_idx);
2836         }
2837 #endif
2838       }
2839     }
2840 #ifndef PRODUCT
2841     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print("\n");
2842 #endif
2843 }
2844 
2845 // Clean up a pinch node for reuse.
2846 void Scheduling::cleanup_pinch( Node *pinch ) {
2847   assert (pinch &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp; pinch-&gt;req() == 1, "just checking");
2848 
2849   for (DUIterator_Last imin, i = pinch-&gt;last_outs(imin); i &gt;= imin; ) {
2850     Node* use = pinch-&gt;last_out(i);
2851     uint uses_found = 0;
2852     for (uint j = use-&gt;req(); j &lt; use-&gt;len(); j++) {
2853       if (use-&gt;in(j) == pinch) {
2854         use-&gt;rm_prec(j);
2855         uses_found++;
2856       }
2857     }
2858     assert(uses_found &gt; 0, "must be a precedence edge");
2859     i -= uses_found;    // we deleted 1 or more copies of this edge
2860   }
2861   // May have a later_def entry
2862   pinch-&gt;set_req(0, NULL);
2863 }
2864 
2865 #ifndef PRODUCT
2866 
2867 void Scheduling::dump_available() const {
2868   tty-&gt;print("#Availist  ");
2869   for (uint i = 0; i &lt; _available.size(); i++)
2870     tty-&gt;print(" N%d/l%d", _available[i]-&gt;_idx,_current_latency[_available[i]-&gt;_idx]);
2871   tty-&gt;cr();
2872 }
2873 
2874 // Print Scheduling Statistics
2875 void Scheduling::print_statistics() {
2876   // Print the size added by nops for bundling
2877   tty-&gt;print("Nops added %d bytes to total of %d bytes",
2878     _total_nop_size, _total_method_size);
2879   if (_total_method_size &gt; 0)
2880     tty-&gt;print(", for %.2f%%",
2881       ((double)_total_nop_size) / ((double) _total_method_size) * 100.0);
2882   tty-&gt;print("\n");
2883 
2884   // Print the number of branch shadows filled
2885   if (Pipeline::_branch_has_delay_slot) {
2886     tty-&gt;print("Of %d branches, %d had unconditional delay slots filled",
2887       _total_branches, _total_unconditional_delays);
2888     if (_total_branches &gt; 0)
2889       tty-&gt;print(", for %.2f%%",
2890         ((double)_total_unconditional_delays) / ((double)_total_branches) * 100.0);
2891     tty-&gt;print("\n");
2892   }
2893 
2894   uint total_instructions = 0, total_bundles = 0;
2895 
2896   for (uint i = 1; i &lt;= Pipeline::_max_instrs_per_cycle; i++) {
2897     uint bundle_count   = _total_instructions_per_bundle[i];
2898     total_instructions += bundle_count * i;
2899     total_bundles      += bundle_count;
2900   }
2901 
2902   if (total_bundles &gt; 0)
2903     tty-&gt;print("Average ILP (excluding nops) is %.2f\n",
2904       ((double)total_instructions) / ((double)total_bundles));
2905 }
2906 #endif
</pre></body></html>
