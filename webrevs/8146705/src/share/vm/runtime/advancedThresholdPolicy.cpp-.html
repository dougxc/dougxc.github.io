<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/runtime/advancedThresholdPolicy.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2010, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "code/codeCache.hpp"
  27 #include "compiler/compileTask.hpp"
  28 #include "runtime/advancedThresholdPolicy.hpp"
  29 #include "runtime/simpleThresholdPolicy.inline.hpp"
  30 
  31 #ifdef TIERED
  32 // Print an event.
  33 void AdvancedThresholdPolicy::print_specific(EventType type, methodHandle mh, methodHandle imh,
  34                                              int bci, CompLevel level) {
  35   tty-&gt;print(" rate=");
  36   if (mh-&gt;prev_time() == 0) tty-&gt;print("n/a");
  37   else tty-&gt;print("%f", mh-&gt;rate());
  38 
  39   tty-&gt;print(" k=%.2lf,%.2lf", threshold_scale(CompLevel_full_profile, Tier3LoadFeedback),
  40                                threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback));
  41 
  42 }
  43 
  44 void AdvancedThresholdPolicy::initialize() {
  45   // Turn on ergonomic compiler count selection
  46   if (FLAG_IS_DEFAULT(CICompilerCountPerCPU) &amp;&amp; FLAG_IS_DEFAULT(CICompilerCount)) {
  47     FLAG_SET_DEFAULT(CICompilerCountPerCPU, true);
  48   }
  49   int count = CICompilerCount;
  50   if (CICompilerCountPerCPU) {
  51     // Simple log n seems to grow too slowly for tiered, try something faster: log n * log log n
  52     int log_cpu = log2_intptr(os::active_processor_count());
  53     int loglog_cpu = log2_intptr(MAX2(log_cpu, 1));
  54     count = MAX2(log_cpu * loglog_cpu, 1) * 3 / 2;
  55   }
  56 
  57   set_c1_count(MAX2(count / 3, 1));
  58   set_c2_count(MAX2(count - c1_count(), 1));
  59   FLAG_SET_ERGO(intx, CICompilerCount, c1_count() + c2_count());
  60 
  61   // Some inlining tuning
  62 #ifdef X86
  63   if (FLAG_IS_DEFAULT(InlineSmallCode)) {
  64     FLAG_SET_DEFAULT(InlineSmallCode, 2000);
  65   }
  66 #endif
  67 
  68 #if defined SPARC || defined AARCH64
  69   if (FLAG_IS_DEFAULT(InlineSmallCode)) {
  70     FLAG_SET_DEFAULT(InlineSmallCode, 2500);
  71   }
  72 #endif
  73 
  74   set_increase_threshold_at_ratio();
  75   set_start_time(os::javaTimeMillis());
  76 }
  77 
  78 // update_rate() is called from select_task() while holding a compile queue lock.
  79 void AdvancedThresholdPolicy::update_rate(jlong t, Method* m) {
  80   // Skip update if counters are absent.
  81   // Can't allocate them since we are holding compile queue lock.
  82   if (m-&gt;method_counters() == NULL)  return;
  83 
  84   if (is_old(m)) {
  85     // We don't remove old methods from the queue,
  86     // so we can just zero the rate.
  87     m-&gt;set_rate(0);
  88     return;
  89   }
  90 
  91   // We don't update the rate if we've just came out of a safepoint.
  92   // delta_s is the time since last safepoint in milliseconds.
  93   jlong delta_s = t - SafepointSynchronize::end_of_last_safepoint();
  94   jlong delta_t = t - (m-&gt;prev_time() != 0 ? m-&gt;prev_time() : start_time()); // milliseconds since the last measurement
  95   // How many events were there since the last time?
  96   int event_count = m-&gt;invocation_count() + m-&gt;backedge_count();
  97   int delta_e = event_count - m-&gt;prev_event_count();
  98 
  99   // We should be running for at least 1ms.
 100   if (delta_s &gt;= TieredRateUpdateMinTime) {
 101     // And we must've taken the previous point at least 1ms before.
 102     if (delta_t &gt;= TieredRateUpdateMinTime &amp;&amp; delta_e &gt; 0) {
 103       m-&gt;set_prev_time(t);
 104       m-&gt;set_prev_event_count(event_count);
 105       m-&gt;set_rate((float)delta_e / (float)delta_t); // Rate is events per millisecond
 106     } else {
 107       if (delta_t &gt; TieredRateUpdateMaxTime &amp;&amp; delta_e == 0) {
 108         // If nothing happened for 25ms, zero the rate. Don't modify prev values.
 109         m-&gt;set_rate(0);
 110       }
 111     }
 112   }
 113 }
 114 
 115 // Check if this method has been stale from a given number of milliseconds.
 116 // See select_task().
 117 bool AdvancedThresholdPolicy::is_stale(jlong t, jlong timeout, Method* m) {
 118   jlong delta_s = t - SafepointSynchronize::end_of_last_safepoint();
 119   jlong delta_t = t - m-&gt;prev_time();
 120   if (delta_t &gt; timeout &amp;&amp; delta_s &gt; timeout) {
 121     int event_count = m-&gt;invocation_count() + m-&gt;backedge_count();
 122     int delta_e = event_count - m-&gt;prev_event_count();
 123     // Return true if there were no events.
 124     return delta_e == 0;
 125   }
 126   return false;
 127 }
 128 
 129 // We don't remove old methods from the compile queue even if they have
 130 // very low activity. See select_task().
 131 bool AdvancedThresholdPolicy::is_old(Method* method) {
 132   return method-&gt;invocation_count() &gt; 50000 || method-&gt;backedge_count() &gt; 500000;
 133 }
 134 
 135 double AdvancedThresholdPolicy::weight(Method* method) {
 136   return (double)(method-&gt;rate() + 1) *
 137     (method-&gt;invocation_count() + 1) * (method-&gt;backedge_count() + 1);
 138 }
 139 
 140 // Apply heuristics and return true if x should be compiled before y
 141 bool AdvancedThresholdPolicy::compare_methods(Method* x, Method* y) {
 142   if (x-&gt;highest_comp_level() &gt; y-&gt;highest_comp_level()) {
 143     // recompilation after deopt
 144     return true;
 145   } else
 146     if (x-&gt;highest_comp_level() == y-&gt;highest_comp_level()) {
 147       if (weight(x) &gt; weight(y)) {
 148         return true;
 149       }
 150     }
 151   return false;
 152 }
 153 
 154 // Is method profiled enough?
 155 bool AdvancedThresholdPolicy::is_method_profiled(Method* method) {
 156   MethodData* mdo = method-&gt;method_data();
 157   if (mdo != NULL) {
 158     int i = mdo-&gt;invocation_count_delta();
 159     int b = mdo-&gt;backedge_count_delta();
 160     return call_predicate_helper&lt;CompLevel_full_profile&gt;(i, b, 1, method);
 161   }
 162   return false;
 163 }
 164 
 165 // Called with the queue locked and with at least one element
 166 CompileTask* AdvancedThresholdPolicy::select_task(CompileQueue* compile_queue) {
 167 #if INCLUDE_JVMCI
 168   CompileTask *max_non_jvmci_task = NULL;
 169 #endif
 170   CompileTask *max_task = NULL;
 171   Method* max_method = NULL;
 172   jlong t = os::javaTimeMillis();
 173   // Iterate through the queue and find a method with a maximum rate.
 174   for (CompileTask* task = compile_queue-&gt;first(); task != NULL;) {
 175     CompileTask* next_task = task-&gt;next();
 176     Method* method = task-&gt;method();
 177     update_rate(t, method);
 178     if (max_task == NULL) {
 179       max_task = task;
 180       max_method = method;
 181     } else {
 182       // If a method has been stale for some time, remove it from the queue.
 183       if (is_stale(t, TieredCompileTaskTimeout, method) &amp;&amp; !is_old(method)) {
 184         if (PrintTieredEvents) {
 185           print_event(REMOVE_FROM_QUEUE, method, method, task-&gt;osr_bci(), (CompLevel)task-&gt;comp_level());
 186         }
 187         task-&gt;log_task_dequeued("stale");
 188         compile_queue-&gt;remove_and_mark_stale(task);
 189         method-&gt;clear_queued_for_compilation();
 190         task = next_task;
 191         continue;
 192       }
 193 
 194       // Select a method with a higher rate
 195       if (compare_methods(method, max_method)) {
 196         max_task = task;
 197         max_method = method;
 198       }
 199     }
 200     task = next_task;
 201   }
 202 
 203 #if INCLUDE_JVMCI
 204   if (UseJVMCICompiler) {
 205     if (max_non_jvmci_task != NULL) {
 206       max_task = max_non_jvmci_task;
 207       max_method = max_task-&gt;method();
 208     }
 209   }
 210 #endif
 211 
 212   if (max_task-&gt;comp_level() == CompLevel_full_profile &amp;&amp; TieredStopAtLevel &gt; CompLevel_full_profile
 213       &amp;&amp; is_method_profiled(max_method)) {
 214     max_task-&gt;set_comp_level(CompLevel_limited_profile);
 215     if (PrintTieredEvents) {
 216       print_event(UPDATE_IN_QUEUE, max_method, max_method, max_task-&gt;osr_bci(), (CompLevel)max_task-&gt;comp_level());
 217     }
 218   }
 219 
 220   return max_task;
 221 }
 222 
 223 double AdvancedThresholdPolicy::threshold_scale(CompLevel level, int feedback_k) {
 224   double queue_size = CompileBroker::queue_size(level);
 225   int comp_count = compiler_count(level);
 226   double k = queue_size / (feedback_k * comp_count) + 1;
 227 
 228   // Increase C1 compile threshold when the code cache is filled more
 229   // than specified by IncreaseFirstTierCompileThresholdAt percentage.
 230   // The main intention is to keep enough free space for C2 compiled code
 231   // to achieve peak performance if the code cache is under stress.
 232   if ((TieredStopAtLevel == CompLevel_full_optimization) &amp;&amp; (level != CompLevel_full_optimization))  {
 233     double current_reverse_free_ratio = CodeCache::reverse_free_ratio(CodeCache::get_code_blob_type(level));
 234     if (current_reverse_free_ratio &gt; _increase_threshold_at_ratio) {
 235       k *= exp(current_reverse_free_ratio - _increase_threshold_at_ratio);
 236     }
 237   }
 238   return k;
 239 }
 240 
 241 // Call and loop predicates determine whether a transition to a higher
 242 // compilation level should be performed (pointers to predicate functions
 243 // are passed to common()).
 244 // Tier?LoadFeedback is basically a coefficient that determines of
 245 // how many methods per compiler thread can be in the queue before
 246 // the threshold values double.
 247 bool AdvancedThresholdPolicy::loop_predicate(int i, int b, CompLevel cur_level, Method* method) {
 248   switch(cur_level) {
 249   case CompLevel_none:
 250   case CompLevel_limited_profile: {
 251     double k = threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);
 252     return loop_predicate_helper&lt;CompLevel_none&gt;(i, b, k, method);
 253   }
 254   case CompLevel_full_profile: {
 255     double k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);
 256     return loop_predicate_helper&lt;CompLevel_full_profile&gt;(i, b, k, method);
 257   }
 258   default:
 259     return true;
 260   }
 261 }
 262 
 263 bool AdvancedThresholdPolicy::call_predicate(int i, int b, CompLevel cur_level, Method* method) {
 264   switch(cur_level) {
 265   case CompLevel_none:
 266   case CompLevel_limited_profile: {
 267     double k = threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);
 268     return call_predicate_helper&lt;CompLevel_none&gt;(i, b, k, method);
 269   }
 270   case CompLevel_full_profile: {
 271     double k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);
 272     return call_predicate_helper&lt;CompLevel_full_profile&gt;(i, b, k, method);
 273   }
 274   default:
 275     return true;
 276   }
 277 }
 278 
 279 // If a method is old enough and is still in the interpreter we would want to
 280 // start profiling without waiting for the compiled method to arrive.
 281 // We also take the load on compilers into the account.
 282 bool AdvancedThresholdPolicy::should_create_mdo(Method* method, CompLevel cur_level) {
 283   if (cur_level == CompLevel_none &amp;&amp;
 284       CompileBroker::queue_size(CompLevel_full_optimization) &lt;=
 285       Tier3DelayOn * compiler_count(CompLevel_full_optimization)) {
 286     int i = method-&gt;invocation_count();
 287     int b = method-&gt;backedge_count();
 288     double k = Tier0ProfilingStartPercentage / 100.0;
 289     return call_predicate_helper&lt;CompLevel_none&gt;(i, b, k, method) || loop_predicate_helper&lt;CompLevel_none&gt;(i, b, k, method);
 290   }
 291   return false;
 292 }
 293 
 294 // Inlining control: if we're compiling a profiled method with C1 and the callee
 295 // is known to have OSRed in a C2 version, don't inline it.
 296 bool AdvancedThresholdPolicy::should_not_inline(ciEnv* env, ciMethod* callee) {
 297   CompLevel comp_level = (CompLevel)env-&gt;comp_level();
 298   if (comp_level == CompLevel_full_profile ||
 299       comp_level == CompLevel_limited_profile) {
 300     return callee-&gt;highest_osr_comp_level() == CompLevel_full_optimization;
 301   }
 302   return false;
 303 }
 304 
 305 // Create MDO if necessary.
 306 void AdvancedThresholdPolicy::create_mdo(methodHandle mh, JavaThread* THREAD) {
 307   if (mh-&gt;is_native() ||
 308       mh-&gt;is_abstract() ||
 309       mh-&gt;is_accessor() ||
 310       mh-&gt;is_constant_getter()) {
 311     return;
 312   }
 313   if (mh-&gt;method_data() == NULL) {
 314     Method::build_interpreter_method_data(mh, CHECK_AND_CLEAR);
 315   }
 316 }
 317 
 318 
 319 /*
 320  * Method states:
 321  *   0 - interpreter (CompLevel_none)
 322  *   1 - pure C1 (CompLevel_simple)
 323  *   2 - C1 with invocation and backedge counting (CompLevel_limited_profile)
 324  *   3 - C1 with full profiling (CompLevel_full_profile)
 325  *   4 - C2 (CompLevel_full_optimization)
 326  *
 327  * Common state transition patterns:
 328  * a. 0 -&gt; 3 -&gt; 4.
 329  *    The most common path. But note that even in this straightforward case
 330  *    profiling can start at level 0 and finish at level 3.
 331  *
 332  * b. 0 -&gt; 2 -&gt; 3 -&gt; 4.
 333  *    This case occurs when the load on C2 is deemed too high. So, instead of transitioning
 334  *    into state 3 directly and over-profiling while a method is in the C2 queue we transition to
 335  *    level 2 and wait until the load on C2 decreases. This path is disabled for OSRs.
 336  *
 337  * c. 0 -&gt; (3-&gt;2) -&gt; 4.
 338  *    In this case we enqueue a method for compilation at level 3, but the C1 queue is long enough
 339  *    to enable the profiling to fully occur at level 0. In this case we change the compilation level
 340  *    of the method to 2 while the request is still in-queue, because it'll allow it to run much faster
 341  *    without full profiling while c2 is compiling.
 342  *
 343  * d. 0 -&gt; 3 -&gt; 1 or 0 -&gt; 2 -&gt; 1.
 344  *    After a method was once compiled with C1 it can be identified as trivial and be compiled to
 345  *    level 1. These transition can also occur if a method can't be compiled with C2 but can with C1.
 346  *
 347  * e. 0 -&gt; 4.
 348  *    This can happen if a method fails C1 compilation (it will still be profiled in the interpreter)
 349  *    or because of a deopt that didn't require reprofiling (compilation won't happen in this case because
 350  *    the compiled version already exists).
 351  *
 352  * Note that since state 0 can be reached from any other state via deoptimization different loops
 353  * are possible.
 354  *
 355  */
 356 
 357 // Common transition function. Given a predicate determines if a method should transition to another level.
 358 CompLevel AdvancedThresholdPolicy::common(Predicate p, Method* method, CompLevel cur_level, bool disable_feedback) {
 359   CompLevel next_level = cur_level;
 360   int i = method-&gt;invocation_count();
 361   int b = method-&gt;backedge_count();
 362 
 363   if (is_trivial(method)) {
 364     next_level = CompLevel_simple;
 365   } else {
 366     switch(cur_level) {
 367     case CompLevel_none:
 368       // If we were at full profile level, would we switch to full opt?
 369       if (common(p, method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {
 370         next_level = CompLevel_full_optimization;
 371       } else if ((this-&gt;*p)(i, b, cur_level, method)) {
 372 #if INCLUDE_JVMCI
 373         if (UseJVMCICompiler) {
 374           // Since JVMCI takes a while to warm up, its queue inevitably backs up during
 375           // early VM execution.
 376           next_level = CompLevel_full_profile;
 377           break;
 378         }
 379 #endif
 380         // C1-generated fully profiled code is about 30% slower than the limited profile
 381         // code that has only invocation and backedge counters. The observation is that
 382         // if C2 queue is large enough we can spend too much time in the fully profiled code
 383         // while waiting for C2 to pick the method from the queue. To alleviate this problem
 384         // we introduce a feedback on the C2 queue size. If the C2 queue is sufficiently long
 385         // we choose to compile a limited profiled version and then recompile with full profiling
 386         // when the load on C2 goes down.
 387         if (!disable_feedback &amp;&amp; CompileBroker::queue_size(CompLevel_full_optimization) &gt;
 388             Tier3DelayOn * compiler_count(CompLevel_full_optimization)) {
 389           next_level = CompLevel_limited_profile;
 390         } else {
 391           next_level = CompLevel_full_profile;
 392         }
 393       }
 394       break;
 395     case CompLevel_limited_profile:
 396       if (is_method_profiled(method)) {
 397         // Special case: we got here because this method was fully profiled in the interpreter.
 398         next_level = CompLevel_full_optimization;
 399       } else {
 400         MethodData* mdo = method-&gt;method_data();
 401         if (mdo != NULL) {
 402           if (mdo-&gt;would_profile()) {
 403             if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) &lt;=
 404                                      Tier3DelayOff * compiler_count(CompLevel_full_optimization) &amp;&amp;
 405                                      (this-&gt;*p)(i, b, cur_level, method))) {
 406               next_level = CompLevel_full_profile;
 407             }
 408           } else {
 409             next_level = CompLevel_full_optimization;
 410           }
 411         }
 412       }
 413       break;
 414     case CompLevel_full_profile:
 415       {
 416         MethodData* mdo = method-&gt;method_data();
 417         if (mdo != NULL) {
 418           if (mdo-&gt;would_profile()) {
 419             int mdo_i = mdo-&gt;invocation_count_delta();
 420             int mdo_b = mdo-&gt;backedge_count_delta();
 421             if ((this-&gt;*p)(mdo_i, mdo_b, cur_level, method)) {
 422               next_level = CompLevel_full_optimization;
 423             }
 424           } else {
 425             next_level = CompLevel_full_optimization;
 426           }
 427         }
 428       }
 429       break;
 430     }
 431   }
 432   return MIN2(next_level, (CompLevel)TieredStopAtLevel);
 433 }
 434 
 435 // Determine if a method should be compiled with a normal entry point at a different level.
 436 CompLevel AdvancedThresholdPolicy::call_event(Method* method, CompLevel cur_level) {
 437   CompLevel osr_level = MIN2((CompLevel) method-&gt;highest_osr_comp_level(),
 438                              common(&amp;AdvancedThresholdPolicy::loop_predicate, method, cur_level, true));
 439   CompLevel next_level = common(&amp;AdvancedThresholdPolicy::call_predicate, method, cur_level);
 440 
 441   // If OSR method level is greater than the regular method level, the levels should be
 442   // equalized by raising the regular method level in order to avoid OSRs during each
 443   // invocation of the method.
 444   if (osr_level == CompLevel_full_optimization &amp;&amp; cur_level == CompLevel_full_profile) {
 445     MethodData* mdo = method-&gt;method_data();
 446     guarantee(mdo != NULL, "MDO should not be NULL");
 447     if (mdo-&gt;invocation_count() &gt;= 1) {
 448       next_level = CompLevel_full_optimization;
 449     }
 450   } else {
 451     next_level = MAX2(osr_level, next_level);
 452   }
 453   return next_level;
 454 }
 455 
 456 // Determine if we should do an OSR compilation of a given method.
 457 CompLevel AdvancedThresholdPolicy::loop_event(Method* method, CompLevel cur_level) {
 458   CompLevel next_level = common(&amp;AdvancedThresholdPolicy::loop_predicate, method, cur_level, true);
 459   if (cur_level == CompLevel_none) {
 460     // If there is a live OSR method that means that we deopted to the interpreter
 461     // for the transition.
 462     CompLevel osr_level = MIN2((CompLevel)method-&gt;highest_osr_comp_level(), next_level);
 463     if (osr_level &gt; CompLevel_none) {
 464       return osr_level;
 465     }
 466   }
 467   return next_level;
 468 }
 469 
 470 // Update the rate and submit compile
 471 void AdvancedThresholdPolicy::submit_compile(const methodHandle&amp; mh, int bci, CompLevel level, JavaThread* thread) {
 472   int hot_count = (bci == InvocationEntryBci) ? mh-&gt;invocation_count() : mh-&gt;backedge_count();
 473   update_rate(os::javaTimeMillis(), mh());
 474   CompileBroker::compile_method(mh, bci, level, mh, hot_count, "tiered", thread);
 475 }
 476 
 477 // Handle the invocation event.
 478 void AdvancedThresholdPolicy::method_invocation_event(const methodHandle&amp; mh, const methodHandle&amp; imh,
 479                                                       CompLevel level, nmethod* nm, JavaThread* thread) {
 480   if (should_create_mdo(mh(), level)) {
 481     create_mdo(mh, thread);
 482   }
 483   if (is_compilation_enabled() &amp;&amp; !CompileBroker::compilation_is_in_queue(mh)) {
 484     CompLevel next_level = call_event(mh(), level);
 485     if (next_level != level) {
 486       compile(mh, InvocationEntryBci, next_level, thread);
 487     }
 488   }
 489 }
 490 
 491 // Handle the back branch event. Notice that we can compile the method
 492 // with a regular entry from here.
 493 void AdvancedThresholdPolicy::method_back_branch_event(const methodHandle&amp; mh, const methodHandle&amp; imh,
 494                                                        int bci, CompLevel level, nmethod* nm, JavaThread* thread) {
 495   if (should_create_mdo(mh(), level)) {
 496     create_mdo(mh, thread);
 497   }
 498   // Check if MDO should be created for the inlined method
 499   if (should_create_mdo(imh(), level)) {
 500     create_mdo(imh, thread);
 501   }
 502 
 503   if (is_compilation_enabled()) {
 504     CompLevel next_osr_level = loop_event(imh(), level);
 505     CompLevel max_osr_level = (CompLevel)imh-&gt;highest_osr_comp_level();
 506     // At the very least compile the OSR version
 507     if (!CompileBroker::compilation_is_in_queue(imh) &amp;&amp; (next_osr_level != level)) {
 508       compile(imh, bci, next_osr_level, thread);
 509     }
 510 
 511     // Use loop event as an opportunity to also check if there's been
 512     // enough calls.
 513     CompLevel cur_level, next_level;
 514     if (mh() != imh()) { // If there is an enclosing method
 515       guarantee(nm != NULL, "Should have nmethod here");
 516       cur_level = comp_level(mh());
 517       next_level = call_event(mh(), cur_level);
 518 
 519       if (max_osr_level == CompLevel_full_optimization) {
 520         // The inlinee OSRed to full opt, we need to modify the enclosing method to avoid deopts
 521         bool make_not_entrant = false;
 522         if (nm-&gt;is_osr_method()) {
 523           // This is an osr method, just make it not entrant and recompile later if needed
 524           make_not_entrant = true;
 525         } else {
 526           if (next_level != CompLevel_full_optimization) {
 527             // next_level is not full opt, so we need to recompile the
 528             // enclosing method without the inlinee
 529             cur_level = CompLevel_none;
 530             make_not_entrant = true;
 531           }
 532         }
 533         if (make_not_entrant) {
 534           if (PrintTieredEvents) {
 535             int osr_bci = nm-&gt;is_osr_method() ? nm-&gt;osr_entry_bci() : InvocationEntryBci;
 536             print_event(MAKE_NOT_ENTRANT, mh(), mh(), osr_bci, level);
 537           }
 538           nm-&gt;make_not_entrant();
 539         }
 540       }
 541       if (!CompileBroker::compilation_is_in_queue(mh)) {
 542         // Fix up next_level if necessary to avoid deopts
 543         if (next_level == CompLevel_limited_profile &amp;&amp; max_osr_level == CompLevel_full_profile) {
 544           next_level = CompLevel_full_profile;
 545         }
 546         if (cur_level != next_level) {
 547           compile(mh, InvocationEntryBci, next_level, thread);
 548         }
 549       }
 550     } else {
 551       cur_level = comp_level(imh());
 552       next_level = call_event(imh(), cur_level);
 553       if (!CompileBroker::compilation_is_in_queue(imh) &amp;&amp; (next_level != cur_level)) {
 554         compile(imh, InvocationEntryBci, next_level, thread);
 555       }
 556     }
 557   }
 558 }
 559 
 560 #endif // TIERED
</pre></body></html>
