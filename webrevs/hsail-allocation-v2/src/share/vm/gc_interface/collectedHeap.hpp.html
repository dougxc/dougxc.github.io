<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/gc_interface/collectedHeap.hpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2001, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_VM_GC_INTERFACE_COLLECTEDHEAP_HPP
  26 #define SHARE_VM_GC_INTERFACE_COLLECTEDHEAP_HPP
  27 
  28 #include "gc_interface/gcCause.hpp"
  29 #include "gc_implementation/shared/gcWhen.hpp"
  30 #include "memory/allocation.hpp"
  31 #include "memory/barrierSet.hpp"
  32 #include "runtime/handles.hpp"
  33 #include "runtime/perfData.hpp"
  34 #include "runtime/safepoint.hpp"
  35 #include "utilities/events.hpp"
  36 
  37 // A "CollectedHeap" is an implementation of a java heap for HotSpot.  This
  38 // is an abstract class: there may be many different kinds of heaps.  This
  39 // class defines the functions that a heap must implement, and contains
  40 // infrastructure common to all heaps.
  41 
  42 class AdaptiveSizePolicy;
  43 class BarrierSet;
  44 class CollectorPolicy;
  45 class GCHeapSummary;
  46 class GCTimer;
  47 class GCTracer;
  48 class MetaspaceSummary;
  49 class Thread;
  50 class ThreadClosure;
  51 class VirtualSpaceSummary;
  52 class nmethod;
  53 
  54 class GCMessage : public FormatBuffer&lt;1024&gt; {
  55  public:
  56   bool is_before;
  57 
  58  public:
  59   GCMessage() {}
  60 };
  61 
  62 class GCHeapLog : public EventLogBase&lt;GCMessage&gt; {
  63  private:
  64   void log_heap(bool before);
  65 
  66  public:
  67   GCHeapLog() : EventLogBase&lt;GCMessage&gt;("GC Heap History") {}
  68 
  69   void log_heap_before() {
  70     log_heap(true);
  71   }
  72   void log_heap_after() {
  73     log_heap(false);
  74   }
  75 };
  76 
  77 //
  78 // CollectedHeap
  79 //   SharedHeap
  80 //     GenCollectedHeap
  81 //     G1CollectedHeap
  82 //   ParallelScavengeHeap
  83 //
  84 class CollectedHeap : public CHeapObj&lt;mtInternal&gt; {
  85   friend class VMStructs;
  86   friend class IsGCActiveMark; // Block structured external access to _is_gc_active
  87   friend class Hsail;  // access to allocate_new_tlab
  88 
  89 #ifdef ASSERT
  90   static int       _fire_out_of_memory_count;
  91 #endif
  92 
  93   // Used for filler objects (static, but initialized in ctor).
  94   static size_t _filler_array_max_size;
  95 
  96   GCHeapLog* _gc_heap_log;
  97 
  98   // Used in support of ReduceInitialCardMarks; only consulted if COMPILER2 is being used
  99   bool _defer_initial_card_mark;
 100 
 101  protected:
 102   MemRegion _reserved;
 103   BarrierSet* _barrier_set;
 104   bool _is_gc_active;
 105   uint _n_par_threads;
 106 
 107   unsigned int _total_collections;          // ... started
 108   unsigned int _total_full_collections;     // ... started
 109   NOT_PRODUCT(volatile size_t _promotion_failure_alot_count;)
 110   NOT_PRODUCT(volatile size_t _promotion_failure_alot_gc_number;)
 111 
 112   // Reason for current garbage collection.  Should be set to
 113   // a value reflecting no collection between collections.
 114   GCCause::Cause _gc_cause;
 115   GCCause::Cause _gc_lastcause;
 116   PerfStringVariable* _perf_gc_cause;
 117   PerfStringVariable* _perf_gc_lastcause;
 118 
 119   // Constructor
 120   CollectedHeap();
 121 
 122   // Do common initializations that must follow instance construction,
 123   // for example, those needing virtual calls.
 124   // This code could perhaps be moved into initialize() but would
 125   // be slightly more awkward because we want the latter to be a
 126   // pure virtual.
 127   void pre_initialize();
 128 
 129   // Create a new tlab. All TLAB allocations must go through this.
 130   virtual HeapWord* allocate_new_tlab(size_t size);
 131 
 132   // Accumulate statistics on all tlabs.
 133   virtual void accumulate_statistics_all_tlabs();
 134 
 135   // Reinitialize tlabs before resuming mutators.
 136   virtual void resize_all_tlabs();
 137 
 138   // Allocate from the current thread's TLAB, with broken-out slow path.
 139   inline static HeapWord* allocate_from_tlab(KlassHandle klass, Thread* thread, size_t size);
 140   static HeapWord* allocate_from_tlab_slow(KlassHandle klass, Thread* thread, size_t size);
 141 
 142   // Allocate an uninitialized block of the given size, or returns NULL if
 143   // this is impossible.
 144   inline static HeapWord* common_mem_allocate_noinit(KlassHandle klass, size_t size, TRAPS);
 145 
 146   // Like allocate_init, but the block returned by a successful allocation
 147   // is guaranteed initialized to zeros.
 148   inline static HeapWord* common_mem_allocate_init(KlassHandle klass, size_t size, TRAPS);
 149 
 150   // Helper functions for (VM) allocation.
 151   inline static void post_allocation_setup_common(KlassHandle klass, HeapWord* obj);
 152   inline static void post_allocation_setup_no_klass_install(KlassHandle klass,
 153                                                             HeapWord* objPtr);
 154 
 155   inline static void post_allocation_setup_obj(KlassHandle klass, HeapWord* obj);
 156 
 157   inline static void post_allocation_setup_array(KlassHandle klass,
 158                                                  HeapWord* obj, int length);
 159 
 160   // Clears an allocated object.
 161   inline static void init_obj(HeapWord* obj, size_t size);
 162 
 163   // Filler object utilities.
 164   static inline size_t filler_array_hdr_size();
 165   static inline size_t filler_array_min_size();
 166 
 167   DEBUG_ONLY(static void fill_args_check(HeapWord* start, size_t words);)
 168   DEBUG_ONLY(static void zap_filler_array(HeapWord* start, size_t words, bool zap = true);)
 169 
 170   // Fill with a single array; caller must ensure filler_array_min_size() &lt;=
 171   // words &lt;= filler_array_max_size().
 172   static inline void fill_with_array(HeapWord* start, size_t words, bool zap = true);
 173 
 174   // Fill with a single object (either an int array or a java.lang.Object).
 175   static inline void fill_with_object_impl(HeapWord* start, size_t words, bool zap = true);
 176 
 177   virtual void trace_heap(GCWhen::Type when, GCTracer* tracer);
 178 
 179   // Verification functions
 180   virtual void check_for_bad_heap_word_value(HeapWord* addr, size_t size)
 181     PRODUCT_RETURN;
 182   virtual void check_for_non_bad_heap_word_value(HeapWord* addr, size_t size)
 183     PRODUCT_RETURN;
 184   debug_only(static void check_for_valid_allocation_state();)
 185 
 186  public:
 187   enum Name {
 188     Abstract,
 189     SharedHeap,
 190     GenCollectedHeap,
 191     ParallelScavengeHeap,
 192     G1CollectedHeap
 193   };
 194 
 195   static inline size_t filler_array_max_size() {
 196     return _filler_array_max_size;
 197   }
 198 
 199   virtual CollectedHeap::Name kind() const { return CollectedHeap::Abstract; }
 200 
 201   /**
 202    * Returns JNI error code JNI_ENOMEM if memory could not be allocated,
 203    * and JNI_OK on success.
 204    */
 205   virtual jint initialize() = 0;
 206 
 207   // In many heaps, there will be a need to perform some initialization activities
 208   // after the Universe is fully formed, but before general heap allocation is allowed.
 209   // This is the correct place to place such initialization methods.
 210   virtual void post_initialize() = 0;
 211 
 212   MemRegion reserved_region() const { return _reserved; }
 213   address base() const { return (address)reserved_region().start(); }
 214 
 215   virtual size_t capacity() const = 0;
 216   virtual size_t used() const = 0;
 217 
 218   // Return "true" if the part of the heap that allocates Java
 219   // objects has reached the maximal committed limit that it can
 220   // reach, without a garbage collection.
 221   virtual bool is_maximal_no_gc() const = 0;
 222 
 223   // Support for java.lang.Runtime.maxMemory():  return the maximum amount of
 224   // memory that the vm could make available for storing 'normal' java objects.
 225   // This is based on the reserved address space, but should not include space
 226   // that the vm uses internally for bookkeeping or temporary storage
 227   // (e.g., in the case of the young gen, one of the survivor
 228   // spaces).
 229   virtual size_t max_capacity() const = 0;
 230 
 231   // Returns "TRUE" if "p" points into the reserved area of the heap.
 232   bool is_in_reserved(const void* p) const {
 233     return _reserved.contains(p);
 234   }
 235 
 236   bool is_in_reserved_or_null(const void* p) const {
 237     return p == NULL || is_in_reserved(p);
 238   }
 239 
 240   // Returns "TRUE" iff "p" points into the committed areas of the heap.
 241   // Since this method can be expensive in general, we restrict its
 242   // use to assertion checking only.
 243   virtual bool is_in(const void* p) const = 0;
 244 
 245   bool is_in_or_null(const void* p) const {
 246     return p == NULL || is_in(p);
 247   }
 248 
 249   bool is_in_place(Metadata** p) {
 250     return !Universe::heap()-&gt;is_in(p);
 251   }
 252   bool is_in_place(oop* p) { return Universe::heap()-&gt;is_in(p); }
 253   bool is_in_place(narrowOop* p) {
 254     oop o = oopDesc::load_decode_heap_oop_not_null(p);
 255     return Universe::heap()-&gt;is_in((const void*)o);
 256   }
 257 
 258   // Let's define some terms: a "closed" subset of a heap is one that
 259   //
 260   // 1) contains all currently-allocated objects, and
 261   //
 262   // 2) is closed under reference: no object in the closed subset
 263   //    references one outside the closed subset.
 264   //
 265   // Membership in a heap's closed subset is useful for assertions.
 266   // Clearly, the entire heap is a closed subset, so the default
 267   // implementation is to use "is_in_reserved".  But this may not be too
 268   // liberal to perform useful checking.  Also, the "is_in" predicate
 269   // defines a closed subset, but may be too expensive, since "is_in"
 270   // verifies that its argument points to an object head.  The
 271   // "closed_subset" method allows a heap to define an intermediate
 272   // predicate, allowing more precise checking than "is_in_reserved" at
 273   // lower cost than "is_in."
 274 
 275   // One important case is a heap composed of disjoint contiguous spaces,
 276   // such as the Garbage-First collector.  Such heaps have a convenient
 277   // closed subset consisting of the allocated portions of those
 278   // contiguous spaces.
 279 
 280   // Return "TRUE" iff the given pointer points into the heap's defined
 281   // closed subset (which defaults to the entire heap).
 282   virtual bool is_in_closed_subset(const void* p) const {
 283     return is_in_reserved(p);
 284   }
 285 
 286   bool is_in_closed_subset_or_null(const void* p) const {
 287     return p == NULL || is_in_closed_subset(p);
 288   }
 289 
 290 #ifdef ASSERT
 291   // Returns true if "p" is in the part of the
 292   // heap being collected.
 293   virtual bool is_in_partial_collection(const void *p) = 0;
 294 #endif
 295 
 296   // An object is scavengable if its location may move during a scavenge.
 297   // (A scavenge is a GC which is not a full GC.)
 298   virtual bool is_scavengable(const void *p) = 0;
 299 
 300   void set_gc_cause(GCCause::Cause v) {
 301      if (UsePerfData) {
 302        _gc_lastcause = _gc_cause;
 303        _perf_gc_lastcause-&gt;set_value(GCCause::to_string(_gc_lastcause));
 304        _perf_gc_cause-&gt;set_value(GCCause::to_string(v));
 305      }
 306     _gc_cause = v;
 307   }
 308   GCCause::Cause gc_cause() { return _gc_cause; }
 309 
 310   // Number of threads currently working on GC tasks.
 311   uint n_par_threads() { return _n_par_threads; }
 312 
 313   // May be overridden to set additional parallelism.
 314   virtual void set_par_threads(uint t) { _n_par_threads = t; };
 315 
 316   // Allocate and initialize instances of Class
 317   static oop Class_obj_allocate(KlassHandle klass, int size, KlassHandle real_klass, TRAPS);
 318 
 319   // General obj/array allocation facilities.
 320   inline static oop obj_allocate(KlassHandle klass, int size, TRAPS);
 321   inline static oop array_allocate(KlassHandle klass, int size, int length, TRAPS);
 322   inline static oop array_allocate_nozero(KlassHandle klass, int size, int length, TRAPS);
 323 
 324   inline static void post_allocation_install_obj_klass(KlassHandle klass,
 325                                                        oop obj);
 326 
 327   // Raw memory allocation facilities
 328   // The obj and array allocate methods are covers for these methods.
 329   // mem_allocate() should never be
 330   // called to allocate TLABs, only individual objects.
 331   virtual HeapWord* mem_allocate(size_t size,
 332                                  bool* gc_overhead_limit_was_exceeded) = 0;
 333 
 334   // Utilities for turning raw memory into filler objects.
 335   //
 336   // min_fill_size() is the smallest region that can be filled.
 337   // fill_with_objects() can fill arbitrary-sized regions of the heap using
 338   // multiple objects.  fill_with_object() is for regions known to be smaller
 339   // than the largest array of integers; it uses a single object to fill the
 340   // region and has slightly less overhead.
 341   static size_t min_fill_size() {
 342     return size_t(align_object_size(oopDesc::header_size()));
 343   }
 344 
 345   static void fill_with_objects(HeapWord* start, size_t words, bool zap = true);
 346 
 347   static void fill_with_object(HeapWord* start, size_t words, bool zap = true);
 348   static void fill_with_object(MemRegion region, bool zap = true) {
 349     fill_with_object(region.start(), region.word_size(), zap);
 350   }
 351   static void fill_with_object(HeapWord* start, HeapWord* end, bool zap = true) {
 352     fill_with_object(start, pointer_delta(end, start), zap);
 353   }
 354 
 355   // Some heaps may offer a contiguous region for shared non-blocking
 356   // allocation, via inlined code (by exporting the address of the top and
 357   // end fields defining the extent of the contiguous allocation region.)
 358 
 359   // This function returns "true" iff the heap supports this kind of
 360   // allocation.  (Default is "no".)
 361   virtual bool supports_inline_contig_alloc() const {
 362     return false;
 363   }
 364   // These functions return the addresses of the fields that define the
 365   // boundaries of the contiguous allocation area.  (These fields should be
 366   // physically near to one another.)
 367   virtual HeapWord** top_addr() const {
 368     guarantee(false, "inline contiguous allocation not supported");
 369     return NULL;
 370   }
 371   virtual HeapWord** end_addr() const {
 372     guarantee(false, "inline contiguous allocation not supported");
 373     return NULL;
 374   }
 375 
 376   // Some heaps may be in an unparseable state at certain times between
 377   // collections. This may be necessary for efficient implementation of
 378   // certain allocation-related activities. Calling this function before
 379   // attempting to parse a heap ensures that the heap is in a parsable
 380   // state (provided other concurrent activity does not introduce
 381   // unparsability). It is normally expected, therefore, that this
 382   // method is invoked with the world stopped.
 383   // NOTE: if you override this method, make sure you call
 384   // super::ensure_parsability so that the non-generational
 385   // part of the work gets done. See implementation of
 386   // CollectedHeap::ensure_parsability and, for instance,
 387   // that of GenCollectedHeap::ensure_parsability().
 388   // The argument "retire_tlabs" controls whether existing TLABs
 389   // are merely filled or also retired, thus preventing further
 390   // allocation from them and necessitating allocation of new TLABs.
 391   virtual void ensure_parsability(bool retire_tlabs);
 392 
 393   // Return an estimate of the maximum allocation that could be performed
 394   // without triggering any collection or expansion activity.  In a
 395   // generational collector, for example, this is probably the largest
 396   // allocation that could be supported (without expansion) in the youngest
 397   // generation.  It is "unsafe" because no locks are taken; the result
 398   // should be treated as an approximation, not a guarantee, for use in
 399   // heuristic resizing decisions.
 400   virtual size_t unsafe_max_alloc() = 0;
 401 
 402   // Section on thread-local allocation buffers (TLABs)
 403   // If the heap supports thread-local allocation buffers, it should override
 404   // the following methods:
 405   // Returns "true" iff the heap supports thread-local allocation buffers.
 406   // The default is "no".
 407   virtual bool supports_tlab_allocation() const {
 408     return false;
 409   }
 410   // The amount of space available for thread-local allocation buffers.
 411   virtual size_t tlab_capacity(Thread *thr) const {
 412     guarantee(false, "thread-local allocation buffers not supported");
 413     return 0;
 414   }
 415   // An estimate of the maximum allocation that could be performed
 416   // for thread-local allocation buffers without triggering any
 417   // collection or expansion activity.
 418   virtual size_t unsafe_max_tlab_alloc(Thread *thr) const {
 419     guarantee(false, "thread-local allocation buffers not supported");
 420     return 0;
 421   }
 422 
 423   // Can a compiler initialize a new object without store barriers?
 424   // This permission only extends from the creation of a new object
 425   // via a TLAB up to the first subsequent safepoint. If such permission
 426   // is granted for this heap type, the compiler promises to call
 427   // defer_store_barrier() below on any slow path allocation of
 428   // a new object for which such initializing store barriers will
 429   // have been elided.
 430   virtual bool can_elide_tlab_store_barriers() const = 0;
 431 
 432   // If a compiler is eliding store barriers for TLAB-allocated objects,
 433   // there is probably a corresponding slow path which can produce
 434   // an object allocated anywhere.  The compiler's runtime support
 435   // promises to call this function on such a slow-path-allocated
 436   // object before performing initializations that have elided
 437   // store barriers. Returns new_obj, or maybe a safer copy thereof.
 438   virtual oop new_store_pre_barrier(JavaThread* thread, oop new_obj);
 439 
 440   // Answers whether an initializing store to a new object currently
 441   // allocated at the given address doesn't need a store
 442   // barrier. Returns "true" if it doesn't need an initializing
 443   // store barrier; answers "false" if it does.
 444   virtual bool can_elide_initializing_store_barrier(oop new_obj) = 0;
 445 
 446   // If a compiler is eliding store barriers for TLAB-allocated objects,
 447   // we will be informed of a slow-path allocation by a call
 448   // to new_store_pre_barrier() above. Such a call precedes the
 449   // initialization of the object itself, and no post-store-barriers will
 450   // be issued. Some heap types require that the barrier strictly follows
 451   // the initializing stores. (This is currently implemented by deferring the
 452   // barrier until the next slow-path allocation or gc-related safepoint.)
 453   // This interface answers whether a particular heap type needs the card
 454   // mark to be thus strictly sequenced after the stores.
 455   virtual bool card_mark_must_follow_store() const = 0;
 456 
 457   // If the CollectedHeap was asked to defer a store barrier above,
 458   // this informs it to flush such a deferred store barrier to the
 459   // remembered set.
 460   virtual void flush_deferred_store_barrier(JavaThread* thread);
 461 
 462   // Does this heap support heap inspection (+PrintClassHistogram?)
 463   virtual bool supports_heap_inspection() const = 0;
 464 
 465   // Perform a collection of the heap; intended for use in implementing
 466   // "System.gc".  This probably implies as full a collection as the
 467   // "CollectedHeap" supports.
 468   virtual void collect(GCCause::Cause cause) = 0;
 469 
 470   // Perform a full collection
 471   virtual void do_full_collection(bool clear_all_soft_refs) = 0;
 472 
 473   // This interface assumes that it's being called by the
 474   // vm thread. It collects the heap assuming that the
 475   // heap lock is already held and that we are executing in
 476   // the context of the vm thread.
 477   virtual void collect_as_vm_thread(GCCause::Cause cause);
 478 
 479   // Returns the barrier set for this heap
 480   BarrierSet* barrier_set() { return _barrier_set; }
 481 
 482   // Returns "true" iff there is a stop-world GC in progress.  (I assume
 483   // that it should answer "false" for the concurrent part of a concurrent
 484   // collector -- dld).
 485   bool is_gc_active() const { return _is_gc_active; }
 486 
 487   // Total number of GC collections (started)
 488   unsigned int total_collections() const { return _total_collections; }
 489   unsigned int total_full_collections() const { return _total_full_collections;}
 490 
 491   // Increment total number of GC collections (started)
 492   // Should be protected but used by PSMarkSweep - cleanup for 1.4.2
 493   void increment_total_collections(bool full = false) {
 494     _total_collections++;
 495     if (full) {
 496       increment_total_full_collections();
 497     }
 498   }
 499 
 500   void increment_total_full_collections() { _total_full_collections++; }
 501 
 502   // Return the AdaptiveSizePolicy for the heap.
 503   virtual AdaptiveSizePolicy* size_policy() = 0;
 504 
 505   // Return the CollectorPolicy for the heap
 506   virtual CollectorPolicy* collector_policy() const = 0;
 507 
 508   void oop_iterate_no_header(OopClosure* cl);
 509 
 510   // Iterate over all the ref-containing fields of all objects, calling
 511   // "cl.do_oop" on each.
 512   virtual void oop_iterate(ExtendedOopClosure* cl) = 0;
 513 
 514   // Iterate over all objects, calling "cl.do_object" on each.
 515   virtual void object_iterate(ObjectClosure* cl) = 0;
 516 
 517   // Similar to object_iterate() except iterates only
 518   // over live objects.
 519   virtual void safe_object_iterate(ObjectClosure* cl) = 0;
 520 
 521   // NOTE! There is no requirement that a collector implement these
 522   // functions.
 523   //
 524   // A CollectedHeap is divided into a dense sequence of "blocks"; that is,
 525   // each address in the (reserved) heap is a member of exactly
 526   // one block.  The defining characteristic of a block is that it is
 527   // possible to find its size, and thus to progress forward to the next
 528   // block.  (Blocks may be of different sizes.)  Thus, blocks may
 529   // represent Java objects, or they might be free blocks in a
 530   // free-list-based heap (or subheap), as long as the two kinds are
 531   // distinguishable and the size of each is determinable.
 532 
 533   // Returns the address of the start of the "block" that contains the
 534   // address "addr".  We say "blocks" instead of "object" since some heaps
 535   // may not pack objects densely; a chunk may either be an object or a
 536   // non-object.
 537   virtual HeapWord* block_start(const void* addr) const = 0;
 538 
 539   // Requires "addr" to be the start of a chunk, and returns its size.
 540   // "addr + size" is required to be the start of a new chunk, or the end
 541   // of the active area of the heap.
 542   virtual size_t block_size(const HeapWord* addr) const = 0;
 543 
 544   // Requires "addr" to be the start of a block, and returns "TRUE" iff
 545   // the block is an object.
 546   virtual bool block_is_obj(const HeapWord* addr) const = 0;
 547 
 548   // Returns the longest time (in ms) that has elapsed since the last
 549   // time that any part of the heap was examined by a garbage collection.
 550   virtual jlong millis_since_last_gc() = 0;
 551 
 552   // Perform any cleanup actions necessary before allowing a verification.
 553   virtual void prepare_for_verify() = 0;
 554 
 555   // Generate any dumps preceding or following a full gc
 556   void pre_full_gc_dump(GCTimer* timer);
 557   void post_full_gc_dump(GCTimer* timer);
 558 
 559   VirtualSpaceSummary create_heap_space_summary();
 560   GCHeapSummary create_heap_summary();
 561 
 562   MetaspaceSummary create_metaspace_summary();
 563 
 564   // Print heap information on the given outputStream.
 565   virtual void print_on(outputStream* st) const = 0;
 566   // The default behavior is to call print_on() on tty.
 567   virtual void print() const {
 568     print_on(tty);
 569   }
 570   // Print more detailed heap information on the given
 571   // outputStream. The default behavior is to call print_on(). It is
 572   // up to each subclass to override it and add any additional output
 573   // it needs.
 574   virtual void print_extended_on(outputStream* st) const {
 575     print_on(st);
 576   }
 577 
 578   virtual void print_on_error(outputStream* st) const {
 579     st-&gt;print_cr("Heap:");
 580     print_extended_on(st);
 581     st-&gt;cr();
 582 
 583     _barrier_set-&gt;print_on(st);
 584   }
 585 
 586   // Print all GC threads (other than the VM thread)
 587   // used by this heap.
 588   virtual void print_gc_threads_on(outputStream* st) const = 0;
 589   // The default behavior is to call print_gc_threads_on() on tty.
 590   void print_gc_threads() {
 591     print_gc_threads_on(tty);
 592   }
 593   // Iterator for all GC threads (other than VM thread)
 594   virtual void gc_threads_do(ThreadClosure* tc) const = 0;
 595 
 596   // Print any relevant tracing info that flags imply.
 597   // Default implementation does nothing.
 598   virtual void print_tracing_info() const = 0;
 599 
 600   void print_heap_before_gc();
 601   void print_heap_after_gc();
 602 
 603   // Registering and unregistering an nmethod (compiled code) with the heap.
 604   // Override with specific mechanism for each specialized heap type.
 605   virtual void register_nmethod(nmethod* nm);
 606   virtual void unregister_nmethod(nmethod* nm);
 607 
 608   void trace_heap_before_gc(GCTracer* gc_tracer);
 609   void trace_heap_after_gc(GCTracer* gc_tracer);
 610 
 611   // Heap verification
 612   virtual void verify(bool silent, VerifyOption option) = 0;
 613 
 614   // Non product verification and debugging.
 615 #ifndef PRODUCT
 616   // Support for PromotionFailureALot.  Return true if it's time to cause a
 617   // promotion failure.  The no-argument version uses
 618   // this-&gt;_promotion_failure_alot_count as the counter.
 619   inline bool promotion_should_fail(volatile size_t* count);
 620   inline bool promotion_should_fail();
 621 
 622   // Reset the PromotionFailureALot counters.  Should be called at the end of a
 623   // GC in which promotion failure occurred.
 624   inline void reset_promotion_should_fail(volatile size_t* count);
 625   inline void reset_promotion_should_fail();
 626 #endif  // #ifndef PRODUCT
 627 
 628 #ifdef ASSERT
 629   static int fired_fake_oom() {
 630     return (CIFireOOMAt &gt; 1 &amp;&amp; _fire_out_of_memory_count &gt;= CIFireOOMAt);
 631   }
 632 #endif
 633 
 634  public:
 635   // This is a convenience method that is used in cases where
 636   // the actual number of GC worker threads is not pertinent but
 637   // only whether there more than 0.  Use of this method helps
 638   // reduce the occurrence of ParallelGCThreads to uses where the
 639   // actual number may be germane.
 640   static bool use_parallel_gc_threads() { return ParallelGCThreads &gt; 0; }
 641 
 642   /////////////// Unit tests ///////////////
 643 
 644   NOT_PRODUCT(static void test_is_in();)
 645 };
 646 
 647 // Class to set and reset the GC cause for a CollectedHeap.
 648 
 649 class GCCauseSetter : StackObj {
 650   CollectedHeap* _heap;
 651   GCCause::Cause _previous_cause;
 652  public:
 653   GCCauseSetter(CollectedHeap* heap, GCCause::Cause cause) {
 654     assert(SafepointSynchronize::is_at_safepoint(),
 655            "This method manipulates heap state without locking");
 656     _heap = heap;
 657     _previous_cause = _heap-&gt;gc_cause();
 658     _heap-&gt;set_gc_cause(cause);
 659   }
 660 
 661   ~GCCauseSetter() {
 662     assert(SafepointSynchronize::is_at_safepoint(),
 663           "This method manipulates heap state without locking");
 664     _heap-&gt;set_gc_cause(_previous_cause);
 665   }
 666 };
 667 
 668 #endif // SHARE_VM_GC_INTERFACE_COLLECTEDHEAP_HPP
</pre></body></html>
