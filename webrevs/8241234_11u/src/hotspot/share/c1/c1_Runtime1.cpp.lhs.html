<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-52917">52917</a> : <a href="https://bugs.openjdk.java.net/browse/JDK-8241234">8241234</a>: Unify monitor enter/exit runtime entries.</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/codeBuffer.hpp"
  27 #include "c1/c1_CodeStubs.hpp"
  28 #include "c1/c1_Defs.hpp"
  29 #include "c1/c1_FrameMap.hpp"
  30 #include "c1/c1_LIRAssembler.hpp"
  31 #include "c1/c1_MacroAssembler.hpp"
  32 #include "c1/c1_Runtime1.hpp"
  33 #include "classfile/systemDictionary.hpp"
  34 #include "classfile/vmSymbols.hpp"
  35 #include "code/codeBlob.hpp"
  36 #include "code/compiledIC.hpp"
  37 #include "code/pcDesc.hpp"
  38 #include "code/scopeDesc.hpp"
  39 #include "code/vtableStubs.hpp"
  40 #include "compiler/disassembler.hpp"
  41 #include "gc/shared/barrierSet.hpp"
  42 #include "gc/shared/c1/barrierSetC1.hpp"
  43 #include "gc/shared/collectedHeap.hpp"
  44 #include "interpreter/bytecode.hpp"
  45 #include "interpreter/interpreter.hpp"
  46 #include "jfr/support/jfrIntrinsics.hpp"
  47 #include "logging/log.hpp"
  48 #include "memory/allocation.inline.hpp"
  49 #include "memory/oopFactory.hpp"
  50 #include "memory/resourceArea.hpp"
  51 #include "oops/access.inline.hpp"
  52 #include "oops/objArrayOop.inline.hpp"
  53 #include "oops/objArrayKlass.hpp"
  54 #include "oops/oop.inline.hpp"
  55 #include "runtime/atomic.hpp"
  56 #include "runtime/biasedLocking.hpp"
  57 #include "runtime/compilationPolicy.hpp"
  58 #include "runtime/interfaceSupport.inline.hpp"
  59 #include "runtime/frame.inline.hpp"
  60 #include "runtime/javaCalls.hpp"
  61 #include "runtime/sharedRuntime.hpp"
  62 #include "runtime/threadCritical.hpp"
  63 #include "runtime/vframe.inline.hpp"
  64 #include "runtime/vframeArray.hpp"
  65 #include "runtime/vm_version.hpp"
  66 #include "utilities/copy.hpp"
  67 #include "utilities/events.hpp"
  68 
  69 
  70 // Implementation of StubAssembler
  71 
  72 StubAssembler::StubAssembler(CodeBuffer* code, const char * name, int stub_id) : C1_MacroAssembler(code) {
  73   _name = name;
  74   _must_gc_arguments = false;
  75   _frame_size = no_frame_size;
  76   _num_rt_args = 0;
  77   _stub_id = stub_id;
  78 }
  79 
  80 
  81 void StubAssembler::set_info(const char* name, bool must_gc_arguments) {
  82   _name = name;
  83   _must_gc_arguments = must_gc_arguments;
  84 }
  85 
  86 
  87 void StubAssembler::set_frame_size(int size) {
  88   if (_frame_size == no_frame_size) {
  89     _frame_size = size;
  90   }
  91   assert(_frame_size == size, "can't change the frame size");
  92 }
  93 
  94 
  95 void StubAssembler::set_num_rt_args(int args) {
  96   if (_num_rt_args == 0) {
  97     _num_rt_args = args;
  98   }
  99   assert(_num_rt_args == args, "can't change the number of args");
 100 }
 101 
 102 // Implementation of Runtime1
 103 
 104 CodeBlob* Runtime1::_blobs[Runtime1::number_of_ids];
 105 const char *Runtime1::_blob_names[] = {
 106   RUNTIME1_STUBS(STUB_NAME, LAST_STUB_NAME)
 107 };
 108 
 109 #ifndef PRODUCT
 110 // statistics
 111 int Runtime1::_generic_arraycopy_cnt = 0;
 112 int Runtime1::_generic_arraycopystub_cnt = 0;
 113 int Runtime1::_arraycopy_slowcase_cnt = 0;
 114 int Runtime1::_arraycopy_checkcast_cnt = 0;
 115 int Runtime1::_arraycopy_checkcast_attempt_cnt = 0;
 116 int Runtime1::_new_type_array_slowcase_cnt = 0;
 117 int Runtime1::_new_object_array_slowcase_cnt = 0;
 118 int Runtime1::_new_instance_slowcase_cnt = 0;
 119 int Runtime1::_new_multi_array_slowcase_cnt = 0;
 120 int Runtime1::_monitorenter_slowcase_cnt = 0;
 121 int Runtime1::_monitorexit_slowcase_cnt = 0;
 122 int Runtime1::_patch_code_slowcase_cnt = 0;
 123 int Runtime1::_throw_range_check_exception_count = 0;
 124 int Runtime1::_throw_index_exception_count = 0;
 125 int Runtime1::_throw_div0_exception_count = 0;
 126 int Runtime1::_throw_null_pointer_exception_count = 0;
 127 int Runtime1::_throw_class_cast_exception_count = 0;
 128 int Runtime1::_throw_incompatible_class_change_error_count = 0;
 129 int Runtime1::_throw_array_store_exception_count = 0;
 130 int Runtime1::_throw_count = 0;
 131 
 132 static int _byte_arraycopy_stub_cnt = 0;
 133 static int _short_arraycopy_stub_cnt = 0;
 134 static int _int_arraycopy_stub_cnt = 0;
 135 static int _long_arraycopy_stub_cnt = 0;
 136 static int _oop_arraycopy_stub_cnt = 0;
 137 
 138 address Runtime1::arraycopy_count_address(BasicType type) {
 139   switch (type) {
 140   case T_BOOLEAN:
 141   case T_BYTE:   return (address)&amp;_byte_arraycopy_stub_cnt;
 142   case T_CHAR:
 143   case T_SHORT:  return (address)&amp;_short_arraycopy_stub_cnt;
 144   case T_FLOAT:
 145   case T_INT:    return (address)&amp;_int_arraycopy_stub_cnt;
 146   case T_DOUBLE:
 147   case T_LONG:   return (address)&amp;_long_arraycopy_stub_cnt;
 148   case T_ARRAY:
 149   case T_OBJECT: return (address)&amp;_oop_arraycopy_stub_cnt;
 150   default:
 151     ShouldNotReachHere();
 152     return NULL;
 153   }
 154 }
 155 
 156 
 157 #endif
 158 
 159 // Simple helper to see if the caller of a runtime stub which
 160 // entered the VM has been deoptimized
 161 
 162 static bool caller_is_deopted() {
 163   JavaThread* thread = JavaThread::current();
 164   RegisterMap reg_map(thread, false);
 165   frame runtime_frame = thread-&gt;last_frame();
 166   frame caller_frame = runtime_frame.sender(&amp;reg_map);
 167   assert(caller_frame.is_compiled_frame(), "must be compiled");
 168   return caller_frame.is_deoptimized_frame();
 169 }
 170 
 171 // Stress deoptimization
 172 static void deopt_caller() {
 173   if ( !caller_is_deopted()) {
 174     JavaThread* thread = JavaThread::current();
 175     RegisterMap reg_map(thread, false);
 176     frame runtime_frame = thread-&gt;last_frame();
 177     frame caller_frame = runtime_frame.sender(&amp;reg_map);
 178     Deoptimization::deoptimize_frame(thread, caller_frame.id());
 179     assert(caller_is_deopted(), "Must be deoptimized");
 180   }
 181 }
 182 
 183 class StubIDStubAssemblerCodeGenClosure: public StubAssemblerCodeGenClosure {
 184  private:
 185   Runtime1::StubID _id;
 186  public:
 187   StubIDStubAssemblerCodeGenClosure(Runtime1::StubID id) : _id(id) {}
 188   virtual OopMapSet* generate_code(StubAssembler* sasm) {
 189     return Runtime1::generate_code_for(_id, sasm);
 190   }
 191 };
 192 
 193 CodeBlob* Runtime1::generate_blob(BufferBlob* buffer_blob, int stub_id, const char* name, bool expect_oop_map, StubAssemblerCodeGenClosure* cl) {
 194   ResourceMark rm;
 195   // create code buffer for code storage
 196   CodeBuffer code(buffer_blob);
 197 
 198   OopMapSet* oop_maps;
 199   int frame_size;
 200   bool must_gc_arguments;
 201 
 202   Compilation::setup_code_buffer(&amp;code, 0);
 203 
 204   // create assembler for code generation
 205   StubAssembler* sasm = new StubAssembler(&amp;code, name, stub_id);
 206   // generate code for runtime stub
 207   oop_maps = cl-&gt;generate_code(sasm);
 208   assert(oop_maps == NULL || sasm-&gt;frame_size() != no_frame_size,
 209          "if stub has an oop map it must have a valid frame size");
 210   assert(!expect_oop_map || oop_maps != NULL, "must have an oopmap");
 211 
 212   // align so printing shows nop's instead of random code at the end (SimpleStubs are aligned)
 213   sasm-&gt;align(BytesPerWord);
 214   // make sure all code is in code buffer
 215   sasm-&gt;flush();
 216 
 217   frame_size = sasm-&gt;frame_size();
 218   must_gc_arguments = sasm-&gt;must_gc_arguments();
 219   // create blob - distinguish a few special cases
 220   CodeBlob* blob = RuntimeStub::new_runtime_stub(name,
 221                                                  &amp;code,
 222                                                  CodeOffsets::frame_never_safe,
 223                                                  frame_size,
 224                                                  oop_maps,
 225                                                  must_gc_arguments);
 226   assert(blob != NULL, "blob must exist");
 227   return blob;
 228 }
 229 
 230 void Runtime1::generate_blob_for(BufferBlob* buffer_blob, StubID id) {
 231   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, "illegal stub id");
 232   bool expect_oop_map = true;
 233 #ifdef ASSERT
 234   // Make sure that stubs that need oopmaps have them
 235   switch (id) {
 236     // These stubs don't need to have an oopmap
 237   case dtrace_object_alloc_id:
 238   case slow_subtype_check_id:
 239   case fpu2long_stub_id:
 240   case unwind_exception_id:
 241   case counter_overflow_id:
 242 #if defined(SPARC) || defined(PPC32)
 243   case handle_exception_nofpu_id:  // Unused on sparc
 244 #endif
 245     expect_oop_map = false;
 246     break;
 247   default:
 248     break;
 249   }
 250 #endif
 251   StubIDStubAssemblerCodeGenClosure cl(id);
 252   CodeBlob* blob = generate_blob(buffer_blob, id, name_for(id), expect_oop_map, &amp;cl);
 253   // install blob
 254   _blobs[id] = blob;
 255 }
 256 
 257 void Runtime1::initialize(BufferBlob* blob) {
 258   // platform-dependent initialization
 259   initialize_pd();
 260   // generate stubs
 261   for (int id = 0; id &lt; number_of_ids; id++) generate_blob_for(blob, (StubID)id);
 262   // printing
 263 #ifndef PRODUCT
 264   if (PrintSimpleStubs) {
 265     ResourceMark rm;
 266     for (int id = 0; id &lt; number_of_ids; id++) {
 267       _blobs[id]-&gt;print();
 268       if (_blobs[id]-&gt;oop_maps() != NULL) {
 269         _blobs[id]-&gt;oop_maps()-&gt;print();
 270       }
 271     }
 272   }
 273 #endif
 274   BarrierSetC1* bs = BarrierSet::barrier_set()-&gt;barrier_set_c1();
 275   bs-&gt;generate_c1_runtime_stubs(blob);
 276 }
 277 
 278 CodeBlob* Runtime1::blob_for(StubID id) {
 279   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, "illegal stub id");
 280   return _blobs[id];
 281 }
 282 
 283 
 284 const char* Runtime1::name_for(StubID id) {
 285   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, "illegal stub id");
 286   return _blob_names[id];
 287 }
 288 
 289 const char* Runtime1::name_for_address(address entry) {
 290   for (int id = 0; id &lt; number_of_ids; id++) {
 291     if (entry == entry_for((StubID)id)) return name_for((StubID)id);
 292   }
 293 
 294 #define FUNCTION_CASE(a, f) \
 295   if ((intptr_t)a == CAST_FROM_FN_PTR(intptr_t, f))  return #f
 296 
 297   FUNCTION_CASE(entry, os::javaTimeMillis);
 298   FUNCTION_CASE(entry, os::javaTimeNanos);
 299   FUNCTION_CASE(entry, SharedRuntime::OSR_migration_end);
 300   FUNCTION_CASE(entry, SharedRuntime::d2f);
 301   FUNCTION_CASE(entry, SharedRuntime::d2i);
 302   FUNCTION_CASE(entry, SharedRuntime::d2l);
 303   FUNCTION_CASE(entry, SharedRuntime::dcos);
 304   FUNCTION_CASE(entry, SharedRuntime::dexp);
 305   FUNCTION_CASE(entry, SharedRuntime::dlog);
 306   FUNCTION_CASE(entry, SharedRuntime::dlog10);
 307   FUNCTION_CASE(entry, SharedRuntime::dpow);
 308   FUNCTION_CASE(entry, SharedRuntime::drem);
 309   FUNCTION_CASE(entry, SharedRuntime::dsin);
 310   FUNCTION_CASE(entry, SharedRuntime::dtan);
 311   FUNCTION_CASE(entry, SharedRuntime::f2i);
 312   FUNCTION_CASE(entry, SharedRuntime::f2l);
 313   FUNCTION_CASE(entry, SharedRuntime::frem);
 314   FUNCTION_CASE(entry, SharedRuntime::l2d);
 315   FUNCTION_CASE(entry, SharedRuntime::l2f);
 316   FUNCTION_CASE(entry, SharedRuntime::ldiv);
 317   FUNCTION_CASE(entry, SharedRuntime::lmul);
 318   FUNCTION_CASE(entry, SharedRuntime::lrem);
 319   FUNCTION_CASE(entry, SharedRuntime::lrem);
 320   FUNCTION_CASE(entry, SharedRuntime::dtrace_method_entry);
 321   FUNCTION_CASE(entry, SharedRuntime::dtrace_method_exit);
 322   FUNCTION_CASE(entry, is_instance_of);
 323   FUNCTION_CASE(entry, trace_block_entry);
 324 #ifdef JFR_HAVE_INTRINSICS
 325   FUNCTION_CASE(entry, JFR_TIME_FUNCTION);
 326 #endif
 327   FUNCTION_CASE(entry, StubRoutines::updateBytesCRC32());
 328   FUNCTION_CASE(entry, StubRoutines::updateBytesCRC32C());
 329   FUNCTION_CASE(entry, StubRoutines::vectorizedMismatch());
 330   FUNCTION_CASE(entry, StubRoutines::dexp());
 331   FUNCTION_CASE(entry, StubRoutines::dlog());
 332   FUNCTION_CASE(entry, StubRoutines::dlog10());
 333   FUNCTION_CASE(entry, StubRoutines::dpow());
 334   FUNCTION_CASE(entry, StubRoutines::dsin());
 335   FUNCTION_CASE(entry, StubRoutines::dcos());
 336   FUNCTION_CASE(entry, StubRoutines::dtan());
 337 
 338 #undef FUNCTION_CASE
 339 
 340   // Soft float adds more runtime names.
 341   return pd_name_for_address(entry);
 342 }
 343 
 344 
 345 JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))
 346   NOT_PRODUCT(_new_instance_slowcase_cnt++;)
 347 
 348   assert(klass-&gt;is_klass(), "not a class");
 349   Handle holder(THREAD, klass-&gt;klass_holder()); // keep the klass alive
 350   InstanceKlass* h = InstanceKlass::cast(klass);
 351   h-&gt;check_valid_for_instantiation(true, CHECK);
 352   // make sure klass is initialized
 353   h-&gt;initialize(CHECK);
 354   // allocate instance and return via TLS
 355   oop obj = h-&gt;allocate_instance(CHECK);
 356   thread-&gt;set_vm_result(obj);
 357 JRT_END
 358 
 359 
 360 JRT_ENTRY(void, Runtime1::new_type_array(JavaThread* thread, Klass* klass, jint length))
 361   NOT_PRODUCT(_new_type_array_slowcase_cnt++;)
 362   // Note: no handle for klass needed since they are not used
 363   //       anymore after new_typeArray() and no GC can happen before.
 364   //       (This may have to change if this code changes!)
 365   assert(klass-&gt;is_klass(), "not a class");
 366   BasicType elt_type = TypeArrayKlass::cast(klass)-&gt;element_type();
 367   oop obj = oopFactory::new_typeArray(elt_type, length, CHECK);
 368   thread-&gt;set_vm_result(obj);
 369   // This is pretty rare but this runtime patch is stressful to deoptimization
 370   // if we deoptimize here so force a deopt to stress the path.
 371   if (DeoptimizeALot) {
 372     deopt_caller();
 373   }
 374 
 375 JRT_END
 376 
 377 
 378 JRT_ENTRY(void, Runtime1::new_object_array(JavaThread* thread, Klass* array_klass, jint length))
 379   NOT_PRODUCT(_new_object_array_slowcase_cnt++;)
 380 
 381   // Note: no handle for klass needed since they are not used
 382   //       anymore after new_objArray() and no GC can happen before.
 383   //       (This may have to change if this code changes!)
 384   assert(array_klass-&gt;is_klass(), "not a class");
 385   Handle holder(THREAD, array_klass-&gt;klass_holder()); // keep the klass alive
 386   Klass* elem_klass = ObjArrayKlass::cast(array_klass)-&gt;element_klass();
 387   objArrayOop obj = oopFactory::new_objArray(elem_klass, length, CHECK);
 388   thread-&gt;set_vm_result(obj);
 389   // This is pretty rare but this runtime patch is stressful to deoptimization
 390   // if we deoptimize here so force a deopt to stress the path.
 391   if (DeoptimizeALot) {
 392     deopt_caller();
 393   }
 394 JRT_END
 395 
 396 
 397 JRT_ENTRY(void, Runtime1::new_multi_array(JavaThread* thread, Klass* klass, int rank, jint* dims))
 398   NOT_PRODUCT(_new_multi_array_slowcase_cnt++;)
 399 
 400   assert(klass-&gt;is_klass(), "not a class");
 401   assert(rank &gt;= 1, "rank must be nonzero");
 402   Handle holder(THREAD, klass-&gt;klass_holder()); // keep the klass alive
 403   oop obj = ArrayKlass::cast(klass)-&gt;multi_allocate(rank, dims, CHECK);
 404   thread-&gt;set_vm_result(obj);
 405 JRT_END
 406 
 407 
 408 JRT_ENTRY(void, Runtime1::unimplemented_entry(JavaThread* thread, StubID id))
 409   tty-&gt;print_cr("Runtime1::entry_for(%d) returned unimplemented entry point", id);
 410 JRT_END
 411 
 412 
 413 JRT_ENTRY(void, Runtime1::throw_array_store_exception(JavaThread* thread, oopDesc* obj))
 414   ResourceMark rm(thread);
 415   const char* klass_name = obj-&gt;klass()-&gt;external_name();
 416   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArrayStoreException(), klass_name);
 417 JRT_END
 418 
 419 
 420 // counter_overflow() is called from within C1-compiled methods. The enclosing method is the method
 421 // associated with the top activation record. The inlinee (that is possibly included in the enclosing
 422 // method) method oop is passed as an argument. In order to do that it is embedded in the code as
 423 // a constant.
 424 static nmethod* counter_overflow_helper(JavaThread* THREAD, int branch_bci, Method* m) {
 425   nmethod* osr_nm = NULL;
 426   methodHandle method(THREAD, m);
 427 
 428   RegisterMap map(THREAD, false);
 429   frame fr =  THREAD-&gt;last_frame().sender(&amp;map);
 430   nmethod* nm = (nmethod*) fr.cb();
 431   assert(nm!= NULL &amp;&amp; nm-&gt;is_nmethod(), "Sanity check");
 432   methodHandle enclosing_method(THREAD, nm-&gt;method());
 433 
 434   CompLevel level = (CompLevel)nm-&gt;comp_level();
 435   int bci = InvocationEntryBci;
 436   if (branch_bci != InvocationEntryBci) {
 437     // Compute destination bci
 438     address pc = method()-&gt;code_base() + branch_bci;
 439     Bytecodes::Code branch = Bytecodes::code_at(method(), pc);
 440     int offset = 0;
 441     switch (branch) {
 442       case Bytecodes::_if_icmplt: case Bytecodes::_iflt:
 443       case Bytecodes::_if_icmpgt: case Bytecodes::_ifgt:
 444       case Bytecodes::_if_icmple: case Bytecodes::_ifle:
 445       case Bytecodes::_if_icmpge: case Bytecodes::_ifge:
 446       case Bytecodes::_if_icmpeq: case Bytecodes::_if_acmpeq: case Bytecodes::_ifeq:
 447       case Bytecodes::_if_icmpne: case Bytecodes::_if_acmpne: case Bytecodes::_ifne:
 448       case Bytecodes::_ifnull: case Bytecodes::_ifnonnull: case Bytecodes::_goto:
 449         offset = (int16_t)Bytes::get_Java_u2(pc + 1);
 450         break;
 451       case Bytecodes::_goto_w:
 452         offset = Bytes::get_Java_u4(pc + 1);
 453         break;
 454       default: ;
 455     }
 456     bci = branch_bci + offset;
 457   }
 458   assert(!HAS_PENDING_EXCEPTION, "Should not have any exceptions pending");
 459   osr_nm = CompilationPolicy::policy()-&gt;event(enclosing_method, method, branch_bci, bci, level, nm, THREAD);
 460   assert(!HAS_PENDING_EXCEPTION, "Event handler should not throw any exceptions");
 461   return osr_nm;
 462 }
 463 
 464 JRT_BLOCK_ENTRY(address, Runtime1::counter_overflow(JavaThread* thread, int bci, Method* method))
 465   nmethod* osr_nm;
 466   JRT_BLOCK
 467     osr_nm = counter_overflow_helper(thread, bci, method);
 468     if (osr_nm != NULL) {
 469       RegisterMap map(thread, false);
 470       frame fr =  thread-&gt;last_frame().sender(&amp;map);
 471       Deoptimization::deoptimize_frame(thread, fr.id());
 472     }
 473   JRT_BLOCK_END
 474   return NULL;
 475 JRT_END
 476 
 477 extern void vm_exit(int code);
 478 
 479 // Enter this method from compiled code handler below. This is where we transition
 480 // to VM mode. This is done as a helper routine so that the method called directly
 481 // from compiled code does not have to transition to VM. This allows the entry
 482 // method to see if the nmethod that we have just looked up a handler for has
 483 // been deoptimized while we were in the vm. This simplifies the assembly code
 484 // cpu directories.
 485 //
 486 // We are entering here from exception stub (via the entry method below)
 487 // If there is a compiled exception handler in this method, we will continue there;
 488 // otherwise we will unwind the stack and continue at the caller of top frame method
 489 // Note: we enter in Java using a special JRT wrapper. This wrapper allows us to
 490 // control the area where we can allow a safepoint. After we exit the safepoint area we can
 491 // check to see if the handler we are going to return is now in a nmethod that has
 492 // been deoptimized. If that is the case we return the deopt blob
 493 // unpack_with_exception entry instead. This makes life for the exception blob easier
 494 // because making that same check and diverting is painful from assembly language.
 495 JRT_ENTRY_NO_ASYNC(static address, exception_handler_for_pc_helper(JavaThread* thread, oopDesc* ex, address pc, nmethod*&amp; nm))
 496   // Reset method handle flag.
 497   thread-&gt;set_is_method_handle_return(false);
 498 
 499   Handle exception(thread, ex);
 500   nm = CodeCache::find_nmethod(pc);
 501   assert(nm != NULL, "this is not an nmethod");
 502   // Adjust the pc as needed/
 503   if (nm-&gt;is_deopt_pc(pc)) {
 504     RegisterMap map(thread, false);
 505     frame exception_frame = thread-&gt;last_frame().sender(&amp;map);
 506     // if the frame isn't deopted then pc must not correspond to the caller of last_frame
 507     assert(exception_frame.is_deoptimized_frame(), "must be deopted");
 508     pc = exception_frame.pc();
 509   }
 510 #ifdef ASSERT
 511   assert(exception.not_null(), "NULL exceptions should be handled by throw_exception");
 512   // Check that exception is a subclass of Throwable, otherwise we have a VerifyError
 513   if (!(exception-&gt;is_a(SystemDictionary::Throwable_klass()))) {
 514     if (ExitVMOnVerifyError) vm_exit(-1);
 515     ShouldNotReachHere();
 516   }
 517 #endif
 518 
 519   // Check the stack guard pages and reenable them if necessary and there is
 520   // enough space on the stack to do so.  Use fast exceptions only if the guard
 521   // pages are enabled.
 522   bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
 523   if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
 524 
 525   if (JvmtiExport::can_post_on_exceptions()) {
 526     // To ensure correct notification of exception catches and throws
 527     // we have to deoptimize here.  If we attempted to notify the
 528     // catches and throws during this exception lookup it's possible
 529     // we could deoptimize on the way out of the VM and end back in
 530     // the interpreter at the throw site.  This would result in double
 531     // notifications since the interpreter would also notify about
 532     // these same catches and throws as it unwound the frame.
 533 
 534     RegisterMap reg_map(thread);
 535     frame stub_frame = thread-&gt;last_frame();
 536     frame caller_frame = stub_frame.sender(&amp;reg_map);
 537 
 538     // We don't really want to deoptimize the nmethod itself since we
 539     // can actually continue in the exception handler ourselves but I
 540     // don't see an easy way to have the desired effect.
 541     Deoptimization::deoptimize_frame(thread, caller_frame.id());
 542     assert(caller_is_deopted(), "Must be deoptimized");
 543 
 544     return SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
 545   }
 546 
 547   // ExceptionCache is used only for exceptions at call sites and not for implicit exceptions
 548   if (guard_pages_enabled) {
 549     address fast_continuation = nm-&gt;handler_for_exception_and_pc(exception, pc);
 550     if (fast_continuation != NULL) {
 551       // Set flag if return address is a method handle call site.
 552       thread-&gt;set_is_method_handle_return(nm-&gt;is_method_handle_return(pc));
 553       return fast_continuation;
 554     }
 555   }
 556 
 557   // If the stack guard pages are enabled, check whether there is a handler in
 558   // the current method.  Otherwise (guard pages disabled), force an unwind and
 559   // skip the exception cache update (i.e., just leave continuation==NULL).
 560   address continuation = NULL;
 561   if (guard_pages_enabled) {
 562 
 563     // New exception handling mechanism can support inlined methods
 564     // with exception handlers since the mappings are from PC to PC
 565 
 566     // debugging support
 567     // tracing
 568     if (log_is_enabled(Info, exceptions)) {
 569       ResourceMark rm;
 570       stringStream tempst;
 571       tempst.print("compiled method &lt;%s&gt;\n"
 572                    " at PC" INTPTR_FORMAT " for thread " INTPTR_FORMAT,
 573                    nm-&gt;method()-&gt;print_value_string(), p2i(pc), p2i(thread));
 574       Exceptions::log_exception(exception, tempst.as_string());
 575     }
 576     // for AbortVMOnException flag
 577     Exceptions::debug_check_abort(exception);
 578 
 579     // Clear out the exception oop and pc since looking up an
 580     // exception handler can cause class loading, which might throw an
 581     // exception and those fields are expected to be clear during
 582     // normal bytecode execution.
 583     thread-&gt;clear_exception_oop_and_pc();
 584 
 585     bool recursive_exception = false;
 586     continuation = SharedRuntime::compute_compiled_exc_handler(nm, pc, exception, false, false, recursive_exception);
 587     // If an exception was thrown during exception dispatch, the exception oop may have changed
 588     thread-&gt;set_exception_oop(exception());
 589     thread-&gt;set_exception_pc(pc);
 590 
 591     // the exception cache is used only by non-implicit exceptions
 592     // Update the exception cache only when there didn't happen
 593     // another exception during the computation of the compiled
 594     // exception handler. Checking for exception oop equality is not
 595     // sufficient because some exceptions are pre-allocated and reused.
 596     if (continuation != NULL &amp;&amp; !recursive_exception) {
 597       nm-&gt;add_handler_for_exception_and_pc(exception, pc, continuation);
 598     }
 599   }
 600 
 601   thread-&gt;set_vm_result(exception());
 602   // Set flag if return address is a method handle call site.
 603   thread-&gt;set_is_method_handle_return(nm-&gt;is_method_handle_return(pc));
 604 
 605   if (log_is_enabled(Info, exceptions)) {
 606     ResourceMark rm;
 607     log_info(exceptions)("Thread " PTR_FORMAT " continuing at PC " PTR_FORMAT
 608                          " for exception thrown at PC " PTR_FORMAT,
 609                          p2i(thread), p2i(continuation), p2i(pc));
 610   }
 611 
 612   return continuation;
 613 JRT_END
 614 
 615 // Enter this method from compiled code only if there is a Java exception handler
 616 // in the method handling the exception.
 617 // We are entering here from exception stub. We don't do a normal VM transition here.
 618 // We do it in a helper. This is so we can check to see if the nmethod we have just
 619 // searched for an exception handler has been deoptimized in the meantime.
 620 address Runtime1::exception_handler_for_pc(JavaThread* thread) {
 621   oop exception = thread-&gt;exception_oop();
 622   address pc = thread-&gt;exception_pc();
 623   // Still in Java mode
 624   DEBUG_ONLY(ResetNoHandleMark rnhm);
 625   nmethod* nm = NULL;
 626   address continuation = NULL;
 627   {
 628     // Enter VM mode by calling the helper
 629     ResetNoHandleMark rnhm;
 630     continuation = exception_handler_for_pc_helper(thread, exception, pc, nm);
 631   }
 632   // Back in JAVA, use no oops DON'T safepoint
 633 
 634   // Now check to see if the nmethod we were called from is now deoptimized.
 635   // If so we must return to the deopt blob and deoptimize the nmethod
 636   if (nm != NULL &amp;&amp; caller_is_deopted()) {
 637     continuation = SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
 638   }
 639 
 640   assert(continuation != NULL, "no handler found");
 641   return continuation;
 642 }
 643 
 644 
 645 JRT_ENTRY(void, Runtime1::throw_range_check_exception(JavaThread* thread, int index, arrayOopDesc* a))
 646   NOT_PRODUCT(_throw_range_check_exception_count++;)
 647   const int len = 35;
 648   assert(len &lt; strlen("Index %d out of bounds for length %d"), "Must allocate more space for message.");
 649   char message[2 * jintAsStringSize + len];
 650   sprintf(message, "Index %d out of bounds for length %d", index, a-&gt;length());
 651   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), message);
 652 JRT_END
 653 
 654 
 655 JRT_ENTRY(void, Runtime1::throw_index_exception(JavaThread* thread, int index))
 656   NOT_PRODUCT(_throw_index_exception_count++;)
 657   char message[16];
 658   sprintf(message, "%d", index);
 659   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IndexOutOfBoundsException(), message);
 660 JRT_END
 661 
 662 
 663 JRT_ENTRY(void, Runtime1::throw_div0_exception(JavaThread* thread))
 664   NOT_PRODUCT(_throw_div0_exception_count++;)
 665   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArithmeticException(), "/ by zero");
 666 JRT_END
 667 
 668 
 669 JRT_ENTRY(void, Runtime1::throw_null_pointer_exception(JavaThread* thread))
 670   NOT_PRODUCT(_throw_null_pointer_exception_count++;)
 671   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());
 672 JRT_END
 673 
 674 
 675 JRT_ENTRY(void, Runtime1::throw_class_cast_exception(JavaThread* thread, oopDesc* object))
 676   NOT_PRODUCT(_throw_class_cast_exception_count++;)
 677   ResourceMark rm(thread);
 678   char* message = SharedRuntime::generate_class_cast_message(
 679     thread, object-&gt;klass());
 680   SharedRuntime::throw_and_post_jvmti_exception(
 681     thread, vmSymbols::java_lang_ClassCastException(), message);
 682 JRT_END
 683 
 684 
 685 JRT_ENTRY(void, Runtime1::throw_incompatible_class_change_error(JavaThread* thread))
 686   NOT_PRODUCT(_throw_incompatible_class_change_error_count++;)
 687   ResourceMark rm(thread);
 688   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IncompatibleClassChangeError());
 689 JRT_END
 690 
 691 
<a name="1" id="anc1"></a><span class="changed"> 692 JRT_ENTRY_NO_ASYNC(void, Runtime1::monitorenter(JavaThread* thread, oopDesc* obj, BasicObjectLock* lock))</span>
 693   NOT_PRODUCT(_monitorenter_slowcase_cnt++;)
<a name="2" id="anc2"></a><span class="changed"> 694   if (PrintBiasedLockingStatistics) {</span>
<span class="changed"> 695     Atomic::inc(BiasedLocking::slow_path_entry_count_addr());</span>
<span class="changed"> 696   }</span>
<span class="changed"> 697   Handle h_obj(thread, obj);</span>
<span class="changed"> 698   if (UseBiasedLocking) {</span>
<span class="changed"> 699     // Retry fast entry if bias is revoked to avoid unnecessary inflation</span>
<span class="changed"> 700     ObjectSynchronizer::fast_enter(h_obj, lock-&gt;lock(), true, CHECK);</span>
<span class="changed"> 701   } else {</span>
 702     if (UseFastLocking) {
<a name="3" id="anc3"></a><span class="removed"> 703       // When using fast locking, the compiled code has already tried the fast case</span>
 704       assert(obj == lock-&gt;obj(), "must match");
<a name="4" id="anc4"></a><span class="removed"> 705       ObjectSynchronizer::slow_enter(h_obj, lock-&gt;lock(), THREAD);</span>
 706     } else {
 707       lock-&gt;set_obj(obj);
<a name="5" id="anc5"></a><span class="removed"> 708       ObjectSynchronizer::fast_enter(h_obj, lock-&gt;lock(), false, THREAD);</span>
 709     }
 710   }
<a name="6" id="anc6"></a>
 711 JRT_END
 712 
 713 
 714 JRT_LEAF(void, Runtime1::monitorexit(JavaThread* thread, BasicObjectLock* lock))
 715   NOT_PRODUCT(_monitorexit_slowcase_cnt++;)
<a name="7" id="anc7"></a><span class="removed"> 716   assert(thread == JavaThread::current(), "threads must correspond");</span>
 717   assert(thread-&gt;last_Java_sp(), "last_Java_sp must be set");
<a name="8" id="anc8"></a><span class="removed"> 718   // monitorexit is non-blocking (leaf routine) =&gt; no exceptions can be thrown</span>
<span class="removed"> 719   EXCEPTION_MARK;</span>
<span class="removed"> 720 </span>
 721   oop obj = lock-&gt;obj();
 722   assert(oopDesc::is_oop(obj), "must be NULL or an object");
<a name="9" id="anc9"></a><span class="changed"> 723   if (UseFastLocking) {</span>
<span class="changed"> 724     // When using fast locking, the compiled code has already tried the fast case</span>
<span class="changed"> 725     ObjectSynchronizer::slow_exit(obj, lock-&gt;lock(), THREAD);</span>
<span class="changed"> 726   } else {</span>
<span class="changed"> 727     ObjectSynchronizer::fast_exit(obj, lock-&gt;lock(), THREAD);</span>
<span class="changed"> 728   }</span>
 729 JRT_END
 730 
 731 // Cf. OptoRuntime::deoptimize_caller_frame
 732 JRT_ENTRY(void, Runtime1::deoptimize(JavaThread* thread, jint trap_request))
 733   // Called from within the owner thread, so no need for safepoint
 734   RegisterMap reg_map(thread, false);
 735   frame stub_frame = thread-&gt;last_frame();
 736   assert(stub_frame.is_runtime_frame(), "Sanity check");
 737   frame caller_frame = stub_frame.sender(&amp;reg_map);
 738   nmethod* nm = caller_frame.cb()-&gt;as_nmethod_or_null();
 739   assert(nm != NULL, "Sanity check");
 740   methodHandle method(thread, nm-&gt;method());
 741   assert(nm == CodeCache::find_nmethod(caller_frame.pc()), "Should be the same");
 742   Deoptimization::DeoptAction action = Deoptimization::trap_request_action(trap_request);
 743   Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(trap_request);
 744 
 745   if (action == Deoptimization::Action_make_not_entrant) {
 746     if (nm-&gt;make_not_entrant()) {
 747       if (reason == Deoptimization::Reason_tenured) {
 748         MethodData* trap_mdo = Deoptimization::get_method_data(thread, method, true /*create_if_missing*/);
 749         if (trap_mdo != NULL) {
 750           trap_mdo-&gt;inc_tenure_traps();
 751         }
 752       }
 753     }
 754   }
 755 
 756   // Deoptimize the caller frame.
 757   Deoptimization::deoptimize_frame(thread, caller_frame.id());
 758   // Return to the now deoptimized frame.
 759 JRT_END
 760 
 761 
 762 #ifndef DEOPTIMIZE_WHEN_PATCHING
 763 
 764 static Klass* resolve_field_return_klass(const methodHandle&amp; caller, int bci, TRAPS) {
 765   Bytecode_field field_access(caller, bci);
 766   // This can be static or non-static field access
 767   Bytecodes::Code code       = field_access.code();
 768 
 769   // We must load class, initialize class and resolve the field
 770   fieldDescriptor result; // initialize class if needed
 771   constantPoolHandle constants(THREAD, caller-&gt;constants());
 772   LinkResolver::resolve_field_access(result, constants, field_access.index(), caller, Bytecodes::java_code(code), CHECK_NULL);
 773   return result.field_holder();
 774 }
 775 
 776 
 777 //
 778 // This routine patches sites where a class wasn't loaded or
 779 // initialized at the time the code was generated.  It handles
 780 // references to classes, fields and forcing of initialization.  Most
 781 // of the cases are straightforward and involving simply forcing
 782 // resolution of a class, rewriting the instruction stream with the
 783 // needed constant and replacing the call in this function with the
 784 // patched code.  The case for static field is more complicated since
 785 // the thread which is in the process of initializing a class can
 786 // access it's static fields but other threads can't so the code
 787 // either has to deoptimize when this case is detected or execute a
 788 // check that the current thread is the initializing thread.  The
 789 // current
 790 //
 791 // Patches basically look like this:
 792 //
 793 //
 794 // patch_site: jmp patch stub     ;; will be patched
 795 // continue:   ...
 796 //             ...
 797 //             ...
 798 //             ...
 799 //
 800 // They have a stub which looks like this:
 801 //
 802 //             ;; patch body
 803 //             movl &lt;const&gt;, reg           (for class constants)
 804 //        &lt;or&gt; movl [reg1 + &lt;const&gt;], reg  (for field offsets)
 805 //        &lt;or&gt; movl reg, [reg1 + &lt;const&gt;]  (for field offsets)
 806 //             &lt;being_init offset&gt; &lt;bytes to copy&gt; &lt;bytes to skip&gt;
 807 // patch_stub: call Runtime1::patch_code (through a runtime stub)
 808 //             jmp patch_site
 809 //
 810 //
 811 // A normal patch is done by rewriting the patch body, usually a move,
 812 // and then copying it into place over top of the jmp instruction
 813 // being careful to flush caches and doing it in an MP-safe way.  The
 814 // constants following the patch body are used to find various pieces
 815 // of the patch relative to the call site for Runtime1::patch_code.
 816 // The case for getstatic and putstatic is more complicated because
 817 // getstatic and putstatic have special semantics when executing while
 818 // the class is being initialized.  getstatic/putstatic on a class
 819 // which is being_initialized may be executed by the initializing
 820 // thread but other threads have to block when they execute it.  This
 821 // is accomplished in compiled code by executing a test of the current
 822 // thread against the initializing thread of the class.  It's emitted
 823 // as boilerplate in their stub which allows the patched code to be
 824 // executed before it's copied back into the main body of the nmethod.
 825 //
 826 // being_init: get_thread(&lt;tmp reg&gt;
 827 //             cmpl [reg1 + &lt;init_thread_offset&gt;], &lt;tmp reg&gt;
 828 //             jne patch_stub
 829 //             movl [reg1 + &lt;const&gt;], reg  (for field offsets)  &lt;or&gt;
 830 //             movl reg, [reg1 + &lt;const&gt;]  (for field offsets)
 831 //             jmp continue
 832 //             &lt;being_init offset&gt; &lt;bytes to copy&gt; &lt;bytes to skip&gt;
 833 // patch_stub: jmp Runtim1::patch_code (through a runtime stub)
 834 //             jmp patch_site
 835 //
 836 // If the class is being initialized the patch body is rewritten and
 837 // the patch site is rewritten to jump to being_init, instead of
 838 // patch_stub.  Whenever this code is executed it checks the current
 839 // thread against the intializing thread so other threads will enter
 840 // the runtime and end up blocked waiting the class to finish
 841 // initializing inside the calls to resolve_field below.  The
 842 // initializing class will continue on it's way.  Once the class is
 843 // fully_initialized, the intializing_thread of the class becomes
 844 // NULL, so the next thread to execute this code will fail the test,
 845 // call into patch_code and complete the patching process by copying
 846 // the patch body back into the main part of the nmethod and resume
 847 // executing.
 848 //
 849 //
 850 
 851 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
 852   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
 853 
 854   ResourceMark rm(thread);
 855   RegisterMap reg_map(thread, false);
 856   frame runtime_frame = thread-&gt;last_frame();
 857   frame caller_frame = runtime_frame.sender(&amp;reg_map);
 858 
 859   // last java frame on stack
 860   vframeStream vfst(thread, true);
 861   assert(!vfst.at_end(), "Java frame must exist");
 862 
 863   methodHandle caller_method(THREAD, vfst.method());
 864   // Note that caller_method-&gt;code() may not be same as caller_code because of OSR's
 865   // Note also that in the presence of inlining it is not guaranteed
 866   // that caller_method() == caller_code-&gt;method()
 867 
 868   int bci = vfst.bci();
 869   Bytecodes::Code code = caller_method()-&gt;java_code_at(bci);
 870 
 871   // this is used by assertions in the access_field_patching_id
 872   BasicType patch_field_type = T_ILLEGAL;
 873   bool deoptimize_for_volatile = false;
 874   bool deoptimize_for_atomic = false;
 875   int patch_field_offset = -1;
 876   Klass* init_klass = NULL; // klass needed by load_klass_patching code
 877   Klass* load_klass = NULL; // klass needed by load_klass_patching code
 878   Handle mirror(THREAD, NULL);                    // oop needed by load_mirror_patching code
 879   Handle appendix(THREAD, NULL);                  // oop needed by appendix_patching code
 880   bool load_klass_or_mirror_patch_id =
 881     (stub_id == Runtime1::load_klass_patching_id || stub_id == Runtime1::load_mirror_patching_id);
 882 
 883   if (stub_id == Runtime1::access_field_patching_id) {
 884 
 885     Bytecode_field field_access(caller_method, bci);
 886     fieldDescriptor result; // initialize class if needed
 887     Bytecodes::Code code = field_access.code();
 888     constantPoolHandle constants(THREAD, caller_method-&gt;constants());
 889     LinkResolver::resolve_field_access(result, constants, field_access.index(), caller_method, Bytecodes::java_code(code), CHECK);
 890     patch_field_offset = result.offset();
 891 
 892     // If we're patching a field which is volatile then at compile it
 893     // must not have been know to be volatile, so the generated code
 894     // isn't correct for a volatile reference.  The nmethod has to be
 895     // deoptimized so that the code can be regenerated correctly.
 896     // This check is only needed for access_field_patching since this
 897     // is the path for patching field offsets.  load_klass is only
 898     // used for patching references to oops which don't need special
 899     // handling in the volatile case.
 900 
 901     deoptimize_for_volatile = result.access_flags().is_volatile();
 902 
 903     // If we are patching a field which should be atomic, then
 904     // the generated code is not correct either, force deoptimizing.
 905     // We need to only cover T_LONG and T_DOUBLE fields, as we can
 906     // break access atomicity only for them.
 907 
 908     // Strictly speaking, the deoptimizaation on 64-bit platforms
 909     // is unnecessary, and T_LONG stores on 32-bit platforms need
 910     // to be handled by special patching code when AlwaysAtomicAccesses
 911     // becomes product feature. At this point, we are still going
 912     // for the deoptimization for consistency against volatile
 913     // accesses.
 914 
 915     patch_field_type = result.field_type();
 916     deoptimize_for_atomic = (AlwaysAtomicAccesses &amp;&amp; (patch_field_type == T_DOUBLE || patch_field_type == T_LONG));
 917 
 918   } else if (load_klass_or_mirror_patch_id) {
 919     Klass* k = NULL;
 920     switch (code) {
 921       case Bytecodes::_putstatic:
 922       case Bytecodes::_getstatic:
 923         { Klass* klass = resolve_field_return_klass(caller_method, bci, CHECK);
 924           init_klass = klass;
 925           mirror = Handle(THREAD, klass-&gt;java_mirror());
 926         }
 927         break;
 928       case Bytecodes::_new:
 929         { Bytecode_new bnew(caller_method(), caller_method-&gt;bcp_from(bci));
 930           k = caller_method-&gt;constants()-&gt;klass_at(bnew.index(), CHECK);
 931         }
 932         break;
 933       case Bytecodes::_multianewarray:
 934         { Bytecode_multianewarray mna(caller_method(), caller_method-&gt;bcp_from(bci));
 935           k = caller_method-&gt;constants()-&gt;klass_at(mna.index(), CHECK);
 936         }
 937         break;
 938       case Bytecodes::_instanceof:
 939         { Bytecode_instanceof io(caller_method(), caller_method-&gt;bcp_from(bci));
 940           k = caller_method-&gt;constants()-&gt;klass_at(io.index(), CHECK);
 941         }
 942         break;
 943       case Bytecodes::_checkcast:
 944         { Bytecode_checkcast cc(caller_method(), caller_method-&gt;bcp_from(bci));
 945           k = caller_method-&gt;constants()-&gt;klass_at(cc.index(), CHECK);
 946         }
 947         break;
 948       case Bytecodes::_anewarray:
 949         { Bytecode_anewarray anew(caller_method(), caller_method-&gt;bcp_from(bci));
 950           Klass* ek = caller_method-&gt;constants()-&gt;klass_at(anew.index(), CHECK);
 951           k = ek-&gt;array_klass(CHECK);
 952         }
 953         break;
 954       case Bytecodes::_ldc:
 955       case Bytecodes::_ldc_w:
 956         {
 957           Bytecode_loadconstant cc(caller_method, bci);
 958           oop m = cc.resolve_constant(CHECK);
 959           mirror = Handle(THREAD, m);
 960         }
 961         break;
 962       default: fatal("unexpected bytecode for load_klass_or_mirror_patch_id");
 963     }
 964     load_klass = k;
 965   } else if (stub_id == load_appendix_patching_id) {
 966     Bytecode_invoke bytecode(caller_method, bci);
 967     Bytecodes::Code bc = bytecode.invoke_code();
 968 
 969     CallInfo info;
 970     constantPoolHandle pool(thread, caller_method-&gt;constants());
 971     int index = bytecode.index();
 972     LinkResolver::resolve_invoke(info, Handle(), pool, index, bc, CHECK);
 973     switch (bc) {
 974       case Bytecodes::_invokehandle: {
 975         int cache_index = ConstantPool::decode_cpcache_index(index, true);
 976         assert(cache_index &gt;= 0 &amp;&amp; cache_index &lt; pool-&gt;cache()-&gt;length(), "unexpected cache index");
 977         ConstantPoolCacheEntry* cpce = pool-&gt;cache()-&gt;entry_at(cache_index);
 978         cpce-&gt;set_method_handle(pool, info);
 979         appendix = Handle(THREAD, cpce-&gt;appendix_if_resolved(pool)); // just in case somebody already resolved the entry
 980         break;
 981       }
 982       case Bytecodes::_invokedynamic: {
 983         ConstantPoolCacheEntry* cpce = pool-&gt;invokedynamic_cp_cache_entry_at(index);
 984         cpce-&gt;set_dynamic_call(pool, info);
 985         appendix = Handle(THREAD, cpce-&gt;appendix_if_resolved(pool)); // just in case somebody already resolved the entry
 986         break;
 987       }
 988       default: fatal("unexpected bytecode for load_appendix_patching_id");
 989     }
 990   } else {
 991     ShouldNotReachHere();
 992   }
 993 
 994   if (deoptimize_for_volatile || deoptimize_for_atomic) {
 995     // At compile time we assumed the field wasn't volatile/atomic but after
 996     // loading it turns out it was volatile/atomic so we have to throw the
 997     // compiled code out and let it be regenerated.
 998     if (TracePatching) {
 999       if (deoptimize_for_volatile) {
1000         tty-&gt;print_cr("Deoptimizing for patching volatile field reference");
1001       }
1002       if (deoptimize_for_atomic) {
1003         tty-&gt;print_cr("Deoptimizing for patching atomic field reference");
1004       }
1005     }
1006 
1007     // It's possible the nmethod was invalidated in the last
1008     // safepoint, but if it's still alive then make it not_entrant.
1009     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1010     if (nm != NULL) {
1011       nm-&gt;make_not_entrant();
1012     }
1013 
1014     Deoptimization::deoptimize_frame(thread, caller_frame.id());
1015 
1016     // Return to the now deoptimized frame.
1017   }
1018 
1019   // Now copy code back
1020 
1021   {
1022     MutexLockerEx ml_patch (Patching_lock, Mutex::_no_safepoint_check_flag);
1023     //
1024     // Deoptimization may have happened while we waited for the lock.
1025     // In that case we don't bother to do any patching we just return
1026     // and let the deopt happen
1027     if (!caller_is_deopted()) {
1028       NativeGeneralJump* jump = nativeGeneralJump_at(caller_frame.pc());
1029       address instr_pc = jump-&gt;jump_destination();
1030       NativeInstruction* ni = nativeInstruction_at(instr_pc);
1031       if (ni-&gt;is_jump() ) {
1032         // the jump has not been patched yet
1033         // The jump destination is slow case and therefore not part of the stubs
1034         // (stubs are only for StaticCalls)
1035 
1036         // format of buffer
1037         //    ....
1038         //    instr byte 0     &lt;-- copy_buff
1039         //    instr byte 1
1040         //    ..
1041         //    instr byte n-1
1042         //      n
1043         //    ....             &lt;-- call destination
1044 
1045         address stub_location = caller_frame.pc() + PatchingStub::patch_info_offset();
1046         unsigned char* byte_count = (unsigned char*) (stub_location - 1);
1047         unsigned char* byte_skip = (unsigned char*) (stub_location - 2);
1048         unsigned char* being_initialized_entry_offset = (unsigned char*) (stub_location - 3);
1049         address copy_buff = stub_location - *byte_skip - *byte_count;
1050         address being_initialized_entry = stub_location - *being_initialized_entry_offset;
1051         if (TracePatching) {
1052           ttyLocker ttyl;
1053           tty-&gt;print_cr(" Patching %s at bci %d at address " INTPTR_FORMAT "  (%s)", Bytecodes::name(code), bci,
1054                         p2i(instr_pc), (stub_id == Runtime1::access_field_patching_id) ? "field" : "klass");
1055           nmethod* caller_code = CodeCache::find_nmethod(caller_frame.pc());
1056           assert(caller_code != NULL, "nmethod not found");
1057 
1058           // NOTE we use pc() not original_pc() because we already know they are
1059           // identical otherwise we'd have never entered this block of code
1060 
1061           const ImmutableOopMap* map = caller_code-&gt;oop_map_for_return_address(caller_frame.pc());
1062           assert(map != NULL, "null check");
1063           map-&gt;print();
1064           tty-&gt;cr();
1065 
1066           Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1067         }
1068         // depending on the code below, do_patch says whether to copy the patch body back into the nmethod
1069         bool do_patch = true;
1070         if (stub_id == Runtime1::access_field_patching_id) {
1071           // The offset may not be correct if the class was not loaded at code generation time.
1072           // Set it now.
1073           NativeMovRegMem* n_move = nativeMovRegMem_at(copy_buff);
1074           assert(n_move-&gt;offset() == 0 || (n_move-&gt;offset() == 4 &amp;&amp; (patch_field_type == T_DOUBLE || patch_field_type == T_LONG)), "illegal offset for type");
1075           assert(patch_field_offset &gt;= 0, "illegal offset");
1076           n_move-&gt;add_offset_in_bytes(patch_field_offset);
1077         } else if (load_klass_or_mirror_patch_id) {
1078           // If a getstatic or putstatic is referencing a klass which
1079           // isn't fully initialized, the patch body isn't copied into
1080           // place until initialization is complete.  In this case the
1081           // patch site is setup so that any threads besides the
1082           // initializing thread are forced to come into the VM and
1083           // block.
1084           do_patch = (code != Bytecodes::_getstatic &amp;&amp; code != Bytecodes::_putstatic) ||
1085                      InstanceKlass::cast(init_klass)-&gt;is_initialized();
1086           NativeGeneralJump* jump = nativeGeneralJump_at(instr_pc);
1087           if (jump-&gt;jump_destination() == being_initialized_entry) {
1088             assert(do_patch == true, "initialization must be complete at this point");
1089           } else {
1090             // patch the instruction &lt;move reg, klass&gt;
1091             NativeMovConstReg* n_copy = nativeMovConstReg_at(copy_buff);
1092 
1093             assert(n_copy-&gt;data() == 0 ||
1094                    n_copy-&gt;data() == (intptr_t)Universe::non_oop_word(),
1095                    "illegal init value");
1096             if (stub_id == Runtime1::load_klass_patching_id) {
1097               assert(load_klass != NULL, "klass not set");
1098               n_copy-&gt;set_data((intx) (load_klass));
1099             } else {
1100               assert(mirror() != NULL, "klass not set");
1101               // Don't need a G1 pre-barrier here since we assert above that data isn't an oop.
1102               n_copy-&gt;set_data(cast_from_oop&lt;intx&gt;(mirror()));
1103             }
1104 
1105             if (TracePatching) {
1106               Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1107             }
1108           }
1109         } else if (stub_id == Runtime1::load_appendix_patching_id) {
1110           NativeMovConstReg* n_copy = nativeMovConstReg_at(copy_buff);
1111           assert(n_copy-&gt;data() == 0 ||
1112                  n_copy-&gt;data() == (intptr_t)Universe::non_oop_word(),
1113                  "illegal init value");
1114           n_copy-&gt;set_data(cast_from_oop&lt;intx&gt;(appendix()));
1115 
1116           if (TracePatching) {
1117             Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1118           }
1119         } else {
1120           ShouldNotReachHere();
1121         }
1122 
1123 #if defined(SPARC) || defined(PPC32)
1124         if (load_klass_or_mirror_patch_id ||
1125             stub_id == Runtime1::load_appendix_patching_id) {
1126           // Update the location in the nmethod with the proper
1127           // metadata.  When the code was generated, a NULL was stuffed
1128           // in the metadata table and that table needs to be update to
1129           // have the right value.  On intel the value is kept
1130           // directly in the instruction instead of in the metadata
1131           // table, so set_data above effectively updated the value.
1132           nmethod* nm = CodeCache::find_nmethod(instr_pc);
1133           assert(nm != NULL, "invalid nmethod_pc");
1134           RelocIterator mds(nm, copy_buff, copy_buff + 1);
1135           bool found = false;
1136           while (mds.next() &amp;&amp; !found) {
1137             if (mds.type() == relocInfo::oop_type) {
1138               assert(stub_id == Runtime1::load_mirror_patching_id ||
1139                      stub_id == Runtime1::load_appendix_patching_id, "wrong stub id");
1140               oop_Relocation* r = mds.oop_reloc();
1141               oop* oop_adr = r-&gt;oop_addr();
1142               *oop_adr = stub_id == Runtime1::load_mirror_patching_id ? mirror() : appendix();
1143               r-&gt;fix_oop_relocation();
1144               found = true;
1145             } else if (mds.type() == relocInfo::metadata_type) {
1146               assert(stub_id == Runtime1::load_klass_patching_id, "wrong stub id");
1147               metadata_Relocation* r = mds.metadata_reloc();
1148               Metadata** metadata_adr = r-&gt;metadata_addr();
1149               *metadata_adr = load_klass;
1150               r-&gt;fix_metadata_relocation();
1151               found = true;
1152             }
1153           }
1154           assert(found, "the metadata must exist!");
1155         }
1156 #endif
1157         if (do_patch) {
1158           // replace instructions
1159           // first replace the tail, then the call
1160 #ifdef ARM
1161           if((load_klass_or_mirror_patch_id ||
1162               stub_id == Runtime1::load_appendix_patching_id) &amp;&amp;
1163               nativeMovConstReg_at(copy_buff)-&gt;is_pc_relative()) {
1164             nmethod* nm = CodeCache::find_nmethod(instr_pc);
1165             address addr = NULL;
1166             assert(nm != NULL, "invalid nmethod_pc");
1167             RelocIterator mds(nm, copy_buff, copy_buff + 1);
1168             while (mds.next()) {
1169               if (mds.type() == relocInfo::oop_type) {
1170                 assert(stub_id == Runtime1::load_mirror_patching_id ||
1171                        stub_id == Runtime1::load_appendix_patching_id, "wrong stub id");
1172                 oop_Relocation* r = mds.oop_reloc();
1173                 addr = (address)r-&gt;oop_addr();
1174                 break;
1175               } else if (mds.type() == relocInfo::metadata_type) {
1176                 assert(stub_id == Runtime1::load_klass_patching_id, "wrong stub id");
1177                 metadata_Relocation* r = mds.metadata_reloc();
1178                 addr = (address)r-&gt;metadata_addr();
1179                 break;
1180               }
1181             }
1182             assert(addr != NULL, "metadata relocation must exist");
1183             copy_buff -= *byte_count;
1184             NativeMovConstReg* n_copy2 = nativeMovConstReg_at(copy_buff);
1185             n_copy2-&gt;set_pc_relative_offset(addr, instr_pc);
1186           }
1187 #endif
1188 
1189           for (int i = NativeGeneralJump::instruction_size; i &lt; *byte_count; i++) {
1190             address ptr = copy_buff + i;
1191             int a_byte = (*ptr) &amp; 0xFF;
1192             address dst = instr_pc + i;
1193             *(unsigned char*)dst = (unsigned char) a_byte;
1194           }
1195           ICache::invalidate_range(instr_pc, *byte_count);
1196           NativeGeneralJump::replace_mt_safe(instr_pc, copy_buff);
1197 
1198           if (load_klass_or_mirror_patch_id ||
1199               stub_id == Runtime1::load_appendix_patching_id) {
1200             relocInfo::relocType rtype =
1201               (stub_id == Runtime1::load_klass_patching_id) ?
1202                                    relocInfo::metadata_type :
1203                                    relocInfo::oop_type;
1204             // update relocInfo to metadata
1205             nmethod* nm = CodeCache::find_nmethod(instr_pc);
1206             assert(nm != NULL, "invalid nmethod_pc");
1207 
1208             // The old patch site is now a move instruction so update
1209             // the reloc info so that it will get updated during
1210             // future GCs.
1211             RelocIterator iter(nm, (address)instr_pc, (address)(instr_pc + 1));
1212             relocInfo::change_reloc_info_for_address(&amp;iter, (address) instr_pc,
1213                                                      relocInfo::none, rtype);
1214 #ifdef SPARC
1215             // Sparc takes two relocations for an metadata so update the second one.
1216             address instr_pc2 = instr_pc + NativeMovConstReg::add_offset;
1217             RelocIterator iter2(nm, instr_pc2, instr_pc2 + 1);
1218             relocInfo::change_reloc_info_for_address(&amp;iter2, (address) instr_pc2,
1219                                                      relocInfo::none, rtype);
1220 #endif
1221 #ifdef PPC32
1222           { address instr_pc2 = instr_pc + NativeMovConstReg::lo_offset;
1223             RelocIterator iter2(nm, instr_pc2, instr_pc2 + 1);
1224             relocInfo::change_reloc_info_for_address(&amp;iter2, (address) instr_pc2,
1225                                                      relocInfo::none, rtype);
1226           }
1227 #endif
1228           }
1229 
1230         } else {
1231           ICache::invalidate_range(copy_buff, *byte_count);
1232           NativeGeneralJump::insert_unconditional(instr_pc, being_initialized_entry);
1233         }
1234       }
1235     }
1236   }
1237 
1238   // If we are patching in a non-perm oop, make sure the nmethod
1239   // is on the right list.
1240   if (ScavengeRootsInCode) {
1241     MutexLockerEx ml_code (CodeCache_lock, Mutex::_no_safepoint_check_flag);
1242     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1243     guarantee(nm != NULL, "only nmethods can contain non-perm oops");
1244 
1245     // Since we've patched some oops in the nmethod,
1246     // (re)register it with the heap.
1247     Universe::heap()-&gt;register_nmethod(nm);
1248   }
1249 JRT_END
1250 
1251 #else // DEOPTIMIZE_WHEN_PATCHING
1252 
1253 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
1254   RegisterMap reg_map(thread, false);
1255 
1256   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
1257   if (TracePatching) {
1258     tty-&gt;print_cr("Deoptimizing because patch is needed");
1259   }
1260 
1261   frame runtime_frame = thread-&gt;last_frame();
1262   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1263 
1264   // It's possible the nmethod was invalidated in the last
1265   // safepoint, but if it's still alive then make it not_entrant.
1266   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1267   if (nm != NULL) {
1268     nm-&gt;make_not_entrant();
1269   }
1270 
1271   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1272 
1273   // Return to the now deoptimized frame.
1274 JRT_END
1275 
1276 #endif // DEOPTIMIZE_WHEN_PATCHING
1277 
1278 //
1279 // Entry point for compiled code. We want to patch a nmethod.
1280 // We don't do a normal VM transition here because we want to
1281 // know after the patching is complete and any safepoint(s) are taken
1282 // if the calling nmethod was deoptimized. We do this by calling a
1283 // helper method which does the normal VM transition and when it
1284 // completes we can check for deoptimization. This simplifies the
1285 // assembly code in the cpu directories.
1286 //
1287 int Runtime1::move_klass_patching(JavaThread* thread) {
1288 //
1289 // NOTE: we are still in Java
1290 //
1291   Thread* THREAD = thread;
1292   debug_only(NoHandleMark nhm;)
1293   {
1294     // Enter VM mode
1295 
1296     ResetNoHandleMark rnhm;
1297     patch_code(thread, load_klass_patching_id);
1298   }
1299   // Back in JAVA, use no oops DON'T safepoint
1300 
1301   // Return true if calling code is deoptimized
1302 
1303   return caller_is_deopted();
1304 }
1305 
1306 int Runtime1::move_mirror_patching(JavaThread* thread) {
1307 //
1308 // NOTE: we are still in Java
1309 //
1310   Thread* THREAD = thread;
1311   debug_only(NoHandleMark nhm;)
1312   {
1313     // Enter VM mode
1314 
1315     ResetNoHandleMark rnhm;
1316     patch_code(thread, load_mirror_patching_id);
1317   }
1318   // Back in JAVA, use no oops DON'T safepoint
1319 
1320   // Return true if calling code is deoptimized
1321 
1322   return caller_is_deopted();
1323 }
1324 
1325 int Runtime1::move_appendix_patching(JavaThread* thread) {
1326 //
1327 // NOTE: we are still in Java
1328 //
1329   Thread* THREAD = thread;
1330   debug_only(NoHandleMark nhm;)
1331   {
1332     // Enter VM mode
1333 
1334     ResetNoHandleMark rnhm;
1335     patch_code(thread, load_appendix_patching_id);
1336   }
1337   // Back in JAVA, use no oops DON'T safepoint
1338 
1339   // Return true if calling code is deoptimized
1340 
1341   return caller_is_deopted();
1342 }
1343 //
1344 // Entry point for compiled code. We want to patch a nmethod.
1345 // We don't do a normal VM transition here because we want to
1346 // know after the patching is complete and any safepoint(s) are taken
1347 // if the calling nmethod was deoptimized. We do this by calling a
1348 // helper method which does the normal VM transition and when it
1349 // completes we can check for deoptimization. This simplifies the
1350 // assembly code in the cpu directories.
1351 //
1352 
1353 int Runtime1::access_field_patching(JavaThread* thread) {
1354 //
1355 // NOTE: we are still in Java
1356 //
1357   Thread* THREAD = thread;
1358   debug_only(NoHandleMark nhm;)
1359   {
1360     // Enter VM mode
1361 
1362     ResetNoHandleMark rnhm;
1363     patch_code(thread, access_field_patching_id);
1364   }
1365   // Back in JAVA, use no oops DON'T safepoint
1366 
1367   // Return true if calling code is deoptimized
1368 
1369   return caller_is_deopted();
1370 JRT_END
1371 
1372 
1373 JRT_LEAF(void, Runtime1::trace_block_entry(jint block_id))
1374   // for now we just print out the block id
1375   tty-&gt;print("%d ", block_id);
1376 JRT_END
1377 
1378 
1379 JRT_LEAF(int, Runtime1::is_instance_of(oopDesc* mirror, oopDesc* obj))
1380   // had to return int instead of bool, otherwise there may be a mismatch
1381   // between the C calling convention and the Java one.
1382   // e.g., on x86, GCC may clear only %al when returning a bool false, but
1383   // JVM takes the whole %eax as the return value, which may misinterpret
1384   // the return value as a boolean true.
1385 
1386   assert(mirror != NULL, "should null-check on mirror before calling");
1387   Klass* k = java_lang_Class::as_Klass(mirror);
1388   return (k != NULL &amp;&amp; obj != NULL &amp;&amp; obj-&gt;is_a(k)) ? 1 : 0;
1389 JRT_END
1390 
1391 JRT_ENTRY(void, Runtime1::predicate_failed_trap(JavaThread* thread))
1392   ResourceMark rm;
1393 
1394   assert(!TieredCompilation, "incompatible with tiered compilation");
1395 
1396   RegisterMap reg_map(thread, false);
1397   frame runtime_frame = thread-&gt;last_frame();
1398   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1399 
1400   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1401   assert (nm != NULL, "no more nmethod?");
1402   nm-&gt;make_not_entrant();
1403 
1404   methodHandle m(nm-&gt;method());
1405   MethodData* mdo = m-&gt;method_data();
1406 
1407   if (mdo == NULL &amp;&amp; !HAS_PENDING_EXCEPTION) {
1408     // Build an MDO.  Ignore errors like OutOfMemory;
1409     // that simply means we won't have an MDO to update.
1410     Method::build_interpreter_method_data(m, THREAD);
1411     if (HAS_PENDING_EXCEPTION) {
1412       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), "we expect only an OOM error here");
1413       CLEAR_PENDING_EXCEPTION;
1414     }
1415     mdo = m-&gt;method_data();
1416   }
1417 
1418   if (mdo != NULL) {
1419     mdo-&gt;inc_trap_count(Deoptimization::Reason_none);
1420   }
1421 
1422   if (TracePredicateFailedTraps) {
1423     stringStream ss1, ss2;
1424     vframeStream vfst(thread);
1425     methodHandle inlinee = methodHandle(vfst.method());
1426     inlinee-&gt;print_short_name(&amp;ss1);
1427     m-&gt;print_short_name(&amp;ss2);
1428     tty-&gt;print_cr("Predicate failed trap in method %s at bci %d inlined in %s at pc " INTPTR_FORMAT, ss1.as_string(), vfst.bci(), ss2.as_string(), p2i(caller_frame.pc()));
1429   }
1430 
1431 
1432   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1433 
1434 JRT_END
1435 
1436 #ifndef PRODUCT
1437 void Runtime1::print_statistics() {
1438   tty-&gt;print_cr("C1 Runtime statistics:");
1439   tty-&gt;print_cr(" _resolve_invoke_virtual_cnt:     %d", SharedRuntime::_resolve_virtual_ctr);
1440   tty-&gt;print_cr(" _resolve_invoke_opt_virtual_cnt: %d", SharedRuntime::_resolve_opt_virtual_ctr);
1441   tty-&gt;print_cr(" _resolve_invoke_static_cnt:      %d", SharedRuntime::_resolve_static_ctr);
1442   tty-&gt;print_cr(" _handle_wrong_method_cnt:        %d", SharedRuntime::_wrong_method_ctr);
1443   tty-&gt;print_cr(" _ic_miss_cnt:                    %d", SharedRuntime::_ic_miss_ctr);
1444   tty-&gt;print_cr(" _generic_arraycopy_cnt:          %d", _generic_arraycopy_cnt);
1445   tty-&gt;print_cr(" _generic_arraycopystub_cnt:      %d", _generic_arraycopystub_cnt);
1446   tty-&gt;print_cr(" _byte_arraycopy_cnt:             %d", _byte_arraycopy_stub_cnt);
1447   tty-&gt;print_cr(" _short_arraycopy_cnt:            %d", _short_arraycopy_stub_cnt);
1448   tty-&gt;print_cr(" _int_arraycopy_cnt:              %d", _int_arraycopy_stub_cnt);
1449   tty-&gt;print_cr(" _long_arraycopy_cnt:             %d", _long_arraycopy_stub_cnt);
1450   tty-&gt;print_cr(" _oop_arraycopy_cnt:              %d", _oop_arraycopy_stub_cnt);
1451   tty-&gt;print_cr(" _arraycopy_slowcase_cnt:         %d", _arraycopy_slowcase_cnt);
1452   tty-&gt;print_cr(" _arraycopy_checkcast_cnt:        %d", _arraycopy_checkcast_cnt);
1453   tty-&gt;print_cr(" _arraycopy_checkcast_attempt_cnt:%d", _arraycopy_checkcast_attempt_cnt);
1454 
1455   tty-&gt;print_cr(" _new_type_array_slowcase_cnt:    %d", _new_type_array_slowcase_cnt);
1456   tty-&gt;print_cr(" _new_object_array_slowcase_cnt:  %d", _new_object_array_slowcase_cnt);
1457   tty-&gt;print_cr(" _new_instance_slowcase_cnt:      %d", _new_instance_slowcase_cnt);
1458   tty-&gt;print_cr(" _new_multi_array_slowcase_cnt:   %d", _new_multi_array_slowcase_cnt);
1459   tty-&gt;print_cr(" _monitorenter_slowcase_cnt:      %d", _monitorenter_slowcase_cnt);
1460   tty-&gt;print_cr(" _monitorexit_slowcase_cnt:       %d", _monitorexit_slowcase_cnt);
1461   tty-&gt;print_cr(" _patch_code_slowcase_cnt:        %d", _patch_code_slowcase_cnt);
1462 
1463   tty-&gt;print_cr(" _throw_range_check_exception_count:            %d:", _throw_range_check_exception_count);
1464   tty-&gt;print_cr(" _throw_index_exception_count:                  %d:", _throw_index_exception_count);
1465   tty-&gt;print_cr(" _throw_div0_exception_count:                   %d:", _throw_div0_exception_count);
1466   tty-&gt;print_cr(" _throw_null_pointer_exception_count:           %d:", _throw_null_pointer_exception_count);
1467   tty-&gt;print_cr(" _throw_class_cast_exception_count:             %d:", _throw_class_cast_exception_count);
1468   tty-&gt;print_cr(" _throw_incompatible_class_change_error_count:  %d:", _throw_incompatible_class_change_error_count);
1469   tty-&gt;print_cr(" _throw_array_store_exception_count:            %d:", _throw_array_store_exception_count);
1470   tty-&gt;print_cr(" _throw_count:                                  %d:", _throw_count);
1471 
1472   SharedRuntime::print_ic_miss_histogram();
1473   tty-&gt;cr();
1474 }
1475 #endif // PRODUCT
<a name="10" id="anc10"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="10" type="hidden" /></form></body></html>
